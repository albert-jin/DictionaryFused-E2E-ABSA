{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "many_model_absa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/albert-jin/DictionaryFused-E2E-ABSA 仓库中的所有模型的训练记事本"
      ],
      "metadata": {
        "id": "OFOKxkAadSNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 包含了对lstm tdlstm tclstm ataelstm  ian memnet cabasc\n",
        "## 对 acl2014data + semeval 2014,+15,+16 + aclshortdata 的 ABSA 模型表现实验"
      ],
      "metadata": {
        "id": "J-SVg4o9eDKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTBSIS15U9zA",
        "outputId": "98c6c7fe-736f-4533-e6c4-9e3080c96da6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Aug 17 15:04:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    41W /  70W |      0MiB / 15109MiB |     12%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "所有模型 include lstm,td_lstm,tc_lstm,atae_lstm,ian,memnet,ram,cabasc,tnet_lf,aoa,mgan,bert_spc,aen_bert,lcf_bert"
      ],
      "metadata": {
        "id": "nIIL8EEidqsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 下载实验项目\n",
        "!git clone https://github.com/albert-jin/DictionaryFused-E2E-ABSA.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Whu5t87el5zC",
        "outputId": "cbd276e9-0fd1-4b11-f8b9-4d0c2adb2f0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DictionaryFused-E2E-ABSA'...\n",
            "remote: Enumerating objects: 123, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 123 (delta 57), reused 115 (delta 52), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (123/123), 1.39 MiB | 1.20 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "安装python解释器环境"
      ],
      "metadata": {
        "id": "l1afDUM4ywog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/DictionaryFused-E2E-ABSA/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1m-MGqSy0S7",
        "outputId": "3d7243cc-7b1e-41fe-c35c-c73579b56539"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 1)) (1.21.6)\n",
            "Collecting torch==1.7\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.5 kB/s \n",
            "\u001b[?25hCollecting transformers<4.0.0,>=3.5.1\n",
            "  Downloading transformers-3.5.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 66.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 4)) (0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 2)) (0.16.0)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 2)) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (4.64.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (3.8.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (3.17.3)\n",
            "Collecting tokenizers==0.9.3\n",
            "  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.0.0,>=3.5.1->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r /content/DictionaryFused-E2E-ABSA/requirements.txt (line 4)) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=4c7eab90d9d8fe3c9f7879ac14fbe64dc36d2f27dec48df53e39e5d3289631dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, dataclasses, transformers, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 sacremoses-0.0.53 sentencepiece-0.1.91 tokenizers-0.9.3 torch-1.7.0 transformers-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch==1.7 # 需要兼容transformer包"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KCeMqoD1r64",
        "outputId": "6ed53367-0bc7-4432-915f-1047631cdba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.7\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 3.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (1.21.6)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (4.2.0)\n",
            "Installing collected packages: dataclasses, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRyTlscNdDTQ",
        "outputId": "899ef295-ddca-464f-d407-5a35ed6dcebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-17 08:05:32--  https://huggingface.co/stanfordnlp/glove/resolve/main/glove.twitter.27B.zip\n",
            "Resolving huggingface.co (huggingface.co)... 52.6.16.131, 52.202.207.64, 2600:1f18:147f:e850:db35:e0c7:187b:c770, ...\n",
            "Connecting to huggingface.co (huggingface.co)|52.6.16.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/stanfordnlp/glove/3123e7f5c3f6a30095d413b12fc3284bbf717acd2a9bed63d1c7c13bf5223352?response-content-disposition=attachment%3B%20filename%3D%22glove.twitter.27B.zip%22 [following]\n",
            "--2022-08-17 08:05:33--  https://cdn-lfs.huggingface.co/stanfordnlp/glove/3123e7f5c3f6a30095d413b12fc3284bbf717acd2a9bed63d1c7c13bf5223352?response-content-disposition=attachment%3B%20filename%3D%22glove.twitter.27B.zip%22\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.227.219.41, 13.227.219.2, 13.227.219.4, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.227.219.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408741 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  40.5MB/s    in 70s     \n",
            "\n",
            "2022-08-17 08:06:43 (20.7 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408741/1520408741]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: /content/DictionaryFused-E2E-ABSA/glove_embeddings/glove.twitter.27B.100d.txt  \n",
            "  inflating: /content/DictionaryFused-E2E-ABSA/glove_embeddings/glove.twitter.27B.200d.txt  \n",
            "  inflating: /content/DictionaryFused-E2E-ABSA/glove_embeddings/glove.twitter.27B.25d.txt  \n",
            "  inflating: /content/DictionaryFused-E2E-ABSA/glove_embeddings/glove.twitter.27B.50d.txt  \n"
          ]
        }
      ],
      "source": [
        "# train.py embed_size == 200 则需要下载\n",
        "!wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.twitter.27B.zip #twitter数据集,不区分大小写\n",
        "!unzip glove.twitter.27B.zip -d /content/DictionaryFused-E2E-ABSA/glove_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 5 /content/DictionaryFused-E2E-ABSA/glove_embeddings/glove.twitter.27B.200d.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrzJEdZCx6ZF",
        "outputId": "10cd580c-bcdc-469e-a4c3-97cfbe77fd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<user> 0.31553 0.53765 0.10177 0.032553 0.003798 0.015364 -0.20344 0.33294 -0.20886 0.10061 0.30976 0.50015 0.32018 0.13537 0.0087039 0.1911 0.24668 -0.060752 -0.43623 0.019302 0.59972 0.13444 0.012801 -0.54052 0.27387 -1.182 -0.27677 0.11279 0.46596 -0.090685 0.24253 0.15654 -0.23618 0.57694 0.17563 -0.01969 0.018295 0.37569 -0.41984 0.22613 -0.20438 -0.076249 0.40356 0.61582 -0.10064 0.23318 0.22808 0.34576 -0.14627 -0.1988 0.033232 -0.84885 -0.25684 0.26369 0.29562 0.1847 -0.20668 -0.013297 0.12233 -0.47751 -0.17202 -0.14577 0.047446 -0.15824 0.054215 -0.19426 -0.081484 0.099009 0.10159 0.043571 0.50245 0.13362 0.065985 0.032969 -0.2017 -0.56905 -0.13203 0.073347 -0.063728 -0.2796 -0.38481 -0.020193 0.22298 -0.059115 0.045198 -0.13995 -0.13299 0.47309 -0.021874 0.38758 -0.074926 -0.0028093 -0.29829 -0.074987 -0.58542 -0.18065 -0.041805 0.41938 0.41004 -0.5911 0.10459 0.10724 0.69768 -0.15901 -0.059596 0.29368 -0.19609 0.39124 -0.29333 -0.0050833 -0.37854 0.33858 0.24782 0.29144 -0.22833 0.19421 -0.055409 0.10322 0.38963 -0.27813 0.21963 0.40014 0.071036 -0.079786 0.19534 -0.69432 -0.093075 -0.13729 -0.54014 0.57165 0.24443 0.036741 0.033606 -0.14398 0.25873 0.089008 0.11109 0.38387 0.19013 -0.23252 -0.26271 -0.26936 -0.32409 0.55236 -0.46158 -0.11086 -0.38417 0.59605 -0.14479 -0.28762 -0.29638 0.21889 -4.1257 0.69382 -0.26307 -0.013691 0.032916 0.017627 -0.02909 0.42807 -0.20815 0.50598 -0.080836 0.45083 -0.11466 -0.27782 -0.038373 0.15672 -0.010899 0.082053 -0.19766 0.20574 -0.075329 0.083754 -0.72767 0.10403 0.42831 0.41023 -0.097314 0.15128 0.39287 -0.17807 -0.029196 0.57502 -0.17823 -0.76488 -0.47383 -0.21984 0.2119 -0.015729 -0.062927 0.26674 -0.23617 0.18109 -0.26583 0.090904 -0.81205 -0.45664 -0.4654 0.52066\n",
            ". 0.35132 0.00056084 -0.21488 -0.04707 -0.17777 0.66162 -0.0074805 -0.15963 -0.22129 0.65523 0.346 -0.22968 -0.078954 0.27465 0.36018 0.20373 -0.048134 0.091749 -0.093562 -0.088653 -0.61514 -0.1124 -0.21046 0.13129 0.11224 -0.92995 -0.18006 0.0096874 0.93647 -0.19493 -0.13873 0.18719 -0.022502 0.39516 0.33007 0.36089 0.0078608 -0.024064 -0.3289 -0.28101 -0.20223 0.30049 -0.22843 0.0639 0.59025 -0.17992 0.72733 0.028216 -0.43656 -0.06433 0.087363 0.0054825 0.34902 0.081738 0.35089 0.053459 0.05537 0.10797 0.19663 0.63077 -0.074041 0.13848 0.24849 -0.10183 0.42992 -0.41886 0.010814 -0.48654 0.19154 -0.18615 0.17851 0.4053 0.14635 0.22446 0.12614 -0.45649 -0.16633 0.15915 -0.12543 -0.055649 -0.30481 0.12712 -0.015809 -0.062656 0.87436 0.4834 -0.16578 0.39106 0.11319 -0.1763 0.01411 -0.40942 0.32281 -0.22064 0.022058 0.076171 0.13255 -0.067011 0.45695 0.04404 0.076988 -0.1565 0.35628 0.18131 -0.040064 0.56617 -0.37482 -0.36297 -0.18098 0.026808 -0.16885 0.24951 -0.14548 -0.23878 -0.074018 -0.032295 0.16866 0.33322 0.023116 -0.4722 0.27615 0.25764 -0.14382 -0.3359 0.029436 -0.15695 -0.084597 -0.10684 -0.24784 0.021849 -0.14485 -0.12874 -0.17975 -0.20351 -0.085811 0.31607 0.28477 -0.011954 0.1486 -0.15402 0.36125 -0.082339 0.27478 0.056358 -0.21401 0.74459 0.048518 0.06175 0.17656 -0.075019 -0.61491 0.063787 -3.9376 0.54504 0.15964 -0.41187 0.51993 0.1342 -0.031554 -0.26429 0.040603 0.2661 0.082515 -0.090602 0.0057244 0.40602 -0.30109 0.37078 0.11716 -0.26164 -0.38366 -0.13616 -0.30152 -0.17193 0.14108 0.31278 0.16425 0.17671 -0.29942 0.43029 0.057682 -0.1411 -0.031367 0.058953 -0.3055 -0.88512 -0.47983 -0.056183 -0.10459 0.039792 -0.4437 0.14186 -0.42475 0.023551 -0.096965 0.079513 -1.4683 0.036684 -0.037206 0.85384\n",
            ": 0.80767 0.49786 0.082696 -0.0079298 0.082471 -0.5936 -0.18753 0.48645 0.10719 -0.31299 0.17609 0.046026 0.036029 0.47656 -0.1215 -0.098428 -0.18983 0.038915 -0.20902 -0.316 0.5007 -0.22646 0.12353 -0.27012 0.70739 -0.98348 -0.54111 0.32431 -0.19345 -0.18299 -0.45738 -0.44089 -0.47504 -0.090636 0.085478 0.028124 -0.28045 0.21273 0.0060152 -0.37948 -0.68825 -0.59779 -0.046953 0.65887 0.44564 -0.25169 -0.030223 0.25673 -0.293 -0.047083 -0.43578 -0.45254 -0.15416 0.35722 0.48247 0.42448 -0.32445 -0.27119 0.31039 -0.40884 -0.4227 0.080102 0.28311 0.013829 0.94008 -0.58542 0.34853 -0.15336 0.1654 0.093904 0.46697 0.11781 0.091151 -0.41349 -0.19446 -0.58665 0.094378 0.14077 0.35482 -0.17125 -0.050602 0.21653 -0.11619 0.45904 -0.30238 0.072354 -0.46345 1.3963 -0.21264 -0.25539 -0.31002 -0.27831 -0.40129 0.10612 -0.42636 0.035816 0.20376 0.22088 0.47981 -0.0767 0.3361 0.34746 0.088655 0.65177 0.45665 0.091346 -0.53175 0.14724 -0.4377 0.27911 -0.67202 0.38052 -0.011494 -0.11314 -0.29098 0.43887 -0.004587 0.079091 -0.0045291 -0.63007 0.1432 0.1942 -0.076888 -0.21021 0.25376 -0.47742 0.14886 0.2257 -0.40912 0.18141 -0.19515 -0.3797 -0.3215 0.0075749 0.68672 0.06226 0.0010575 0.30921 0.10123 -0.28578 0.20308 -0.13243 -0.8974 0.70887 -0.04753 0.54342 -0.40847 0.14928 0.23341 0.024772 0.30833 -0.14153 -3.6293 0.40686 0.22129 -0.2993 -0.01763 0.045388 -0.03538 -0.14189 -0.098312 0.083338 -0.20842 0.59749 0.039259 -0.046968 -0.038719 0.04147 0.40501 -0.13983 -0.40245 0.30089 -0.03762 0.12831 -0.15879 0.17023 0.75995 0.3549 0.0046914 -0.053001 0.13518 0.40568 -0.22714 -0.063414 -0.36388 -0.4503 -0.47098 -0.0973 0.77199 -0.01085 -0.15552 0.11186 0.068971 -0.55048 -0.5001 0.1886 -0.85631 -0.073302 -0.47785 0.67059\n",
            "rt 0.55687 0.63284 -0.15609 0.26397 0.28015 -0.36506 -0.12128 0.45217 -0.16123 0.015791 0.49511 0.054643 0.66865 0.44101 -0.48446 0.12594 0.40596 -0.086889 -0.2352 -0.0073657 0.58835 -0.12101 -0.11671 -0.47415 0.47753 -1.3069 -0.42765 -0.0055401 0.2247 0.061141 0.010707 0.20652 -0.48375 0.40042 0.35776 0.17049 -0.23819 0.28888 -0.069899 0.094793 -0.33917 -0.11422 0.016834 0.53581 0.25739 -0.2775 0.056731 0.34448 -0.10715 -0.21423 0.020693 -0.59172 -0.39452 -0.1925 0.20454 0.40205 -0.096825 -0.037272 0.32491 -0.26628 0.0072484 0.19783 0.17485 -0.071738 0.084738 -0.43062 0.016838 0.14359 -0.13822 0.27964 0.31179 0.33653 0.12314 -0.15906 -0.62741 -0.83838 -0.069363 0.28698 -0.11668 -0.43706 -0.063269 -0.19582 0.42072 0.19197 0.057278 0.32468 -0.59688 0.82594 -0.7103 0.47354 0.11517 -0.11188 -0.53444 0.015748 -0.57038 -0.1253 -0.45733 0.42662 0.41064 -0.27635 -0.2032 0.14295 0.46532 0.16804 0.022694 -0.24602 0.059308 0.73777 -0.29451 -0.33315 -0.29938 0.33871 0.37681 0.31939 -0.014027 0.32298 0.054817 0.13213 0.3252 -0.6153 -0.12941 0.28465 0.32881 0.25362 0.27244 -0.82169 0.41908 -0.62524 -1.2176 0.27902 -0.33003 -0.20213 -0.28881 -0.071473 0.23549 0.38409 0.13101 0.17312 0.018403 -0.29669 0.17959 -0.33733 -0.82358 0.1042 -0.51619 0.035018 -0.19027 0.50867 0.023113 -0.53112 0.10043 0.27881 -4.2515 0.44271 0.29931 -0.23305 0.011018 0.07118 0.14536 0.2012 -0.046338 0.36196 -0.33985 0.65517 -0.045043 -0.39308 0.22788 0.30343 -0.058125 0.18367 -0.22463 -0.46903 -0.20435 -0.13256 -0.73335 -0.42302 0.14038 0.59022 -0.28843 -0.09194 0.18957 0.039595 -0.11694 0.67706 -0.23313 -0.079736 -0.46375 -0.054137 0.0557 -0.12069 -0.30669 -0.15438 -0.19857 0.064394 -0.14346 -0.10524 -0.57253 0.23857 -0.79235 0.23761\n",
            ", 0.3927 -0.084181 -0.6075 0.3231 -0.35919 0.62664 0.29758 -0.21039 -0.23201 0.11897 0.41827 -0.21206 -0.036036 0.39526 0.53432 -0.33607 -0.23529 0.027524 -0.21025 -0.49712 0.12486 -0.14865 -0.45697 0.35445 -0.31083 -1.8664 -0.32149 -0.41014 0.35596 0.25467 -0.35302 0.059504 0.1571 0.61731 -0.13229 0.18362 0.1329 -0.13754 -0.068064 -0.29138 -0.53293 -0.048187 -0.1334 0.11968 -0.045649 -0.42278 0.69403 0.30971 -0.66289 0.017076 -0.45864 -0.23924 0.24663 0.13362 -0.34587 0.031223 -0.43579 0.16547 -0.25115 0.48124 -0.21432 0.22534 0.20446 -0.45843 0.60826 0.20866 -0.46438 -0.34758 0.088277 -0.30944 0.41937 0.12139 -0.46349 0.08518 -0.070756 -0.84094 -0.32331 -0.53684 0.28473 0.22625 0.092721 0.039561 -0.48258 0.069858 0.49982 0.54392 -0.34694 0.72118 -0.30233 -0.049354 -0.47386 -0.26428 0.27401 0.1476 0.091779 0.10067 0.15366 -0.65149 -0.085811 -0.22351 0.28081 0.046785 0.42656 0.12003 0.13375 -0.035338 0.0067347 -0.36015 -0.70746 -0.033635 -0.11495 0.45643 0.11071 -0.18302 -0.335 -0.13371 -0.32718 0.28352 0.19098 -0.53333 0.49566 -0.18797 -0.1495 -0.027403 -0.0051272 -1.2365 0.049948 -0.0711 -0.74201 -0.13454 -0.35839 -0.20315 -0.20721 -0.10175 -0.13152 0.089342 -0.13269 0.36283 0.03283 -0.36339 -0.0097338 -0.20782 -0.17021 0.44278 -0.29718 0.76858 0.29221 0.21189 0.39831 -0.12685 -0.49607 -0.21184 -4.3586 -0.0046878 -0.01809 -0.13526 -0.44054 -0.496 0.11789 -1.0292 0.17799 -0.021813 0.2141 -0.27944 0.024458 0.28278 -0.49199 0.49358 0.34139 -0.67217 -0.44666 -0.46249 -0.56888 -0.71003 -0.39095 0.2495 0.46399 0.15134 -0.5881 -0.1399 -0.11789 0.17936 -0.22037 -0.27648 0.16274 -1.1748 -0.99164 0.13503 0.016681 -0.45423 -0.61859 -0.32079 -0.25119 0.27893 0.25774 -0.46615 -0.48912 0.093868 -0.58506 0.19544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 测试Glove模型在Twitter数据集上的表现"
      ],
      "metadata": {
        "id": "IFrdga2_rc3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **twitter** dataset on model(**LSTM**)"
      ],
      "metadata": {
        "id": "RU2FcO2vyQLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset twitter --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2FpJek8yQb0",
        "outputId": "43bb9911-df73-4223-8944-1c0ef30365a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1687.\n",
            "> testing dataset count: 422.\n",
            "cuda memory allocated: 11166720\n",
            "> n_trainable_params: 603303, n_nontrainable_params: 2188000\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f35a1e08b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:12\n",
            "loss: 1.1314, acc: 0.0917\n",
            "E2E-ABSA >>> 2022-05-21 02:26:12\n",
            "loss: 1.1013, acc: 0.2729\n",
            "E2E-ABSA >>> 2022-05-21 02:26:12\n",
            "loss: 1.0769, acc: 0.3868\n",
            "E2E-ABSA >>> 2022-05-21 02:26:12\n",
            ">>> val_acc: 0.6777, val_precision: 0.6777 val_recall: 0.6777, val_f1: 0.6777\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.6777\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:13\n",
            "loss: 0.9604, acc: 0.6786\n",
            "E2E-ABSA >>> 2022-05-21 02:26:13\n",
            "loss: 0.9397, acc: 0.6804\n",
            "E2E-ABSA >>> 2022-05-21 02:26:13\n",
            "loss: 0.9346, acc: 0.6647\n",
            "E2E-ABSA >>> 2022-05-21 02:26:13\n",
            "loss: 0.9149, acc: 0.6695\n",
            "E2E-ABSA >>> 2022-05-21 02:26:13\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:13\n",
            "loss: 0.8692, acc: 0.6540\n",
            "E2E-ABSA >>> 2022-05-21 02:26:14\n",
            "loss: 0.8658, acc: 0.6487\n",
            "E2E-ABSA >>> 2022-05-21 02:26:14\n",
            "loss: 0.8444, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 02:26:14\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:14\n",
            "loss: 0.7951, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:26:14\n",
            "loss: 0.7879, acc: 0.6845\n",
            "E2E-ABSA >>> 2022-05-21 02:26:14\n",
            "loss: 0.8036, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 02:26:14\n",
            "loss: 0.8098, acc: 0.6697\n",
            "E2E-ABSA >>> 2022-05-21 02:26:15\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:15\n",
            "loss: 0.8222, acc: 0.6178\n",
            "E2E-ABSA >>> 2022-05-21 02:26:15\n",
            "loss: 0.7987, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-05-21 02:26:15\n",
            "loss: 0.7821, acc: 0.6824\n",
            "E2E-ABSA >>> 2022-05-21 02:26:15\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:15\n",
            "loss: 0.7110, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 02:26:16\n",
            "loss: 0.7873, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-05-21 02:26:16\n",
            "loss: 0.7732, acc: 0.6839\n",
            "E2E-ABSA >>> 2022-05-21 02:26:16\n",
            "loss: 0.7874, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-05-21 02:26:16\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:16\n",
            "loss: 0.7587, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 02:26:16\n",
            "loss: 0.7677, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-05-21 02:26:16\n",
            "loss: 0.7774, acc: 0.6734\n",
            "E2E-ABSA >>> 2022-05-21 02:26:17\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:17\n",
            "loss: 0.6992, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-05-21 02:26:17\n",
            "loss: 0.7789, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-05-21 02:26:17\n",
            "loss: 0.7723, acc: 0.6801\n",
            "E2E-ABSA >>> 2022-05-21 02:26:17\n",
            "loss: 0.7794, acc: 0.6716\n",
            "E2E-ABSA >>> 2022-05-21 02:26:17\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:18\n",
            "loss: 0.7626, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-21 02:26:18\n",
            "loss: 0.7519, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-05-21 02:26:18\n",
            "loss: 0.7637, acc: 0.6822\n",
            "E2E-ABSA >>> 2022-05-21 02:26:18\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:18\n",
            "loss: 0.8213, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 02:26:18\n",
            "loss: 0.7503, acc: 0.6892\n",
            "E2E-ABSA >>> 2022-05-21 02:26:18\n",
            "loss: 0.7860, acc: 0.6610\n",
            "E2E-ABSA >>> 2022-05-21 02:26:19\n",
            "loss: 0.7729, acc: 0.6725\n",
            "E2E-ABSA >>> 2022-05-21 02:26:19\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:19\n",
            "loss: 0.7528, acc: 0.6531\n",
            "E2E-ABSA >>> 2022-05-21 02:26:19\n",
            "loss: 0.7656, acc: 0.6763\n",
            "E2E-ABSA >>> 2022-05-21 02:26:19\n",
            "loss: 0.7635, acc: 0.6734\n",
            "E2E-ABSA >>> 2022-05-21 02:26:19\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:19\n",
            "loss: 0.7286, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 02:26:20\n",
            "loss: 0.7436, acc: 0.6801\n",
            "E2E-ABSA >>> 2022-05-21 02:26:20\n",
            "loss: 0.7407, acc: 0.6865\n",
            "E2E-ABSA >>> 2022-05-21 02:26:20\n",
            "loss: 0.7596, acc: 0.6749\n",
            "E2E-ABSA >>> 2022-05-21 02:26:20\n",
            ">>> val_acc: 0.6825, val_precision: 0.6825 val_recall: 0.6825, val_f1: 0.6825\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:20\n",
            "loss: 0.8012, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:26:20\n",
            "loss: 0.7551, acc: 0.6810\n",
            "E2E-ABSA >>> 2022-05-21 02:26:21\n",
            "loss: 0.7640, acc: 0.6715\n",
            "E2E-ABSA >>> 2022-05-21 02:26:21\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:21\n",
            "loss: 0.6574, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:26:21\n",
            "loss: 0.7530, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-05-21 02:26:21\n",
            "loss: 0.7639, acc: 0.6694\n",
            "E2E-ABSA >>> 2022-05-21 02:26:21\n",
            "loss: 0.7549, acc: 0.6766\n",
            "E2E-ABSA >>> 2022-05-21 02:26:21\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:22\n",
            "loss: 0.7693, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-05-21 02:26:22\n",
            "loss: 0.7785, acc: 0.6590\n",
            "E2E-ABSA >>> 2022-05-21 02:26:22\n",
            "loss: 0.7530, acc: 0.6826\n",
            "E2E-ABSA >>> 2022-05-21 02:26:22\n",
            "loss: 0.7471, acc: 0.6811\n",
            "E2E-ABSA >>> 2022-05-21 02:26:22\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:22\n",
            "loss: 0.7503, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-05-21 02:26:23\n",
            "loss: 0.7545, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:26:23\n",
            "loss: 0.7501, acc: 0.6778\n",
            "E2E-ABSA >>> 2022-05-21 02:26:23\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:23\n",
            "loss: 0.7168, acc: 0.6920\n",
            "E2E-ABSA >>> 2022-05-21 02:26:23\n",
            "loss: 0.7456, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-05-21 02:26:23\n",
            "loss: 0.7476, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-05-21 02:26:23\n",
            "loss: 0.7372, acc: 0.6857\n",
            "E2E-ABSA >>> 2022-05-21 02:26:24\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:24\n",
            "loss: 0.7067, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-05-21 02:26:24\n",
            "loss: 0.6923, acc: 0.7037\n",
            "E2E-ABSA >>> 2022-05-21 02:26:24\n",
            "loss: 0.7274, acc: 0.6903\n",
            "E2E-ABSA >>> 2022-05-21 02:26:24\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:24\n",
            "loss: 0.8176, acc: 0.6198\n",
            "E2E-ABSA >>> 2022-05-21 02:26:24\n",
            "loss: 0.7512, acc: 0.6741\n",
            "E2E-ABSA >>> 2022-05-21 02:26:25\n",
            "loss: 0.7305, acc: 0.6918\n",
            "E2E-ABSA >>> 2022-05-21 02:26:25\n",
            "loss: 0.7304, acc: 0.6857\n",
            "E2E-ABSA >>> 2022-05-21 02:26:25\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:25\n",
            "loss: 0.7297, acc: 0.7091\n",
            "E2E-ABSA >>> 2022-05-21 02:26:25\n",
            "loss: 0.7338, acc: 0.6819\n",
            "E2E-ABSA >>> 2022-05-21 02:26:25\n",
            "loss: 0.7415, acc: 0.6810\n",
            "E2E-ABSA >>> 2022-05-21 02:26:26\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:26\n",
            "loss: 0.7646, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-05-21 02:26:26\n",
            "loss: 0.7142, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-05-21 02:26:26\n",
            "loss: 0.7390, acc: 0.6777\n",
            "E2E-ABSA >>> 2022-05-21 02:26:26\n",
            "loss: 0.7231, acc: 0.6900\n",
            "E2E-ABSA >>> 2022-05-21 02:26:26\n",
            ">>> val_acc: 0.6919, val_precision: 0.6919 val_recall: 0.6919, val_f1: 0.6919\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.6919\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:26\n",
            "loss: 0.6905, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-05-21 02:26:27\n",
            "loss: 0.7020, acc: 0.6968\n",
            "E2E-ABSA >>> 2022-05-21 02:26:27\n",
            "loss: 0.7039, acc: 0.7001\n",
            "E2E-ABSA >>> 2022-05-21 02:26:27\n",
            ">>> val_acc: 0.6943, val_precision: 0.6943 val_recall: 0.6943, val_f1: 0.6943\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.6943\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:27\n",
            "loss: 0.6596, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 02:26:27\n",
            "loss: 0.6799, acc: 0.7122\n",
            "E2E-ABSA >>> 2022-05-21 02:26:27\n",
            "loss: 0.7142, acc: 0.6912\n",
            "E2E-ABSA >>> 2022-05-21 02:26:28\n",
            "loss: 0.7145, acc: 0.6926\n",
            "E2E-ABSA >>> 2022-05-21 02:26:28\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:28\n",
            "loss: 0.7280, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-21 02:26:28\n",
            "loss: 0.7368, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-05-21 02:26:28\n",
            "loss: 0.7077, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-05-21 02:26:28\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:28\n",
            "loss: 0.6785, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 02:26:29\n",
            "loss: 0.7095, acc: 0.6788\n",
            "E2E-ABSA >>> 2022-05-21 02:26:29\n",
            "loss: 0.7005, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-05-21 02:26:29\n",
            "loss: 0.7030, acc: 0.6921\n",
            "E2E-ABSA >>> 2022-05-21 02:26:29\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:29\n",
            "loss: 0.7096, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 02:26:29\n",
            "loss: 0.7035, acc: 0.6775\n",
            "E2E-ABSA >>> 2022-05-21 02:26:30\n",
            "loss: 0.6946, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:26:30\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:30\n",
            "loss: 0.6915, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 02:26:30\n",
            "loss: 0.6768, acc: 0.6985\n",
            "E2E-ABSA >>> 2022-05-21 02:26:30\n",
            "loss: 0.6733, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 02:26:30\n",
            "loss: 0.6900, acc: 0.6961\n",
            "E2E-ABSA >>> 2022-05-21 02:26:30\n",
            ">>> val_acc: 0.6919, val_precision: 0.6919 val_recall: 0.6919, val_f1: 0.6919\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:31\n",
            "loss: 0.6739, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-05-21 02:26:31\n",
            "loss: 0.6897, acc: 0.6914\n",
            "E2E-ABSA >>> 2022-05-21 02:26:31\n",
            "loss: 0.6831, acc: 0.6947\n",
            "E2E-ABSA >>> 2022-05-21 02:26:31\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:31\n",
            "loss: 0.6832, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:26:31\n",
            "loss: 0.6615, acc: 0.7012\n",
            "E2E-ABSA >>> 2022-05-21 02:26:32\n",
            "loss: 0.6825, acc: 0.6865\n",
            "E2E-ABSA >>> 2022-05-21 02:26:32\n",
            "loss: 0.6824, acc: 0.6970\n",
            "E2E-ABSA >>> 2022-05-21 02:26:32\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:32\n",
            "loss: 0.7001, acc: 0.6914\n",
            "E2E-ABSA >>> 2022-05-21 02:26:32\n",
            "loss: 0.6917, acc: 0.6889\n",
            "E2E-ABSA >>> 2022-05-21 02:26:32\n",
            "loss: 0.6667, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 02:26:33\n",
            "loss: 0.6737, acc: 0.6989\n",
            "E2E-ABSA >>> 2022-05-21 02:26:33\n",
            ">>> val_acc: 0.7133, val_precision: 0.7133 val_recall: 0.7133, val_f1: 0.7133\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.7133\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:33\n",
            "loss: 0.6939, acc: 0.6833\n",
            "E2E-ABSA >>> 2022-05-21 02:26:33\n",
            "loss: 0.6777, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-05-21 02:26:33\n",
            "loss: 0.6772, acc: 0.6972\n",
            "E2E-ABSA >>> 2022-05-21 02:26:33\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:33\n",
            "loss: 0.6301, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-05-21 02:26:34\n",
            "loss: 0.6368, acc: 0.7116\n",
            "E2E-ABSA >>> 2022-05-21 02:26:34\n",
            "loss: 0.6563, acc: 0.7035\n",
            "E2E-ABSA >>> 2022-05-21 02:26:34\n",
            "loss: 0.6529, acc: 0.7121\n",
            "E2E-ABSA >>> 2022-05-21 02:26:34\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:34\n",
            "loss: 0.6610, acc: 0.7121\n",
            "E2E-ABSA >>> 2022-05-21 02:26:34\n",
            "loss: 0.6388, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-05-21 02:26:35\n",
            "loss: 0.6471, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-05-21 02:26:35\n",
            ">>> val_acc: 0.7204, val_precision: 0.7204 val_recall: 0.7204, val_f1: 0.7204\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.7204\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:35\n",
            "loss: 0.6239, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 02:26:35\n",
            "loss: 0.6528, acc: 0.7217\n",
            "E2E-ABSA >>> 2022-05-21 02:26:35\n",
            "loss: 0.6522, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-21 02:26:35\n",
            "loss: 0.6403, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-05-21 02:26:35\n",
            ">>> val_acc: 0.7275, val_precision: 0.7275 val_recall: 0.7275, val_f1: 0.7275\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.7275\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:36\n",
            "loss: 0.6298, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 02:26:36\n",
            "loss: 0.6577, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-05-21 02:26:36\n",
            "loss: 0.6496, acc: 0.7166\n",
            "E2E-ABSA >>> 2022-05-21 02:26:36\n",
            ">>> val_acc: 0.7133, val_precision: 0.7133 val_recall: 0.7133, val_f1: 0.7133\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:36\n",
            "loss: 0.6408, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-05-21 02:26:36\n",
            "loss: 0.6205, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-05-21 02:26:36\n",
            "loss: 0.6422, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-05-21 02:26:37\n",
            "loss: 0.6336, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-05-21 02:26:37\n",
            ">>> val_acc: 0.7322, val_precision: 0.7322 val_recall: 0.7322, val_f1: 0.7322\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.7322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:37\n",
            "loss: 0.6702, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-05-21 02:26:37\n",
            "loss: 0.6429, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-05-21 02:26:37\n",
            "loss: 0.6233, acc: 0.7329\n",
            "E2E-ABSA >>> 2022-05-21 02:26:37\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:38\n",
            "loss: 0.5909, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 02:26:38\n",
            "loss: 0.6133, acc: 0.7237\n",
            "E2E-ABSA >>> 2022-05-21 02:26:38\n",
            "loss: 0.6144, acc: 0.7307\n",
            "E2E-ABSA >>> 2022-05-21 02:26:38\n",
            "loss: 0.6133, acc: 0.7372\n",
            "E2E-ABSA >>> 2022-05-21 02:26:38\n",
            ">>> val_acc: 0.7180, val_precision: 0.7180 val_recall: 0.7180, val_f1: 0.7180\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:38\n",
            "loss: 0.6463, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-05-21 02:26:38\n",
            "loss: 0.6163, acc: 0.7416\n",
            "E2E-ABSA >>> 2022-05-21 02:26:39\n",
            "loss: 0.6126, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-05-21 02:26:39\n",
            ">>> val_acc: 0.7227, val_precision: 0.7227 val_recall: 0.7227, val_f1: 0.7227\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:39\n",
            "loss: 0.5665, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:26:39\n",
            "loss: 0.5957, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:26:39\n",
            "loss: 0.6122, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-05-21 02:26:39\n",
            "loss: 0.6059, acc: 0.7389\n",
            "E2E-ABSA >>> 2022-05-21 02:26:40\n",
            ">>> val_acc: 0.7227, val_precision: 0.7227 val_recall: 0.7227, val_f1: 0.7227\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:40\n",
            "loss: 0.5921, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 02:26:40\n",
            "loss: 0.5778, acc: 0.7588\n",
            "E2E-ABSA >>> 2022-05-21 02:26:40\n",
            "loss: 0.5969, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-05-21 02:26:40\n",
            ">>> val_acc: 0.7370, val_precision: 0.7370 val_recall: 0.7370, val_f1: 0.7370\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.737\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:40\n",
            "loss: 0.6789, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:26:40\n",
            "loss: 0.6380, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-05-21 02:26:41\n",
            "loss: 0.6137, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 02:26:41\n",
            "loss: 0.6104, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-05-21 02:26:41\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:41\n",
            "loss: 0.6204, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-05-21 02:26:41\n",
            "loss: 0.5981, acc: 0.7565\n",
            "E2E-ABSA >>> 2022-05-21 02:26:41\n",
            "loss: 0.6029, acc: 0.7548\n",
            "E2E-ABSA >>> 2022-05-21 02:26:42\n",
            ">>> val_acc: 0.7417, val_precision: 0.7417 val_recall: 0.7417, val_f1: 0.7417\n",
            ">> saved: state_dict/lstm_twitter_val_f1_0.7417\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:42\n",
            "loss: 0.3869, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:26:42\n",
            "loss: 0.6023, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-05-21 02:26:42\n",
            "loss: 0.6054, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-05-21 02:26:42\n",
            "loss: 0.6041, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-05-21 02:26:42\n",
            ">>> val_acc: 0.7299, val_precision: 0.7299 val_recall: 0.7299, val_f1: 0.7299\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:42\n",
            "loss: 0.6104, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-05-21 02:26:43\n",
            "loss: 0.5751, acc: 0.7527\n",
            "E2E-ABSA >>> 2022-05-21 02:26:43\n",
            "loss: 0.5744, acc: 0.7549\n",
            "E2E-ABSA >>> 2022-05-21 02:26:43\n",
            "loss: 0.5954, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-05-21 02:26:43\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:43\n",
            "loss: 0.6179, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 02:26:43\n",
            "loss: 0.5887, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 02:26:44\n",
            "loss: 0.5748, acc: 0.7611\n",
            "E2E-ABSA >>> 2022-05-21 02:26:44\n",
            ">>> val_acc: 0.7251, val_precision: 0.7251 val_recall: 0.7251, val_f1: 0.7251\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:44\n",
            "loss: 0.5229, acc: 0.8036\n",
            "E2E-ABSA >>> 2022-05-21 02:26:44\n",
            "loss: 0.5469, acc: 0.7926\n",
            "E2E-ABSA >>> 2022-05-21 02:26:44\n",
            "loss: 0.5732, acc: 0.7745\n",
            "E2E-ABSA >>> 2022-05-21 02:26:44\n",
            "loss: 0.5877, acc: 0.7590\n",
            "E2E-ABSA >>> 2022-05-21 02:26:44\n",
            ">>> val_acc: 0.7180, val_precision: 0.7180 val_recall: 0.7180, val_f1: 0.7180\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:45\n",
            "loss: 0.5837, acc: 0.7567\n",
            "E2E-ABSA >>> 2022-05-21 02:26:45\n",
            "loss: 0.5918, acc: 0.7586\n",
            "E2E-ABSA >>> 2022-05-21 02:26:45\n",
            "loss: 0.5876, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-05-21 02:26:45\n",
            ">>> val_acc: 0.7299, val_precision: 0.7299 val_recall: 0.7299, val_f1: 0.7299\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:45\n",
            "loss: 0.5224, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-05-21 02:26:45\n",
            "loss: 0.6141, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-05-21 02:26:45\n",
            "loss: 0.5925, acc: 0.7457\n",
            "E2E-ABSA >>> 2022-05-21 02:26:46\n",
            "loss: 0.5859, acc: 0.7506\n",
            "E2E-ABSA >>> 2022-05-21 02:26:46\n",
            ">>> val_acc: 0.7275, val_precision: 0.7275 val_recall: 0.7275, val_f1: 0.7275\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:46\n",
            "loss: 0.5953, acc: 0.7428\n",
            "E2E-ABSA >>> 2022-05-21 02:26:46\n",
            "loss: 0.5625, acc: 0.7701\n",
            "E2E-ABSA >>> 2022-05-21 02:26:46\n",
            "loss: 0.5786, acc: 0.7544\n",
            "E2E-ABSA >>> 2022-05-21 02:26:46\n",
            ">>> val_acc: 0.7251, val_precision: 0.7251 val_recall: 0.7251, val_f1: 0.7251\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:47\n",
            "loss: 0.6169, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:26:47\n",
            "loss: 0.5981, acc: 0.7453\n",
            "E2E-ABSA >>> 2022-05-21 02:26:47\n",
            "loss: 0.5898, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-05-21 02:26:47\n",
            "loss: 0.5793, acc: 0.7600\n",
            "E2E-ABSA >>> 2022-05-21 02:26:47\n",
            ">>> val_acc: 0.7275, val_precision: 0.7275 val_recall: 0.7275, val_f1: 0.7275\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:47\n",
            "loss: 0.5817, acc: 0.7786\n",
            "E2E-ABSA >>> 2022-05-21 02:26:47\n",
            "loss: 0.5879, acc: 0.7546\n",
            "E2E-ABSA >>> 2022-05-21 02:26:48\n",
            "loss: 0.5862, acc: 0.7560\n",
            "E2E-ABSA >>> 2022-05-21 02:26:48\n",
            ">>> val_acc: 0.7062, val_precision: 0.7062 val_recall: 0.7062, val_f1: 0.7062\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:48\n",
            "loss: 0.5439, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 02:26:48\n",
            "loss: 0.5877, acc: 0.7401\n",
            "E2E-ABSA >>> 2022-05-21 02:26:48\n",
            "loss: 0.5710, acc: 0.7564\n",
            "E2E-ABSA >>> 2022-05-21 02:26:48\n",
            "loss: 0.5799, acc: 0.7532\n",
            "E2E-ABSA >>> 2022-05-21 02:26:49\n",
            ">>> val_acc: 0.7133, val_precision: 0.7133 val_recall: 0.7133, val_f1: 0.7133\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:49\n",
            "loss: 0.5558, acc: 0.7898\n",
            "E2E-ABSA >>> 2022-05-21 02:26:49\n",
            "loss: 0.5870, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:26:49\n",
            "loss: 0.5826, acc: 0.7569\n",
            "E2E-ABSA >>> 2022-05-21 02:26:49\n",
            ">>> val_acc: 0.7227, val_precision: 0.7227 val_recall: 0.7227, val_f1: 0.7227\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:49\n",
            "loss: 0.5528, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-05-21 02:26:49\n",
            "loss: 0.5871, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 02:26:50\n",
            "loss: 0.5806, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:26:50\n",
            "loss: 0.5788, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-05-21 02:26:50\n",
            ">>> val_acc: 0.7014, val_precision: 0.7014 val_recall: 0.7014, val_f1: 0.7014\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:50\n",
            "loss: 0.5559, acc: 0.7906\n",
            "E2E-ABSA >>> 2022-05-21 02:26:50\n",
            "loss: 0.5656, acc: 0.7700\n",
            "E2E-ABSA >>> 2022-05-21 02:26:50\n",
            "loss: 0.5689, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-05-21 02:26:51\n",
            ">>> val_acc: 0.7014, val_precision: 0.7014 val_recall: 0.7014, val_f1: 0.7014\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:51\n",
            "loss: 0.6327, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 02:26:51\n",
            "loss: 0.5645, acc: 0.7592\n",
            "E2E-ABSA >>> 2022-05-21 02:26:51\n",
            "loss: 0.5789, acc: 0.7559\n",
            "E2E-ABSA >>> 2022-05-21 02:26:51\n",
            "loss: 0.5765, acc: 0.7533\n",
            "E2E-ABSA >>> 2022-05-21 02:26:51\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:51\n",
            "loss: 0.5562, acc: 0.7674\n",
            "E2E-ABSA >>> 2022-05-21 02:26:52\n",
            "loss: 0.5768, acc: 0.7617\n",
            "E2E-ABSA >>> 2022-05-21 02:26:52\n",
            "loss: 0.5683, acc: 0.7572\n",
            "E2E-ABSA >>> 2022-05-21 02:26:52\n",
            ">>> val_acc: 0.7180, val_precision: 0.7180 val_recall: 0.7180, val_f1: 0.7180\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:52\n",
            "loss: 0.3746, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-21 02:26:52\n",
            "loss: 0.5685, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-05-21 02:26:52\n",
            "loss: 0.5520, acc: 0.7752\n",
            "E2E-ABSA >>> 2022-05-21 02:26:53\n",
            "loss: 0.5684, acc: 0.7615\n",
            "E2E-ABSA >>> 2022-05-21 02:26:53\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:53\n",
            "loss: 0.5707, acc: 0.7617\n",
            "E2E-ABSA >>> 2022-05-21 02:26:53\n",
            "loss: 0.5615, acc: 0.7568\n",
            "E2E-ABSA >>> 2022-05-21 02:26:53\n",
            "loss: 0.5554, acc: 0.7648\n",
            "E2E-ABSA >>> 2022-05-21 02:26:53\n",
            "loss: 0.5643, acc: 0.7605\n",
            "E2E-ABSA >>> 2022-05-21 02:26:53\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:54\n",
            "loss: 0.5466, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-05-21 02:26:54\n",
            "loss: 0.5716, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-05-21 02:26:54\n",
            "loss: 0.5693, acc: 0.7646\n",
            "E2E-ABSA >>> 2022-05-21 02:26:54\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:54\n",
            "loss: 0.5820, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-05-21 02:26:54\n",
            "loss: 0.5666, acc: 0.7614\n",
            "E2E-ABSA >>> 2022-05-21 02:26:54\n",
            "loss: 0.5676, acc: 0.7559\n",
            "E2E-ABSA >>> 2022-05-21 02:26:55\n",
            "loss: 0.5645, acc: 0.7590\n",
            "E2E-ABSA >>> 2022-05-21 02:26:55\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:55\n",
            "loss: 0.5497, acc: 0.7723\n",
            "E2E-ABSA >>> 2022-05-21 02:26:55\n",
            "loss: 0.5421, acc: 0.7802\n",
            "E2E-ABSA >>> 2022-05-21 02:26:55\n",
            "loss: 0.5614, acc: 0.7621\n",
            "E2E-ABSA >>> 2022-05-21 02:26:55\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:55\n",
            "loss: 0.5786, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 02:26:56\n",
            "loss: 0.5619, acc: 0.7649\n",
            "E2E-ABSA >>> 2022-05-21 02:26:56\n",
            "loss: 0.5606, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-05-21 02:26:56\n",
            "loss: 0.5605, acc: 0.7586\n",
            "E2E-ABSA >>> 2022-05-21 02:26:56\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:56\n",
            "loss: 0.5547, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-05-21 02:26:56\n",
            "loss: 0.5681, acc: 0.7645\n",
            "E2E-ABSA >>> 2022-05-21 02:26:57\n",
            "loss: 0.5639, acc: 0.7536\n",
            "E2E-ABSA >>> 2022-05-21 02:26:57\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:57\n",
            "loss: 0.5853, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-05-21 02:26:57\n",
            "loss: 0.5674, acc: 0.7547\n",
            "E2E-ABSA >>> 2022-05-21 02:26:57\n",
            "loss: 0.5437, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-05-21 02:26:57\n",
            "loss: 0.5527, acc: 0.7644\n",
            "E2E-ABSA >>> 2022-05-21 02:26:58\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:58\n",
            "loss: 0.5386, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 02:26:58\n",
            "loss: 0.5506, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-05-21 02:26:58\n",
            "loss: 0.5530, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 02:26:58\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:58\n",
            "loss: 0.5797, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 02:26:58\n",
            "loss: 0.5624, acc: 0.7632\n",
            "E2E-ABSA >>> 2022-05-21 02:26:59\n",
            "loss: 0.5450, acc: 0.7757\n",
            "E2E-ABSA >>> 2022-05-21 02:26:59\n",
            "loss: 0.5545, acc: 0.7666\n",
            "E2E-ABSA >>> 2022-05-21 02:26:59\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-05-21 02:26:59\n",
            "loss: 0.5742, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-05-21 02:26:59\n",
            "loss: 0.5642, acc: 0.7488\n",
            "E2E-ABSA >>> 2022-05-21 02:26:59\n",
            "loss: 0.5492, acc: 0.7561\n",
            "E2E-ABSA >>> 2022-05-21 02:27:00\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:00\n",
            "loss: 0.5610, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-05-21 02:27:00\n",
            "loss: 0.5393, acc: 0.7674\n",
            "E2E-ABSA >>> 2022-05-21 02:27:00\n",
            "loss: 0.5511, acc: 0.7585\n",
            "E2E-ABSA >>> 2022-05-21 02:27:00\n",
            "loss: 0.5519, acc: 0.7630\n",
            "E2E-ABSA >>> 2022-05-21 02:27:00\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:00\n",
            "loss: 0.5821, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:27:01\n",
            "loss: 0.5857, acc: 0.7450\n",
            "E2E-ABSA >>> 2022-05-21 02:27:01\n",
            "loss: 0.5470, acc: 0.7664\n",
            "E2E-ABSA >>> 2022-05-21 02:27:01\n",
            ">>> val_acc: 0.6967, val_precision: 0.6967 val_recall: 0.6967, val_f1: 0.6967\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:01\n",
            "loss: 0.5392, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 02:27:01\n",
            "loss: 0.5322, acc: 0.7776\n",
            "E2E-ABSA >>> 2022-05-21 02:27:01\n",
            "loss: 0.5432, acc: 0.7744\n",
            "E2E-ABSA >>> 2022-05-21 02:27:02\n",
            "loss: 0.5565, acc: 0.7586\n",
            "E2E-ABSA >>> 2022-05-21 02:27:02\n",
            ">>> val_acc: 0.6943, val_precision: 0.6943 val_recall: 0.6943, val_f1: 0.6943\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:02\n",
            "loss: 0.5062, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 02:27:02\n",
            "loss: 0.5609, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-05-21 02:27:02\n",
            "loss: 0.5591, acc: 0.7540\n",
            "E2E-ABSA >>> 2022-05-21 02:27:02\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:02\n",
            "loss: 0.5037, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:27:03\n",
            "loss: 0.5264, acc: 0.8008\n",
            "E2E-ABSA >>> 2022-05-21 02:27:03\n",
            "loss: 0.5323, acc: 0.7853\n",
            "E2E-ABSA >>> 2022-05-21 02:27:03\n",
            "loss: 0.5437, acc: 0.7731\n",
            "E2E-ABSA >>> 2022-05-21 02:27:03\n",
            ">>> val_acc: 0.7014, val_precision: 0.7014 val_recall: 0.7014, val_f1: 0.7014\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:03\n",
            "loss: 0.6276, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-05-21 02:27:03\n",
            "loss: 0.5412, acc: 0.7636\n",
            "E2E-ABSA >>> 2022-05-21 02:27:04\n",
            "loss: 0.5437, acc: 0.7607\n",
            "E2E-ABSA >>> 2022-05-21 02:27:04\n",
            "loss: 0.5462, acc: 0.7629\n",
            "E2E-ABSA >>> 2022-05-21 02:27:04\n",
            ">>> val_acc: 0.6943, val_precision: 0.6943 val_recall: 0.6943, val_f1: 0.6943\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:04\n",
            "loss: 0.5329, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:27:04\n",
            "loss: 0.5278, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-05-21 02:27:04\n",
            "loss: 0.5363, acc: 0.7694\n",
            "E2E-ABSA >>> 2022-05-21 02:27:04\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:05\n",
            "loss: 0.5722, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-05-21 02:27:05\n",
            "loss: 0.5544, acc: 0.7599\n",
            "E2E-ABSA >>> 2022-05-21 02:27:05\n",
            "loss: 0.5545, acc: 0.7576\n",
            "E2E-ABSA >>> 2022-05-21 02:27:05\n",
            "loss: 0.5419, acc: 0.7680\n",
            "E2E-ABSA >>> 2022-05-21 02:27:05\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:05\n",
            "loss: 0.5269, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-05-21 02:27:05\n",
            "loss: 0.5253, acc: 0.7769\n",
            "E2E-ABSA >>> 2022-05-21 02:27:06\n",
            "loss: 0.5298, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-05-21 02:27:06\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:06\n",
            "loss: 0.5373, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-05-21 02:27:06\n",
            "loss: 0.5393, acc: 0.7574\n",
            "E2E-ABSA >>> 2022-05-21 02:27:06\n",
            "loss: 0.5403, acc: 0.7595\n",
            "E2E-ABSA >>> 2022-05-21 02:27:06\n",
            "loss: 0.5332, acc: 0.7702\n",
            "E2E-ABSA >>> 2022-05-21 02:27:07\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:07\n",
            "loss: 0.4913, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:27:07\n",
            "loss: 0.5256, acc: 0.7712\n",
            "E2E-ABSA >>> 2022-05-21 02:27:07\n",
            "loss: 0.5306, acc: 0.7718\n",
            "E2E-ABSA >>> 2022-05-21 02:27:07\n",
            ">>> val_acc: 0.6919, val_precision: 0.6919 val_recall: 0.6919, val_f1: 0.6919\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:07\n",
            "loss: 0.6357, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 02:27:07\n",
            "loss: 0.5492, acc: 0.7766\n",
            "E2E-ABSA >>> 2022-05-21 02:27:08\n",
            "loss: 0.5202, acc: 0.7804\n",
            "E2E-ABSA >>> 2022-05-21 02:27:08\n",
            "loss: 0.5366, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-05-21 02:27:08\n",
            ">>> val_acc: 0.6919, val_precision: 0.6919 val_recall: 0.6919, val_f1: 0.6919\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:08\n",
            "loss: 0.5476, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-05-21 02:27:08\n",
            "loss: 0.5334, acc: 0.7743\n",
            "E2E-ABSA >>> 2022-05-21 02:27:08\n",
            "loss: 0.5291, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 02:27:09\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:09\n",
            "loss: 0.5272, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-05-21 02:27:09\n",
            "loss: 0.4795, acc: 0.7928\n",
            "E2E-ABSA >>> 2022-05-21 02:27:09\n",
            "loss: 0.5164, acc: 0.7757\n",
            "E2E-ABSA >>> 2022-05-21 02:27:09\n",
            "loss: 0.5345, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-05-21 02:27:09\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:09\n",
            "loss: 0.5460, acc: 0.7614\n",
            "E2E-ABSA >>> 2022-05-21 02:27:10\n",
            "loss: 0.5534, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-05-21 02:27:10\n",
            "loss: 0.5425, acc: 0.7569\n",
            "E2E-ABSA >>> 2022-05-21 02:27:10\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:10\n",
            "loss: 0.4705, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-05-21 02:27:10\n",
            "loss: 0.5370, acc: 0.7691\n",
            "E2E-ABSA >>> 2022-05-21 02:27:10\n",
            "loss: 0.5481, acc: 0.7576\n",
            "E2E-ABSA >>> 2022-05-21 02:27:11\n",
            "loss: 0.5303, acc: 0.7715\n",
            "E2E-ABSA >>> 2022-05-21 02:27:11\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:11\n",
            "loss: 0.5670, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:27:11\n",
            "loss: 0.5545, acc: 0.7550\n",
            "E2E-ABSA >>> 2022-05-21 02:27:11\n",
            "loss: 0.5313, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-05-21 02:27:11\n",
            ">>> val_acc: 0.7014, val_precision: 0.7014 val_recall: 0.7014, val_f1: 0.7014\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:11\n",
            "loss: 0.4949, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:27:12\n",
            "loss: 0.5360, acc: 0.7739\n",
            "E2E-ABSA >>> 2022-05-21 02:27:12\n",
            "loss: 0.5248, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-05-21 02:27:12\n",
            "loss: 0.5308, acc: 0.7706\n",
            "E2E-ABSA >>> 2022-05-21 02:27:12\n",
            ">>> val_acc: 0.6967, val_precision: 0.6967 val_recall: 0.6967, val_f1: 0.6967\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:12\n",
            "loss: 0.4755, acc: 0.7951\n",
            "E2E-ABSA >>> 2022-05-21 02:27:12\n",
            "loss: 0.5071, acc: 0.7799\n",
            "E2E-ABSA >>> 2022-05-21 02:27:13\n",
            "loss: 0.5373, acc: 0.7700\n",
            "E2E-ABSA >>> 2022-05-21 02:27:13\n",
            ">>> val_acc: 0.6967, val_precision: 0.6967 val_recall: 0.6967, val_f1: 0.6967\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:13\n",
            "loss: 0.6599, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 02:27:13\n",
            "loss: 0.5266, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-05-21 02:27:13\n",
            "loss: 0.5203, acc: 0.7732\n",
            "E2E-ABSA >>> 2022-05-21 02:27:13\n",
            "loss: 0.5295, acc: 0.7697\n",
            "E2E-ABSA >>> 2022-05-21 02:27:13\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:14\n",
            "loss: 0.5209, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 02:27:14\n",
            "loss: 0.5397, acc: 0.7514\n",
            "E2E-ABSA >>> 2022-05-21 02:27:14\n",
            "loss: 0.5266, acc: 0.7623\n",
            "E2E-ABSA >>> 2022-05-21 02:27:14\n",
            "loss: 0.5296, acc: 0.7623\n",
            "E2E-ABSA >>> 2022-05-21 02:27:14\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:14\n",
            "loss: 0.5390, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-05-21 02:27:15\n",
            "loss: 0.5274, acc: 0.7729\n",
            "E2E-ABSA >>> 2022-05-21 02:27:15\n",
            "loss: 0.5258, acc: 0.7715\n",
            "E2E-ABSA >>> 2022-05-21 02:27:15\n",
            ">>> val_acc: 0.6943, val_precision: 0.6943 val_recall: 0.6943, val_f1: 0.6943\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:15\n",
            "loss: 0.5226, acc: 0.7768\n",
            "E2E-ABSA >>> 2022-05-21 02:27:15\n",
            "loss: 0.5385, acc: 0.7685\n",
            "E2E-ABSA >>> 2022-05-21 02:27:15\n",
            "loss: 0.5235, acc: 0.7728\n",
            "E2E-ABSA >>> 2022-05-21 02:27:15\n",
            "loss: 0.5274, acc: 0.7704\n",
            "E2E-ABSA >>> 2022-05-21 02:27:16\n",
            ">>> val_acc: 0.6919, val_precision: 0.6919 val_recall: 0.6919, val_f1: 0.6919\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-05-21 02:27:16\n",
            "loss: 0.5673, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:27:16\n",
            "loss: 0.5415, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-05-21 02:27:16\n",
            "loss: 0.5343, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-05-21 02:27:16\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            "E2E-ABSA >>> 2022-05-21 02:27:16\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7417, val_precision: 0.7417 val_recall: 0.7417, val_f1: 0.7417\n",
            "you can download the best model from state_dict/lstm_twitter_val_f1_0.7417\n",
            ">>> test_acc: 0.7417, test_precision: 0.7417, test_recall: 0.7417, test_f1: 0.7417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **twitter** dataset on model**(LSTM)**"
      ],
      "metadata": {
        "id": "z2TAF_B5QZ-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset twitter_know --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWCXsluCQaLH",
        "outputId": "78ce0328-77f8-4827-da37-16061b0d61a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1687.\n",
            "> testing dataset count: 422.\n",
            "cuda memory allocated: 17928192\n",
            "> n_trainable_params: 603303, n_nontrainable_params: 3878400\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f9b6b0fbb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:13\n",
            "loss: 1.1229, acc: 0.1292\n",
            "E2E-ABSA >>> 2022-05-21 02:28:13\n",
            "loss: 1.0896, acc: 0.3396\n",
            "E2E-ABSA >>> 2022-05-21 02:28:14\n",
            "loss: 1.0614, acc: 0.4361\n",
            "E2E-ABSA >>> 2022-05-21 02:28:14\n",
            ">>> val_acc: 0.6801, val_precision: 0.6801 val_recall: 0.6801, val_f1: 0.6801\n",
            ">> saved: state_dict/lstm_twitter_know_val_f1_0.6801\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:14\n",
            "loss: 0.9223, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 02:28:14\n",
            "loss: 0.9033, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-21 02:28:14\n",
            "loss: 0.9005, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-05-21 02:28:15\n",
            "loss: 0.8852, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-05-21 02:28:15\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">> saved: state_dict/lstm_twitter_know_val_f1_0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:15\n",
            "loss: 0.8506, acc: 0.6540\n",
            "E2E-ABSA >>> 2022-05-21 02:28:15\n",
            "loss: 0.8557, acc: 0.6487\n",
            "E2E-ABSA >>> 2022-05-21 02:28:15\n",
            "loss: 0.8352, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 02:28:16\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:16\n",
            "loss: 0.7885, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:28:16\n",
            "loss: 0.7895, acc: 0.6845\n",
            "E2E-ABSA >>> 2022-05-21 02:28:16\n",
            "loss: 0.8038, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 02:28:16\n",
            "loss: 0.8108, acc: 0.6697\n",
            "E2E-ABSA >>> 2022-05-21 02:28:17\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:17\n",
            "loss: 0.8334, acc: 0.6178\n",
            "E2E-ABSA >>> 2022-05-21 02:28:17\n",
            "loss: 0.8075, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-05-21 02:28:17\n",
            "loss: 0.7888, acc: 0.6824\n",
            "E2E-ABSA >>> 2022-05-21 02:28:17\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:17\n",
            "loss: 0.7266, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 02:28:18\n",
            "loss: 0.8013, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 02:28:18\n",
            "loss: 0.7837, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 02:28:18\n",
            "loss: 0.8000, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:28:18\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:18\n",
            "loss: 0.7961, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 02:28:19\n",
            "loss: 0.7931, acc: 0.6725\n",
            "E2E-ABSA >>> 2022-05-21 02:28:19\n",
            "loss: 0.7977, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-05-21 02:28:19\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:19\n",
            "loss: 0.7385, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:28:19\n",
            "loss: 0.8017, acc: 0.6661\n",
            "E2E-ABSA >>> 2022-05-21 02:28:20\n",
            "loss: 0.7961, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-05-21 02:28:20\n",
            "loss: 0.8031, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-05-21 02:28:20\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:20\n",
            "loss: 0.7790, acc: 0.6761\n",
            "E2E-ABSA >>> 2022-05-21 02:28:20\n",
            "loss: 0.7726, acc: 0.6827\n",
            "E2E-ABSA >>> 2022-05-21 02:28:21\n",
            "loss: 0.7935, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-05-21 02:28:21\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:21\n",
            "loss: 0.8087, acc: 0.6354\n",
            "E2E-ABSA >>> 2022-05-21 02:28:21\n",
            "loss: 0.7709, acc: 0.6858\n",
            "E2E-ABSA >>> 2022-05-21 02:28:21\n",
            "loss: 0.8161, acc: 0.6572\n",
            "E2E-ABSA >>> 2022-05-21 02:28:22\n",
            "loss: 0.8036, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 02:28:22\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:22\n",
            "loss: 0.7803, acc: 0.6594\n",
            "E2E-ABSA >>> 2022-05-21 02:28:22\n",
            "loss: 0.7978, acc: 0.6700\n",
            "E2E-ABSA >>> 2022-05-21 02:28:22\n",
            "loss: 0.7934, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-05-21 02:28:23\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:23\n",
            "loss: 0.7624, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:28:23\n",
            "loss: 0.7809, acc: 0.6654\n",
            "E2E-ABSA >>> 2022-05-21 02:28:23\n",
            "loss: 0.7822, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-05-21 02:28:23\n",
            "loss: 0.7981, acc: 0.6682\n",
            "E2E-ABSA >>> 2022-05-21 02:28:24\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:24\n",
            "loss: 0.8424, acc: 0.6632\n",
            "E2E-ABSA >>> 2022-05-21 02:28:24\n",
            "loss: 0.7837, acc: 0.6810\n",
            "E2E-ABSA >>> 2022-05-21 02:28:24\n",
            "loss: 0.7960, acc: 0.6675\n",
            "E2E-ABSA >>> 2022-05-21 02:28:25\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:25\n",
            "loss: 0.7266, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:28:25\n",
            "loss: 0.7919, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:28:25\n",
            "loss: 0.8084, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-05-21 02:28:25\n",
            "loss: 0.7972, acc: 0.6698\n",
            "E2E-ABSA >>> 2022-05-21 02:28:25\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:26\n",
            "loss: 0.8183, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-05-21 02:28:26\n",
            "loss: 0.8313, acc: 0.6413\n",
            "E2E-ABSA >>> 2022-05-21 02:28:26\n",
            "loss: 0.7979, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-05-21 02:28:26\n",
            "loss: 0.7925, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 02:28:26\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:27\n",
            "loss: 0.8137, acc: 0.6417\n",
            "E2E-ABSA >>> 2022-05-21 02:28:27\n",
            "loss: 0.8084, acc: 0.6583\n",
            "E2E-ABSA >>> 2022-05-21 02:28:27\n",
            "loss: 0.8026, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-05-21 02:28:27\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:27\n",
            "loss: 0.7470, acc: 0.6741\n",
            "E2E-ABSA >>> 2022-05-21 02:28:28\n",
            "loss: 0.7928, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-05-21 02:28:28\n",
            "loss: 0.7963, acc: 0.6639\n",
            "E2E-ABSA >>> 2022-05-21 02:28:28\n",
            "loss: 0.7892, acc: 0.6689\n",
            "E2E-ABSA >>> 2022-05-21 02:28:28\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:28\n",
            "loss: 0.7539, acc: 0.6786\n",
            "E2E-ABSA >>> 2022-05-21 02:28:29\n",
            "loss: 0.7527, acc: 0.6853\n",
            "E2E-ABSA >>> 2022-05-21 02:28:29\n",
            "loss: 0.7848, acc: 0.6783\n",
            "E2E-ABSA >>> 2022-05-21 02:28:29\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:29\n",
            "loss: 0.8571, acc: 0.5885\n",
            "E2E-ABSA >>> 2022-05-21 02:28:29\n",
            "loss: 0.8107, acc: 0.6503\n",
            "E2E-ABSA >>> 2022-05-21 02:28:29\n",
            "loss: 0.7872, acc: 0.6727\n",
            "E2E-ABSA >>> 2022-05-21 02:28:30\n",
            "loss: 0.7888, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 02:28:30\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:30\n",
            "loss: 0.7671, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-05-21 02:28:30\n",
            "loss: 0.7911, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-05-21 02:28:30\n",
            "loss: 0.8027, acc: 0.6584\n",
            "E2E-ABSA >>> 2022-05-21 02:28:31\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:31\n",
            "loss: 0.8193, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-05-21 02:28:31\n",
            "loss: 0.7771, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 02:28:31\n",
            "loss: 0.8022, acc: 0.6598\n",
            "E2E-ABSA >>> 2022-05-21 02:28:31\n",
            "loss: 0.7893, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-05-21 02:28:32\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:32\n",
            "loss: 0.7522, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-05-21 02:28:32\n",
            "loss: 0.7657, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-05-21 02:28:32\n",
            "loss: 0.7694, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 02:28:32\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:33\n",
            "loss: 0.7458, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 02:28:33\n",
            "loss: 0.7394, acc: 0.6908\n",
            "E2E-ABSA >>> 2022-05-21 02:28:33\n",
            "loss: 0.7854, acc: 0.6673\n",
            "E2E-ABSA >>> 2022-05-21 02:28:33\n",
            "loss: 0.7862, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 02:28:33\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:34\n",
            "loss: 0.8015, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-05-21 02:28:34\n",
            "loss: 0.8022, acc: 0.6514\n",
            "E2E-ABSA >>> 2022-05-21 02:28:34\n",
            "loss: 0.7797, acc: 0.6700\n",
            "E2E-ABSA >>> 2022-05-21 02:28:34\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:34\n",
            "loss: 0.7565, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:28:34\n",
            "loss: 0.7885, acc: 0.6597\n",
            "E2E-ABSA >>> 2022-05-21 02:28:35\n",
            "loss: 0.7767, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-05-21 02:28:35\n",
            "loss: 0.7783, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-05-21 02:28:35\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:35\n",
            "loss: 0.7860, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-05-21 02:28:35\n",
            "loss: 0.7952, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-05-21 02:28:36\n",
            "loss: 0.7770, acc: 0.6727\n",
            "E2E-ABSA >>> 2022-05-21 02:28:36\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:36\n",
            "loss: 0.8059, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:28:36\n",
            "loss: 0.7561, acc: 0.6838\n",
            "E2E-ABSA >>> 2022-05-21 02:28:36\n",
            "loss: 0.7574, acc: 0.6914\n",
            "E2E-ABSA >>> 2022-05-21 02:28:37\n",
            "loss: 0.7819, acc: 0.6722\n",
            "E2E-ABSA >>> 2022-05-21 02:28:37\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:37\n",
            "loss: 0.7588, acc: 0.6806\n",
            "E2E-ABSA >>> 2022-05-21 02:28:37\n",
            "loss: 0.7675, acc: 0.6784\n",
            "E2E-ABSA >>> 2022-05-21 02:28:37\n",
            "loss: 0.7742, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-05-21 02:28:38\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:38\n",
            "loss: 0.8086, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:28:38\n",
            "loss: 0.7833, acc: 0.6660\n",
            "E2E-ABSA >>> 2022-05-21 02:28:38\n",
            "loss: 0.7965, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:28:38\n",
            "loss: 0.7869, acc: 0.6678\n",
            "E2E-ABSA >>> 2022-05-21 02:28:39\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:39\n",
            "loss: 0.8475, acc: 0.6211\n",
            "E2E-ABSA >>> 2022-05-21 02:28:39\n",
            "loss: 0.8054, acc: 0.6427\n",
            "E2E-ABSA >>> 2022-05-21 02:28:39\n",
            "loss: 0.7715, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-05-21 02:28:39\n",
            "loss: 0.7810, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 02:28:39\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:40\n",
            "loss: 0.8162, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-05-21 02:28:40\n",
            "loss: 0.7858, acc: 0.6573\n",
            "E2E-ABSA >>> 2022-05-21 02:28:40\n",
            "loss: 0.7929, acc: 0.6583\n",
            "E2E-ABSA >>> 2022-05-21 02:28:40\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:40\n",
            "loss: 0.7730, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:28:41\n",
            "loss: 0.7773, acc: 0.6747\n",
            "E2E-ABSA >>> 2022-05-21 02:28:41\n",
            "loss: 0.7964, acc: 0.6622\n",
            "E2E-ABSA >>> 2022-05-21 02:28:41\n",
            "loss: 0.7791, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 02:28:41\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:41\n",
            "loss: 0.7985, acc: 0.6607\n",
            "E2E-ABSA >>> 2022-05-21 02:28:42\n",
            "loss: 0.7747, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 02:28:42\n",
            "loss: 0.7843, acc: 0.6683\n",
            "E2E-ABSA >>> 2022-05-21 02:28:42\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:42\n",
            "loss: 0.7398, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 02:28:42\n",
            "loss: 0.7853, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 02:28:43\n",
            "loss: 0.7938, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-05-21 02:28:43\n",
            "loss: 0.7767, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 02:28:43\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:43\n",
            "loss: 0.8018, acc: 0.6466\n",
            "E2E-ABSA >>> 2022-05-21 02:28:43\n",
            "loss: 0.8140, acc: 0.6462\n",
            "E2E-ABSA >>> 2022-05-21 02:28:44\n",
            "loss: 0.7979, acc: 0.6519\n",
            "E2E-ABSA >>> 2022-05-21 02:28:44\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:44\n",
            "loss: 0.7806, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 02:28:44\n",
            "loss: 0.7589, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-05-21 02:28:44\n",
            "loss: 0.7747, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-05-21 02:28:45\n",
            "loss: 0.7725, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-05-21 02:28:45\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:45\n",
            "loss: 0.8163, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-05-21 02:28:45\n",
            "loss: 0.8059, acc: 0.6516\n",
            "E2E-ABSA >>> 2022-05-21 02:28:45\n",
            "loss: 0.7899, acc: 0.6607\n",
            "E2E-ABSA >>> 2022-05-21 02:28:46\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:46\n",
            "loss: 0.7902, acc: 0.6328\n",
            "E2E-ABSA >>> 2022-05-21 02:28:46\n",
            "loss: 0.7861, acc: 0.6497\n",
            "E2E-ABSA >>> 2022-05-21 02:28:46\n",
            "loss: 0.7812, acc: 0.6590\n",
            "E2E-ABSA >>> 2022-05-21 02:28:46\n",
            "loss: 0.7735, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-05-21 02:28:47\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:47\n",
            "loss: 0.7954, acc: 0.6477\n",
            "E2E-ABSA >>> 2022-05-21 02:28:47\n",
            "loss: 0.7715, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-05-21 02:28:47\n",
            "loss: 0.7710, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-05-21 02:28:47\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:47\n",
            "loss: 0.6583, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 02:28:48\n",
            "loss: 0.7435, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-05-21 02:28:48\n",
            "loss: 0.7837, acc: 0.6638\n",
            "E2E-ABSA >>> 2022-05-21 02:28:48\n",
            "loss: 0.7752, acc: 0.6706\n",
            "E2E-ABSA >>> 2022-05-21 02:28:48\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:48\n",
            "loss: 0.7511, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-05-21 02:28:49\n",
            "loss: 0.7617, acc: 0.6775\n",
            "E2E-ABSA >>> 2022-05-21 02:28:49\n",
            "loss: 0.7652, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-05-21 02:28:49\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:49\n",
            "loss: 0.7962, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 02:28:49\n",
            "loss: 0.8138, acc: 0.6452\n",
            "E2E-ABSA >>> 2022-05-21 02:28:50\n",
            "loss: 0.7838, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-05-21 02:28:50\n",
            "loss: 0.7756, acc: 0.6695\n",
            "E2E-ABSA >>> 2022-05-21 02:28:50\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:50\n",
            "loss: 0.7780, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-05-21 02:28:50\n",
            "loss: 0.7627, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-05-21 02:28:51\n",
            "loss: 0.7659, acc: 0.6715\n",
            "E2E-ABSA >>> 2022-05-21 02:28:51\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:51\n",
            "loss: 0.6849, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:28:51\n",
            "loss: 0.7559, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:28:51\n",
            "loss: 0.7713, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-05-21 02:28:52\n",
            "loss: 0.7702, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 02:28:52\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:52\n",
            "loss: 0.8107, acc: 0.6133\n",
            "E2E-ABSA >>> 2022-05-21 02:28:52\n",
            "loss: 0.7866, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-05-21 02:28:52\n",
            "loss: 0.7658, acc: 0.6702\n",
            "E2E-ABSA >>> 2022-05-21 02:28:53\n",
            "loss: 0.7722, acc: 0.6698\n",
            "E2E-ABSA >>> 2022-05-21 02:28:53\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:53\n",
            "loss: 0.7751, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 02:28:53\n",
            "loss: 0.7614, acc: 0.6729\n",
            "E2E-ABSA >>> 2022-05-21 02:28:53\n",
            "loss: 0.7614, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 02:28:54\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:54\n",
            "loss: 0.7419, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 02:28:54\n",
            "loss: 0.7426, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-05-21 02:28:54\n",
            "loss: 0.7557, acc: 0.6740\n",
            "E2E-ABSA >>> 2022-05-21 02:28:54\n",
            "loss: 0.7698, acc: 0.6707\n",
            "E2E-ABSA >>> 2022-05-21 02:28:54\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:55\n",
            "loss: 0.7659, acc: 0.6763\n",
            "E2E-ABSA >>> 2022-05-21 02:28:55\n",
            "loss: 0.7749, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-05-21 02:28:55\n",
            "loss: 0.7680, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:28:55\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:55\n",
            "loss: 0.7094, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 02:28:56\n",
            "loss: 0.7903, acc: 0.6533\n",
            "E2E-ABSA >>> 2022-05-21 02:28:56\n",
            "loss: 0.7700, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 02:28:56\n",
            "loss: 0.7704, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-05-21 02:28:56\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:56\n",
            "loss: 0.7775, acc: 0.6803\n",
            "E2E-ABSA >>> 2022-05-21 02:28:57\n",
            "loss: 0.7528, acc: 0.6897\n",
            "E2E-ABSA >>> 2022-05-21 02:28:57\n",
            "loss: 0.7699, acc: 0.6751\n",
            "E2E-ABSA >>> 2022-05-21 02:28:57\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:57\n",
            "loss: 0.7944, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-05-21 02:28:57\n",
            "loss: 0.7812, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 02:28:58\n",
            "loss: 0.7658, acc: 0.6732\n",
            "E2E-ABSA >>> 2022-05-21 02:28:58\n",
            "loss: 0.7650, acc: 0.6706\n",
            "E2E-ABSA >>> 2022-05-21 02:28:58\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 02:28:58\n",
            "loss: 0.7024, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 02:28:58\n",
            "loss: 0.7467, acc: 0.6782\n",
            "E2E-ABSA >>> 2022-05-21 02:28:59\n",
            "loss: 0.7752, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-05-21 02:28:59\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            "E2E-ABSA >>> 2022-05-21 02:28:59\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            "you can download the best model from state_dict/lstm_twitter_know_val_f1_0.6848\n",
            ">>> test_acc: 0.6848, test_precision: 0.6848, test_recall: 0.6848, test_f1: 0.6848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **twitter** dataset on model**(td_LSTM)**"
      ],
      "metadata": {
        "id": "VugQPrLU9wfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset twitter --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nE0nu8h9x0q",
        "outputId": "9b395719-e125-419c-b4ad-39cec9b4df2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1154.\n",
            "> testing dataset count: 310.\n",
            "cuda memory allocated: 13580288\n",
            "> n_trainable_params: 1206603, n_nontrainable_params: 2188000\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f2c5f994b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 02:45:56\n",
            "loss: 1.2871, acc: 0.0729\n",
            "E2E-ABSA >>> 2022-05-21 02:45:56\n",
            "loss: 1.2349, acc: 0.0844\n",
            "E2E-ABSA >>> 2022-05-21 02:45:57\n",
            ">>> val_acc: 0.3258, val_precision: 0.3258 val_recall: 0.3258, val_f1: 0.3258\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.3258\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 02:45:57\n",
            "loss: 1.0624, acc: 0.4191\n",
            "E2E-ABSA >>> 2022-05-21 02:45:57\n",
            "loss: 1.0542, acc: 0.4734\n",
            "E2E-ABSA >>> 2022-05-21 02:45:57\n",
            ">>> val_acc: 0.6290, val_precision: 0.6290 val_recall: 0.6290, val_f1: 0.6290\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.629\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 02:45:57\n",
            "loss: 0.8840, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 02:45:58\n",
            "loss: 0.9466, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 02:45:58\n",
            "loss: 0.9332, acc: 0.6387\n",
            "E2E-ABSA >>> 2022-05-21 02:45:58\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 02:45:58\n",
            "loss: 0.9005, acc: 0.6310\n",
            "E2E-ABSA >>> 2022-05-21 02:45:59\n",
            "loss: 0.8904, acc: 0.6360\n",
            "E2E-ABSA >>> 2022-05-21 02:45:59\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 02:45:59\n",
            "loss: 0.8486, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:45:59\n",
            "loss: 0.8251, acc: 0.6859\n",
            "E2E-ABSA >>> 2022-05-21 02:45:59\n",
            "loss: 0.8552, acc: 0.6498\n",
            "E2E-ABSA >>> 2022-05-21 02:46:00\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:00\n",
            "loss: 0.8301, acc: 0.6575\n",
            "E2E-ABSA >>> 2022-05-21 02:46:00\n",
            "loss: 0.8429, acc: 0.6466\n",
            "E2E-ABSA >>> 2022-05-21 02:46:00\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:00\n",
            "loss: 0.7455, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 02:46:01\n",
            "loss: 0.8090, acc: 0.6622\n",
            "E2E-ABSA >>> 2022-05-21 02:46:01\n",
            "loss: 0.8259, acc: 0.6476\n",
            "E2E-ABSA >>> 2022-05-21 02:46:01\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:01\n",
            "loss: 0.8005, acc: 0.6552\n",
            "E2E-ABSA >>> 2022-05-21 02:46:01\n",
            "loss: 0.8187, acc: 0.6409\n",
            "E2E-ABSA >>> 2022-05-21 02:46:02\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:02\n",
            "loss: 0.8209, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-05-21 02:46:02\n",
            "loss: 0.7972, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-05-21 02:46:02\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:02\n",
            "loss: 0.7819, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-05-21 02:46:03\n",
            "loss: 0.8270, acc: 0.6288\n",
            "E2E-ABSA >>> 2022-05-21 02:46:03\n",
            "loss: 0.8056, acc: 0.6468\n",
            "E2E-ABSA >>> 2022-05-21 02:46:03\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:03\n",
            "loss: 0.8241, acc: 0.6469\n",
            "E2E-ABSA >>> 2022-05-21 02:46:04\n",
            "loss: 0.8206, acc: 0.6338\n",
            "E2E-ABSA >>> 2022-05-21 02:46:04\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:04\n",
            "loss: 0.7878, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-05-21 02:46:04\n",
            "loss: 0.7869, acc: 0.6605\n",
            "E2E-ABSA >>> 2022-05-21 02:46:04\n",
            "loss: 0.7864, acc: 0.6567\n",
            "E2E-ABSA >>> 2022-05-21 02:46:05\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:05\n",
            "loss: 0.8066, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-05-21 02:46:05\n",
            "loss: 0.7836, acc: 0.6644\n",
            "E2E-ABSA >>> 2022-05-21 02:46:05\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:05\n",
            "loss: 0.8304, acc: 0.5852\n",
            "E2E-ABSA >>> 2022-05-21 02:46:06\n",
            "loss: 0.7850, acc: 0.6524\n",
            "E2E-ABSA >>> 2022-05-21 02:46:06\n",
            "loss: 0.7860, acc: 0.6540\n",
            "E2E-ABSA >>> 2022-05-21 02:46:06\n",
            ">>> val_acc: 0.6548, val_precision: 0.6548 val_recall: 0.6548, val_f1: 0.6548\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:06\n",
            "loss: 0.7577, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-05-21 02:46:07\n",
            "loss: 0.7810, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:46:07\n",
            ">>> val_acc: 0.6548, val_precision: 0.6548 val_recall: 0.6548, val_f1: 0.6548\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:07\n",
            "loss: 0.7500, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 02:46:07\n",
            "loss: 0.7548, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-05-21 02:46:08\n",
            ">>> val_acc: 0.6516, val_precision: 0.6516 val_recall: 0.6516, val_f1: 0.6516\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:08\n",
            "loss: 0.7546, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:46:08\n",
            "loss: 0.7826, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-05-21 02:46:08\n",
            "loss: 0.7642, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-05-21 02:46:08\n",
            ">>> val_acc: 0.6548, val_precision: 0.6548 val_recall: 0.6548, val_f1: 0.6548\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:09\n",
            "loss: 0.7901, acc: 0.6513\n",
            "E2E-ABSA >>> 2022-05-21 02:46:09\n",
            "loss: 0.7800, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-05-21 02:46:09\n",
            ">>> val_acc: 0.6548, val_precision: 0.6548 val_recall: 0.6548, val_f1: 0.6548\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:09\n",
            "loss: 0.7768, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 02:46:09\n",
            "loss: 0.7540, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 02:46:10\n",
            "loss: 0.7667, acc: 0.6619\n",
            "E2E-ABSA >>> 2022-05-21 02:46:10\n",
            ">>> val_acc: 0.6516, val_precision: 0.6516 val_recall: 0.6516, val_f1: 0.6516\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:10\n",
            "loss: 0.7404, acc: 0.6821\n",
            "E2E-ABSA >>> 2022-05-21 02:46:10\n",
            "loss: 0.7574, acc: 0.6627\n",
            "E2E-ABSA >>> 2022-05-21 02:46:11\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:11\n",
            "loss: 0.7328, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-05-21 02:46:11\n",
            "loss: 0.7626, acc: 0.6609\n",
            "E2E-ABSA >>> 2022-05-21 02:46:11\n",
            "loss: 0.7511, acc: 0.6741\n",
            "E2E-ABSA >>> 2022-05-21 02:46:11\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:12\n",
            "loss: 0.7487, acc: 0.6574\n",
            "E2E-ABSA >>> 2022-05-21 02:46:12\n",
            "loss: 0.7461, acc: 0.6743\n",
            "E2E-ABSA >>> 2022-05-21 02:46:12\n",
            ">>> val_acc: 0.6645, val_precision: 0.6645 val_recall: 0.6645, val_f1: 0.6645\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6645\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:12\n",
            "loss: 0.7369, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-05-21 02:46:12\n",
            "loss: 0.7396, acc: 0.6747\n",
            "E2E-ABSA >>> 2022-05-21 02:46:13\n",
            ">>> val_acc: 0.6645, val_precision: 0.6645 val_recall: 0.6645, val_f1: 0.6645\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:13\n",
            "loss: 0.8817, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 02:46:13\n",
            "loss: 0.7525, acc: 0.6694\n",
            "E2E-ABSA >>> 2022-05-21 02:46:13\n",
            "loss: 0.7433, acc: 0.6773\n",
            "E2E-ABSA >>> 2022-05-21 02:46:14\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:14\n",
            "loss: 0.7887, acc: 0.6458\n",
            "E2E-ABSA >>> 2022-05-21 02:46:14\n",
            "loss: 0.7585, acc: 0.6589\n",
            "E2E-ABSA >>> 2022-05-21 02:46:14\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:14\n",
            "loss: 0.6410, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-05-21 02:46:15\n",
            "loss: 0.7242, acc: 0.6839\n",
            "E2E-ABSA >>> 2022-05-21 02:46:15\n",
            "loss: 0.7319, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:46:15\n",
            ">>> val_acc: 0.6645, val_precision: 0.6645 val_recall: 0.6645, val_f1: 0.6645\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:15\n",
            "loss: 0.7028, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-05-21 02:46:15\n",
            "loss: 0.7149, acc: 0.6911\n",
            "E2E-ABSA >>> 2022-05-21 02:46:16\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:16\n",
            "loss: 0.6613, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-05-21 02:46:16\n",
            "loss: 0.7359, acc: 0.6715\n",
            "E2E-ABSA >>> 2022-05-21 02:46:16\n",
            "loss: 0.7298, acc: 0.6821\n",
            "E2E-ABSA >>> 2022-05-21 02:46:16\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.671\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:17\n",
            "loss: 0.7441, acc: 0.6755\n",
            "E2E-ABSA >>> 2022-05-21 02:46:17\n",
            "loss: 0.7327, acc: 0.6864\n",
            "E2E-ABSA >>> 2022-05-21 02:46:17\n",
            ">>> val_acc: 0.6742, val_precision: 0.6742 val_recall: 0.6742, val_f1: 0.6742\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6742\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:17\n",
            "loss: 0.7058, acc: 0.7067\n",
            "E2E-ABSA >>> 2022-05-21 02:46:18\n",
            "loss: 0.7192, acc: 0.6948\n",
            "E2E-ABSA >>> 2022-05-21 02:46:18\n",
            "loss: 0.7205, acc: 0.6837\n",
            "E2E-ABSA >>> 2022-05-21 02:46:18\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:18\n",
            "loss: 0.7195, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-05-21 02:46:18\n",
            "loss: 0.7213, acc: 0.6896\n",
            "E2E-ABSA >>> 2022-05-21 02:46:19\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:19\n",
            "loss: 0.6743, acc: 0.7059\n",
            "E2E-ABSA >>> 2022-05-21 02:46:19\n",
            "loss: 0.7236, acc: 0.6822\n",
            "E2E-ABSA >>> 2022-05-21 02:46:19\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:19\n",
            "loss: 0.7389, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-05-21 02:46:20\n",
            "loss: 0.7098, acc: 0.6893\n",
            "E2E-ABSA >>> 2022-05-21 02:46:20\n",
            "loss: 0.6987, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-05-21 02:46:20\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:20\n",
            "loss: 0.7009, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-05-21 02:46:21\n",
            "loss: 0.6987, acc: 0.6949\n",
            "E2E-ABSA >>> 2022-05-21 02:46:21\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:21\n",
            "loss: 0.6928, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-05-21 02:46:21\n",
            "loss: 0.6890, acc: 0.7023\n",
            "E2E-ABSA >>> 2022-05-21 02:46:21\n",
            "loss: 0.7029, acc: 0.6939\n",
            "E2E-ABSA >>> 2022-05-21 02:46:22\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:22\n",
            "loss: 0.6666, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-05-21 02:46:22\n",
            "loss: 0.6930, acc: 0.7011\n",
            "E2E-ABSA >>> 2022-05-21 02:46:22\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:22\n",
            "loss: 0.6148, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 02:46:23\n",
            "loss: 0.6836, acc: 0.7024\n",
            "E2E-ABSA >>> 2022-05-21 02:46:23\n",
            "loss: 0.6959, acc: 0.6936\n",
            "E2E-ABSA >>> 2022-05-21 02:46:23\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:23\n",
            "loss: 0.7045, acc: 0.6789\n",
            "E2E-ABSA >>> 2022-05-21 02:46:24\n",
            "loss: 0.6993, acc: 0.6907\n",
            "E2E-ABSA >>> 2022-05-21 02:46:24\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:24\n",
            "loss: 0.6396, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-05-21 02:46:24\n",
            "loss: 0.6757, acc: 0.7024\n",
            "E2E-ABSA >>> 2022-05-21 02:46:25\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:25\n",
            "loss: 0.8041, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 02:46:25\n",
            "loss: 0.6811, acc: 0.6989\n",
            "E2E-ABSA >>> 2022-05-21 02:46:25\n",
            "loss: 0.6710, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-05-21 02:46:25\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:26\n",
            "loss: 0.7137, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:46:26\n",
            "loss: 0.6967, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-05-21 02:46:26\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:26\n",
            "loss: 0.6784, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-05-21 02:46:26\n",
            "loss: 0.6584, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-05-21 02:46:27\n",
            "loss: 0.6718, acc: 0.7080\n",
            "E2E-ABSA >>> 2022-05-21 02:46:27\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:27\n",
            "loss: 0.6645, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 02:46:27\n",
            "loss: 0.6598, acc: 0.7072\n",
            "E2E-ABSA >>> 2022-05-21 02:46:28\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:28\n",
            "loss: 0.6972, acc: 0.6932\n",
            "E2E-ABSA >>> 2022-05-21 02:46:28\n",
            "loss: 0.6692, acc: 0.7210\n",
            "E2E-ABSA >>> 2022-05-21 02:46:28\n",
            "loss: 0.6672, acc: 0.7139\n",
            "E2E-ABSA >>> 2022-05-21 02:46:28\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.7\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:29\n",
            "loss: 0.6245, acc: 0.7478\n",
            "E2E-ABSA >>> 2022-05-21 02:46:29\n",
            "loss: 0.6463, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-05-21 02:46:29\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:29\n",
            "loss: 0.6462, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-05-21 02:46:29\n",
            "loss: 0.6384, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-05-21 02:46:30\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:30\n",
            "loss: 0.6989, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:46:30\n",
            "loss: 0.6365, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-05-21 02:46:30\n",
            "loss: 0.6542, acc: 0.7258\n",
            "E2E-ABSA >>> 2022-05-21 02:46:31\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:31\n",
            "loss: 0.6440, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-05-21 02:46:31\n",
            "loss: 0.6569, acc: 0.7105\n",
            "E2E-ABSA >>> 2022-05-21 02:46:31\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:31\n",
            "loss: 0.5974, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-05-21 02:46:32\n",
            "loss: 0.6266, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 02:46:32\n",
            "loss: 0.6507, acc: 0.7206\n",
            "E2E-ABSA >>> 2022-05-21 02:46:32\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:32\n",
            "loss: 0.6553, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-05-21 02:46:32\n",
            "loss: 0.6415, acc: 0.7335\n",
            "E2E-ABSA >>> 2022-05-21 02:46:33\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:33\n",
            "loss: 0.5970, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 02:46:33\n",
            "loss: 0.6193, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-05-21 02:46:33\n",
            "loss: 0.6366, acc: 0.7304\n",
            "E2E-ABSA >>> 2022-05-21 02:46:33\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:34\n",
            "loss: 0.5996, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:46:34\n",
            "loss: 0.6001, acc: 0.7544\n",
            "E2E-ABSA >>> 2022-05-21 02:46:34\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:34\n",
            "loss: 0.6372, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 02:46:35\n",
            "loss: 0.6275, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-05-21 02:46:35\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:35\n",
            "loss: 0.6982, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 02:46:35\n",
            "loss: 0.6080, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-05-21 02:46:35\n",
            "loss: 0.6323, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-05-21 02:46:36\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:36\n",
            "loss: 0.6258, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 02:46:36\n",
            "loss: 0.6229, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 02:46:36\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:36\n",
            "loss: 0.6902, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 02:46:37\n",
            "loss: 0.6156, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-05-21 02:46:37\n",
            "loss: 0.6211, acc: 0.7317\n",
            "E2E-ABSA >>> 2022-05-21 02:46:37\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:37\n",
            "loss: 0.6345, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-05-21 02:46:38\n",
            "loss: 0.6212, acc: 0.7428\n",
            "E2E-ABSA >>> 2022-05-21 02:46:38\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:38\n",
            "loss: 0.5662, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 02:46:38\n",
            "loss: 0.6129, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 02:46:38\n",
            "loss: 0.6135, acc: 0.7373\n",
            "E2E-ABSA >>> 2022-05-21 02:46:39\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:39\n",
            "loss: 0.6060, acc: 0.7524\n",
            "E2E-ABSA >>> 2022-05-21 02:46:39\n",
            "loss: 0.6138, acc: 0.7489\n",
            "E2E-ABSA >>> 2022-05-21 02:46:39\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:39\n",
            "loss: 0.5913, acc: 0.7933\n",
            "E2E-ABSA >>> 2022-05-21 02:46:40\n",
            "loss: 0.5936, acc: 0.7529\n",
            "E2E-ABSA >>> 2022-05-21 02:46:40\n",
            "loss: 0.6048, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-05-21 02:46:40\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:40\n",
            "loss: 0.5917, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-05-21 02:46:41\n",
            "loss: 0.6044, acc: 0.7490\n",
            "E2E-ABSA >>> 2022-05-21 02:46:41\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:41\n",
            "loss: 0.5935, acc: 0.7647\n",
            "E2E-ABSA >>> 2022-05-21 02:46:41\n",
            "loss: 0.6017, acc: 0.7566\n",
            "E2E-ABSA >>> 2022-05-21 02:46:42\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:42\n",
            "loss: 0.5395, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 02:46:42\n",
            "loss: 0.5921, acc: 0.7739\n",
            "E2E-ABSA >>> 2022-05-21 02:46:42\n",
            "loss: 0.5955, acc: 0.7529\n",
            "E2E-ABSA >>> 2022-05-21 02:46:42\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:42\n",
            "loss: 0.5749, acc: 0.7768\n",
            "E2E-ABSA >>> 2022-05-21 02:46:43\n",
            "loss: 0.5845, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 02:46:43\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:43\n",
            "loss: 0.5298, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-05-21 02:46:43\n",
            "loss: 0.5610, acc: 0.7796\n",
            "E2E-ABSA >>> 2022-05-21 02:46:44\n",
            "loss: 0.5832, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-05-21 02:46:44\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:44\n",
            "loss: 0.5838, acc: 0.7600\n",
            "E2E-ABSA >>> 2022-05-21 02:46:44\n",
            "loss: 0.5819, acc: 0.7591\n",
            "E2E-ABSA >>> 2022-05-21 02:46:44\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:45\n",
            "loss: 0.5888, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 02:46:45\n",
            "loss: 0.5952, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-05-21 02:46:45\n",
            "loss: 0.5823, acc: 0.7630\n",
            "E2E-ABSA >>> 2022-05-21 02:46:45\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:45\n",
            "loss: 0.5637, acc: 0.7608\n",
            "E2E-ABSA >>> 2022-05-21 02:46:46\n",
            "loss: 0.5722, acc: 0.7648\n",
            "E2E-ABSA >>> 2022-05-21 02:46:46\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:46\n",
            "loss: 0.5593, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-05-21 02:46:46\n",
            "loss: 0.5586, acc: 0.7731\n",
            "E2E-ABSA >>> 2022-05-21 02:46:47\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:47\n",
            "loss: 0.6486, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 02:46:47\n",
            "loss: 0.5686, acc: 0.7595\n",
            "E2E-ABSA >>> 2022-05-21 02:46:47\n",
            "loss: 0.5662, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-05-21 02:46:47\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:48\n",
            "loss: 0.5459, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 02:46:48\n",
            "loss: 0.5628, acc: 0.7675\n",
            "E2E-ABSA >>> 2022-05-21 02:46:48\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:48\n",
            "loss: 0.5352, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-05-21 02:46:48\n",
            "loss: 0.5598, acc: 0.7838\n",
            "E2E-ABSA >>> 2022-05-21 02:46:49\n",
            "loss: 0.5569, acc: 0.7733\n",
            "E2E-ABSA >>> 2022-05-21 02:46:49\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:49\n",
            "loss: 0.5827, acc: 0.7474\n",
            "E2E-ABSA >>> 2022-05-21 02:46:49\n",
            "loss: 0.5589, acc: 0.7697\n",
            "E2E-ABSA >>> 2022-05-21 02:46:50\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:50\n",
            "loss: 0.5871, acc: 0.7614\n",
            "E2E-ABSA >>> 2022-05-21 02:46:50\n",
            "loss: 0.5718, acc: 0.7668\n",
            "E2E-ABSA >>> 2022-05-21 02:46:50\n",
            "loss: 0.5592, acc: 0.7738\n",
            "E2E-ABSA >>> 2022-05-21 02:46:50\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:51\n",
            "loss: 0.5481, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 02:46:51\n",
            "loss: 0.5431, acc: 0.7834\n",
            "E2E-ABSA >>> 2022-05-21 02:46:51\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:51\n",
            "loss: 0.5579, acc: 0.8042\n",
            "E2E-ABSA >>> 2022-05-21 02:46:51\n",
            "loss: 0.5641, acc: 0.7722\n",
            "E2E-ABSA >>> 2022-05-21 02:46:52\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:52\n",
            "loss: 0.4239, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-05-21 02:46:52\n",
            "loss: 0.5758, acc: 0.7637\n",
            "E2E-ABSA >>> 2022-05-21 02:46:52\n",
            "loss: 0.5478, acc: 0.7823\n",
            "E2E-ABSA >>> 2022-05-21 02:46:52\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:53\n",
            "loss: 0.5574, acc: 0.7829\n",
            "E2E-ABSA >>> 2022-05-21 02:46:53\n",
            "loss: 0.5303, acc: 0.7832\n",
            "E2E-ABSA >>> 2022-05-21 02:46:53\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:53\n",
            "loss: 0.5542, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:46:54\n",
            "loss: 0.5516, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:46:54\n",
            "loss: 0.5400, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:46:54\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:54\n",
            "loss: 0.5307, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 02:46:54\n",
            "loss: 0.5366, acc: 0.7948\n",
            "E2E-ABSA >>> 2022-05-21 02:46:55\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:55\n",
            "loss: 0.4706, acc: 0.8063\n",
            "E2E-ABSA >>> 2022-05-21 02:46:55\n",
            "loss: 0.5078, acc: 0.8016\n",
            "E2E-ABSA >>> 2022-05-21 02:46:55\n",
            "loss: 0.5347, acc: 0.7946\n",
            "E2E-ABSA >>> 2022-05-21 02:46:55\n",
            ">>> val_acc: 0.7161, val_precision: 0.7161 val_recall: 0.7161, val_f1: 0.7161\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.7161\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:56\n",
            "loss: 0.5423, acc: 0.7847\n",
            "E2E-ABSA >>> 2022-05-21 02:46:56\n",
            "loss: 0.5393, acc: 0.7884\n",
            "E2E-ABSA >>> 2022-05-21 02:46:56\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:56\n",
            "loss: 0.5070, acc: 0.8170\n",
            "E2E-ABSA >>> 2022-05-21 02:46:57\n",
            "loss: 0.5353, acc: 0.7983\n",
            "E2E-ABSA >>> 2022-05-21 02:46:57\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:57\n",
            "loss: 0.6473, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-21 02:46:57\n",
            "loss: 0.5146, acc: 0.8044\n",
            "E2E-ABSA >>> 2022-05-21 02:46:57\n",
            "loss: 0.5270, acc: 0.7941\n",
            "E2E-ABSA >>> 2022-05-21 02:46:58\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:58\n",
            "loss: 0.4754, acc: 0.8090\n",
            "E2E-ABSA >>> 2022-05-21 02:46:58\n",
            "loss: 0.4875, acc: 0.8190\n",
            "E2E-ABSA >>> 2022-05-21 02:46:58\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:58\n",
            "loss: 0.4772, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-05-21 02:46:59\n",
            "loss: 0.4887, acc: 0.8196\n",
            "E2E-ABSA >>> 2022-05-21 02:46:59\n",
            "loss: 0.5223, acc: 0.7990\n",
            "E2E-ABSA >>> 2022-05-21 02:46:59\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-05-21 02:46:59\n",
            "loss: 0.5177, acc: 0.8040\n",
            "E2E-ABSA >>> 2022-05-21 02:47:00\n",
            "loss: 0.5333, acc: 0.7885\n",
            "E2E-ABSA >>> 2022-05-21 02:47:00\n",
            ">>> val_acc: 0.7226, val_precision: 0.7226 val_recall: 0.7226, val_f1: 0.7226\n",
            ">> saved: state_dict/td_lstm_twitter_val_f1_0.7226\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:00\n",
            "loss: 0.5259, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 02:47:00\n",
            "loss: 0.5325, acc: 0.7901\n",
            "E2E-ABSA >>> 2022-05-21 02:47:00\n",
            "loss: 0.5159, acc: 0.7998\n",
            "E2E-ABSA >>> 2022-05-21 02:47:01\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:01\n",
            "loss: 0.5351, acc: 0.7933\n",
            "E2E-ABSA >>> 2022-05-21 02:47:01\n",
            "loss: 0.5194, acc: 0.7980\n",
            "E2E-ABSA >>> 2022-05-21 02:47:01\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:01\n",
            "loss: 0.5040, acc: 0.8029\n",
            "E2E-ABSA >>> 2022-05-21 02:47:02\n",
            "loss: 0.5104, acc: 0.8009\n",
            "E2E-ABSA >>> 2022-05-21 02:47:02\n",
            "loss: 0.5105, acc: 0.8033\n",
            "E2E-ABSA >>> 2022-05-21 02:47:02\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:02\n",
            "loss: 0.5077, acc: 0.7958\n",
            "E2E-ABSA >>> 2022-05-21 02:47:03\n",
            "loss: 0.5123, acc: 0.8031\n",
            "E2E-ABSA >>> 2022-05-21 02:47:03\n",
            ">>> val_acc: 0.7129, val_precision: 0.7129 val_recall: 0.7129, val_f1: 0.7129\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:03\n",
            "loss: 0.4975, acc: 0.8015\n",
            "E2E-ABSA >>> 2022-05-21 02:47:03\n",
            "loss: 0.4957, acc: 0.8098\n",
            "E2E-ABSA >>> 2022-05-21 02:47:03\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:04\n",
            "loss: 0.5068, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-05-21 02:47:04\n",
            "loss: 0.5154, acc: 0.7941\n",
            "E2E-ABSA >>> 2022-05-21 02:47:04\n",
            "loss: 0.5065, acc: 0.7998\n",
            "E2E-ABSA >>> 2022-05-21 02:47:04\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:04\n",
            "loss: 0.4596, acc: 0.8452\n",
            "E2E-ABSA >>> 2022-05-21 02:47:05\n",
            "loss: 0.4926, acc: 0.8100\n",
            "E2E-ABSA >>> 2022-05-21 02:47:05\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:05\n",
            "loss: 0.4449, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 02:47:05\n",
            "loss: 0.5103, acc: 0.7845\n",
            "E2E-ABSA >>> 2022-05-21 02:47:06\n",
            "loss: 0.4986, acc: 0.8024\n",
            "E2E-ABSA >>> 2022-05-21 02:47:06\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:06\n",
            "loss: 0.4924, acc: 0.8075\n",
            "E2E-ABSA >>> 2022-05-21 02:47:06\n",
            "loss: 0.4943, acc: 0.8057\n",
            "E2E-ABSA >>> 2022-05-21 02:47:06\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:07\n",
            "loss: 0.4414, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-21 02:47:07\n",
            "loss: 0.4681, acc: 0.8289\n",
            "E2E-ABSA >>> 2022-05-21 02:47:07\n",
            "loss: 0.4945, acc: 0.8160\n",
            "E2E-ABSA >>> 2022-05-21 02:47:07\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:07\n",
            "loss: 0.4429, acc: 0.8341\n",
            "E2E-ABSA >>> 2022-05-21 02:47:08\n",
            "loss: 0.4757, acc: 0.8146\n",
            "E2E-ABSA >>> 2022-05-21 02:47:08\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:08\n",
            "loss: 0.5006, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 02:47:08\n",
            "loss: 0.4895, acc: 0.8084\n",
            "E2E-ABSA >>> 2022-05-21 02:47:09\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:09\n",
            "loss: 0.5077, acc: 0.8542\n",
            "E2E-ABSA >>> 2022-05-21 02:47:09\n",
            "loss: 0.4908, acc: 0.8163\n",
            "E2E-ABSA >>> 2022-05-21 02:47:09\n",
            "loss: 0.4893, acc: 0.8135\n",
            "E2E-ABSA >>> 2022-05-21 02:47:09\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            "you can download the best model from state_dict/td_lstm_twitter_val_f1_0.7226\n",
            ">>> test_acc: 0.7226, test_precision: 0.7226, test_recall: 0.7226, test_f1: 0.7226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **twitter** dataset on model**(td_LSTM)**"
      ],
      "metadata": {
        "id": "RaOWTTM19wxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset twitter_know --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koFxkweS9xIs",
        "outputId": "34cee23d-698d-4090-bc82-c8f96808a623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            "加载Bert...\n",
            "loading tokenizer: twitter_know_tokenizer.dat\n",
            "加载预训练向量...\n",
            "loading embedding_matrix: 200_twitter_know_embedding_matrix.dat\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1687.\n",
            "> testing dataset count: 422.\n",
            "cuda memory allocated: 14997504\n",
            "> n_trainable_params: 603303, n_nontrainable_params: 2998000\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fe9797d2b90>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:10\n",
            "loss: 1.1225, acc: 0.1333\n",
            "E2E-ABSA >>> 2022-05-21 00:55:10\n",
            "loss: 1.0898, acc: 0.3396\n",
            "E2E-ABSA >>> 2022-05-21 00:55:11\n",
            "loss: 1.0616, acc: 0.4361\n",
            "E2E-ABSA >>> 2022-05-21 00:55:11\n",
            ">>> val_acc: 0.6801, val_precision: 0.6801 val_recall: 0.6801, val_f1: 0.6801\n",
            ">> saved: state_dict/lstm_twitter_know_val_f1_0.6801\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:11\n",
            "loss: 0.9220, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 00:55:11\n",
            "loss: 0.9037, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-21 00:55:12\n",
            "loss: 0.9009, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-05-21 00:55:12\n",
            "loss: 0.8861, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-05-21 00:55:12\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">> saved: state_dict/lstm_twitter_know_val_f1_0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:12\n",
            "loss: 0.8501, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-05-21 00:55:12\n",
            "loss: 0.8563, acc: 0.6476\n",
            "E2E-ABSA >>> 2022-05-21 00:55:13\n",
            "loss: 0.8358, acc: 0.6634\n",
            "E2E-ABSA >>> 2022-05-21 00:55:13\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:13\n",
            "loss: 0.7871, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 00:55:13\n",
            "loss: 0.7902, acc: 0.6845\n",
            "E2E-ABSA >>> 2022-05-21 00:55:13\n",
            "loss: 0.8043, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 00:55:14\n",
            "loss: 0.8112, acc: 0.6697\n",
            "E2E-ABSA >>> 2022-05-21 00:55:14\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:14\n",
            "loss: 0.8338, acc: 0.6178\n",
            "E2E-ABSA >>> 2022-05-21 00:55:14\n",
            "loss: 0.8075, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-05-21 00:55:14\n",
            "loss: 0.7889, acc: 0.6824\n",
            "E2E-ABSA >>> 2022-05-21 00:55:15\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:15\n",
            "loss: 0.7274, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 00:55:15\n",
            "loss: 0.8012, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 00:55:15\n",
            "loss: 0.7840, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 00:55:15\n",
            "loss: 0.8004, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 00:55:15\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:16\n",
            "loss: 0.7958, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 00:55:16\n",
            "loss: 0.7935, acc: 0.6725\n",
            "E2E-ABSA >>> 2022-05-21 00:55:16\n",
            "loss: 0.7980, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-05-21 00:55:16\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:16\n",
            "loss: 0.7382, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 00:55:17\n",
            "loss: 0.8026, acc: 0.6661\n",
            "E2E-ABSA >>> 2022-05-21 00:55:17\n",
            "loss: 0.7967, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-05-21 00:55:17\n",
            "loss: 0.8034, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-05-21 00:55:17\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:17\n",
            "loss: 0.7790, acc: 0.6761\n",
            "E2E-ABSA >>> 2022-05-21 00:55:18\n",
            "loss: 0.7730, acc: 0.6827\n",
            "E2E-ABSA >>> 2022-05-21 00:55:18\n",
            "loss: 0.7938, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-05-21 00:55:18\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:18\n",
            "loss: 0.8103, acc: 0.6354\n",
            "E2E-ABSA >>> 2022-05-21 00:55:18\n",
            "loss: 0.7718, acc: 0.6858\n",
            "E2E-ABSA >>> 2022-05-21 00:55:19\n",
            "loss: 0.8169, acc: 0.6572\n",
            "E2E-ABSA >>> 2022-05-21 00:55:19\n",
            "loss: 0.8041, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 00:55:19\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:19\n",
            "loss: 0.7808, acc: 0.6594\n",
            "E2E-ABSA >>> 2022-05-21 00:55:19\n",
            "loss: 0.7987, acc: 0.6700\n",
            "E2E-ABSA >>> 2022-05-21 00:55:20\n",
            "loss: 0.7941, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-05-21 00:55:20\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:20\n",
            "loss: 0.7632, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 00:55:20\n",
            "loss: 0.7813, acc: 0.6654\n",
            "E2E-ABSA >>> 2022-05-21 00:55:20\n",
            "loss: 0.7830, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-05-21 00:55:21\n",
            "loss: 0.7991, acc: 0.6682\n",
            "E2E-ABSA >>> 2022-05-21 00:55:21\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:21\n",
            "loss: 0.8426, acc: 0.6632\n",
            "E2E-ABSA >>> 2022-05-21 00:55:21\n",
            "loss: 0.7841, acc: 0.6810\n",
            "E2E-ABSA >>> 2022-05-21 00:55:21\n",
            "loss: 0.7967, acc: 0.6675\n",
            "E2E-ABSA >>> 2022-05-21 00:55:22\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:22\n",
            "loss: 0.7232, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 00:55:22\n",
            "loss: 0.7924, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 00:55:22\n",
            "loss: 0.8090, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-05-21 00:55:22\n",
            "loss: 0.7981, acc: 0.6698\n",
            "E2E-ABSA >>> 2022-05-21 00:55:23\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:23\n",
            "loss: 0.8201, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-05-21 00:55:23\n",
            "loss: 0.8326, acc: 0.6413\n",
            "E2E-ABSA >>> 2022-05-21 00:55:23\n",
            "loss: 0.7988, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-05-21 00:55:24\n",
            "loss: 0.7933, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 00:55:24\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:24\n",
            "loss: 0.8144, acc: 0.6417\n",
            "E2E-ABSA >>> 2022-05-21 00:55:24\n",
            "loss: 0.8091, acc: 0.6583\n",
            "E2E-ABSA >>> 2022-05-21 00:55:24\n",
            "loss: 0.8036, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-05-21 00:55:24\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:25\n",
            "loss: 0.7481, acc: 0.6741\n",
            "E2E-ABSA >>> 2022-05-21 00:55:25\n",
            "loss: 0.7934, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-05-21 00:55:25\n",
            "loss: 0.7968, acc: 0.6639\n",
            "E2E-ABSA >>> 2022-05-21 00:55:25\n",
            "loss: 0.7900, acc: 0.6689\n",
            "E2E-ABSA >>> 2022-05-21 00:55:25\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:26\n",
            "loss: 0.7542, acc: 0.6786\n",
            "E2E-ABSA >>> 2022-05-21 00:55:26\n",
            "loss: 0.7534, acc: 0.6853\n",
            "E2E-ABSA >>> 2022-05-21 00:55:26\n",
            "loss: 0.7857, acc: 0.6783\n",
            "E2E-ABSA >>> 2022-05-21 00:55:26\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:26\n",
            "loss: 0.8597, acc: 0.5885\n",
            "E2E-ABSA >>> 2022-05-21 00:55:27\n",
            "loss: 0.8120, acc: 0.6503\n",
            "E2E-ABSA >>> 2022-05-21 00:55:27\n",
            "loss: 0.7882, acc: 0.6727\n",
            "E2E-ABSA >>> 2022-05-21 00:55:27\n",
            "loss: 0.7896, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 00:55:27\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:27\n",
            "loss: 0.7677, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-05-21 00:55:28\n",
            "loss: 0.7918, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-05-21 00:55:28\n",
            "loss: 0.8039, acc: 0.6584\n",
            "E2E-ABSA >>> 2022-05-21 00:55:28\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:28\n",
            "loss: 0.8198, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-05-21 00:55:28\n",
            "loss: 0.7784, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 00:55:29\n",
            "loss: 0.8033, acc: 0.6598\n",
            "E2E-ABSA >>> 2022-05-21 00:55:29\n",
            "loss: 0.7902, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-05-21 00:55:29\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:29\n",
            "loss: 0.7536, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-05-21 00:55:29\n",
            "loss: 0.7667, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-05-21 00:55:30\n",
            "loss: 0.7703, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 00:55:30\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:30\n",
            "loss: 0.7478, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 00:55:30\n",
            "loss: 0.7408, acc: 0.6908\n",
            "E2E-ABSA >>> 2022-05-21 00:55:30\n",
            "loss: 0.7866, acc: 0.6673\n",
            "E2E-ABSA >>> 2022-05-21 00:55:30\n",
            "loss: 0.7872, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 00:55:31\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:31\n",
            "loss: 0.8032, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-05-21 00:55:31\n",
            "loss: 0.8040, acc: 0.6514\n",
            "E2E-ABSA >>> 2022-05-21 00:55:31\n",
            "loss: 0.7811, acc: 0.6700\n",
            "E2E-ABSA >>> 2022-05-21 00:55:32\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:32\n",
            "loss: 0.7594, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 00:55:32\n",
            "loss: 0.7899, acc: 0.6597\n",
            "E2E-ABSA >>> 2022-05-21 00:55:32\n",
            "loss: 0.7778, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-05-21 00:55:32\n",
            "loss: 0.7797, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-05-21 00:55:32\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:33\n",
            "loss: 0.7872, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-05-21 00:55:33\n",
            "loss: 0.7964, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-05-21 00:55:33\n",
            "loss: 0.7781, acc: 0.6727\n",
            "E2E-ABSA >>> 2022-05-21 00:55:33\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:33\n",
            "loss: 0.8046, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 00:55:34\n",
            "loss: 0.7579, acc: 0.6838\n",
            "E2E-ABSA >>> 2022-05-21 00:55:34\n",
            "loss: 0.7587, acc: 0.6914\n",
            "E2E-ABSA >>> 2022-05-21 00:55:34\n",
            "loss: 0.7833, acc: 0.6722\n",
            "E2E-ABSA >>> 2022-05-21 00:55:34\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:34\n",
            "loss: 0.7614, acc: 0.6806\n",
            "E2E-ABSA >>> 2022-05-21 00:55:35\n",
            "loss: 0.7690, acc: 0.6784\n",
            "E2E-ABSA >>> 2022-05-21 00:55:35\n",
            "loss: 0.7758, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-05-21 00:55:35\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:35\n",
            "loss: 0.8120, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 00:55:35\n",
            "loss: 0.7848, acc: 0.6660\n",
            "E2E-ABSA >>> 2022-05-21 00:55:36\n",
            "loss: 0.7985, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 00:55:36\n",
            "loss: 0.7884, acc: 0.6678\n",
            "E2E-ABSA >>> 2022-05-21 00:55:36\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:36\n",
            "loss: 0.8516, acc: 0.6211\n",
            "E2E-ABSA >>> 2022-05-21 00:55:36\n",
            "loss: 0.8076, acc: 0.6427\n",
            "E2E-ABSA >>> 2022-05-21 00:55:37\n",
            "loss: 0.7732, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-05-21 00:55:37\n",
            "loss: 0.7825, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 00:55:37\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:37\n",
            "loss: 0.8177, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-05-21 00:55:37\n",
            "loss: 0.7871, acc: 0.6573\n",
            "E2E-ABSA >>> 2022-05-21 00:55:38\n",
            "loss: 0.7943, acc: 0.6583\n",
            "E2E-ABSA >>> 2022-05-21 00:55:38\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:38\n",
            "loss: 0.7752, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 00:55:38\n",
            "loss: 0.7786, acc: 0.6747\n",
            "E2E-ABSA >>> 2022-05-21 00:55:38\n",
            "loss: 0.7979, acc: 0.6622\n",
            "E2E-ABSA >>> 2022-05-21 00:55:39\n",
            "loss: 0.7807, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 00:55:39\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:39\n",
            "loss: 0.8000, acc: 0.6607\n",
            "E2E-ABSA >>> 2022-05-21 00:55:39\n",
            "loss: 0.7769, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 00:55:39\n",
            "loss: 0.7861, acc: 0.6683\n",
            "E2E-ABSA >>> 2022-05-21 00:55:40\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:40\n",
            "loss: 0.7432, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 00:55:40\n",
            "loss: 0.7876, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 00:55:40\n",
            "loss: 0.7958, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-05-21 00:55:40\n",
            "loss: 0.7786, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 00:55:41\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:41\n",
            "loss: 0.8036, acc: 0.6466\n",
            "E2E-ABSA >>> 2022-05-21 00:55:41\n",
            "loss: 0.8154, acc: 0.6462\n",
            "E2E-ABSA >>> 2022-05-21 00:55:41\n",
            "loss: 0.7997, acc: 0.6519\n",
            "E2E-ABSA >>> 2022-05-21 00:55:41\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:42\n",
            "loss: 0.7793, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 00:55:42\n",
            "loss: 0.7593, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-05-21 00:55:42\n",
            "loss: 0.7760, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-05-21 00:55:42\n",
            "loss: 0.7742, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-05-21 00:55:42\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:43\n",
            "loss: 0.8171, acc: 0.6380\n",
            "E2E-ABSA >>> 2022-05-21 00:55:43\n",
            "loss: 0.8068, acc: 0.6493\n",
            "E2E-ABSA >>> 2022-05-21 00:55:43\n",
            "loss: 0.7919, acc: 0.6592\n",
            "E2E-ABSA >>> 2022-05-21 00:55:43\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:43\n",
            "loss: 0.7892, acc: 0.6328\n",
            "E2E-ABSA >>> 2022-05-21 00:55:44\n",
            "loss: 0.7875, acc: 0.6497\n",
            "E2E-ABSA >>> 2022-05-21 00:55:44\n",
            "loss: 0.7832, acc: 0.6590\n",
            "E2E-ABSA >>> 2022-05-21 00:55:44\n",
            "loss: 0.7753, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-05-21 00:55:44\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:44\n",
            "loss: 0.7974, acc: 0.6477\n",
            "E2E-ABSA >>> 2022-05-21 00:55:45\n",
            "loss: 0.7733, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-05-21 00:55:45\n",
            "loss: 0.7730, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-05-21 00:55:45\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:45\n",
            "loss: 0.6580, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 00:55:45\n",
            "loss: 0.7454, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-05-21 00:55:46\n",
            "loss: 0.7860, acc: 0.6638\n",
            "E2E-ABSA >>> 2022-05-21 00:55:46\n",
            "loss: 0.7773, acc: 0.6706\n",
            "E2E-ABSA >>> 2022-05-21 00:55:46\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:46\n",
            "loss: 0.7517, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-05-21 00:55:46\n",
            "loss: 0.7629, acc: 0.6775\n",
            "E2E-ABSA >>> 2022-05-21 00:55:47\n",
            "loss: 0.7667, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-05-21 00:55:47\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:47\n",
            "loss: 0.7991, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 00:55:47\n",
            "loss: 0.8163, acc: 0.6452\n",
            "E2E-ABSA >>> 2022-05-21 00:55:47\n",
            "loss: 0.7865, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-05-21 00:55:48\n",
            "loss: 0.7781, acc: 0.6695\n",
            "E2E-ABSA >>> 2022-05-21 00:55:48\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:48\n",
            "loss: 0.7804, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-05-21 00:55:48\n",
            "loss: 0.7642, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-05-21 00:55:48\n",
            "loss: 0.7680, acc: 0.6715\n",
            "E2E-ABSA >>> 2022-05-21 00:55:49\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:49\n",
            "loss: 0.6824, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 00:55:49\n",
            "loss: 0.7586, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 00:55:49\n",
            "loss: 0.7736, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-05-21 00:55:49\n",
            "loss: 0.7729, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 00:55:50\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:50\n",
            "loss: 0.8150, acc: 0.6133\n",
            "E2E-ABSA >>> 2022-05-21 00:55:50\n",
            "loss: 0.7895, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-05-21 00:55:50\n",
            "loss: 0.7685, acc: 0.6702\n",
            "E2E-ABSA >>> 2022-05-21 00:55:50\n",
            "loss: 0.7745, acc: 0.6698\n",
            "E2E-ABSA >>> 2022-05-21 00:55:51\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:51\n",
            "loss: 0.7789, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 00:55:51\n",
            "loss: 0.7643, acc: 0.6729\n",
            "E2E-ABSA >>> 2022-05-21 00:55:51\n",
            "loss: 0.7639, acc: 0.6743\n",
            "E2E-ABSA >>> 2022-05-21 00:55:51\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:51\n",
            "loss: 0.7472, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 00:55:52\n",
            "loss: 0.7465, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-05-21 00:55:52\n",
            "loss: 0.7587, acc: 0.6740\n",
            "E2E-ABSA >>> 2022-05-21 00:55:52\n",
            "loss: 0.7724, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 00:55:52\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:52\n",
            "loss: 0.7683, acc: 0.6763\n",
            "E2E-ABSA >>> 2022-05-21 00:55:53\n",
            "loss: 0.7778, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-05-21 00:55:53\n",
            "loss: 0.7707, acc: 0.6712\n",
            "E2E-ABSA >>> 2022-05-21 00:55:53\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:53\n",
            "loss: 0.7122, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 00:55:53\n",
            "loss: 0.7919, acc: 0.6533\n",
            "E2E-ABSA >>> 2022-05-21 00:55:54\n",
            "loss: 0.7722, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 00:55:54\n",
            "loss: 0.7734, acc: 0.6679\n",
            "E2E-ABSA >>> 2022-05-21 00:55:54\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:54\n",
            "loss: 0.7814, acc: 0.6803\n",
            "E2E-ABSA >>> 2022-05-21 00:55:54\n",
            "loss: 0.7558, acc: 0.6897\n",
            "E2E-ABSA >>> 2022-05-21 00:55:55\n",
            "loss: 0.7725, acc: 0.6751\n",
            "E2E-ABSA >>> 2022-05-21 00:55:55\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:55\n",
            "loss: 0.7960, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-05-21 00:55:55\n",
            "loss: 0.7849, acc: 0.6656\n",
            "E2E-ABSA >>> 2022-05-21 00:55:55\n",
            "loss: 0.7685, acc: 0.6741\n",
            "E2E-ABSA >>> 2022-05-21 00:55:56\n",
            "loss: 0.7681, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-05-21 00:55:56\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 00:55:56\n",
            "loss: 0.7032, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 00:55:56\n",
            "loss: 0.7491, acc: 0.6782\n",
            "E2E-ABSA >>> 2022-05-21 00:55:56\n",
            "loss: 0.7785, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-05-21 00:55:57\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            "E2E-ABSA >>> 2022-05-21 00:55:57\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            "you can download the best model from state_dict/lstm_twitter_know_val_f1_0.6848\n",
            ">>> test_acc: 0.6848, test_precision: 0.6848, test_recall: 0.6848, test_f1: 0.6848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **twitter** dataset on model**(tc_LSTM)**"
      ],
      "metadata": {
        "id": "nQ_N8DaFHNAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset twitter --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZmaYFZ1HOj-",
        "outputId": "e6d0f1d3-2a46-4163-bae7-a69f58dccc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            "加载Bert...\n",
            "loading tokenizer: twitter_tokenizer.dat\n",
            "加载预训练向量...\n",
            "loading word vectors...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "building embedding_matrix: 200_twitter_embedding_matrix.dat\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1154.\n",
            "> testing dataset count: 310.\n",
            "cuda memory allocated: 13278720\n",
            "> n_trainable_params: 1686603, n_nontrainable_params: 1632600\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fb3a225ab90>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:02\n",
            "loss: 1.0022, acc: 0.6375\n",
            "E2E-ABSA >>> 2022-05-21 01:04:02\n",
            "loss: 0.9777, acc: 0.6281\n",
            "E2E-ABSA >>> 2022-05-21 01:04:03\n",
            ">>> val_acc: 0.6548, val_precision: 0.6548 val_recall: 0.6548, val_f1: 0.6548\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.6548\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:03\n",
            "loss: 0.8824, acc: 0.6691\n",
            "E2E-ABSA >>> 2022-05-21 01:04:03\n",
            "loss: 0.8783, acc: 0.6569\n",
            "E2E-ABSA >>> 2022-05-21 01:04:03\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:03\n",
            "loss: 0.7771, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 01:04:04\n",
            "loss: 0.8178, acc: 0.6820\n",
            "E2E-ABSA >>> 2022-05-21 01:04:04\n",
            "loss: 0.8346, acc: 0.6553\n",
            "E2E-ABSA >>> 2022-05-21 01:04:04\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:04\n",
            "loss: 0.8196, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-05-21 01:04:05\n",
            "loss: 0.8175, acc: 0.6520\n",
            "E2E-ABSA >>> 2022-05-21 01:04:05\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:05\n",
            "loss: 0.8851, acc: 0.6328\n",
            "E2E-ABSA >>> 2022-05-21 01:04:05\n",
            "loss: 0.7985, acc: 0.6595\n",
            "E2E-ABSA >>> 2022-05-21 01:04:05\n",
            "loss: 0.8093, acc: 0.6443\n",
            "E2E-ABSA >>> 2022-05-21 01:04:06\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:06\n",
            "loss: 0.7668, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-05-21 01:04:06\n",
            "loss: 0.7761, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 01:04:06\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:06\n",
            "loss: 0.7189, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-05-21 01:04:07\n",
            "loss: 0.7776, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-05-21 01:04:07\n",
            "loss: 0.7847, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 01:04:07\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:07\n",
            "loss: 0.7875, acc: 0.6573\n",
            "E2E-ABSA >>> 2022-05-21 01:04:08\n",
            "loss: 0.7836, acc: 0.6525\n",
            "E2E-ABSA >>> 2022-05-21 01:04:08\n",
            ">>> val_acc: 0.6645, val_precision: 0.6645 val_recall: 0.6645, val_f1: 0.6645\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.6645\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:08\n",
            "loss: 0.7730, acc: 0.6680\n",
            "E2E-ABSA >>> 2022-05-21 01:04:08\n",
            "loss: 0.7708, acc: 0.6658\n",
            "E2E-ABSA >>> 2022-05-21 01:04:09\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:09\n",
            "loss: 0.8606, acc: 0.5000\n",
            "E2E-ABSA >>> 2022-05-21 01:04:09\n",
            "loss: 0.7394, acc: 0.6799\n",
            "E2E-ABSA >>> 2022-05-21 01:04:09\n",
            "loss: 0.7618, acc: 0.6677\n",
            "E2E-ABSA >>> 2022-05-21 01:04:09\n",
            ">>> val_acc: 0.6742, val_precision: 0.6742 val_recall: 0.6742, val_f1: 0.6742\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:10\n",
            "loss: 0.7325, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-05-21 01:04:10\n",
            "loss: 0.7502, acc: 0.6725\n",
            "E2E-ABSA >>> 2022-05-21 01:04:10\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:10\n",
            "loss: 0.6938, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-05-21 01:04:10\n",
            "loss: 0.7382, acc: 0.6858\n",
            "E2E-ABSA >>> 2022-05-21 01:04:11\n",
            "loss: 0.7411, acc: 0.6782\n",
            "E2E-ABSA >>> 2022-05-21 01:04:11\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:11\n",
            "loss: 0.7509, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-05-21 01:04:11\n",
            "loss: 0.7634, acc: 0.6516\n",
            "E2E-ABSA >>> 2022-05-21 01:04:12\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:12\n",
            "loss: 0.7432, acc: 0.6932\n",
            "E2E-ABSA >>> 2022-05-21 01:04:12\n",
            "loss: 0.7470, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 01:04:12\n",
            "loss: 0.7305, acc: 0.6778\n",
            "E2E-ABSA >>> 2022-05-21 01:04:12\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:13\n",
            "loss: 0.7172, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 01:04:13\n",
            "loss: 0.7276, acc: 0.6756\n",
            "E2E-ABSA >>> 2022-05-21 01:04:13\n",
            ">>> val_acc: 0.6645, val_precision: 0.6645 val_recall: 0.6645, val_f1: 0.6645\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:13\n",
            "loss: 0.7349, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-05-21 01:04:14\n",
            "loss: 0.7130, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-21 01:04:14\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:14\n",
            "loss: 0.8215, acc: 0.5000\n",
            "E2E-ABSA >>> 2022-05-21 01:04:14\n",
            "loss: 0.7435, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-05-21 01:04:15\n",
            "loss: 0.7264, acc: 0.6754\n",
            "E2E-ABSA >>> 2022-05-21 01:04:15\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:15\n",
            "loss: 0.7040, acc: 0.6645\n",
            "E2E-ABSA >>> 2022-05-21 01:04:15\n",
            "loss: 0.7196, acc: 0.6786\n",
            "E2E-ABSA >>> 2022-05-21 01:04:16\n",
            ">>> val_acc: 0.6742, val_precision: 0.6742 val_recall: 0.6742, val_f1: 0.6742\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:16\n",
            "loss: 0.7043, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 01:04:16\n",
            "loss: 0.6977, acc: 0.6840\n",
            "E2E-ABSA >>> 2022-05-21 01:04:16\n",
            "loss: 0.7080, acc: 0.6799\n",
            "E2E-ABSA >>> 2022-05-21 01:04:16\n",
            ">>> val_acc: 0.6742, val_precision: 0.6742 val_recall: 0.6742, val_f1: 0.6742\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:17\n",
            "loss: 0.6832, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 01:04:17\n",
            "loss: 0.6954, acc: 0.6828\n",
            "E2E-ABSA >>> 2022-05-21 01:04:17\n",
            ">>> val_acc: 0.6742, val_precision: 0.6742 val_recall: 0.6742, val_f1: 0.6742\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:17\n",
            "loss: 0.7001, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 01:04:17\n",
            "loss: 0.6863, acc: 0.6984\n",
            "E2E-ABSA >>> 2022-05-21 01:04:18\n",
            "loss: 0.6999, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 01:04:18\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:18\n",
            "loss: 0.6772, acc: 0.7037\n",
            "E2E-ABSA >>> 2022-05-21 01:04:18\n",
            "loss: 0.6928, acc: 0.6952\n",
            "E2E-ABSA >>> 2022-05-21 01:04:19\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:19\n",
            "loss: 0.7122, acc: 0.7009\n",
            "E2E-ABSA >>> 2022-05-21 01:04:19\n",
            "loss: 0.6923, acc: 0.6974\n",
            "E2E-ABSA >>> 2022-05-21 01:04:19\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:19\n",
            "loss: 0.6187, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 01:04:20\n",
            "loss: 0.7031, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 01:04:20\n",
            "loss: 0.6924, acc: 0.6967\n",
            "E2E-ABSA >>> 2022-05-21 01:04:20\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:20\n",
            "loss: 0.6450, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-05-21 01:04:21\n",
            "loss: 0.6521, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-05-21 01:04:21\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:21\n",
            "loss: 0.7117, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 01:04:21\n",
            "loss: 0.6897, acc: 0.6893\n",
            "E2E-ABSA >>> 2022-05-21 01:04:22\n",
            "loss: 0.6733, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-05-21 01:04:22\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:22\n",
            "loss: 0.6744, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-05-21 01:04:22\n",
            "loss: 0.6604, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-05-21 01:04:22\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:23\n",
            "loss: 0.7021, acc: 0.6528\n",
            "E2E-ABSA >>> 2022-05-21 01:04:23\n",
            "loss: 0.6742, acc: 0.7051\n",
            "E2E-ABSA >>> 2022-05-21 01:04:23\n",
            "loss: 0.6563, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-05-21 01:04:23\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:23\n",
            "loss: 0.6338, acc: 0.7524\n",
            "E2E-ABSA >>> 2022-05-21 01:04:24\n",
            "loss: 0.6353, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-05-21 01:04:24\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:24\n",
            "loss: 0.6537, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-05-21 01:04:24\n",
            "loss: 0.6601, acc: 0.7209\n",
            "E2E-ABSA >>> 2022-05-21 01:04:25\n",
            "loss: 0.6487, acc: 0.7296\n",
            "E2E-ABSA >>> 2022-05-21 01:04:25\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:25\n",
            "loss: 0.6370, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 01:04:25\n",
            "loss: 0.6412, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 01:04:25\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:26\n",
            "loss: 0.6883, acc: 0.6765\n",
            "E2E-ABSA >>> 2022-05-21 01:04:26\n",
            "loss: 0.6470, acc: 0.7234\n",
            "E2E-ABSA >>> 2022-05-21 01:04:26\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:26\n",
            "loss: 0.5563, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 01:04:27\n",
            "loss: 0.6655, acc: 0.6949\n",
            "E2E-ABSA >>> 2022-05-21 01:04:27\n",
            "loss: 0.6380, acc: 0.7324\n",
            "E2E-ABSA >>> 2022-05-21 01:04:27\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:27\n",
            "loss: 0.5986, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-05-21 01:04:27\n",
            "loss: 0.6410, acc: 0.7206\n",
            "E2E-ABSA >>> 2022-05-21 01:04:28\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:28\n",
            "loss: 0.6065, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 01:04:28\n",
            "loss: 0.6011, acc: 0.7434\n",
            "E2E-ABSA >>> 2022-05-21 01:04:28\n",
            "loss: 0.6169, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-05-21 01:04:28\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:29\n",
            "loss: 0.5995, acc: 0.7575\n",
            "E2E-ABSA >>> 2022-05-21 01:04:29\n",
            "loss: 0.6084, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-05-21 01:04:29\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:29\n",
            "loss: 0.5499, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 01:04:30\n",
            "loss: 0.5976, acc: 0.7530\n",
            "E2E-ABSA >>> 2022-05-21 01:04:30\n",
            "loss: 0.6005, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-05-21 01:04:30\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:30\n",
            "loss: 0.6026, acc: 0.7608\n",
            "E2E-ABSA >>> 2022-05-21 01:04:31\n",
            "loss: 0.6012, acc: 0.7511\n",
            "E2E-ABSA >>> 2022-05-21 01:04:31\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:31\n",
            "loss: 0.5558, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-05-21 01:04:31\n",
            "loss: 0.5707, acc: 0.7677\n",
            "E2E-ABSA >>> 2022-05-21 01:04:32\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:32\n",
            "loss: 0.4267, acc: 0.8542\n",
            "E2E-ABSA >>> 2022-05-21 01:04:32\n",
            "loss: 0.5867, acc: 0.7538\n",
            "E2E-ABSA >>> 2022-05-21 01:04:32\n",
            "loss: 0.5771, acc: 0.7579\n",
            "E2E-ABSA >>> 2022-05-21 01:04:32\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:33\n",
            "loss: 0.5669, acc: 0.7719\n",
            "E2E-ABSA >>> 2022-05-21 01:04:33\n",
            "loss: 0.5748, acc: 0.7650\n",
            "E2E-ABSA >>> 2022-05-21 01:04:33\n",
            ">>> val_acc: 0.7129, val_precision: 0.7129 val_recall: 0.7129, val_f1: 0.7129\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.7129\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:33\n",
            "loss: 0.5509, acc: 0.7946\n",
            "E2E-ABSA >>> 2022-05-21 01:04:33\n",
            "loss: 0.5653, acc: 0.7787\n",
            "E2E-ABSA >>> 2022-05-21 01:04:34\n",
            "loss: 0.5656, acc: 0.7696\n",
            "E2E-ABSA >>> 2022-05-21 01:04:34\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:34\n",
            "loss: 0.5558, acc: 0.7682\n",
            "E2E-ABSA >>> 2022-05-21 01:04:34\n",
            "loss: 0.5656, acc: 0.7616\n",
            "E2E-ABSA >>> 2022-05-21 01:04:35\n",
            ">>> val_acc: 0.7129, val_precision: 0.7129 val_recall: 0.7129, val_f1: 0.7129\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:35\n",
            "loss: 0.5131, acc: 0.8011\n",
            "E2E-ABSA >>> 2022-05-21 01:04:35\n",
            "loss: 0.5530, acc: 0.7866\n",
            "E2E-ABSA >>> 2022-05-21 01:04:35\n",
            "loss: 0.5478, acc: 0.7755\n",
            "E2E-ABSA >>> 2022-05-21 01:04:35\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:36\n",
            "loss: 0.5036, acc: 0.8058\n",
            "E2E-ABSA >>> 2022-05-21 01:04:36\n",
            "loss: 0.5249, acc: 0.7899\n",
            "E2E-ABSA >>> 2022-05-21 01:04:36\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:36\n",
            "loss: 0.5539, acc: 0.7667\n",
            "E2E-ABSA >>> 2022-05-21 01:04:37\n",
            "loss: 0.5159, acc: 0.8069\n",
            "E2E-ABSA >>> 2022-05-21 01:04:37\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:37\n",
            "loss: 0.5693, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 01:04:37\n",
            "loss: 0.5267, acc: 0.7910\n",
            "E2E-ABSA >>> 2022-05-21 01:04:38\n",
            "loss: 0.5119, acc: 0.7964\n",
            "E2E-ABSA >>> 2022-05-21 01:04:38\n",
            ">>> val_acc: 0.7194, val_precision: 0.7194 val_recall: 0.7194, val_f1: 0.7194\n",
            ">> saved: state_dict/tc_lstm_twitter_val_f1_0.7194\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:38\n",
            "loss: 0.5494, acc: 0.7928\n",
            "E2E-ABSA >>> 2022-05-21 01:04:38\n",
            "loss: 0.5101, acc: 0.8048\n",
            "E2E-ABSA >>> 2022-05-21 01:04:38\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:39\n",
            "loss: 0.5691, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 01:04:39\n",
            "loss: 0.5220, acc: 0.7882\n",
            "E2E-ABSA >>> 2022-05-21 01:04:39\n",
            "loss: 0.5091, acc: 0.7992\n",
            "E2E-ABSA >>> 2022-05-21 01:04:39\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:39\n",
            "loss: 0.5157, acc: 0.8016\n",
            "E2E-ABSA >>> 2022-05-21 01:04:40\n",
            "loss: 0.4880, acc: 0.8101\n",
            "E2E-ABSA >>> 2022-05-21 01:04:40\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:40\n",
            "loss: 0.4843, acc: 0.8187\n",
            "E2E-ABSA >>> 2022-05-21 01:04:40\n",
            "loss: 0.4824, acc: 0.8063\n",
            "E2E-ABSA >>> 2022-05-21 01:04:41\n",
            "loss: 0.4955, acc: 0.8054\n",
            "E2E-ABSA >>> 2022-05-21 01:04:41\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:41\n",
            "loss: 0.4879, acc: 0.8194\n",
            "E2E-ABSA >>> 2022-05-21 01:04:41\n",
            "loss: 0.4838, acc: 0.8169\n",
            "E2E-ABSA >>> 2022-05-21 01:04:42\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:42\n",
            "loss: 0.4880, acc: 0.8170\n",
            "E2E-ABSA >>> 2022-05-21 01:04:42\n",
            "loss: 0.4810, acc: 0.8068\n",
            "E2E-ABSA >>> 2022-05-21 01:04:42\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:42\n",
            "loss: 0.3823, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 01:04:43\n",
            "loss: 0.4536, acc: 0.8246\n",
            "E2E-ABSA >>> 2022-05-21 01:04:43\n",
            "loss: 0.4677, acc: 0.8227\n",
            "E2E-ABSA >>> 2022-05-21 01:04:43\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:43\n",
            "loss: 0.4678, acc: 0.8160\n",
            "E2E-ABSA >>> 2022-05-21 01:04:43\n",
            "loss: 0.4680, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-05-21 01:04:44\n",
            ">>> val_acc: 0.7129, val_precision: 0.7129 val_recall: 0.7129, val_f1: 0.7129\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:44\n",
            "loss: 0.3840, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-21 01:04:44\n",
            "loss: 0.4677, acc: 0.8232\n",
            "E2E-ABSA >>> 2022-05-21 01:04:44\n",
            "loss: 0.4554, acc: 0.8298\n",
            "E2E-ABSA >>> 2022-05-21 01:04:45\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:45\n",
            "loss: 0.4067, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-05-21 01:04:45\n",
            "loss: 0.4237, acc: 0.8450\n",
            "E2E-ABSA >>> 2022-05-21 01:04:45\n",
            ">>> val_acc: 0.7161, val_precision: 0.7161 val_recall: 0.7161, val_f1: 0.7161\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:45\n",
            "loss: 0.4302, acc: 0.8264\n",
            "E2E-ABSA >>> 2022-05-21 01:04:46\n",
            "loss: 0.4616, acc: 0.8189\n",
            "E2E-ABSA >>> 2022-05-21 01:04:46\n",
            "loss: 0.4369, acc: 0.8351\n",
            "E2E-ABSA >>> 2022-05-21 01:04:46\n",
            ">>> val_acc: 0.7161, val_precision: 0.7161 val_recall: 0.7161, val_f1: 0.7161\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:46\n",
            "loss: 0.4197, acc: 0.8365\n",
            "E2E-ABSA >>> 2022-05-21 01:04:47\n",
            "loss: 0.4223, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-05-21 01:04:47\n",
            ">>> val_acc: 0.7194, val_precision: 0.7194 val_recall: 0.7194, val_f1: 0.7194\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:47\n",
            "loss: 0.4551, acc: 0.7933\n",
            "E2E-ABSA >>> 2022-05-21 01:04:47\n",
            "loss: 0.4440, acc: 0.8256\n",
            "E2E-ABSA >>> 2022-05-21 01:04:48\n",
            "loss: 0.4263, acc: 0.8319\n",
            "E2E-ABSA >>> 2022-05-21 01:04:48\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:48\n",
            "loss: 0.4449, acc: 0.8396\n",
            "E2E-ABSA >>> 2022-05-21 01:04:48\n",
            "loss: 0.4188, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-05-21 01:04:48\n",
            ">>> val_acc: 0.7129, val_precision: 0.7129 val_recall: 0.7129, val_f1: 0.7129\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:49\n",
            "loss: 0.4001, acc: 0.8676\n",
            "E2E-ABSA >>> 2022-05-21 01:04:49\n",
            "loss: 0.4063, acc: 0.8551\n",
            "E2E-ABSA >>> 2022-05-21 01:04:49\n",
            ">>> val_acc: 0.7161, val_precision: 0.7161 val_recall: 0.7161, val_f1: 0.7161\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:49\n",
            "loss: 0.4442, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 01:04:49\n",
            "loss: 0.4454, acc: 0.8290\n",
            "E2E-ABSA >>> 2022-05-21 01:04:50\n",
            "loss: 0.4123, acc: 0.8516\n",
            "E2E-ABSA >>> 2022-05-21 01:04:50\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:50\n",
            "loss: 0.3636, acc: 0.8661\n",
            "E2E-ABSA >>> 2022-05-21 01:04:50\n",
            "loss: 0.3883, acc: 0.8640\n",
            "E2E-ABSA >>> 2022-05-21 01:04:51\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:51\n",
            "loss: 0.3153, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 01:04:51\n",
            "loss: 0.3932, acc: 0.8536\n",
            "E2E-ABSA >>> 2022-05-21 01:04:51\n",
            "loss: 0.3879, acc: 0.8631\n",
            "E2E-ABSA >>> 2022-05-21 01:04:51\n",
            ">>> val_acc: 0.7065, val_precision: 0.7065 val_recall: 0.7065, val_f1: 0.7065\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:52\n",
            "loss: 0.3436, acc: 0.8800\n",
            "E2E-ABSA >>> 2022-05-21 01:04:52\n",
            "loss: 0.3835, acc: 0.8659\n",
            "E2E-ABSA >>> 2022-05-21 01:04:52\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:52\n",
            "loss: 0.3771, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-05-21 01:04:53\n",
            "loss: 0.3959, acc: 0.8616\n",
            "E2E-ABSA >>> 2022-05-21 01:04:53\n",
            "loss: 0.3774, acc: 0.8681\n",
            "E2E-ABSA >>> 2022-05-21 01:04:53\n",
            ">>> val_acc: 0.7097, val_precision: 0.7097 val_recall: 0.7097, val_f1: 0.7097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:53\n",
            "loss: 0.3481, acc: 0.8772\n",
            "E2E-ABSA >>> 2022-05-21 01:04:54\n",
            "loss: 0.3485, acc: 0.8803\n",
            "E2E-ABSA >>> 2022-05-21 01:04:54\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:54\n",
            "loss: 0.3498, acc: 0.8867\n",
            "E2E-ABSA >>> 2022-05-21 01:04:54\n",
            "loss: 0.3458, acc: 0.8913\n",
            "E2E-ABSA >>> 2022-05-21 01:04:55\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:55\n",
            "loss: 0.3074, acc: 0.9167\n",
            "E2E-ABSA >>> 2022-05-21 01:04:55\n",
            "loss: 0.3488, acc: 0.8807\n",
            "E2E-ABSA >>> 2022-05-21 01:04:55\n",
            "loss: 0.3492, acc: 0.8800\n",
            "E2E-ABSA >>> 2022-05-21 01:04:55\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:56\n",
            "loss: 0.3496, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-05-21 01:04:56\n",
            "loss: 0.3451, acc: 0.8838\n",
            "E2E-ABSA >>> 2022-05-21 01:04:56\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:56\n",
            "loss: 0.4626, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 01:04:56\n",
            "loss: 0.3597, acc: 0.8784\n",
            "E2E-ABSA >>> 2022-05-21 01:04:57\n",
            "loss: 0.3382, acc: 0.8843\n",
            "E2E-ABSA >>> 2022-05-21 01:04:57\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:57\n",
            "loss: 0.3338, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-21 01:04:57\n",
            "loss: 0.3318, acc: 0.8866\n",
            "E2E-ABSA >>> 2022-05-21 01:04:58\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:58\n",
            "loss: 0.2823, acc: 0.8864\n",
            "E2E-ABSA >>> 2022-05-21 01:04:58\n",
            "loss: 0.3096, acc: 0.8948\n",
            "E2E-ABSA >>> 2022-05-21 01:04:58\n",
            "loss: 0.3268, acc: 0.8891\n",
            "E2E-ABSA >>> 2022-05-21 01:04:58\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:59\n",
            "loss: 0.2962, acc: 0.9107\n",
            "E2E-ABSA >>> 2022-05-21 01:04:59\n",
            "loss: 0.3068, acc: 0.8955\n",
            "E2E-ABSA >>> 2022-05-21 01:04:59\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-05-21 01:04:59\n",
            "loss: 0.2998, acc: 0.9000\n",
            "E2E-ABSA >>> 2022-05-21 01:05:00\n",
            "loss: 0.2926, acc: 0.9181\n",
            "E2E-ABSA >>> 2022-05-21 01:05:00\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:00\n",
            "loss: 0.2968, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-21 01:05:00\n",
            "loss: 0.3105, acc: 0.9004\n",
            "E2E-ABSA >>> 2022-05-21 01:05:01\n",
            "loss: 0.3122, acc: 0.8962\n",
            "E2E-ABSA >>> 2022-05-21 01:05:01\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:01\n",
            "loss: 0.2895, acc: 0.9145\n",
            "E2E-ABSA >>> 2022-05-21 01:05:01\n",
            "loss: 0.3088, acc: 0.8954\n",
            "E2E-ABSA >>> 2022-05-21 01:05:01\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:02\n",
            "loss: 0.2272, acc: 0.9167\n",
            "E2E-ABSA >>> 2022-05-21 01:05:02\n",
            "loss: 0.2763, acc: 0.9097\n",
            "E2E-ABSA >>> 2022-05-21 01:05:02\n",
            "loss: 0.2923, acc: 0.9034\n",
            "E2E-ABSA >>> 2022-05-21 01:05:02\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:02\n",
            "loss: 0.2836, acc: 0.9212\n",
            "E2E-ABSA >>> 2022-05-21 01:05:03\n",
            "loss: 0.2909, acc: 0.9021\n",
            "E2E-ABSA >>> 2022-05-21 01:05:03\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:03\n",
            "loss: 0.3123, acc: 0.8812\n",
            "E2E-ABSA >>> 2022-05-21 01:05:03\n",
            "loss: 0.2964, acc: 0.8984\n",
            "E2E-ABSA >>> 2022-05-21 01:05:04\n",
            "loss: 0.2854, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 01:05:04\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:04\n",
            "loss: 0.2870, acc: 0.9074\n",
            "E2E-ABSA >>> 2022-05-21 01:05:04\n",
            "loss: 0.2759, acc: 0.9068\n",
            "E2E-ABSA >>> 2022-05-21 01:05:05\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:05\n",
            "loss: 0.2824, acc: 0.9107\n",
            "E2E-ABSA >>> 2022-05-21 01:05:05\n",
            "loss: 0.2860, acc: 0.8991\n",
            "E2E-ABSA >>> 2022-05-21 01:05:05\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:05\n",
            "loss: 0.1330, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-21 01:05:06\n",
            "loss: 0.2682, acc: 0.9113\n",
            "E2E-ABSA >>> 2022-05-21 01:05:06\n",
            "loss: 0.2580, acc: 0.9201\n",
            "E2E-ABSA >>> 2022-05-21 01:05:06\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:06\n",
            "loss: 0.2560, acc: 0.9167\n",
            "E2E-ABSA >>> 2022-05-21 01:05:07\n",
            "loss: 0.2561, acc: 0.9180\n",
            "E2E-ABSA >>> 2022-05-21 01:05:07\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:07\n",
            "loss: 0.2072, acc: 0.9500\n",
            "E2E-ABSA >>> 2022-05-21 01:05:07\n",
            "loss: 0.2554, acc: 0.9179\n",
            "E2E-ABSA >>> 2022-05-21 01:05:07\n",
            "loss: 0.2540, acc: 0.9183\n",
            "E2E-ABSA >>> 2022-05-21 01:05:08\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:08\n",
            "loss: 0.2727, acc: 0.9034\n",
            "E2E-ABSA >>> 2022-05-21 01:05:08\n",
            "loss: 0.2520, acc: 0.9231\n",
            "E2E-ABSA >>> 2022-05-21 01:05:08\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:08\n",
            "loss: 0.2232, acc: 0.9236\n",
            "E2E-ABSA >>> 2022-05-21 01:05:09\n",
            "loss: 0.2467, acc: 0.9167\n",
            "E2E-ABSA >>> 2022-05-21 01:05:09\n",
            "loss: 0.2464, acc: 0.9230\n",
            "E2E-ABSA >>> 2022-05-21 01:05:09\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:09\n",
            "loss: 0.2140, acc: 0.9399\n",
            "E2E-ABSA >>> 2022-05-21 01:05:10\n",
            "loss: 0.2318, acc: 0.9308\n",
            "E2E-ABSA >>> 2022-05-21 01:05:10\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:10\n",
            "loss: 0.2194, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-21 01:05:10\n",
            "loss: 0.2417, acc: 0.9186\n",
            "E2E-ABSA >>> 2022-05-21 01:05:11\n",
            "loss: 0.2437, acc: 0.9203\n",
            "E2E-ABSA >>> 2022-05-21 01:05:11\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:11\n",
            "loss: 0.2386, acc: 0.9208\n",
            "E2E-ABSA >>> 2022-05-21 01:05:11\n",
            "loss: 0.2454, acc: 0.9208\n",
            "E2E-ABSA >>> 2022-05-21 01:05:11\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:12\n",
            "loss: 0.2535, acc: 0.8934\n",
            "E2E-ABSA >>> 2022-05-21 01:05:12\n",
            "loss: 0.2336, acc: 0.9242\n",
            "E2E-ABSA >>> 2022-05-21 01:05:12\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:12\n",
            "loss: 0.2123, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-21 01:05:12\n",
            "loss: 0.2445, acc: 0.9228\n",
            "E2E-ABSA >>> 2022-05-21 01:05:13\n",
            "loss: 0.2315, acc: 0.9287\n",
            "E2E-ABSA >>> 2022-05-21 01:05:13\n",
            ">>> val_acc: 0.6548, val_precision: 0.6548 val_recall: 0.6548, val_f1: 0.6548\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:13\n",
            "loss: 0.2190, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-21 01:05:13\n",
            "loss: 0.2284, acc: 0.9252\n",
            "E2E-ABSA >>> 2022-05-21 01:05:14\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:14\n",
            "loss: 0.2151, acc: 0.9453\n",
            "E2E-ABSA >>> 2022-05-21 01:05:14\n",
            "loss: 0.2205, acc: 0.9391\n",
            "E2E-ABSA >>> 2022-05-21 01:05:14\n",
            "loss: 0.2252, acc: 0.9301\n",
            "E2E-ABSA >>> 2022-05-21 01:05:14\n",
            ">>> val_acc: 0.6516, val_precision: 0.6516 val_recall: 0.6516, val_f1: 0.6516\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:15\n",
            "loss: 0.2145, acc: 0.9300\n",
            "E2E-ABSA >>> 2022-05-21 01:05:15\n",
            "loss: 0.2198, acc: 0.9307\n",
            "E2E-ABSA >>> 2022-05-21 01:05:15\n",
            ">>> val_acc: 0.6516, val_precision: 0.6516 val_recall: 0.6516, val_f1: 0.6516\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-05-21 01:05:15\n",
            "loss: 0.1917, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-21 01:05:16\n",
            "loss: 0.2171, acc: 0.9301\n",
            "E2E-ABSA >>> 2022-05-21 01:05:16\n",
            "loss: 0.2176, acc: 0.9323\n",
            "E2E-ABSA >>> 2022-05-21 01:05:16\n",
            ">>> val_acc: 0.6516, val_precision: 0.6516 val_recall: 0.6516, val_f1: 0.6516\n",
            "E2E-ABSA >>> 2022-05-21 01:05:16\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7194, val_precision: 0.7194 val_recall: 0.7194, val_f1: 0.7194\n",
            "you can download the best model from state_dict/tc_lstm_twitter_val_f1_0.7194\n",
            ">>> test_acc: 0.7194, test_precision: 0.7194, test_recall: 0.7194, test_f1: 0.7194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **twitter** dataset on model**(tc_LSTM)**"
      ],
      "metadata": {
        "id": "oYlSWV6QHI5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset twitter_know --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxCExZUeHK7b",
        "outputId": "538030d2-1482-4262-86f1-d522c7c2eaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1245.\n",
            "> testing dataset count: 332.\n",
            "cuda memory allocated: 22261760\n",
            "> n_trainable_params: 1686603, n_nontrainable_params: 3878400\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f9b67e6bb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:37\n",
            "loss: 0.9957, acc: 0.6333\n",
            "E2E-ABSA >>> 2022-05-21 02:47:37\n",
            "loss: 0.9602, acc: 0.6531\n",
            "E2E-ABSA >>> 2022-05-21 02:47:38\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:38\n",
            "loss: 0.8734, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-05-21 02:47:38\n",
            "loss: 0.8670, acc: 0.6458\n",
            "E2E-ABSA >>> 2022-05-21 02:47:39\n",
            "loss: 0.8617, acc: 0.6502\n",
            "E2E-ABSA >>> 2022-05-21 02:47:39\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:39\n",
            "loss: 0.8214, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-05-21 02:47:39\n",
            "loss: 0.8372, acc: 0.6447\n",
            "E2E-ABSA >>> 2022-05-21 02:47:40\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:40\n",
            "loss: 0.7658, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 02:47:40\n",
            "loss: 0.8007, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:47:40\n",
            "loss: 0.8048, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 02:47:41\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:41\n",
            "loss: 0.7801, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-05-21 02:47:41\n",
            "loss: 0.7750, acc: 0.6706\n",
            "E2E-ABSA >>> 2022-05-21 02:47:42\n",
            "loss: 0.7896, acc: 0.6570\n",
            "E2E-ABSA >>> 2022-05-21 02:47:42\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:42\n",
            "loss: 0.7669, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-05-21 02:47:42\n",
            "loss: 0.7853, acc: 0.6552\n",
            "E2E-ABSA >>> 2022-05-21 02:47:43\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:43\n",
            "loss: 0.8089, acc: 0.6354\n",
            "E2E-ABSA >>> 2022-05-21 02:47:43\n",
            "loss: 0.7804, acc: 0.6458\n",
            "E2E-ABSA >>> 2022-05-21 02:47:44\n",
            "loss: 0.7693, acc: 0.6658\n",
            "E2E-ABSA >>> 2022-05-21 02:47:44\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:44\n",
            "loss: 0.7463, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-05-21 02:47:44\n",
            "loss: 0.7626, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-05-21 02:47:45\n",
            ">>> val_acc: 0.6747, val_precision: 0.6747 val_recall: 0.6747, val_f1: 0.6747\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.6747\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:45\n",
            "loss: 0.7522, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 02:47:45\n",
            "loss: 0.7450, acc: 0.6788\n",
            "E2E-ABSA >>> 2022-05-21 02:47:46\n",
            "loss: 0.7590, acc: 0.6657\n",
            "E2E-ABSA >>> 2022-05-21 02:47:46\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:46\n",
            "loss: 0.7341, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 02:47:47\n",
            "loss: 0.7309, acc: 0.6862\n",
            "E2E-ABSA >>> 2022-05-21 02:47:47\n",
            "loss: 0.7453, acc: 0.6715\n",
            "E2E-ABSA >>> 2022-05-21 02:47:47\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:47\n",
            "loss: 0.7483, acc: 0.6708\n",
            "E2E-ABSA >>> 2022-05-21 02:47:48\n",
            "loss: 0.7546, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 02:47:48\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:48\n",
            "loss: 0.7821, acc: 0.6354\n",
            "E2E-ABSA >>> 2022-05-21 02:47:49\n",
            "loss: 0.7330, acc: 0.6890\n",
            "E2E-ABSA >>> 2022-05-21 02:47:49\n",
            "loss: 0.7283, acc: 0.6884\n",
            "E2E-ABSA >>> 2022-05-21 02:47:49\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:49\n",
            "loss: 0.7176, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 02:47:50\n",
            "loss: 0.7053, acc: 0.6956\n",
            "E2E-ABSA >>> 2022-05-21 02:47:50\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:50\n",
            "loss: 0.7956, acc: 0.6354\n",
            "E2E-ABSA >>> 2022-05-21 02:47:50\n",
            "loss: 0.7282, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-05-21 02:47:51\n",
            "loss: 0.7089, acc: 0.6894\n",
            "E2E-ABSA >>> 2022-05-21 02:47:51\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:51\n",
            "loss: 0.7096, acc: 0.6840\n",
            "E2E-ABSA >>> 2022-05-21 02:47:52\n",
            "loss: 0.7283, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 02:47:52\n",
            "loss: 0.7128, acc: 0.6859\n",
            "E2E-ABSA >>> 2022-05-21 02:47:52\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:52\n",
            "loss: 0.7207, acc: 0.6708\n",
            "E2E-ABSA >>> 2022-05-21 02:47:53\n",
            "loss: 0.7125, acc: 0.6833\n",
            "E2E-ABSA >>> 2022-05-21 02:47:53\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:53\n",
            "loss: 0.6752, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-05-21 02:47:54\n",
            "loss: 0.7114, acc: 0.6786\n",
            "E2E-ABSA >>> 2022-05-21 02:47:54\n",
            "loss: 0.6973, acc: 0.6901\n",
            "E2E-ABSA >>> 2022-05-21 02:47:54\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:54\n",
            "loss: 0.7199, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-05-21 02:47:55\n",
            "loss: 0.6828, acc: 0.6933\n",
            "E2E-ABSA >>> 2022-05-21 02:47:55\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:55\n",
            "loss: 0.6885, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 02:47:56\n",
            "loss: 0.6868, acc: 0.6962\n",
            "E2E-ABSA >>> 2022-05-21 02:47:56\n",
            "loss: 0.6934, acc: 0.6903\n",
            "E2E-ABSA >>> 2022-05-21 02:47:56\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:56\n",
            "loss: 0.6754, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-05-21 02:47:57\n",
            "loss: 0.6772, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-05-21 02:47:57\n",
            "loss: 0.6830, acc: 0.6996\n",
            "E2E-ABSA >>> 2022-05-21 02:47:57\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:57\n",
            "loss: 0.6890, acc: 0.7042\n",
            "E2E-ABSA >>> 2022-05-21 02:47:58\n",
            "loss: 0.6775, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-05-21 02:47:58\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:58\n",
            "loss: 0.6936, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 02:47:59\n",
            "loss: 0.6808, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-05-21 02:47:59\n",
            "loss: 0.6653, acc: 0.7127\n",
            "E2E-ABSA >>> 2022-05-21 02:47:59\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 02:47:59\n",
            "loss: 0.6608, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 02:48:00\n",
            "loss: 0.6723, acc: 0.7025\n",
            "E2E-ABSA >>> 2022-05-21 02:48:00\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:00\n",
            "loss: 0.5830, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:48:01\n",
            "loss: 0.6409, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-05-21 02:48:01\n",
            "loss: 0.6664, acc: 0.7102\n",
            "E2E-ABSA >>> 2022-05-21 02:48:01\n",
            ">>> val_acc: 0.6898, val_precision: 0.6898 val_recall: 0.6898, val_f1: 0.6898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:01\n",
            "loss: 0.6758, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-05-21 02:48:02\n",
            "loss: 0.6734, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-05-21 02:48:02\n",
            "loss: 0.6526, acc: 0.7100\n",
            "E2E-ABSA >>> 2022-05-21 02:48:02\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:03\n",
            "loss: 0.6349, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-05-21 02:48:03\n",
            "loss: 0.6451, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-05-21 02:48:03\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:03\n",
            "loss: 0.6749, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 02:48:04\n",
            "loss: 0.6273, acc: 0.7262\n",
            "E2E-ABSA >>> 2022-05-21 02:48:04\n",
            "loss: 0.6470, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-05-21 02:48:04\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:04\n",
            "loss: 0.6372, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-05-21 02:48:05\n",
            "loss: 0.6248, acc: 0.7315\n",
            "E2E-ABSA >>> 2022-05-21 02:48:05\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:05\n",
            "loss: 0.6905, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 02:48:06\n",
            "loss: 0.6310, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 02:48:06\n",
            "loss: 0.6222, acc: 0.7405\n",
            "E2E-ABSA >>> 2022-05-21 02:48:06\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:06\n",
            "loss: 0.6080, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 02:48:07\n",
            "loss: 0.6167, acc: 0.7370\n",
            "E2E-ABSA >>> 2022-05-21 02:48:07\n",
            "loss: 0.6187, acc: 0.7333\n",
            "E2E-ABSA >>> 2022-05-21 02:48:07\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:08\n",
            "loss: 0.5935, acc: 0.7729\n",
            "E2E-ABSA >>> 2022-05-21 02:48:08\n",
            "loss: 0.6073, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-05-21 02:48:08\n",
            ">>> val_acc: 0.6988, val_precision: 0.6988 val_recall: 0.6988, val_f1: 0.6988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:08\n",
            "loss: 0.6060, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 02:48:09\n",
            "loss: 0.5781, acc: 0.7530\n",
            "E2E-ABSA >>> 2022-05-21 02:48:09\n",
            "loss: 0.5939, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-05-21 02:48:09\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:10\n",
            "loss: 0.5632, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-05-21 02:48:10\n",
            "loss: 0.5977, acc: 0.7488\n",
            "E2E-ABSA >>> 2022-05-21 02:48:10\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:10\n",
            "loss: 0.5492, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:48:11\n",
            "loss: 0.5944, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-05-21 02:48:11\n",
            "loss: 0.5830, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 02:48:11\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:12\n",
            "loss: 0.5429, acc: 0.7882\n",
            "E2E-ABSA >>> 2022-05-21 02:48:12\n",
            "loss: 0.5614, acc: 0.7721\n",
            "E2E-ABSA >>> 2022-05-21 02:48:12\n",
            "loss: 0.5758, acc: 0.7614\n",
            "E2E-ABSA >>> 2022-05-21 02:48:12\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">> saved: state_dict/tc_lstm_twitter_know_val_f1_0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:13\n",
            "loss: 0.6017, acc: 0.7458\n",
            "E2E-ABSA >>> 2022-05-21 02:48:13\n",
            "loss: 0.5688, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 02:48:13\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:14\n",
            "loss: 0.5733, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-05-21 02:48:14\n",
            "loss: 0.5902, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-05-21 02:48:14\n",
            "loss: 0.5614, acc: 0.7630\n",
            "E2E-ABSA >>> 2022-05-21 02:48:14\n",
            ">>> val_acc: 0.6988, val_precision: 0.6988 val_recall: 0.6988, val_f1: 0.6988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:15\n",
            "loss: 0.5432, acc: 0.7839\n",
            "E2E-ABSA >>> 2022-05-21 02:48:15\n",
            "loss: 0.5553, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 02:48:15\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:16\n",
            "loss: 0.5094, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-05-21 02:48:16\n",
            "loss: 0.5153, acc: 0.8038\n",
            "E2E-ABSA >>> 2022-05-21 02:48:16\n",
            "loss: 0.5286, acc: 0.7888\n",
            "E2E-ABSA >>> 2022-05-21 02:48:16\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:17\n",
            "loss: 0.4827, acc: 0.8194\n",
            "E2E-ABSA >>> 2022-05-21 02:48:17\n",
            "loss: 0.5195, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-05-21 02:48:17\n",
            "loss: 0.5298, acc: 0.7839\n",
            "E2E-ABSA >>> 2022-05-21 02:48:17\n",
            ">>> val_acc: 0.6988, val_precision: 0.6988 val_recall: 0.6988, val_f1: 0.6988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:18\n",
            "loss: 0.5411, acc: 0.7792\n",
            "E2E-ABSA >>> 2022-05-21 02:48:18\n",
            "loss: 0.5170, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-05-21 02:48:18\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:19\n",
            "loss: 0.4713, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-05-21 02:48:19\n",
            "loss: 0.5274, acc: 0.7991\n",
            "E2E-ABSA >>> 2022-05-21 02:48:19\n",
            "loss: 0.5092, acc: 0.7934\n",
            "E2E-ABSA >>> 2022-05-21 02:48:19\n",
            ">>> val_acc: 0.6898, val_precision: 0.6898 val_recall: 0.6898, val_f1: 0.6898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:20\n",
            "loss: 0.5058, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-05-21 02:48:20\n",
            "loss: 0.4883, acc: 0.8067\n",
            "E2E-ABSA >>> 2022-05-21 02:48:20\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:21\n",
            "loss: 0.4327, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-05-21 02:48:21\n",
            "loss: 0.4891, acc: 0.7899\n",
            "E2E-ABSA >>> 2022-05-21 02:48:21\n",
            "loss: 0.4906, acc: 0.7945\n",
            "E2E-ABSA >>> 2022-05-21 02:48:21\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:22\n",
            "loss: 0.4910, acc: 0.8090\n",
            "E2E-ABSA >>> 2022-05-21 02:48:22\n",
            "loss: 0.4670, acc: 0.8203\n",
            "E2E-ABSA >>> 2022-05-21 02:48:22\n",
            "loss: 0.4875, acc: 0.7976\n",
            "E2E-ABSA >>> 2022-05-21 02:48:22\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:23\n",
            "loss: 0.4859, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 02:48:23\n",
            "loss: 0.4700, acc: 0.8104\n",
            "E2E-ABSA >>> 2022-05-21 02:48:23\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:24\n",
            "loss: 0.4901, acc: 0.8073\n",
            "E2E-ABSA >>> 2022-05-21 02:48:24\n",
            "loss: 0.4751, acc: 0.8095\n",
            "E2E-ABSA >>> 2022-05-21 02:48:24\n",
            "loss: 0.4725, acc: 0.8108\n",
            "E2E-ABSA >>> 2022-05-21 02:48:24\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:25\n",
            "loss: 0.4262, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-05-21 02:48:25\n",
            "loss: 0.4559, acc: 0.8160\n",
            "E2E-ABSA >>> 2022-05-21 02:48:25\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:26\n",
            "loss: 0.3876, acc: 0.8854\n",
            "E2E-ABSA >>> 2022-05-21 02:48:26\n",
            "loss: 0.4371, acc: 0.8403\n",
            "E2E-ABSA >>> 2022-05-21 02:48:26\n",
            "loss: 0.4493, acc: 0.8286\n",
            "E2E-ABSA >>> 2022-05-21 02:48:26\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:27\n",
            "loss: 0.4439, acc: 0.8299\n",
            "E2E-ABSA >>> 2022-05-21 02:48:27\n",
            "loss: 0.4353, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-05-21 02:48:27\n",
            "loss: 0.4461, acc: 0.8321\n",
            "E2E-ABSA >>> 2022-05-21 02:48:27\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:28\n",
            "loss: 0.4001, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-05-21 02:48:28\n",
            "loss: 0.4444, acc: 0.8323\n",
            "E2E-ABSA >>> 2022-05-21 02:48:28\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:29\n",
            "loss: 0.4311, acc: 0.8490\n",
            "E2E-ABSA >>> 2022-05-21 02:48:29\n",
            "loss: 0.4305, acc: 0.8393\n",
            "E2E-ABSA >>> 2022-05-21 02:48:29\n",
            "loss: 0.4329, acc: 0.8394\n",
            "E2E-ABSA >>> 2022-05-21 02:48:29\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:30\n",
            "loss: 0.4247, acc: 0.8411\n",
            "E2E-ABSA >>> 2022-05-21 02:48:30\n",
            "loss: 0.4335, acc: 0.8345\n",
            "E2E-ABSA >>> 2022-05-21 02:48:30\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:31\n",
            "loss: 0.4829, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 02:48:31\n",
            "loss: 0.4002, acc: 0.8542\n",
            "E2E-ABSA >>> 2022-05-21 02:48:31\n",
            "loss: 0.4137, acc: 0.8447\n",
            "E2E-ABSA >>> 2022-05-21 02:48:31\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:32\n",
            "loss: 0.4043, acc: 0.8507\n",
            "E2E-ABSA >>> 2022-05-21 02:48:32\n",
            "loss: 0.4175, acc: 0.8411\n",
            "E2E-ABSA >>> 2022-05-21 02:48:32\n",
            "loss: 0.4160, acc: 0.8458\n",
            "E2E-ABSA >>> 2022-05-21 02:48:32\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:33\n",
            "loss: 0.3892, acc: 0.8458\n",
            "E2E-ABSA >>> 2022-05-21 02:48:33\n",
            "loss: 0.4100, acc: 0.8406\n",
            "E2E-ABSA >>> 2022-05-21 02:48:33\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:34\n",
            "loss: 0.3501, acc: 0.8646\n",
            "E2E-ABSA >>> 2022-05-21 02:48:34\n",
            "loss: 0.3806, acc: 0.8616\n",
            "E2E-ABSA >>> 2022-05-21 02:48:34\n",
            "loss: 0.3941, acc: 0.8559\n",
            "E2E-ABSA >>> 2022-05-21 02:48:34\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:35\n",
            "loss: 0.3894, acc: 0.8620\n",
            "E2E-ABSA >>> 2022-05-21 02:48:35\n",
            "loss: 0.3930, acc: 0.8530\n",
            "E2E-ABSA >>> 2022-05-21 02:48:35\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:36\n",
            "loss: 0.4731, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-05-21 02:48:36\n",
            "loss: 0.3900, acc: 0.8576\n",
            "E2E-ABSA >>> 2022-05-21 02:48:36\n",
            "loss: 0.3862, acc: 0.8598\n",
            "E2E-ABSA >>> 2022-05-21 02:48:36\n",
            ">>> val_acc: 0.6747, val_precision: 0.6747 val_recall: 0.6747, val_f1: 0.6747\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:37\n",
            "loss: 0.3419, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-21 02:48:37\n",
            "loss: 0.3808, acc: 0.8529\n",
            "E2E-ABSA >>> 2022-05-21 02:48:37\n",
            "loss: 0.3823, acc: 0.8570\n",
            "E2E-ABSA >>> 2022-05-21 02:48:37\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:38\n",
            "loss: 0.3661, acc: 0.8729\n",
            "E2E-ABSA >>> 2022-05-21 02:48:38\n",
            "loss: 0.3839, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-05-21 02:48:38\n",
            ">>> val_acc: 0.6777, val_precision: 0.6777 val_recall: 0.6777, val_f1: 0.6777\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:39\n",
            "loss: 0.3920, acc: 0.8698\n",
            "E2E-ABSA >>> 2022-05-21 02:48:39\n",
            "loss: 0.3672, acc: 0.8676\n",
            "E2E-ABSA >>> 2022-05-21 02:48:39\n",
            "loss: 0.3664, acc: 0.8672\n",
            "E2E-ABSA >>> 2022-05-21 02:48:39\n",
            ">>> val_acc: 0.6777, val_precision: 0.6777 val_recall: 0.6777, val_f1: 0.6777\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:40\n",
            "loss: 0.3534, acc: 0.8646\n",
            "E2E-ABSA >>> 2022-05-21 02:48:40\n",
            "loss: 0.3581, acc: 0.8681\n",
            "E2E-ABSA >>> 2022-05-21 02:48:40\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:41\n",
            "loss: 0.2798, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:48:41\n",
            "loss: 0.3736, acc: 0.8576\n",
            "E2E-ABSA >>> 2022-05-21 02:48:41\n",
            "loss: 0.3637, acc: 0.8674\n",
            "E2E-ABSA >>> 2022-05-21 02:48:41\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:42\n",
            "loss: 0.3628, acc: 0.8681\n",
            "E2E-ABSA >>> 2022-05-21 02:48:42\n",
            "loss: 0.3636, acc: 0.8698\n",
            "E2E-ABSA >>> 2022-05-21 02:48:42\n",
            "loss: 0.3600, acc: 0.8683\n",
            "E2E-ABSA >>> 2022-05-21 02:48:42\n",
            ">>> val_acc: 0.6988, val_precision: 0.6988 val_recall: 0.6988, val_f1: 0.6988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:43\n",
            "loss: 0.3372, acc: 0.8771\n",
            "E2E-ABSA >>> 2022-05-21 02:48:43\n",
            "loss: 0.3573, acc: 0.8740\n",
            "E2E-ABSA >>> 2022-05-21 02:48:43\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:44\n",
            "loss: 0.3162, acc: 0.8802\n",
            "E2E-ABSA >>> 2022-05-21 02:48:44\n",
            "loss: 0.3093, acc: 0.8869\n",
            "E2E-ABSA >>> 2022-05-21 02:48:44\n",
            "loss: 0.3314, acc: 0.8828\n",
            "E2E-ABSA >>> 2022-05-21 02:48:44\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:45\n",
            "loss: 0.3469, acc: 0.8724\n",
            "E2E-ABSA >>> 2022-05-21 02:48:45\n",
            "loss: 0.3475, acc: 0.8727\n",
            "E2E-ABSA >>> 2022-05-21 02:48:45\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:45\n",
            "loss: 0.2512, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-05-21 02:48:46\n",
            "loss: 0.2996, acc: 0.9010\n",
            "E2E-ABSA >>> 2022-05-21 02:48:46\n",
            "loss: 0.3218, acc: 0.8902\n",
            "E2E-ABSA >>> 2022-05-21 02:48:46\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:47\n",
            "loss: 0.2946, acc: 0.9132\n",
            "E2E-ABSA >>> 2022-05-21 02:48:47\n",
            "loss: 0.3134, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-05-21 02:48:47\n",
            "loss: 0.3334, acc: 0.8819\n",
            "E2E-ABSA >>> 2022-05-21 02:48:47\n",
            ">>> val_acc: 0.6988, val_precision: 0.6988 val_recall: 0.6988, val_f1: 0.6988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:48\n",
            "loss: 0.2933, acc: 0.9125\n",
            "E2E-ABSA >>> 2022-05-21 02:48:48\n",
            "loss: 0.3213, acc: 0.8865\n",
            "E2E-ABSA >>> 2022-05-21 02:48:48\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:49\n",
            "loss: 0.2941, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-05-21 02:48:49\n",
            "loss: 0.2942, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:48:49\n",
            "loss: 0.3148, acc: 0.8950\n",
            "E2E-ABSA >>> 2022-05-21 02:48:49\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:50\n",
            "loss: 0.2971, acc: 0.8906\n",
            "E2E-ABSA >>> 2022-05-21 02:48:50\n",
            "loss: 0.3076, acc: 0.8900\n",
            "E2E-ABSA >>> 2022-05-21 02:48:50\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:51\n",
            "loss: 0.2398, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-21 02:48:51\n",
            "loss: 0.2914, acc: 0.9115\n",
            "E2E-ABSA >>> 2022-05-21 02:48:51\n",
            "loss: 0.2922, acc: 0.9006\n",
            "E2E-ABSA >>> 2022-05-21 02:48:51\n",
            ">>> val_acc: 0.6898, val_precision: 0.6898 val_recall: 0.6898, val_f1: 0.6898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:52\n",
            "loss: 0.3082, acc: 0.8993\n",
            "E2E-ABSA >>> 2022-05-21 02:48:52\n",
            "loss: 0.3038, acc: 0.8932\n",
            "E2E-ABSA >>> 2022-05-21 02:48:52\n",
            "loss: 0.3046, acc: 0.8988\n",
            "E2E-ABSA >>> 2022-05-21 02:48:52\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:53\n",
            "loss: 0.3363, acc: 0.8854\n",
            "E2E-ABSA >>> 2022-05-21 02:48:53\n",
            "loss: 0.3081, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-05-21 02:48:53\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:54\n",
            "loss: 0.2716, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-05-21 02:48:54\n",
            "loss: 0.2782, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:48:54\n",
            "loss: 0.2897, acc: 0.9002\n",
            "E2E-ABSA >>> 2022-05-21 02:48:54\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:55\n",
            "loss: 0.3010, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:48:55\n",
            "loss: 0.2794, acc: 0.9097\n",
            "E2E-ABSA >>> 2022-05-21 02:48:55\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:55\n",
            "loss: 0.3054, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:48:56\n",
            "loss: 0.2920, acc: 0.9045\n",
            "E2E-ABSA >>> 2022-05-21 02:48:56\n",
            "loss: 0.2889, acc: 0.9025\n",
            "E2E-ABSA >>> 2022-05-21 02:48:56\n",
            ">>> val_acc: 0.6898, val_precision: 0.6898 val_recall: 0.6898, val_f1: 0.6898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:57\n",
            "loss: 0.3025, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:48:57\n",
            "loss: 0.2861, acc: 0.9010\n",
            "E2E-ABSA >>> 2022-05-21 02:48:57\n",
            "loss: 0.2815, acc: 0.9028\n",
            "E2E-ABSA >>> 2022-05-21 02:48:57\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:58\n",
            "loss: 0.3060, acc: 0.8875\n",
            "E2E-ABSA >>> 2022-05-21 02:48:58\n",
            "loss: 0.2728, acc: 0.9104\n",
            "E2E-ABSA >>> 2022-05-21 02:48:58\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-05-21 02:48:59\n",
            "loss: 0.2509, acc: 0.9010\n",
            "E2E-ABSA >>> 2022-05-21 02:48:59\n",
            "loss: 0.2605, acc: 0.9107\n",
            "E2E-ABSA >>> 2022-05-21 02:48:59\n",
            "loss: 0.2632, acc: 0.9149\n",
            "E2E-ABSA >>> 2022-05-21 02:48:59\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-05-21 02:49:00\n",
            "loss: 0.2378, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-05-21 02:49:00\n",
            "loss: 0.2754, acc: 0.9074\n",
            "E2E-ABSA >>> 2022-05-21 02:49:00\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-05-21 02:49:00\n",
            "loss: 0.3037, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 02:49:01\n",
            "loss: 0.2919, acc: 0.9045\n",
            "E2E-ABSA >>> 2022-05-21 02:49:01\n",
            "loss: 0.2855, acc: 0.9110\n",
            "E2E-ABSA >>> 2022-05-21 02:49:01\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-05-21 02:49:02\n",
            "loss: 0.2491, acc: 0.9271\n",
            "E2E-ABSA >>> 2022-05-21 02:49:02\n",
            "loss: 0.2559, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-05-21 02:49:02\n",
            "loss: 0.2638, acc: 0.9181\n",
            "E2E-ABSA >>> 2022-05-21 02:49:02\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            "E2E-ABSA >>> 2022-05-21 02:49:02\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            "you can download the best model from state_dict/tc_lstm_twitter_know_val_f1_0.7078\n",
            ">>> test_acc: 0.7078, test_precision: 0.7078, test_recall: 0.7078, test_f1: 0.7078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Twitter** dataset on model(**atae_lstm**)"
      ],
      "metadata": {
        "id": "HLiGqlQEhdQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset twitter --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJUHV31Yhde6",
        "outputId": "293ba6a1-1ad0-4403-dbd6-6946f8ef5f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1687.\n",
            "> testing dataset count: 422.\n",
            "cuda memory allocated: 16136704\n",
            "> n_trainable_params: 1845303, n_nontrainable_params: 2188000\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1bccbb6b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:02\n",
            "loss: nan, acc: 0.0833\n",
            "E2E-ABSA >>> 2022-05-21 02:57:02\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:57:02\n",
            "loss: nan, acc: 0.0653\n",
            "E2E-ABSA >>> 2022-05-21 02:57:03\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">> saved: state_dict/atae_lstm_twitter_val_f1_0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:03\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:57:03\n",
            "loss: nan, acc: 0.0753\n",
            "E2E-ABSA >>> 2022-05-21 02:57:04\n",
            "loss: nan, acc: 0.0709\n",
            "E2E-ABSA >>> 2022-05-21 02:57:04\n",
            "loss: nan, acc: 0.0691\n",
            "E2E-ABSA >>> 2022-05-21 02:57:04\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:04\n",
            "loss: nan, acc: 0.0558\n",
            "E2E-ABSA >>> 2022-05-21 02:57:05\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:57:05\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:57:05\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:05\n",
            "loss: nan, acc: 0.0990\n",
            "E2E-ABSA >>> 2022-05-21 02:57:06\n",
            "loss: nan, acc: 0.0789\n",
            "E2E-ABSA >>> 2022-05-21 02:57:06\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:57:06\n",
            "loss: nan, acc: 0.0686\n",
            "E2E-ABSA >>> 2022-05-21 02:57:06\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:07\n",
            "loss: nan, acc: 0.0649\n",
            "E2E-ABSA >>> 2022-05-21 02:57:07\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:57:07\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:57:07\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:08\n",
            "loss: nan, acc: 0.0750\n",
            "E2E-ABSA >>> 2022-05-21 02:57:08\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:08\n",
            "loss: nan, acc: 0.0732\n",
            "E2E-ABSA >>> 2022-05-21 02:57:09\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:57:09\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:09\n",
            "loss: nan, acc: 0.0859\n",
            "E2E-ABSA >>> 2022-05-21 02:57:09\n",
            "loss: nan, acc: 0.0775\n",
            "E2E-ABSA >>> 2022-05-21 02:57:10\n",
            "loss: nan, acc: 0.0722\n",
            "E2E-ABSA >>> 2022-05-21 02:57:10\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:10\n",
            "loss: nan, acc: 0.1094\n",
            "E2E-ABSA >>> 2022-05-21 02:57:10\n",
            "loss: nan, acc: 0.0707\n",
            "E2E-ABSA >>> 2022-05-21 02:57:11\n",
            "loss: nan, acc: 0.0717\n",
            "E2E-ABSA >>> 2022-05-21 02:57:11\n",
            "loss: nan, acc: 0.0663\n",
            "E2E-ABSA >>> 2022-05-21 02:57:11\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:11\n",
            "loss: nan, acc: 0.0824\n",
            "E2E-ABSA >>> 2022-05-21 02:57:12\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:57:12\n",
            "loss: nan, acc: 0.0709\n",
            "E2E-ABSA >>> 2022-05-21 02:57:12\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:12\n",
            "loss: nan, acc: 0.0312\n",
            "E2E-ABSA >>> 2022-05-21 02:57:13\n",
            "loss: nan, acc: 0.0660\n",
            "E2E-ABSA >>> 2022-05-21 02:57:13\n",
            "loss: nan, acc: 0.0691\n",
            "E2E-ABSA >>> 2022-05-21 02:57:13\n",
            "loss: nan, acc: 0.0671\n",
            "E2E-ABSA >>> 2022-05-21 02:57:14\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:14\n",
            "loss: nan, acc: 0.0844\n",
            "E2E-ABSA >>> 2022-05-21 02:57:14\n",
            "loss: nan, acc: 0.0638\n",
            "E2E-ABSA >>> 2022-05-21 02:57:14\n",
            "loss: nan, acc: 0.0695\n",
            "E2E-ABSA >>> 2022-05-21 02:57:15\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:15\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:15\n",
            "loss: nan, acc: 0.0699\n",
            "E2E-ABSA >>> 2022-05-21 02:57:15\n",
            "loss: nan, acc: 0.0674\n",
            "E2E-ABSA >>> 2022-05-21 02:57:16\n",
            "loss: nan, acc: 0.0672\n",
            "E2E-ABSA >>> 2022-05-21 02:57:16\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:16\n",
            "loss: nan, acc: 0.0347\n",
            "E2E-ABSA >>> 2022-05-21 02:57:16\n",
            "loss: nan, acc: 0.0573\n",
            "E2E-ABSA >>> 2022-05-21 02:57:17\n",
            "loss: nan, acc: 0.0641\n",
            "E2E-ABSA >>> 2022-05-21 02:57:17\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:17\n",
            "loss: nan, acc: 0.0000\n",
            "E2E-ABSA >>> 2022-05-21 02:57:17\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:57:18\n",
            "loss: nan, acc: 0.0685\n",
            "E2E-ABSA >>> 2022-05-21 02:57:18\n",
            "loss: nan, acc: 0.0693\n",
            "E2E-ABSA >>> 2022-05-21 02:57:18\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:18\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:19\n",
            "loss: nan, acc: 0.0693\n",
            "E2E-ABSA >>> 2022-05-21 02:57:19\n",
            "loss: nan, acc: 0.0683\n",
            "E2E-ABSA >>> 2022-05-21 02:57:19\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:57:19\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:20\n",
            "loss: nan, acc: 0.0563\n",
            "E2E-ABSA >>> 2022-05-21 02:57:20\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:57:20\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:57:21\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:21\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:57:21\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:57:21\n",
            "loss: nan, acc: 0.0693\n",
            "E2E-ABSA >>> 2022-05-21 02:57:22\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:57:22\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:22\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:57:22\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:57:23\n",
            "loss: nan, acc: 0.0668\n",
            "E2E-ABSA >>> 2022-05-21 02:57:23\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:23\n",
            "loss: nan, acc: 0.0573\n",
            "E2E-ABSA >>> 2022-05-21 02:57:23\n",
            "loss: nan, acc: 0.0551\n",
            "E2E-ABSA >>> 2022-05-21 02:57:24\n",
            "loss: nan, acc: 0.0634\n",
            "E2E-ABSA >>> 2022-05-21 02:57:24\n",
            "loss: nan, acc: 0.0699\n",
            "E2E-ABSA >>> 2022-05-21 02:57:24\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:24\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:57:25\n",
            "loss: nan, acc: 0.0781\n",
            "E2E-ABSA >>> 2022-05-21 02:57:25\n",
            "loss: nan, acc: 0.0734\n",
            "E2E-ABSA >>> 2022-05-21 02:57:25\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:25\n",
            "loss: nan, acc: 0.0875\n",
            "E2E-ABSA >>> 2022-05-21 02:57:26\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:26\n",
            "loss: nan, acc: 0.0616\n",
            "E2E-ABSA >>> 2022-05-21 02:57:26\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:57:27\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:27\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:57:27\n",
            "loss: nan, acc: 0.0660\n",
            "E2E-ABSA >>> 2022-05-21 02:57:27\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:57:28\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:28\n",
            "loss: nan, acc: 0.0781\n",
            "E2E-ABSA >>> 2022-05-21 02:57:28\n",
            "loss: nan, acc: 0.0773\n",
            "E2E-ABSA >>> 2022-05-21 02:57:28\n",
            "loss: nan, acc: 0.0689\n",
            "E2E-ABSA >>> 2022-05-21 02:57:29\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:57:29\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:29\n",
            "loss: nan, acc: 0.0653\n",
            "E2E-ABSA >>> 2022-05-21 02:57:30\n",
            "loss: nan, acc: 0.0649\n",
            "E2E-ABSA >>> 2022-05-21 02:57:30\n",
            "loss: nan, acc: 0.0671\n",
            "E2E-ABSA >>> 2022-05-21 02:57:30\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:30\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:31\n",
            "loss: nan, acc: 0.0642\n",
            "E2E-ABSA >>> 2022-05-21 02:57:31\n",
            "loss: nan, acc: 0.0672\n",
            "E2E-ABSA >>> 2022-05-21 02:57:31\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:57:31\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:32\n",
            "loss: nan, acc: 0.0719\n",
            "E2E-ABSA >>> 2022-05-21 02:57:32\n",
            "loss: nan, acc: 0.0775\n",
            "E2E-ABSA >>> 2022-05-21 02:57:32\n",
            "loss: nan, acc: 0.0766\n",
            "E2E-ABSA >>> 2022-05-21 02:57:33\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:33\n",
            "loss: nan, acc: 0.1406\n",
            "E2E-ABSA >>> 2022-05-21 02:57:33\n",
            "loss: nan, acc: 0.0662\n",
            "E2E-ABSA >>> 2022-05-21 02:57:33\n",
            "loss: nan, acc: 0.0742\n",
            "E2E-ABSA >>> 2022-05-21 02:57:34\n",
            "loss: nan, acc: 0.0678\n",
            "E2E-ABSA >>> 2022-05-21 02:57:34\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:34\n",
            "loss: nan, acc: 0.0660\n",
            "E2E-ABSA >>> 2022-05-21 02:57:34\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:57:35\n",
            "loss: nan, acc: 0.0689\n",
            "E2E-ABSA >>> 2022-05-21 02:57:35\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:35\n",
            "loss: nan, acc: 0.2500\n",
            "E2E-ABSA >>> 2022-05-21 02:57:35\n",
            "loss: nan, acc: 0.0859\n",
            "E2E-ABSA >>> 2022-05-21 02:57:36\n",
            "loss: nan, acc: 0.0786\n",
            "E2E-ABSA >>> 2022-05-21 02:57:36\n",
            "loss: nan, acc: 0.0713\n",
            "E2E-ABSA >>> 2022-05-21 02:57:36\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:36\n",
            "loss: nan, acc: 0.0664\n",
            "E2E-ABSA >>> 2022-05-21 02:57:37\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:57:37\n",
            "loss: nan, acc: 0.0773\n",
            "E2E-ABSA >>> 2022-05-21 02:57:37\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:57:37\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:38\n",
            "loss: nan, acc: 0.0604\n",
            "E2E-ABSA >>> 2022-05-21 02:57:38\n",
            "loss: nan, acc: 0.0698\n",
            "E2E-ABSA >>> 2022-05-21 02:57:38\n",
            "loss: nan, acc: 0.0653\n",
            "E2E-ABSA >>> 2022-05-21 02:57:39\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:39\n",
            "loss: nan, acc: 0.0759\n",
            "E2E-ABSA >>> 2022-05-21 02:57:39\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:57:39\n",
            "loss: nan, acc: 0.0701\n",
            "E2E-ABSA >>> 2022-05-21 02:57:40\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:57:40\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:40\n",
            "loss: nan, acc: 0.0759\n",
            "E2E-ABSA >>> 2022-05-21 02:57:40\n",
            "loss: nan, acc: 0.0657\n",
            "E2E-ABSA >>> 2022-05-21 02:57:41\n",
            "loss: nan, acc: 0.0661\n",
            "E2E-ABSA >>> 2022-05-21 02:57:41\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:41\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:41\n",
            "loss: nan, acc: 0.0610\n",
            "E2E-ABSA >>> 2022-05-21 02:57:42\n",
            "loss: nan, acc: 0.0599\n",
            "E2E-ABSA >>> 2022-05-21 02:57:42\n",
            "loss: nan, acc: 0.0662\n",
            "E2E-ABSA >>> 2022-05-21 02:57:42\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:42\n",
            "loss: nan, acc: 0.0649\n",
            "E2E-ABSA >>> 2022-05-21 02:57:43\n",
            "loss: nan, acc: 0.0759\n",
            "E2E-ABSA >>> 2022-05-21 02:57:43\n",
            "loss: nan, acc: 0.0734\n",
            "E2E-ABSA >>> 2022-05-21 02:57:43\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:43\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:44\n",
            "loss: nan, acc: 0.0641\n",
            "E2E-ABSA >>> 2022-05-21 02:57:44\n",
            "loss: nan, acc: 0.0723\n",
            "E2E-ABSA >>> 2022-05-21 02:57:44\n",
            "loss: nan, acc: 0.0706\n",
            "E2E-ABSA >>> 2022-05-21 02:57:44\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:45\n",
            "loss: nan, acc: 0.0729\n",
            "E2E-ABSA >>> 2022-05-21 02:57:45\n",
            "loss: nan, acc: 0.0845\n",
            "E2E-ABSA >>> 2022-05-21 02:57:45\n",
            "loss: nan, acc: 0.0699\n",
            "E2E-ABSA >>> 2022-05-21 02:57:46\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:46\n",
            "loss: nan, acc: 0.0547\n",
            "E2E-ABSA >>> 2022-05-21 02:57:46\n",
            "loss: nan, acc: 0.0658\n",
            "E2E-ABSA >>> 2022-05-21 02:57:46\n",
            "loss: nan, acc: 0.0726\n",
            "E2E-ABSA >>> 2022-05-21 02:57:47\n",
            "loss: nan, acc: 0.0689\n",
            "E2E-ABSA >>> 2022-05-21 02:57:47\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:47\n",
            "loss: nan, acc: 0.0455\n",
            "E2E-ABSA >>> 2022-05-21 02:57:47\n",
            "loss: nan, acc: 0.0589\n",
            "E2E-ABSA >>> 2022-05-21 02:57:48\n",
            "loss: nan, acc: 0.0678\n",
            "E2E-ABSA >>> 2022-05-21 02:57:48\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:48\n",
            "loss: nan, acc: 0.0833\n",
            "E2E-ABSA >>> 2022-05-21 02:57:48\n",
            "loss: nan, acc: 0.0781\n",
            "E2E-ABSA >>> 2022-05-21 02:57:49\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:57:49\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:57:49\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:49\n",
            "loss: nan, acc: 0.0563\n",
            "E2E-ABSA >>> 2022-05-21 02:57:50\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:50\n",
            "loss: nan, acc: 0.0648\n",
            "E2E-ABSA >>> 2022-05-21 02:57:50\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:51\n",
            "loss: nan, acc: 0.0469\n",
            "E2E-ABSA >>> 2022-05-21 02:57:51\n",
            "loss: nan, acc: 0.0496\n",
            "E2E-ABSA >>> 2022-05-21 02:57:51\n",
            "loss: nan, acc: 0.0664\n",
            "E2E-ABSA >>> 2022-05-21 02:57:52\n",
            "loss: nan, acc: 0.0685\n",
            "E2E-ABSA >>> 2022-05-21 02:57:52\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:52\n",
            "loss: nan, acc: 0.0799\n",
            "E2E-ABSA >>> 2022-05-21 02:57:52\n",
            "loss: nan, acc: 0.0755\n",
            "E2E-ABSA >>> 2022-05-21 02:57:53\n",
            "loss: nan, acc: 0.0681\n",
            "E2E-ABSA >>> 2022-05-21 02:57:53\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:53\n",
            "loss: nan, acc: 0.0312\n",
            "E2E-ABSA >>> 2022-05-21 02:57:53\n",
            "loss: nan, acc: 0.0605\n",
            "E2E-ABSA >>> 2022-05-21 02:57:54\n",
            "loss: nan, acc: 0.0736\n",
            "E2E-ABSA >>> 2022-05-21 02:57:54\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:57:54\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:54\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:57:55\n",
            "loss: nan, acc: 0.0707\n",
            "E2E-ABSA >>> 2022-05-21 02:57:55\n",
            "loss: nan, acc: 0.0765\n",
            "E2E-ABSA >>> 2022-05-21 02:57:55\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:57:55\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:56\n",
            "loss: nan, acc: 0.0667\n",
            "E2E-ABSA >>> 2022-05-21 02:57:56\n",
            "loss: nan, acc: 0.0719\n",
            "E2E-ABSA >>> 2022-05-21 02:57:56\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:57:56\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:57\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:57:57\n",
            "loss: nan, acc: 0.0653\n",
            "E2E-ABSA >>> 2022-05-21 02:57:57\n",
            "loss: nan, acc: 0.0659\n",
            "E2E-ABSA >>> 2022-05-21 02:57:57\n",
            "loss: nan, acc: 0.0691\n",
            "E2E-ABSA >>> 2022-05-21 02:57:58\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:58\n",
            "loss: nan, acc: 0.0670\n",
            "E2E-ABSA >>> 2022-05-21 02:57:58\n",
            "loss: nan, acc: 0.0722\n",
            "E2E-ABSA >>> 2022-05-21 02:57:59\n",
            "loss: nan, acc: 0.0732\n",
            "E2E-ABSA >>> 2022-05-21 02:57:59\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 02:57:59\n",
            "loss: nan, acc: 0.0469\n",
            "E2E-ABSA >>> 2022-05-21 02:57:59\n",
            "loss: nan, acc: 0.0506\n",
            "E2E-ABSA >>> 2022-05-21 02:58:00\n",
            "loss: nan, acc: 0.0608\n",
            "E2E-ABSA >>> 2022-05-21 02:58:00\n",
            "loss: nan, acc: 0.0680\n",
            "E2E-ABSA >>> 2022-05-21 02:58:00\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 02:58:00\n",
            "loss: nan, acc: 0.0673\n",
            "E2E-ABSA >>> 2022-05-21 02:58:01\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:58:01\n",
            "loss: nan, acc: 0.0756\n",
            "E2E-ABSA >>> 2022-05-21 02:58:01\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 02:58:01\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:58:02\n",
            "loss: nan, acc: 0.0750\n",
            "E2E-ABSA >>> 2022-05-21 02:58:02\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:58:02\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:58:02\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            "E2E-ABSA >>> 2022-05-21 02:58:02\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            "you can download the best model from state_dict/atae_lstm_twitter_val_f1_0.0664\n",
            ">>> test_acc: 0.0664, test_precision: 0.0664, test_recall: 0.0664, test_f1: 0.0664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后: Training **twitter** dataset on model(**atae_lstm**)"
      ],
      "metadata": {
        "id": "Sfm2Aik3hdsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset twitter_know --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5LG_XXnhd7I",
        "outputId": "f4f44d2f-b948-4ca6-d01b-cb39f7736188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1687.\n",
            "> testing dataset count: 422.\n",
            "cuda memory allocated: 22898176\n",
            "> n_trainable_params: 1845303, n_nontrainable_params: 3878400\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f75e1d7db00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:10\n",
            "loss: nan, acc: 0.0854\n",
            "E2E-ABSA >>> 2022-05-21 02:55:11\n",
            "loss: nan, acc: 0.0698\n",
            "E2E-ABSA >>> 2022-05-21 02:55:11\n",
            "loss: nan, acc: 0.0660\n",
            "E2E-ABSA >>> 2022-05-21 02:55:11\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">> saved: state_dict/atae_lstm_twitter_know_val_f1_0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:12\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:55:12\n",
            "loss: nan, acc: 0.0753\n",
            "E2E-ABSA >>> 2022-05-21 02:55:12\n",
            "loss: nan, acc: 0.0709\n",
            "E2E-ABSA >>> 2022-05-21 02:55:13\n",
            "loss: nan, acc: 0.0691\n",
            "E2E-ABSA >>> 2022-05-21 02:55:13\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:13\n",
            "loss: nan, acc: 0.0558\n",
            "E2E-ABSA >>> 2022-05-21 02:55:14\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:55:14\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:55:14\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:14\n",
            "loss: nan, acc: 0.0990\n",
            "E2E-ABSA >>> 2022-05-21 02:55:15\n",
            "loss: nan, acc: 0.0789\n",
            "E2E-ABSA >>> 2022-05-21 02:55:15\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:55:15\n",
            "loss: nan, acc: 0.0686\n",
            "E2E-ABSA >>> 2022-05-21 02:55:16\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:16\n",
            "loss: nan, acc: 0.0649\n",
            "E2E-ABSA >>> 2022-05-21 02:55:16\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:55:17\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:55:17\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:17\n",
            "loss: nan, acc: 0.0750\n",
            "E2E-ABSA >>> 2022-05-21 02:55:18\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:55:18\n",
            "loss: nan, acc: 0.0732\n",
            "E2E-ABSA >>> 2022-05-21 02:55:18\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:55:19\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:19\n",
            "loss: nan, acc: 0.0859\n",
            "E2E-ABSA >>> 2022-05-21 02:55:19\n",
            "loss: nan, acc: 0.0775\n",
            "E2E-ABSA >>> 2022-05-21 02:55:20\n",
            "loss: nan, acc: 0.0722\n",
            "E2E-ABSA >>> 2022-05-21 02:55:20\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:20\n",
            "loss: nan, acc: 0.1094\n",
            "E2E-ABSA >>> 2022-05-21 02:55:20\n",
            "loss: nan, acc: 0.0707\n",
            "E2E-ABSA >>> 2022-05-21 02:55:21\n",
            "loss: nan, acc: 0.0717\n",
            "E2E-ABSA >>> 2022-05-21 02:55:21\n",
            "loss: nan, acc: 0.0663\n",
            "E2E-ABSA >>> 2022-05-21 02:55:21\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:22\n",
            "loss: nan, acc: 0.0824\n",
            "E2E-ABSA >>> 2022-05-21 02:55:22\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:55:22\n",
            "loss: nan, acc: 0.0709\n",
            "E2E-ABSA >>> 2022-05-21 02:55:23\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:23\n",
            "loss: nan, acc: 0.0312\n",
            "E2E-ABSA >>> 2022-05-21 02:55:23\n",
            "loss: nan, acc: 0.0660\n",
            "E2E-ABSA >>> 2022-05-21 02:55:23\n",
            "loss: nan, acc: 0.0691\n",
            "E2E-ABSA >>> 2022-05-21 02:55:24\n",
            "loss: nan, acc: 0.0671\n",
            "E2E-ABSA >>> 2022-05-21 02:55:24\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:24\n",
            "loss: nan, acc: 0.0844\n",
            "E2E-ABSA >>> 2022-05-21 02:55:25\n",
            "loss: nan, acc: 0.0638\n",
            "E2E-ABSA >>> 2022-05-21 02:55:25\n",
            "loss: nan, acc: 0.0695\n",
            "E2E-ABSA >>> 2022-05-21 02:55:26\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:26\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:55:26\n",
            "loss: nan, acc: 0.0699\n",
            "E2E-ABSA >>> 2022-05-21 02:55:26\n",
            "loss: nan, acc: 0.0674\n",
            "E2E-ABSA >>> 2022-05-21 02:55:27\n",
            "loss: nan, acc: 0.0672\n",
            "E2E-ABSA >>> 2022-05-21 02:55:27\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:27\n",
            "loss: nan, acc: 0.0347\n",
            "E2E-ABSA >>> 2022-05-21 02:55:28\n",
            "loss: nan, acc: 0.0573\n",
            "E2E-ABSA >>> 2022-05-21 02:55:28\n",
            "loss: nan, acc: 0.0641\n",
            "E2E-ABSA >>> 2022-05-21 02:55:28\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:28\n",
            "loss: nan, acc: 0.0000\n",
            "E2E-ABSA >>> 2022-05-21 02:55:29\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:55:29\n",
            "loss: nan, acc: 0.0685\n",
            "E2E-ABSA >>> 2022-05-21 02:55:29\n",
            "loss: nan, acc: 0.0693\n",
            "E2E-ABSA >>> 2022-05-21 02:55:30\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:30\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:55:30\n",
            "loss: nan, acc: 0.0693\n",
            "E2E-ABSA >>> 2022-05-21 02:55:31\n",
            "loss: nan, acc: 0.0683\n",
            "E2E-ABSA >>> 2022-05-21 02:55:31\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:55:31\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:32\n",
            "loss: nan, acc: 0.0563\n",
            "E2E-ABSA >>> 2022-05-21 02:55:32\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:55:32\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:55:33\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:33\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:55:33\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:55:33\n",
            "loss: nan, acc: 0.0693\n",
            "E2E-ABSA >>> 2022-05-21 02:55:34\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:55:34\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:34\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:55:35\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:55:35\n",
            "loss: nan, acc: 0.0668\n",
            "E2E-ABSA >>> 2022-05-21 02:55:35\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:36\n",
            "loss: nan, acc: 0.0573\n",
            "E2E-ABSA >>> 2022-05-21 02:55:36\n",
            "loss: nan, acc: 0.0551\n",
            "E2E-ABSA >>> 2022-05-21 02:55:36\n",
            "loss: nan, acc: 0.0634\n",
            "E2E-ABSA >>> 2022-05-21 02:55:37\n",
            "loss: nan, acc: 0.0699\n",
            "E2E-ABSA >>> 2022-05-21 02:55:37\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:37\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:55:37\n",
            "loss: nan, acc: 0.0781\n",
            "E2E-ABSA >>> 2022-05-21 02:55:38\n",
            "loss: nan, acc: 0.0734\n",
            "E2E-ABSA >>> 2022-05-21 02:55:38\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:38\n",
            "loss: nan, acc: 0.0875\n",
            "E2E-ABSA >>> 2022-05-21 02:55:39\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:55:39\n",
            "loss: nan, acc: 0.0616\n",
            "E2E-ABSA >>> 2022-05-21 02:55:39\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:55:40\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:40\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:55:40\n",
            "loss: nan, acc: 0.0660\n",
            "E2E-ABSA >>> 2022-05-21 02:55:41\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:55:41\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:41\n",
            "loss: nan, acc: 0.0781\n",
            "E2E-ABSA >>> 2022-05-21 02:55:41\n",
            "loss: nan, acc: 0.0773\n",
            "E2E-ABSA >>> 2022-05-21 02:55:42\n",
            "loss: nan, acc: 0.0689\n",
            "E2E-ABSA >>> 2022-05-21 02:55:42\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:55:42\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:43\n",
            "loss: nan, acc: 0.0653\n",
            "E2E-ABSA >>> 2022-05-21 02:55:43\n",
            "loss: nan, acc: 0.0649\n",
            "E2E-ABSA >>> 2022-05-21 02:55:43\n",
            "loss: nan, acc: 0.0671\n",
            "E2E-ABSA >>> 2022-05-21 02:55:44\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:44\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:55:44\n",
            "loss: nan, acc: 0.0642\n",
            "E2E-ABSA >>> 2022-05-21 02:55:45\n",
            "loss: nan, acc: 0.0672\n",
            "E2E-ABSA >>> 2022-05-21 02:55:45\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:55:45\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:45\n",
            "loss: nan, acc: 0.0719\n",
            "E2E-ABSA >>> 2022-05-21 02:55:46\n",
            "loss: nan, acc: 0.0775\n",
            "E2E-ABSA >>> 2022-05-21 02:55:46\n",
            "loss: nan, acc: 0.0766\n",
            "E2E-ABSA >>> 2022-05-21 02:55:47\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:47\n",
            "loss: nan, acc: 0.1406\n",
            "E2E-ABSA >>> 2022-05-21 02:55:47\n",
            "loss: nan, acc: 0.0662\n",
            "E2E-ABSA >>> 2022-05-21 02:55:47\n",
            "loss: nan, acc: 0.0742\n",
            "E2E-ABSA >>> 2022-05-21 02:55:48\n",
            "loss: nan, acc: 0.0678\n",
            "E2E-ABSA >>> 2022-05-21 02:55:48\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:48\n",
            "loss: nan, acc: 0.0660\n",
            "E2E-ABSA >>> 2022-05-21 02:55:49\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:55:49\n",
            "loss: nan, acc: 0.0689\n",
            "E2E-ABSA >>> 2022-05-21 02:55:49\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:49\n",
            "loss: nan, acc: 0.2500\n",
            "E2E-ABSA >>> 2022-05-21 02:55:50\n",
            "loss: nan, acc: 0.0859\n",
            "E2E-ABSA >>> 2022-05-21 02:55:50\n",
            "loss: nan, acc: 0.0786\n",
            "E2E-ABSA >>> 2022-05-21 02:55:50\n",
            "loss: nan, acc: 0.0713\n",
            "E2E-ABSA >>> 2022-05-21 02:55:51\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:51\n",
            "loss: nan, acc: 0.0664\n",
            "E2E-ABSA >>> 2022-05-21 02:55:51\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:55:52\n",
            "loss: nan, acc: 0.0773\n",
            "E2E-ABSA >>> 2022-05-21 02:55:52\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:55:52\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:53\n",
            "loss: nan, acc: 0.0604\n",
            "E2E-ABSA >>> 2022-05-21 02:55:53\n",
            "loss: nan, acc: 0.0698\n",
            "E2E-ABSA >>> 2022-05-21 02:55:53\n",
            "loss: nan, acc: 0.0653\n",
            "E2E-ABSA >>> 2022-05-21 02:55:54\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:54\n",
            "loss: nan, acc: 0.0759\n",
            "E2E-ABSA >>> 2022-05-21 02:55:54\n",
            "loss: nan, acc: 0.0682\n",
            "E2E-ABSA >>> 2022-05-21 02:55:54\n",
            "loss: nan, acc: 0.0701\n",
            "E2E-ABSA >>> 2022-05-21 02:55:55\n",
            "loss: nan, acc: 0.0697\n",
            "E2E-ABSA >>> 2022-05-21 02:55:55\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:55\n",
            "loss: nan, acc: 0.0759\n",
            "E2E-ABSA >>> 2022-05-21 02:55:56\n",
            "loss: nan, acc: 0.0657\n",
            "E2E-ABSA >>> 2022-05-21 02:55:56\n",
            "loss: nan, acc: 0.0661\n",
            "E2E-ABSA >>> 2022-05-21 02:55:56\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:56\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:55:57\n",
            "loss: nan, acc: 0.0610\n",
            "E2E-ABSA >>> 2022-05-21 02:55:57\n",
            "loss: nan, acc: 0.0599\n",
            "E2E-ABSA >>> 2022-05-21 02:55:58\n",
            "loss: nan, acc: 0.0662\n",
            "E2E-ABSA >>> 2022-05-21 02:55:58\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:58\n",
            "loss: nan, acc: 0.0649\n",
            "E2E-ABSA >>> 2022-05-21 02:55:58\n",
            "loss: nan, acc: 0.0759\n",
            "E2E-ABSA >>> 2022-05-21 02:55:59\n",
            "loss: nan, acc: 0.0734\n",
            "E2E-ABSA >>> 2022-05-21 02:55:59\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 02:55:59\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:56:00\n",
            "loss: nan, acc: 0.0641\n",
            "E2E-ABSA >>> 2022-05-21 02:56:00\n",
            "loss: nan, acc: 0.0723\n",
            "E2E-ABSA >>> 2022-05-21 02:56:00\n",
            "loss: nan, acc: 0.0706\n",
            "E2E-ABSA >>> 2022-05-21 02:56:00\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:01\n",
            "loss: nan, acc: 0.0729\n",
            "E2E-ABSA >>> 2022-05-21 02:56:01\n",
            "loss: nan, acc: 0.0845\n",
            "E2E-ABSA >>> 2022-05-21 02:56:01\n",
            "loss: nan, acc: 0.0699\n",
            "E2E-ABSA >>> 2022-05-21 02:56:02\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:02\n",
            "loss: nan, acc: 0.0547\n",
            "E2E-ABSA >>> 2022-05-21 02:56:02\n",
            "loss: nan, acc: 0.0658\n",
            "E2E-ABSA >>> 2022-05-21 02:56:03\n",
            "loss: nan, acc: 0.0726\n",
            "E2E-ABSA >>> 2022-05-21 02:56:03\n",
            "loss: nan, acc: 0.0689\n",
            "E2E-ABSA >>> 2022-05-21 02:56:03\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:03\n",
            "loss: nan, acc: 0.0455\n",
            "E2E-ABSA >>> 2022-05-21 02:56:04\n",
            "loss: nan, acc: 0.0589\n",
            "E2E-ABSA >>> 2022-05-21 02:56:04\n",
            "loss: nan, acc: 0.0678\n",
            "E2E-ABSA >>> 2022-05-21 02:56:05\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:05\n",
            "loss: nan, acc: 0.0833\n",
            "E2E-ABSA >>> 2022-05-21 02:56:05\n",
            "loss: nan, acc: 0.0781\n",
            "E2E-ABSA >>> 2022-05-21 02:56:05\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:56:06\n",
            "loss: nan, acc: 0.0690\n",
            "E2E-ABSA >>> 2022-05-21 02:56:06\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:06\n",
            "loss: nan, acc: 0.0563\n",
            "E2E-ABSA >>> 2022-05-21 02:56:07\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:56:07\n",
            "loss: nan, acc: 0.0648\n",
            "E2E-ABSA >>> 2022-05-21 02:56:07\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:07\n",
            "loss: nan, acc: 0.0469\n",
            "E2E-ABSA >>> 2022-05-21 02:56:08\n",
            "loss: nan, acc: 0.0496\n",
            "E2E-ABSA >>> 2022-05-21 02:56:08\n",
            "loss: nan, acc: 0.0664\n",
            "E2E-ABSA >>> 2022-05-21 02:56:08\n",
            "loss: nan, acc: 0.0685\n",
            "E2E-ABSA >>> 2022-05-21 02:56:09\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:09\n",
            "loss: nan, acc: 0.0799\n",
            "E2E-ABSA >>> 2022-05-21 02:56:09\n",
            "loss: nan, acc: 0.0755\n",
            "E2E-ABSA >>> 2022-05-21 02:56:10\n",
            "loss: nan, acc: 0.0681\n",
            "E2E-ABSA >>> 2022-05-21 02:56:10\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:10\n",
            "loss: nan, acc: 0.0312\n",
            "E2E-ABSA >>> 2022-05-21 02:56:11\n",
            "loss: nan, acc: 0.0605\n",
            "E2E-ABSA >>> 2022-05-21 02:56:11\n",
            "loss: nan, acc: 0.0736\n",
            "E2E-ABSA >>> 2022-05-21 02:56:11\n",
            "loss: nan, acc: 0.0720\n",
            "E2E-ABSA >>> 2022-05-21 02:56:12\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:12\n",
            "loss: nan, acc: 0.0703\n",
            "E2E-ABSA >>> 2022-05-21 02:56:12\n",
            "loss: nan, acc: 0.0707\n",
            "E2E-ABSA >>> 2022-05-21 02:56:12\n",
            "loss: nan, acc: 0.0765\n",
            "E2E-ABSA >>> 2022-05-21 02:56:13\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:56:13\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:13\n",
            "loss: nan, acc: 0.0667\n",
            "E2E-ABSA >>> 2022-05-21 02:56:14\n",
            "loss: nan, acc: 0.0719\n",
            "E2E-ABSA >>> 2022-05-21 02:56:14\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:56:14\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:14\n",
            "loss: nan, acc: 0.0625\n",
            "E2E-ABSA >>> 2022-05-21 02:56:15\n",
            "loss: nan, acc: 0.0653\n",
            "E2E-ABSA >>> 2022-05-21 02:56:15\n",
            "loss: nan, acc: 0.0659\n",
            "E2E-ABSA >>> 2022-05-21 02:56:16\n",
            "loss: nan, acc: 0.0691\n",
            "E2E-ABSA >>> 2022-05-21 02:56:16\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:16\n",
            "loss: nan, acc: 0.0670\n",
            "E2E-ABSA >>> 2022-05-21 02:56:16\n",
            "loss: nan, acc: 0.0722\n",
            "E2E-ABSA >>> 2022-05-21 02:56:17\n",
            "loss: nan, acc: 0.0732\n",
            "E2E-ABSA >>> 2022-05-21 02:56:17\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:17\n",
            "loss: nan, acc: 0.0469\n",
            "E2E-ABSA >>> 2022-05-21 02:56:18\n",
            "loss: nan, acc: 0.0506\n",
            "E2E-ABSA >>> 2022-05-21 02:56:18\n",
            "loss: nan, acc: 0.0608\n",
            "E2E-ABSA >>> 2022-05-21 02:56:18\n",
            "loss: nan, acc: 0.0680\n",
            "E2E-ABSA >>> 2022-05-21 02:56:18\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:19\n",
            "loss: nan, acc: 0.0673\n",
            "E2E-ABSA >>> 2022-05-21 02:56:19\n",
            "loss: nan, acc: 0.0714\n",
            "E2E-ABSA >>> 2022-05-21 02:56:19\n",
            "loss: nan, acc: 0.0756\n",
            "E2E-ABSA >>> 2022-05-21 02:56:20\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 02:56:20\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:56:20\n",
            "loss: nan, acc: 0.0750\n",
            "E2E-ABSA >>> 2022-05-21 02:56:21\n",
            "loss: nan, acc: 0.0688\n",
            "E2E-ABSA >>> 2022-05-21 02:56:21\n",
            "loss: nan, acc: 0.0694\n",
            "E2E-ABSA >>> 2022-05-21 02:56:21\n",
            ">>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            "E2E-ABSA >>> 2022-05-21 02:56:21\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.0664, val_precision: 0.0664 val_recall: 0.0664, val_f1: 0.0664\n",
            "you can download the best model from state_dict/atae_lstm_twitter_know_val_f1_0.0664\n",
            ">>> test_acc: 0.0664, test_precision: 0.0664, test_recall: 0.0664, test_f1: 0.0664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Twitter** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "-HZDgT3bjOa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset twitter --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIUt2Y5UjOpl",
        "outputId": "78b949e4-855d-443a-e6b0-fef94df9a3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1664.\n",
            "> testing dataset count: 419.\n",
            "cuda memory allocated: 16473088\n",
            "> n_trainable_params: 1928403, n_nontrainable_params: 2188000\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7ff57065ab00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:27\n",
            "loss: 1.1629, acc: 0.2708\n",
            "E2E-ABSA >>> 2022-05-21 03:05:27\n",
            "loss: 1.1013, acc: 0.2958\n",
            "E2E-ABSA >>> 2022-05-21 03:05:28\n",
            "loss: 1.0499, acc: 0.3972\n",
            "E2E-ABSA >>> 2022-05-21 03:05:28\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">> saved: state_dict/ian_twitter_val_f1_0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:28\n",
            "loss: 0.8493, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:05:29\n",
            "loss: 0.8768, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-05-21 03:05:29\n",
            "loss: 0.8737, acc: 0.6686\n",
            "E2E-ABSA >>> 2022-05-21 03:05:30\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:30\n",
            "loss: 0.9129, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-05-21 03:05:30\n",
            "loss: 0.8609, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-05-21 03:05:30\n",
            "loss: 0.8365, acc: 0.6744\n",
            "E2E-ABSA >>> 2022-05-21 03:05:31\n",
            "loss: 0.8390, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 03:05:31\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:31\n",
            "loss: 0.8441, acc: 0.6528\n",
            "E2E-ABSA >>> 2022-05-21 03:05:32\n",
            "loss: 0.8359, acc: 0.6589\n",
            "E2E-ABSA >>> 2022-05-21 03:05:32\n",
            "loss: 0.8344, acc: 0.6627\n",
            "E2E-ABSA >>> 2022-05-21 03:05:33\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:33\n",
            "loss: 0.7565, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:05:33\n",
            "loss: 0.7897, acc: 0.6838\n",
            "E2E-ABSA >>> 2022-05-21 03:05:34\n",
            "loss: 0.7930, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-05-21 03:05:34\n",
            "loss: 0.8100, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-05-21 03:05:34\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:35\n",
            "loss: 0.7909, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-05-21 03:05:35\n",
            "loss: 0.8075, acc: 0.6787\n",
            "E2E-ABSA >>> 2022-05-21 03:05:35\n",
            "loss: 0.8041, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:05:36\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:36\n",
            "loss: 0.8301, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:05:36\n",
            "loss: 0.8292, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:05:37\n",
            "loss: 0.8072, acc: 0.6648\n",
            "E2E-ABSA >>> 2022-05-21 03:05:37\n",
            "loss: 0.7960, acc: 0.6699\n",
            "E2E-ABSA >>> 2022-05-21 03:05:37\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:38\n",
            "loss: 0.8264, acc: 0.6591\n",
            "E2E-ABSA >>> 2022-05-21 03:05:38\n",
            "loss: 0.7808, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-05-21 03:05:38\n",
            "loss: 0.7935, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-05-21 03:05:39\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:39\n",
            "loss: 0.8225, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 03:05:39\n",
            "loss: 0.7695, acc: 0.6743\n",
            "E2E-ABSA >>> 2022-05-21 03:05:40\n",
            "loss: 0.7771, acc: 0.6710\n",
            "E2E-ABSA >>> 2022-05-21 03:05:40\n",
            "loss: 0.7744, acc: 0.6728\n",
            "E2E-ABSA >>> 2022-05-21 03:05:40\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:41\n",
            "loss: 0.8261, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-05-21 03:05:41\n",
            "loss: 0.7923, acc: 0.6632\n",
            "E2E-ABSA >>> 2022-05-21 03:05:41\n",
            "loss: 0.7787, acc: 0.6704\n",
            "E2E-ABSA >>> 2022-05-21 03:05:42\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:42\n",
            "loss: 0.8784, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 03:05:42\n",
            "loss: 0.7977, acc: 0.6609\n",
            "E2E-ABSA >>> 2022-05-21 03:05:43\n",
            "loss: 0.7724, acc: 0.6643\n",
            "E2E-ABSA >>> 2022-05-21 03:05:43\n",
            "loss: 0.7696, acc: 0.6706\n",
            "E2E-ABSA >>> 2022-05-21 03:05:43\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:44\n",
            "loss: 0.7859, acc: 0.6635\n",
            "E2E-ABSA >>> 2022-05-21 03:05:44\n",
            "loss: 0.7686, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-05-21 03:05:45\n",
            "loss: 0.7520, acc: 0.6773\n",
            "E2E-ABSA >>> 2022-05-21 03:05:45\n",
            ">>> val_acc: 0.6921, val_precision: 0.6921 val_recall: 0.6921, val_f1: 0.6921\n",
            ">> saved: state_dict/ian_twitter_val_f1_0.6921\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:45\n",
            "loss: 0.8145, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-05-21 03:05:46\n",
            "loss: 0.7757, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-05-21 03:05:46\n",
            "loss: 0.7542, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-05-21 03:05:46\n",
            "loss: 0.7583, acc: 0.6734\n",
            "E2E-ABSA >>> 2022-05-21 03:05:46\n",
            ">>> val_acc: 0.6897, val_precision: 0.6897 val_recall: 0.6897, val_f1: 0.6897\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:47\n",
            "loss: 0.7435, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:05:47\n",
            "loss: 0.7303, acc: 0.6961\n",
            "E2E-ABSA >>> 2022-05-21 03:05:48\n",
            "loss: 0.7423, acc: 0.6776\n",
            "E2E-ABSA >>> 2022-05-21 03:05:48\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">> saved: state_dict/ian_twitter_val_f1_0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:48\n",
            "loss: 0.7161, acc: 0.7143\n",
            "E2E-ABSA >>> 2022-05-21 03:05:49\n",
            "loss: 0.7669, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-05-21 03:05:49\n",
            "loss: 0.7607, acc: 0.6706\n",
            "E2E-ABSA >>> 2022-05-21 03:05:49\n",
            "loss: 0.7486, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-05-21 03:05:50\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:50\n",
            "loss: 0.6958, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-05-21 03:05:50\n",
            "loss: 0.7441, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:05:51\n",
            "loss: 0.7494, acc: 0.6833\n",
            "E2E-ABSA >>> 2022-05-21 03:05:51\n",
            ">>> val_acc: 0.6993, val_precision: 0.6993 val_recall: 0.6993, val_f1: 0.6993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:51\n",
            "loss: 0.6849, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-05-21 03:05:52\n",
            "loss: 0.7310, acc: 0.6902\n",
            "E2E-ABSA >>> 2022-05-21 03:05:52\n",
            "loss: 0.7335, acc: 0.6933\n",
            "E2E-ABSA >>> 2022-05-21 03:05:53\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:53\n",
            "loss: 0.9045, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-05-21 03:05:53\n",
            "loss: 0.7373, acc: 0.6777\n",
            "E2E-ABSA >>> 2022-05-21 03:05:54\n",
            "loss: 0.7319, acc: 0.6865\n",
            "E2E-ABSA >>> 2022-05-21 03:05:54\n",
            "loss: 0.7413, acc: 0.6834\n",
            "E2E-ABSA >>> 2022-05-21 03:05:54\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">> saved: state_dict/ian_twitter_val_f1_0.716\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:55\n",
            "loss: 0.7263, acc: 0.6806\n",
            "E2E-ABSA >>> 2022-05-21 03:05:55\n",
            "loss: 0.7689, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:05:55\n",
            "loss: 0.7451, acc: 0.6883\n",
            "E2E-ABSA >>> 2022-05-21 03:05:56\n",
            ">>> val_acc: 0.7064, val_precision: 0.7064 val_recall: 0.7064, val_f1: 0.7064\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:56\n",
            "loss: 0.7990, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-05-21 03:05:56\n",
            "loss: 0.7472, acc: 0.6838\n",
            "E2E-ABSA >>> 2022-05-21 03:05:57\n",
            "loss: 0.7371, acc: 0.6963\n",
            "E2E-ABSA >>> 2022-05-21 03:05:57\n",
            "loss: 0.7287, acc: 0.6908\n",
            "E2E-ABSA >>> 2022-05-21 03:05:57\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">> saved: state_dict/ian_twitter_val_f1_0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:58\n",
            "loss: 0.7176, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-05-21 03:05:58\n",
            "loss: 0.7225, acc: 0.6925\n",
            "E2E-ABSA >>> 2022-05-21 03:05:58\n",
            "loss: 0.7155, acc: 0.7008\n",
            "E2E-ABSA >>> 2022-05-21 03:05:59\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">> saved: state_dict/ian_twitter_val_f1_0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:05:59\n",
            "loss: 0.7058, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:05:59\n",
            "loss: 0.6977, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-05-21 03:06:00\n",
            "loss: 0.7097, acc: 0.7027\n",
            "E2E-ABSA >>> 2022-05-21 03:06:00\n",
            "loss: 0.7218, acc: 0.6986\n",
            "E2E-ABSA >>> 2022-05-21 03:06:00\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:01\n",
            "loss: 0.7147, acc: 0.6903\n",
            "E2E-ABSA >>> 2022-05-21 03:06:01\n",
            "loss: 0.7050, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-05-21 03:06:02\n",
            "loss: 0.7120, acc: 0.7035\n",
            "E2E-ABSA >>> 2022-05-21 03:06:02\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:02\n",
            "loss: 0.6701, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:06:02\n",
            "loss: 0.7312, acc: 0.6809\n",
            "E2E-ABSA >>> 2022-05-21 03:06:03\n",
            "loss: 0.7085, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-05-21 03:06:03\n",
            "loss: 0.7171, acc: 0.6958\n",
            "E2E-ABSA >>> 2022-05-21 03:06:03\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:04\n",
            "loss: 0.6727, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:06:04\n",
            "loss: 0.7034, acc: 0.7060\n",
            "E2E-ABSA >>> 2022-05-21 03:06:05\n",
            "loss: 0.6994, acc: 0.7046\n",
            "E2E-ABSA >>> 2022-05-21 03:06:05\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:05\n",
            "loss: 0.6538, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-05-21 03:06:06\n",
            "loss: 0.6776, acc: 0.6984\n",
            "E2E-ABSA >>> 2022-05-21 03:06:06\n",
            "loss: 0.7112, acc: 0.6955\n",
            "E2E-ABSA >>> 2022-05-21 03:06:06\n",
            "loss: 0.7078, acc: 0.7013\n",
            "E2E-ABSA >>> 2022-05-21 03:06:07\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:07\n",
            "loss: 0.7202, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-05-21 03:06:07\n",
            "loss: 0.7148, acc: 0.6975\n",
            "E2E-ABSA >>> 2022-05-21 03:06:08\n",
            "loss: 0.6964, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-05-21 03:06:08\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:08\n",
            "loss: 0.8035, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:06:09\n",
            "loss: 0.7301, acc: 0.7009\n",
            "E2E-ABSA >>> 2022-05-21 03:06:09\n",
            "loss: 0.7126, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-05-21 03:06:09\n",
            "loss: 0.6985, acc: 0.7114\n",
            "E2E-ABSA >>> 2022-05-21 03:06:10\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:10\n",
            "loss: 0.6317, acc: 0.7433\n",
            "E2E-ABSA >>> 2022-05-21 03:06:10\n",
            "loss: 0.6688, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-05-21 03:06:11\n",
            "loss: 0.6815, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:06:11\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:11\n",
            "loss: 0.6104, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-05-21 03:06:12\n",
            "loss: 0.7032, acc: 0.7017\n",
            "E2E-ABSA >>> 2022-05-21 03:06:12\n",
            "loss: 0.6846, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-05-21 03:06:12\n",
            "loss: 0.6896, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-05-21 03:06:13\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:13\n",
            "loss: 0.7088, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:06:13\n",
            "loss: 0.7024, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-05-21 03:06:14\n",
            "loss: 0.6936, acc: 0.7132\n",
            "E2E-ABSA >>> 2022-05-21 03:06:14\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:14\n",
            "loss: 0.6772, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-05-21 03:06:15\n",
            "loss: 0.6887, acc: 0.6970\n",
            "E2E-ABSA >>> 2022-05-21 03:06:15\n",
            "loss: 0.6778, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-05-21 03:06:16\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:16\n",
            "loss: 0.6407, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:06:16\n",
            "loss: 0.6738, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-05-21 03:06:17\n",
            "loss: 0.6828, acc: 0.7167\n",
            "E2E-ABSA >>> 2022-05-21 03:06:17\n",
            "loss: 0.6710, acc: 0.7201\n",
            "E2E-ABSA >>> 2022-05-21 03:06:17\n",
            ">>> val_acc: 0.7112, val_precision: 0.7112 val_recall: 0.7112, val_f1: 0.7112\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:17\n",
            "loss: 0.6645, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-05-21 03:06:18\n",
            "loss: 0.6852, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:06:18\n",
            "loss: 0.6803, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-05-21 03:06:19\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:19\n",
            "loss: 0.7073, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:06:19\n",
            "loss: 0.6716, acc: 0.7408\n",
            "E2E-ABSA >>> 2022-05-21 03:06:20\n",
            "loss: 0.6463, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-05-21 03:06:20\n",
            "loss: 0.6633, acc: 0.7307\n",
            "E2E-ABSA >>> 2022-05-21 03:06:20\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:21\n",
            "loss: 0.6242, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-05-21 03:06:21\n",
            "loss: 0.6269, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 03:06:21\n",
            "loss: 0.6596, acc: 0.7258\n",
            "E2E-ABSA >>> 2022-05-21 03:06:22\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:22\n",
            "loss: 0.5910, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:06:22\n",
            "loss: 0.6546, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-05-21 03:06:23\n",
            "loss: 0.6524, acc: 0.7424\n",
            "E2E-ABSA >>> 2022-05-21 03:06:23\n",
            "loss: 0.6752, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-05-21 03:06:23\n",
            ">>> val_acc: 0.7112, val_precision: 0.7112 val_recall: 0.7112, val_f1: 0.7112\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:24\n",
            "loss: 0.6696, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-05-21 03:06:24\n",
            "loss: 0.6557, acc: 0.7368\n",
            "E2E-ABSA >>> 2022-05-21 03:06:24\n",
            "loss: 0.6732, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-05-21 03:06:25\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:25\n",
            "loss: 0.6496, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:06:25\n",
            "loss: 0.6711, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-05-21 03:06:26\n",
            "loss: 0.6630, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-05-21 03:06:26\n",
            "loss: 0.6647, acc: 0.7315\n",
            "E2E-ABSA >>> 2022-05-21 03:06:26\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:27\n",
            "loss: 0.6780, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:06:27\n",
            "loss: 0.6587, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-05-21 03:06:28\n",
            "loss: 0.6505, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-05-21 03:06:28\n",
            ">>> val_acc: 0.6969, val_precision: 0.6969 val_recall: 0.6969, val_f1: 0.6969\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:28\n",
            "loss: 0.6088, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-05-21 03:06:28\n",
            "loss: 0.6520, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-05-21 03:06:29\n",
            "loss: 0.6602, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-05-21 03:06:29\n",
            "loss: 0.6581, acc: 0.7300\n",
            "E2E-ABSA >>> 2022-05-21 03:06:29\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:30\n",
            "loss: 0.6550, acc: 0.7476\n",
            "E2E-ABSA >>> 2022-05-21 03:06:30\n",
            "loss: 0.6737, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:06:31\n",
            "loss: 0.6500, acc: 0.7420\n",
            "E2E-ABSA >>> 2022-05-21 03:06:31\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:31\n",
            "loss: 0.6957, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:06:32\n",
            "loss: 0.6479, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-05-21 03:06:32\n",
            "loss: 0.6417, acc: 0.7413\n",
            "E2E-ABSA >>> 2022-05-21 03:06:32\n",
            "loss: 0.6499, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-05-21 03:06:33\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:33\n",
            "loss: 0.6493, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-05-21 03:06:33\n",
            "loss: 0.6592, acc: 0.7360\n",
            "E2E-ABSA >>> 2022-05-21 03:06:34\n",
            "loss: 0.6468, acc: 0.7415\n",
            "E2E-ABSA >>> 2022-05-21 03:06:34\n",
            ">>> val_acc: 0.6969, val_precision: 0.6969 val_recall: 0.6969, val_f1: 0.6969\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:34\n",
            "loss: 0.6564, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:06:35\n",
            "loss: 0.6587, acc: 0.7301\n",
            "E2E-ABSA >>> 2022-05-21 03:06:35\n",
            "loss: 0.6642, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-05-21 03:06:36\n",
            "loss: 0.6470, acc: 0.7386\n",
            "E2E-ABSA >>> 2022-05-21 03:06:36\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:36\n",
            "loss: 0.6717, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-05-21 03:06:37\n",
            "loss: 0.6586, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:06:37\n",
            "loss: 0.6509, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 03:06:37\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:38\n",
            "loss: 0.6772, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-05-21 03:06:38\n",
            "loss: 0.6332, acc: 0.7364\n",
            "E2E-ABSA >>> 2022-05-21 03:06:38\n",
            "loss: 0.6382, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-05-21 03:06:39\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:39\n",
            "loss: 0.7168, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:06:39\n",
            "loss: 0.6006, acc: 0.7715\n",
            "E2E-ABSA >>> 2022-05-21 03:06:40\n",
            "loss: 0.6297, acc: 0.7460\n",
            "E2E-ABSA >>> 2022-05-21 03:06:40\n",
            "loss: 0.6325, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-05-21 03:06:40\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:41\n",
            "loss: 0.6269, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-05-21 03:06:41\n",
            "loss: 0.6055, acc: 0.7526\n",
            "E2E-ABSA >>> 2022-05-21 03:06:41\n",
            "loss: 0.6298, acc: 0.7516\n",
            "E2E-ABSA >>> 2022-05-21 03:06:42\n",
            ">>> val_acc: 0.7064, val_precision: 0.7064 val_recall: 0.7064, val_f1: 0.7064\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:42\n",
            "loss: 0.5894, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:06:42\n",
            "loss: 0.6639, acc: 0.7445\n",
            "E2E-ABSA >>> 2022-05-21 03:06:43\n",
            "loss: 0.6642, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-05-21 03:06:43\n",
            "loss: 0.6462, acc: 0.7427\n",
            "E2E-ABSA >>> 2022-05-21 03:06:43\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:44\n",
            "loss: 0.6630, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:06:44\n",
            "loss: 0.6324, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-05-21 03:06:44\n",
            "loss: 0.6401, acc: 0.7414\n",
            "E2E-ABSA >>> 2022-05-21 03:06:45\n",
            ">>> val_acc: 0.6921, val_precision: 0.6921 val_recall: 0.6921, val_f1: 0.6921\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:45\n",
            "loss: 0.6726, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:06:45\n",
            "loss: 0.6525, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-05-21 03:06:46\n",
            "loss: 0.6364, acc: 0.7348\n",
            "E2E-ABSA >>> 2022-05-21 03:06:46\n",
            "loss: 0.6257, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-05-21 03:06:46\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:47\n",
            "loss: 0.6481, acc: 0.7472\n",
            "E2E-ABSA >>> 2022-05-21 03:06:47\n",
            "loss: 0.6548, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-05-21 03:06:47\n",
            "loss: 0.6423, acc: 0.7340\n",
            "E2E-ABSA >>> 2022-05-21 03:06:48\n",
            ">>> val_acc: 0.6921, val_precision: 0.6921 val_recall: 0.6921, val_f1: 0.6921\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:48\n",
            "loss: 0.5870, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:06:48\n",
            "loss: 0.6219, acc: 0.7516\n",
            "E2E-ABSA >>> 2022-05-21 03:06:49\n",
            "loss: 0.6392, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-05-21 03:06:49\n",
            "loss: 0.6281, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-05-21 03:06:49\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:50\n",
            "loss: 0.6390, acc: 0.7318\n",
            "E2E-ABSA >>> 2022-05-21 03:06:50\n",
            "loss: 0.6028, acc: 0.7616\n",
            "E2E-ABSA >>> 2022-05-21 03:06:50\n",
            "loss: 0.6215, acc: 0.7433\n",
            "E2E-ABSA >>> 2022-05-21 03:06:51\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:51\n",
            "loss: 0.6090, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:06:51\n",
            "loss: 0.6370, acc: 0.7141\n",
            "E2E-ABSA >>> 2022-05-21 03:06:52\n",
            "loss: 0.6387, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-05-21 03:06:52\n",
            "loss: 0.6202, acc: 0.7388\n",
            "E2E-ABSA >>> 2022-05-21 03:06:52\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:53\n",
            "loss: 0.6188, acc: 0.7356\n",
            "E2E-ABSA >>> 2022-05-21 03:06:53\n",
            "loss: 0.6148, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-05-21 03:06:53\n",
            "loss: 0.6289, acc: 0.7347\n",
            "E2E-ABSA >>> 2022-05-21 03:06:54\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:54\n",
            "loss: 0.6237, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:06:54\n",
            "loss: 0.6277, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-05-21 03:06:55\n",
            "loss: 0.6180, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:06:55\n",
            "loss: 0.6197, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-05-21 03:06:55\n",
            ">>> val_acc: 0.6921, val_precision: 0.6921 val_recall: 0.6921, val_f1: 0.6921\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:56\n",
            "loss: 0.6198, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-05-21 03:06:56\n",
            "loss: 0.6428, acc: 0.7295\n",
            "E2E-ABSA >>> 2022-05-21 03:06:57\n",
            "loss: 0.6081, acc: 0.7493\n",
            "E2E-ABSA >>> 2022-05-21 03:06:57\n",
            ">>> val_acc: 0.6850, val_precision: 0.6850 val_recall: 0.6850, val_f1: 0.6850\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:57\n",
            "loss: 0.6115, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-05-21 03:06:57\n",
            "loss: 0.6207, acc: 0.7372\n",
            "E2E-ABSA >>> 2022-05-21 03:06:58\n",
            "loss: 0.6185, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-05-21 03:06:58\n",
            "loss: 0.6085, acc: 0.7518\n",
            "E2E-ABSA >>> 2022-05-21 03:06:58\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:06:59\n",
            "loss: 0.5912, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-05-21 03:06:59\n",
            "loss: 0.6029, acc: 0.7542\n",
            "E2E-ABSA >>> 2022-05-21 03:07:00\n",
            "loss: 0.6103, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-05-21 03:07:00\n",
            ">>> val_acc: 0.6969, val_precision: 0.6969 val_recall: 0.6969, val_f1: 0.6969\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:00\n",
            "loss: 0.5429, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-05-21 03:07:01\n",
            "loss: 0.5900, acc: 0.7514\n",
            "E2E-ABSA >>> 2022-05-21 03:07:01\n",
            "loss: 0.5991, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:07:01\n",
            ">>> val_acc: 0.6850, val_precision: 0.6850 val_recall: 0.6850, val_f1: 0.6850\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:01\n",
            "loss: 0.4378, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:07:02\n",
            "loss: 0.6131, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:07:02\n",
            "loss: 0.6033, acc: 0.7510\n",
            "E2E-ABSA >>> 2022-05-21 03:07:03\n",
            "loss: 0.6108, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-05-21 03:07:03\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:03\n",
            "loss: 0.6031, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-05-21 03:07:04\n",
            "loss: 0.6071, acc: 0.7513\n",
            "E2E-ABSA >>> 2022-05-21 03:07:04\n",
            "loss: 0.6004, acc: 0.7532\n",
            "E2E-ABSA >>> 2022-05-21 03:07:04\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:05\n",
            "loss: 0.4451, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-05-21 03:07:05\n",
            "loss: 0.5939, acc: 0.7574\n",
            "E2E-ABSA >>> 2022-05-21 03:07:05\n",
            "loss: 0.5910, acc: 0.7598\n",
            "E2E-ABSA >>> 2022-05-21 03:07:06\n",
            "loss: 0.5990, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-05-21 03:07:06\n",
            ">>> val_acc: 0.6969, val_precision: 0.6969 val_recall: 0.6969, val_f1: 0.6969\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:06\n",
            "loss: 0.5679, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 03:07:07\n",
            "loss: 0.5745, acc: 0.7638\n",
            "E2E-ABSA >>> 2022-05-21 03:07:07\n",
            "loss: 0.5839, acc: 0.7508\n",
            "E2E-ABSA >>> 2022-05-21 03:07:08\n",
            ">>> val_acc: 0.6826, val_precision: 0.6826 val_recall: 0.6826, val_f1: 0.6826\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:08\n",
            "loss: 0.5424, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-05-21 03:07:08\n",
            "loss: 0.6055, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-05-21 03:07:08\n",
            "loss: 0.5855, acc: 0.7595\n",
            "E2E-ABSA >>> 2022-05-21 03:07:09\n",
            "loss: 0.5968, acc: 0.7513\n",
            "E2E-ABSA >>> 2022-05-21 03:07:09\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:09\n",
            "loss: 0.5724, acc: 0.7699\n",
            "E2E-ABSA >>> 2022-05-21 03:07:10\n",
            "loss: 0.5732, acc: 0.7668\n",
            "E2E-ABSA >>> 2022-05-21 03:07:10\n",
            "loss: 0.5839, acc: 0.7622\n",
            "E2E-ABSA >>> 2022-05-21 03:07:11\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:11\n",
            "loss: 0.6032, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:07:11\n",
            "loss: 0.5783, acc: 0.7533\n",
            "E2E-ABSA >>> 2022-05-21 03:07:12\n",
            "loss: 0.5848, acc: 0.7509\n",
            "E2E-ABSA >>> 2022-05-21 03:07:12\n",
            "loss: 0.5909, acc: 0.7506\n",
            "E2E-ABSA >>> 2022-05-21 03:07:12\n",
            ">>> val_acc: 0.6993, val_precision: 0.6993 val_recall: 0.6993, val_f1: 0.6993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:13\n",
            "loss: 0.5579, acc: 0.7786\n",
            "E2E-ABSA >>> 2022-05-21 03:07:13\n",
            "loss: 0.5766, acc: 0.7755\n",
            "E2E-ABSA >>> 2022-05-21 03:07:13\n",
            "loss: 0.5917, acc: 0.7612\n",
            "E2E-ABSA >>> 2022-05-21 03:07:14\n",
            ">>> val_acc: 0.6826, val_precision: 0.6826 val_recall: 0.6826, val_f1: 0.6826\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:07:14\n",
            "loss: 0.6516, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:07:14\n",
            "loss: 0.6265, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-05-21 03:07:15\n",
            "loss: 0.5992, acc: 0.7446\n",
            "E2E-ABSA >>> 2022-05-21 03:07:15\n",
            "loss: 0.5888, acc: 0.7494\n",
            "E2E-ABSA >>> 2022-05-21 03:07:15\n",
            ">>> val_acc: 0.6850, val_precision: 0.6850 val_recall: 0.6850, val_f1: 0.6850\n",
            "E2E-ABSA >>> 2022-05-21 03:07:15\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            "you can download the best model from state_dict/ian_twitter_val_f1_0.7232\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.7232, test_precision: 0.7232, test_recall: 0.7232, test_f1: 0.7232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后: Training **Twitter** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "ifPtlhzNjO2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset twitter_know --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z56axXpijPDU",
        "outputId": "15ab8915-8bca-4b38-8990-2a64d34223bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1664.\n",
            "> testing dataset count: 419.\n",
            "cuda memory allocated: 23234560\n",
            "> n_trainable_params: 1928403, n_nontrainable_params: 3878400\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fc668f33b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:12\n",
            "loss: 1.1696, acc: 0.2708\n",
            "E2E-ABSA >>> 2022-05-21 03:08:13\n",
            "loss: 1.1040, acc: 0.3135\n",
            "E2E-ABSA >>> 2022-05-21 03:08:13\n",
            "loss: 1.0503, acc: 0.3986\n",
            "E2E-ABSA >>> 2022-05-21 03:08:13\n",
            ">>> val_acc: 0.6921, val_precision: 0.6921 val_recall: 0.6921, val_f1: 0.6921\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.6921\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:14\n",
            "loss: 0.8382, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:08:14\n",
            "loss: 0.8728, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-05-21 03:08:15\n",
            "loss: 0.8679, acc: 0.6686\n",
            "E2E-ABSA >>> 2022-05-21 03:08:15\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:15\n",
            "loss: 0.9660, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-05-21 03:08:16\n",
            "loss: 0.8542, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-05-21 03:08:16\n",
            "loss: 0.8291, acc: 0.6744\n",
            "E2E-ABSA >>> 2022-05-21 03:08:17\n",
            "loss: 0.8321, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-05-21 03:08:17\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:17\n",
            "loss: 0.8365, acc: 0.6528\n",
            "E2E-ABSA >>> 2022-05-21 03:08:18\n",
            "loss: 0.8298, acc: 0.6589\n",
            "E2E-ABSA >>> 2022-05-21 03:08:18\n",
            "loss: 0.8311, acc: 0.6627\n",
            "E2E-ABSA >>> 2022-05-21 03:08:19\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:19\n",
            "loss: 0.7622, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:08:19\n",
            "loss: 0.7906, acc: 0.6838\n",
            "E2E-ABSA >>> 2022-05-21 03:08:20\n",
            "loss: 0.7967, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-05-21 03:08:20\n",
            "loss: 0.8139, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-05-21 03:08:20\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:21\n",
            "loss: 0.7987, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-05-21 03:08:21\n",
            "loss: 0.8167, acc: 0.6787\n",
            "E2E-ABSA >>> 2022-05-21 03:08:22\n",
            "loss: 0.8134, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:08:22\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:22\n",
            "loss: 0.8398, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:08:23\n",
            "loss: 0.8406, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:08:23\n",
            "loss: 0.8193, acc: 0.6648\n",
            "E2E-ABSA >>> 2022-05-21 03:08:24\n",
            "loss: 0.8089, acc: 0.6699\n",
            "E2E-ABSA >>> 2022-05-21 03:08:24\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:24\n",
            "loss: 0.8337, acc: 0.6591\n",
            "E2E-ABSA >>> 2022-05-21 03:08:25\n",
            "loss: 0.7958, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-05-21 03:08:25\n",
            "loss: 0.8102, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-05-21 03:08:26\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:26\n",
            "loss: 0.8459, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 03:08:26\n",
            "loss: 0.7951, acc: 0.6743\n",
            "E2E-ABSA >>> 2022-05-21 03:08:27\n",
            "loss: 0.8032, acc: 0.6710\n",
            "E2E-ABSA >>> 2022-05-21 03:08:27\n",
            "loss: 0.7975, acc: 0.6728\n",
            "E2E-ABSA >>> 2022-05-21 03:08:27\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:28\n",
            "loss: 0.8550, acc: 0.6510\n",
            "E2E-ABSA >>> 2022-05-21 03:08:28\n",
            "loss: 0.8202, acc: 0.6609\n",
            "E2E-ABSA >>> 2022-05-21 03:08:29\n",
            "loss: 0.8047, acc: 0.6689\n",
            "E2E-ABSA >>> 2022-05-21 03:08:29\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:29\n",
            "loss: 0.9126, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 03:08:30\n",
            "loss: 0.8204, acc: 0.6609\n",
            "E2E-ABSA >>> 2022-05-21 03:08:30\n",
            "loss: 0.8034, acc: 0.6634\n",
            "E2E-ABSA >>> 2022-05-21 03:08:31\n",
            "loss: 0.7993, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-05-21 03:08:31\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:31\n",
            "loss: 0.8116, acc: 0.6611\n",
            "E2E-ABSA >>> 2022-05-21 03:08:32\n",
            "loss: 0.7957, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-05-21 03:08:32\n",
            "loss: 0.7834, acc: 0.6766\n",
            "E2E-ABSA >>> 2022-05-21 03:08:33\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:33\n",
            "loss: 0.8626, acc: 0.5833\n",
            "E2E-ABSA >>> 2022-05-21 03:08:33\n",
            "loss: 0.8072, acc: 0.6473\n",
            "E2E-ABSA >>> 2022-05-21 03:08:34\n",
            "loss: 0.7895, acc: 0.6727\n",
            "E2E-ABSA >>> 2022-05-21 03:08:34\n",
            "loss: 0.7937, acc: 0.6697\n",
            "E2E-ABSA >>> 2022-05-21 03:08:34\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:35\n",
            "loss: 0.7762, acc: 0.6808\n",
            "E2E-ABSA >>> 2022-05-21 03:08:35\n",
            "loss: 0.7683, acc: 0.6929\n",
            "E2E-ABSA >>> 2022-05-21 03:08:36\n",
            "loss: 0.7802, acc: 0.6740\n",
            "E2E-ABSA >>> 2022-05-21 03:08:36\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:36\n",
            "loss: 0.7688, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 03:08:37\n",
            "loss: 0.8123, acc: 0.6534\n",
            "E2E-ABSA >>> 2022-05-21 03:08:37\n",
            "loss: 0.8062, acc: 0.6571\n",
            "E2E-ABSA >>> 2022-05-21 03:08:38\n",
            "loss: 0.7902, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 03:08:38\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:38\n",
            "loss: 0.7388, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:08:39\n",
            "loss: 0.7870, acc: 0.6760\n",
            "E2E-ABSA >>> 2022-05-21 03:08:39\n",
            "loss: 0.7911, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-05-21 03:08:40\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:40\n",
            "loss: 0.7281, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:08:40\n",
            "loss: 0.7730, acc: 0.6821\n",
            "E2E-ABSA >>> 2022-05-21 03:08:41\n",
            "loss: 0.7804, acc: 0.6859\n",
            "E2E-ABSA >>> 2022-05-21 03:08:41\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:41\n",
            "loss: 0.8591, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 03:08:42\n",
            "loss: 0.7743, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-05-21 03:08:42\n",
            "loss: 0.7748, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-05-21 03:08:43\n",
            "loss: 0.7894, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-05-21 03:08:43\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:43\n",
            "loss: 0.7965, acc: 0.6632\n",
            "E2E-ABSA >>> 2022-05-21 03:08:44\n",
            "loss: 0.8197, acc: 0.6471\n",
            "E2E-ABSA >>> 2022-05-21 03:08:44\n",
            "loss: 0.7957, acc: 0.6659\n",
            "E2E-ABSA >>> 2022-05-21 03:08:45\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:45\n",
            "loss: 0.8897, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-05-21 03:08:45\n",
            "loss: 0.7994, acc: 0.6599\n",
            "E2E-ABSA >>> 2022-05-21 03:08:46\n",
            "loss: 0.7888, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-05-21 03:08:46\n",
            "loss: 0.7806, acc: 0.6722\n",
            "E2E-ABSA >>> 2022-05-21 03:08:47\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:47\n",
            "loss: 0.7437, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-05-21 03:08:47\n",
            "loss: 0.7739, acc: 0.6675\n",
            "E2E-ABSA >>> 2022-05-21 03:08:48\n",
            "loss: 0.7655, acc: 0.6820\n",
            "E2E-ABSA >>> 2022-05-21 03:08:48\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:48\n",
            "loss: 0.7191, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:08:49\n",
            "loss: 0.7563, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-05-21 03:08:49\n",
            "loss: 0.7679, acc: 0.6780\n",
            "E2E-ABSA >>> 2022-05-21 03:08:50\n",
            "loss: 0.7751, acc: 0.6751\n",
            "E2E-ABSA >>> 2022-05-21 03:08:50\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:50\n",
            "loss: 0.7824, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:08:51\n",
            "loss: 0.7655, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-05-21 03:08:51\n",
            "loss: 0.7714, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-05-21 03:08:52\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:52\n",
            "loss: 0.7550, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:08:52\n",
            "loss: 0.7903, acc: 0.6694\n",
            "E2E-ABSA >>> 2022-05-21 03:08:53\n",
            "loss: 0.7643, acc: 0.6811\n",
            "E2E-ABSA >>> 2022-05-21 03:08:53\n",
            "loss: 0.7782, acc: 0.6665\n",
            "E2E-ABSA >>> 2022-05-21 03:08:54\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:54\n",
            "loss: 0.7283, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:08:54\n",
            "loss: 0.7605, acc: 0.6794\n",
            "E2E-ABSA >>> 2022-05-21 03:08:55\n",
            "loss: 0.7625, acc: 0.6756\n",
            "E2E-ABSA >>> 2022-05-21 03:08:55\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:56\n",
            "loss: 0.7328, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 03:08:56\n",
            "loss: 0.7447, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 03:08:57\n",
            "loss: 0.7697, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-05-21 03:08:57\n",
            "loss: 0.7715, acc: 0.6700\n",
            "E2E-ABSA >>> 2022-05-21 03:08:57\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:58\n",
            "loss: 0.7790, acc: 0.6538\n",
            "E2E-ABSA >>> 2022-05-21 03:08:58\n",
            "loss: 0.7754, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-05-21 03:08:58\n",
            "loss: 0.7621, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 03:08:59\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:08:59\n",
            "loss: 0.8736, acc: 0.6146\n",
            "E2E-ABSA >>> 2022-05-21 03:09:00\n",
            "loss: 0.7990, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:09:00\n",
            "loss: 0.7760, acc: 0.6649\n",
            "E2E-ABSA >>> 2022-05-21 03:09:00\n",
            "loss: 0.7676, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-05-21 03:09:01\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:01\n",
            "loss: 0.7159, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-05-21 03:09:02\n",
            "loss: 0.7466, acc: 0.6810\n",
            "E2E-ABSA >>> 2022-05-21 03:09:02\n",
            "loss: 0.7580, acc: 0.6747\n",
            "E2E-ABSA >>> 2022-05-21 03:09:02\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:03\n",
            "loss: 0.6827, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-05-21 03:09:03\n",
            "loss: 0.7546, acc: 0.6776\n",
            "E2E-ABSA >>> 2022-05-21 03:09:03\n",
            "loss: 0.7507, acc: 0.6723\n",
            "E2E-ABSA >>> 2022-05-21 03:09:04\n",
            "loss: 0.7592, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 03:09:04\n",
            ">>> val_acc: 0.6969, val_precision: 0.6969 val_recall: 0.6969, val_f1: 0.6969\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.6969\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:05\n",
            "loss: 0.7867, acc: 0.6542\n",
            "E2E-ABSA >>> 2022-05-21 03:09:05\n",
            "loss: 0.7815, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-05-21 03:09:06\n",
            "loss: 0.7676, acc: 0.6660\n",
            "E2E-ABSA >>> 2022-05-21 03:09:06\n",
            ">>> val_acc: 0.6897, val_precision: 0.6897 val_recall: 0.6897, val_f1: 0.6897\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:06\n",
            "loss: 0.7389, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-05-21 03:09:07\n",
            "loss: 0.7615, acc: 0.6658\n",
            "E2E-ABSA >>> 2022-05-21 03:09:07\n",
            "loss: 0.7530, acc: 0.6801\n",
            "E2E-ABSA >>> 2022-05-21 03:09:08\n",
            ">>> val_acc: 0.7112, val_precision: 0.7112 val_recall: 0.7112, val_f1: 0.7112\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.7112\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:08\n",
            "loss: 0.7065, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:09:08\n",
            "loss: 0.7599, acc: 0.6621\n",
            "E2E-ABSA >>> 2022-05-21 03:09:09\n",
            "loss: 0.7642, acc: 0.6593\n",
            "E2E-ABSA >>> 2022-05-21 03:09:09\n",
            "loss: 0.7488, acc: 0.6766\n",
            "E2E-ABSA >>> 2022-05-21 03:09:09\n",
            ">>> val_acc: 0.6921, val_precision: 0.6921 val_recall: 0.6921, val_f1: 0.6921\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:10\n",
            "loss: 0.7234, acc: 0.6944\n",
            "E2E-ABSA >>> 2022-05-21 03:09:10\n",
            "loss: 0.7559, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-05-21 03:09:11\n",
            "loss: 0.7494, acc: 0.6819\n",
            "E2E-ABSA >>> 2022-05-21 03:09:11\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:11\n",
            "loss: 0.8302, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-05-21 03:09:12\n",
            "loss: 0.7562, acc: 0.6765\n",
            "E2E-ABSA >>> 2022-05-21 03:09:12\n",
            "loss: 0.7333, acc: 0.6865\n",
            "E2E-ABSA >>> 2022-05-21 03:09:13\n",
            "loss: 0.7410, acc: 0.6842\n",
            "E2E-ABSA >>> 2022-05-21 03:09:13\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:13\n",
            "loss: 0.6792, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-05-21 03:09:14\n",
            "loss: 0.6889, acc: 0.7075\n",
            "E2E-ABSA >>> 2022-05-21 03:09:14\n",
            "loss: 0.7312, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-05-21 03:09:15\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:15\n",
            "loss: 0.6760, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:09:15\n",
            "loss: 0.7272, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:09:16\n",
            "loss: 0.7304, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-05-21 03:09:16\n",
            "loss: 0.7463, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:09:16\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:17\n",
            "loss: 0.7324, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-05-21 03:09:17\n",
            "loss: 0.7359, acc: 0.6839\n",
            "E2E-ABSA >>> 2022-05-21 03:09:18\n",
            "loss: 0.7475, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-05-21 03:09:18\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:18\n",
            "loss: 0.7262, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 03:09:19\n",
            "loss: 0.7361, acc: 0.6924\n",
            "E2E-ABSA >>> 2022-05-21 03:09:19\n",
            "loss: 0.7385, acc: 0.6866\n",
            "E2E-ABSA >>> 2022-05-21 03:09:20\n",
            "loss: 0.7373, acc: 0.6856\n",
            "E2E-ABSA >>> 2022-05-21 03:09:20\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:20\n",
            "loss: 0.7405, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:09:21\n",
            "loss: 0.7293, acc: 0.6944\n",
            "E2E-ABSA >>> 2022-05-21 03:09:21\n",
            "loss: 0.7219, acc: 0.6957\n",
            "E2E-ABSA >>> 2022-05-21 03:09:22\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:22\n",
            "loss: 0.6919, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 03:09:22\n",
            "loss: 0.7129, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-05-21 03:09:23\n",
            "loss: 0.7301, acc: 0.6902\n",
            "E2E-ABSA >>> 2022-05-21 03:09:23\n",
            "loss: 0.7309, acc: 0.6863\n",
            "E2E-ABSA >>> 2022-05-21 03:09:24\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:24\n",
            "loss: 0.7474, acc: 0.6635\n",
            "E2E-ABSA >>> 2022-05-21 03:09:24\n",
            "loss: 0.7600, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-05-21 03:09:25\n",
            "loss: 0.7293, acc: 0.6853\n",
            "E2E-ABSA >>> 2022-05-21 03:09:25\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:25\n",
            "loss: 0.7511, acc: 0.6510\n",
            "E2E-ABSA >>> 2022-05-21 03:09:26\n",
            "loss: 0.7288, acc: 0.6920\n",
            "E2E-ABSA >>> 2022-05-21 03:09:26\n",
            "loss: 0.7239, acc: 0.6884\n",
            "E2E-ABSA >>> 2022-05-21 03:09:27\n",
            "loss: 0.7212, acc: 0.6930\n",
            "E2E-ABSA >>> 2022-05-21 03:09:27\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:27\n",
            "loss: 0.7029, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-05-21 03:09:28\n",
            "loss: 0.7305, acc: 0.6918\n",
            "E2E-ABSA >>> 2022-05-21 03:09:28\n",
            "loss: 0.7156, acc: 0.7003\n",
            "E2E-ABSA >>> 2022-05-21 03:09:29\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">> saved: state_dict/ian_twitter_know_val_f1_0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:29\n",
            "loss: 0.6563, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-05-21 03:09:29\n",
            "loss: 0.7199, acc: 0.7003\n",
            "E2E-ABSA >>> 2022-05-21 03:09:30\n",
            "loss: 0.7255, acc: 0.6951\n",
            "E2E-ABSA >>> 2022-05-21 03:09:30\n",
            "loss: 0.7142, acc: 0.6995\n",
            "E2E-ABSA >>> 2022-05-21 03:09:31\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:31\n",
            "loss: 0.7450, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 03:09:31\n",
            "loss: 0.7314, acc: 0.6958\n",
            "E2E-ABSA >>> 2022-05-21 03:09:32\n",
            "loss: 0.7145, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-05-21 03:09:32\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:33\n",
            "loss: 0.7417, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-05-21 03:09:33\n",
            "loss: 0.7074, acc: 0.7038\n",
            "E2E-ABSA >>> 2022-05-21 03:09:33\n",
            "loss: 0.7131, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-05-21 03:09:34\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:34\n",
            "loss: 0.7254, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:09:35\n",
            "loss: 0.6498, acc: 0.7324\n",
            "E2E-ABSA >>> 2022-05-21 03:09:35\n",
            "loss: 0.6889, acc: 0.7117\n",
            "E2E-ABSA >>> 2022-05-21 03:09:35\n",
            "loss: 0.6956, acc: 0.7086\n",
            "E2E-ABSA >>> 2022-05-21 03:09:36\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:36\n",
            "loss: 0.6806, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-05-21 03:09:37\n",
            "loss: 0.6683, acc: 0.7201\n",
            "E2E-ABSA >>> 2022-05-21 03:09:37\n",
            "loss: 0.6985, acc: 0.7099\n",
            "E2E-ABSA >>> 2022-05-21 03:09:38\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:38\n",
            "loss: 0.5564, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:09:38\n",
            "loss: 0.7215, acc: 0.7004\n",
            "E2E-ABSA >>> 2022-05-21 03:09:39\n",
            "loss: 0.7229, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-05-21 03:09:39\n",
            "loss: 0.7092, acc: 0.7055\n",
            "E2E-ABSA >>> 2022-05-21 03:09:39\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:40\n",
            "loss: 0.7252, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-05-21 03:09:40\n",
            "loss: 0.7107, acc: 0.7175\n",
            "E2E-ABSA >>> 2022-05-21 03:09:41\n",
            "loss: 0.7079, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-05-21 03:09:41\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:41\n",
            "loss: 0.7154, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:09:42\n",
            "loss: 0.7132, acc: 0.6840\n",
            "E2E-ABSA >>> 2022-05-21 03:09:42\n",
            "loss: 0.6964, acc: 0.7017\n",
            "E2E-ABSA >>> 2022-05-21 03:09:43\n",
            "loss: 0.6903, acc: 0.7077\n",
            "E2E-ABSA >>> 2022-05-21 03:09:43\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:43\n",
            "loss: 0.7233, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-05-21 03:09:44\n",
            "loss: 0.7156, acc: 0.7007\n",
            "E2E-ABSA >>> 2022-05-21 03:09:44\n",
            "loss: 0.7032, acc: 0.7088\n",
            "E2E-ABSA >>> 2022-05-21 03:09:45\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:45\n",
            "loss: 0.6399, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:09:45\n",
            "loss: 0.6991, acc: 0.7089\n",
            "E2E-ABSA >>> 2022-05-21 03:09:46\n",
            "loss: 0.6975, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-05-21 03:09:46\n",
            "loss: 0.6898, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:09:46\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:47\n",
            "loss: 0.7241, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:09:47\n",
            "loss: 0.6862, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-05-21 03:09:48\n",
            "loss: 0.6855, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-05-21 03:09:48\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:48\n",
            "loss: 0.6670, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:09:49\n",
            "loss: 0.6952, acc: 0.7141\n",
            "E2E-ABSA >>> 2022-05-21 03:09:49\n",
            "loss: 0.7017, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-05-21 03:09:50\n",
            "loss: 0.6821, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-05-21 03:09:50\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:50\n",
            "loss: 0.6976, acc: 0.7115\n",
            "E2E-ABSA >>> 2022-05-21 03:09:51\n",
            "loss: 0.6725, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-05-21 03:09:51\n",
            "loss: 0.6922, acc: 0.7144\n",
            "E2E-ABSA >>> 2022-05-21 03:09:52\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:52\n",
            "loss: 0.6607, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:09:52\n",
            "loss: 0.6908, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-05-21 03:09:53\n",
            "loss: 0.6778, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-05-21 03:09:53\n",
            "loss: 0.6834, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-05-21 03:09:53\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:54\n",
            "loss: 0.6865, acc: 0.7210\n",
            "E2E-ABSA >>> 2022-05-21 03:09:54\n",
            "loss: 0.7132, acc: 0.7091\n",
            "E2E-ABSA >>> 2022-05-21 03:09:55\n",
            "loss: 0.6746, acc: 0.7251\n",
            "E2E-ABSA >>> 2022-05-21 03:09:55\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:55\n",
            "loss: 0.6946, acc: 0.7009\n",
            "E2E-ABSA >>> 2022-05-21 03:09:56\n",
            "loss: 0.7012, acc: 0.7074\n",
            "E2E-ABSA >>> 2022-05-21 03:09:56\n",
            "loss: 0.6870, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:09:57\n",
            "loss: 0.6739, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-05-21 03:09:57\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:57\n",
            "loss: 0.6497, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:09:58\n",
            "loss: 0.6653, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-05-21 03:09:58\n",
            "loss: 0.6748, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-05-21 03:09:59\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:09:59\n",
            "loss: 0.6141, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:09:59\n",
            "loss: 0.6674, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-05-21 03:10:00\n",
            "loss: 0.6726, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-05-21 03:10:00\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:00\n",
            "loss: 0.5079, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:10:01\n",
            "loss: 0.6782, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-05-21 03:10:01\n",
            "loss: 0.6740, acc: 0.7238\n",
            "E2E-ABSA >>> 2022-05-21 03:10:02\n",
            "loss: 0.6835, acc: 0.7235\n",
            "E2E-ABSA >>> 2022-05-21 03:10:02\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:02\n",
            "loss: 0.6586, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 03:10:03\n",
            "loss: 0.6708, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-21 03:10:03\n",
            "loss: 0.6686, acc: 0.7268\n",
            "E2E-ABSA >>> 2022-05-21 03:10:04\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:04\n",
            "loss: 0.5550, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-05-21 03:10:04\n",
            "loss: 0.6772, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-05-21 03:10:05\n",
            "loss: 0.6740, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-05-21 03:10:05\n",
            "loss: 0.6740, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-05-21 03:10:06\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:06\n",
            "loss: 0.6757, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-05-21 03:10:06\n",
            "loss: 0.6704, acc: 0.7288\n",
            "E2E-ABSA >>> 2022-05-21 03:10:07\n",
            "loss: 0.6594, acc: 0.7359\n",
            "E2E-ABSA >>> 2022-05-21 03:10:07\n",
            ">>> val_acc: 0.6969, val_precision: 0.6969 val_recall: 0.6969, val_f1: 0.6969\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:07\n",
            "loss: 0.6132, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:10:08\n",
            "loss: 0.6713, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 03:10:08\n",
            "loss: 0.6764, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-05-21 03:10:09\n",
            "loss: 0.6785, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-05-21 03:10:09\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:09\n",
            "loss: 0.6677, acc: 0.7557\n",
            "E2E-ABSA >>> 2022-05-21 03:10:10\n",
            "loss: 0.6612, acc: 0.7296\n",
            "E2E-ABSA >>> 2022-05-21 03:10:10\n",
            "loss: 0.6593, acc: 0.7386\n",
            "E2E-ABSA >>> 2022-05-21 03:10:11\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:11\n",
            "loss: 0.6648, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:10:11\n",
            "loss: 0.6583, acc: 0.7368\n",
            "E2E-ABSA >>> 2022-05-21 03:10:12\n",
            "loss: 0.6599, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-05-21 03:10:12\n",
            "loss: 0.6684, acc: 0.7341\n",
            "E2E-ABSA >>> 2022-05-21 03:10:13\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:13\n",
            "loss: 0.6264, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:10:13\n",
            "loss: 0.6402, acc: 0.7477\n",
            "E2E-ABSA >>> 2022-05-21 03:10:14\n",
            "loss: 0.6644, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-05-21 03:10:14\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:15\n",
            "loss: 0.7419, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:10:15\n",
            "loss: 0.7026, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-05-21 03:10:15\n",
            "loss: 0.6803, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-05-21 03:10:16\n",
            "loss: 0.6678, acc: 0.7300\n",
            "E2E-ABSA >>> 2022-05-21 03:10:16\n",
            ">>> val_acc: 0.7041, val_precision: 0.7041 val_recall: 0.7041, val_f1: 0.7041\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:17\n",
            "loss: 0.6683, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-05-21 03:10:17\n",
            "loss: 0.6462, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-05-21 03:10:18\n",
            "loss: 0.6586, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-05-21 03:10:18\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:18\n",
            "loss: 0.5969, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:10:19\n",
            "loss: 0.6517, acc: 0.7485\n",
            "E2E-ABSA >>> 2022-05-21 03:10:19\n",
            "loss: 0.6544, acc: 0.7439\n",
            "E2E-ABSA >>> 2022-05-21 03:10:20\n",
            "loss: 0.6658, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-05-21 03:10:20\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:20\n",
            "loss: 0.6707, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-05-21 03:10:21\n",
            "loss: 0.6744, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-05-21 03:10:21\n",
            "loss: 0.6666, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:10:21\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:22\n",
            "loss: 0.6382, acc: 0.7768\n",
            "E2E-ABSA >>> 2022-05-21 03:10:22\n",
            "loss: 0.6862, acc: 0.7301\n",
            "E2E-ABSA >>> 2022-05-21 03:10:23\n",
            "loss: 0.6710, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-05-21 03:10:23\n",
            "loss: 0.6632, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-05-21 03:10:23\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:24\n",
            "loss: 0.6864, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-05-21 03:10:24\n",
            "loss: 0.6780, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-05-21 03:10:25\n",
            "loss: 0.6726, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-05-21 03:10:25\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:25\n",
            "loss: 0.6003, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:10:26\n",
            "loss: 0.6640, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-05-21 03:10:26\n",
            "loss: 0.6766, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-05-21 03:10:27\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:27\n",
            "loss: 0.6546, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:10:27\n",
            "loss: 0.6336, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:10:28\n",
            "loss: 0.6747, acc: 0.7117\n",
            "E2E-ABSA >>> 2022-05-21 03:10:28\n",
            "loss: 0.6668, acc: 0.7194\n",
            "E2E-ABSA >>> 2022-05-21 03:10:28\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:29\n",
            "loss: 0.6354, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 03:10:29\n",
            "loss: 0.6542, acc: 0.7279\n",
            "E2E-ABSA >>> 2022-05-21 03:10:30\n",
            "loss: 0.6609, acc: 0.7236\n",
            "E2E-ABSA >>> 2022-05-21 03:10:30\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:30\n",
            "loss: 0.3654, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-21 03:10:31\n",
            "loss: 0.6756, acc: 0.7169\n",
            "E2E-ABSA >>> 2022-05-21 03:10:31\n",
            "loss: 0.6459, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:10:32\n",
            "loss: 0.6557, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-05-21 03:10:32\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:32\n",
            "loss: 0.6546, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-05-21 03:10:33\n",
            "loss: 0.6533, acc: 0.7262\n",
            "E2E-ABSA >>> 2022-05-21 03:10:33\n",
            "loss: 0.6592, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-05-21 03:10:34\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:34\n",
            "loss: 0.6732, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:10:34\n",
            "loss: 0.6410, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:10:35\n",
            "loss: 0.6516, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-05-21 03:10:35\n",
            "loss: 0.6511, acc: 0.7357\n",
            "E2E-ABSA >>> 2022-05-21 03:10:36\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:36\n",
            "loss: 0.6741, acc: 0.7074\n",
            "E2E-ABSA >>> 2022-05-21 03:10:37\n",
            "loss: 0.6502, acc: 0.7272\n",
            "E2E-ABSA >>> 2022-05-21 03:10:37\n",
            "loss: 0.6550, acc: 0.7317\n",
            "E2E-ABSA >>> 2022-05-21 03:10:38\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:38\n",
            "loss: 0.5841, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:10:38\n",
            "loss: 0.6140, acc: 0.7582\n",
            "E2E-ABSA >>> 2022-05-21 03:10:39\n",
            "loss: 0.6511, acc: 0.7362\n",
            "E2E-ABSA >>> 2022-05-21 03:10:39\n",
            "loss: 0.6584, acc: 0.7334\n",
            "E2E-ABSA >>> 2022-05-21 03:10:39\n",
            ">>> val_acc: 0.6969, val_precision: 0.6969 val_recall: 0.6969, val_f1: 0.6969\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:40\n",
            "loss: 0.6549, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-05-21 03:10:40\n",
            "loss: 0.6598, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-05-21 03:10:41\n",
            "loss: 0.6488, acc: 0.7351\n",
            "E2E-ABSA >>> 2022-05-21 03:10:41\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:41\n",
            "loss: 0.5738, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:10:42\n",
            "loss: 0.6095, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-05-21 03:10:42\n",
            "loss: 0.6527, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-05-21 03:10:43\n",
            "loss: 0.6516, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-05-21 03:10:43\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:43\n",
            "loss: 0.6628, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:10:44\n",
            "loss: 0.6504, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-05-21 03:10:44\n",
            "loss: 0.6543, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-05-21 03:10:45\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:45\n",
            "loss: 0.7563, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:10:45\n",
            "loss: 0.6963, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-05-21 03:10:46\n",
            "loss: 0.6650, acc: 0.7300\n",
            "E2E-ABSA >>> 2022-05-21 03:10:46\n",
            "loss: 0.6578, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-05-21 03:10:46\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:47\n",
            "loss: 0.6153, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:10:47\n",
            "loss: 0.6503, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-05-21 03:10:48\n",
            "loss: 0.6535, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:10:48\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:48\n",
            "loss: 0.6283, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-05-21 03:10:49\n",
            "loss: 0.6536, acc: 0.7443\n",
            "E2E-ABSA >>> 2022-05-21 03:10:49\n",
            "loss: 0.6513, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-05-21 03:10:50\n",
            "loss: 0.6515, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-05-21 03:10:50\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:50\n",
            "loss: 0.6496, acc: 0.7479\n",
            "E2E-ABSA >>> 2022-05-21 03:10:51\n",
            "loss: 0.6563, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:10:51\n",
            "loss: 0.6481, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:10:52\n",
            ">>> val_acc: 0.6826, val_precision: 0.6826 val_recall: 0.6826, val_f1: 0.6826\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:52\n",
            "loss: 0.5673, acc: 0.7852\n",
            "E2E-ABSA >>> 2022-05-21 03:10:52\n",
            "loss: 0.6194, acc: 0.7459\n",
            "E2E-ABSA >>> 2022-05-21 03:10:53\n",
            "loss: 0.6527, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-05-21 03:10:54\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:54\n",
            "loss: 0.5358, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 03:10:54\n",
            "loss: 0.6854, acc: 0.7207\n",
            "E2E-ABSA >>> 2022-05-21 03:10:55\n",
            "loss: 0.6648, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-05-21 03:10:55\n",
            "loss: 0.6558, acc: 0.7276\n",
            "E2E-ABSA >>> 2022-05-21 03:10:55\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-05-21 03:10:56\n",
            "loss: 0.5927, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-05-21 03:10:56\n",
            "loss: 0.6229, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-05-21 03:10:57\n",
            "loss: 0.6425, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-05-21 03:10:57\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            "E2E-ABSA >>> 2022-05-21 03:10:57\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            "you can download the best model from state_dict/ian_twitter_know_val_f1_0.7351\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.7351, test_precision: 0.7351, test_recall: 0.7351, test_f1: 0.7351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Twitter** dataset on model(**memnet**)"
      ],
      "metadata": {
        "id": "LmkSqiY8mjxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset twitter --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0_8Gxpimj9L",
        "outputId": "e77ade3a-1113-4e80-e464-f110e8439229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1664.\n",
            "> testing dataset count: 419.\n",
            "cuda memory allocated: 9402368\n",
            "> n_trainable_params: 161803, n_nontrainable_params: 2188000\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fdfcbd5ab00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:26\n",
            "loss: 1.0080, acc: 0.4208\n",
            "E2E-ABSA >>> 2022-05-21 03:13:27\n",
            "loss: 0.9602, acc: 0.5219\n",
            "E2E-ABSA >>> 2022-05-21 03:13:27\n",
            "loss: 0.9279, acc: 0.5597\n",
            "E2E-ABSA >>> 2022-05-21 03:13:27\n",
            ">>> val_acc: 0.6444, val_precision: 0.6444 val_recall: 0.6444, val_f1: 0.6444\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.6444\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:27\n",
            "loss: 0.8353, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-05-21 03:13:27\n",
            "loss: 0.8476, acc: 0.6481\n",
            "E2E-ABSA >>> 2022-05-21 03:13:28\n",
            "loss: 0.8375, acc: 0.6414\n",
            "E2E-ABSA >>> 2022-05-21 03:13:28\n",
            ">>> val_acc: 0.6587, val_precision: 0.6587 val_recall: 0.6587, val_f1: 0.6587\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.6587\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:28\n",
            "loss: 0.9719, acc: 0.5312\n",
            "E2E-ABSA >>> 2022-05-21 03:13:28\n",
            "loss: 0.8029, acc: 0.6445\n",
            "E2E-ABSA >>> 2022-05-21 03:13:29\n",
            "loss: 0.8112, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-05-21 03:13:29\n",
            "loss: 0.8083, acc: 0.6549\n",
            "E2E-ABSA >>> 2022-05-21 03:13:29\n",
            ">>> val_acc: 0.6659, val_precision: 0.6659 val_recall: 0.6659, val_f1: 0.6659\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.6659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:29\n",
            "loss: 0.7221, acc: 0.6944\n",
            "E2E-ABSA >>> 2022-05-21 03:13:29\n",
            "loss: 0.7576, acc: 0.6732\n",
            "E2E-ABSA >>> 2022-05-21 03:13:30\n",
            "loss: 0.7747, acc: 0.6691\n",
            "E2E-ABSA >>> 2022-05-21 03:13:30\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:30\n",
            "loss: 0.7336, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:13:30\n",
            "loss: 0.7550, acc: 0.6801\n",
            "E2E-ABSA >>> 2022-05-21 03:13:30\n",
            "loss: 0.7589, acc: 0.6699\n",
            "E2E-ABSA >>> 2022-05-21 03:13:31\n",
            "loss: 0.7602, acc: 0.6722\n",
            "E2E-ABSA >>> 2022-05-21 03:13:31\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:31\n",
            "loss: 0.7263, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:13:31\n",
            "loss: 0.7480, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 03:13:31\n",
            "loss: 0.7504, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-05-21 03:13:32\n",
            ">>> val_acc: 0.6897, val_precision: 0.6897 val_recall: 0.6897, val_f1: 0.6897\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.6897\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:32\n",
            "loss: 0.7127, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:13:32\n",
            "loss: 0.7441, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:13:32\n",
            "loss: 0.7460, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-05-21 03:13:32\n",
            "loss: 0.7370, acc: 0.6816\n",
            "E2E-ABSA >>> 2022-05-21 03:13:33\n",
            ">>> val_acc: 0.6993, val_precision: 0.6993 val_recall: 0.6993, val_f1: 0.6993\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.6993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:33\n",
            "loss: 0.6768, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-05-21 03:13:33\n",
            "loss: 0.7302, acc: 0.6827\n",
            "E2E-ABSA >>> 2022-05-21 03:13:33\n",
            "loss: 0.7311, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-05-21 03:13:33\n",
            ">>> val_acc: 0.7017, val_precision: 0.7017 val_recall: 0.7017, val_f1: 0.7017\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.7017\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:34\n",
            "loss: 0.6936, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:13:34\n",
            "loss: 0.6773, acc: 0.7138\n",
            "E2E-ABSA >>> 2022-05-21 03:13:34\n",
            "loss: 0.7127, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:13:34\n",
            "loss: 0.7076, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-05-21 03:13:34\n",
            ">>> val_acc: 0.7112, val_precision: 0.7112 val_recall: 0.7112, val_f1: 0.7112\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.7112\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:35\n",
            "loss: 0.6734, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:13:35\n",
            "loss: 0.6951, acc: 0.7130\n",
            "E2E-ABSA >>> 2022-05-21 03:13:35\n",
            "loss: 0.6973, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-05-21 03:13:35\n",
            ">>> val_acc: 0.7112, val_precision: 0.7112 val_recall: 0.7112, val_f1: 0.7112\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:35\n",
            "loss: 0.7207, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:13:36\n",
            "loss: 0.7103, acc: 0.6891\n",
            "E2E-ABSA >>> 2022-05-21 03:13:36\n",
            "loss: 0.6930, acc: 0.7080\n",
            "E2E-ABSA >>> 2022-05-21 03:13:36\n",
            "loss: 0.6958, acc: 0.7056\n",
            "E2E-ABSA >>> 2022-05-21 03:13:36\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.716\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:37\n",
            "loss: 0.6675, acc: 0.7019\n",
            "E2E-ABSA >>> 2022-05-21 03:13:37\n",
            "loss: 0.6689, acc: 0.7132\n",
            "E2E-ABSA >>> 2022-05-21 03:13:37\n",
            "loss: 0.6876, acc: 0.7100\n",
            "E2E-ABSA >>> 2022-05-21 03:13:37\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:37\n",
            "loss: 0.6978, acc: 0.6510\n",
            "E2E-ABSA >>> 2022-05-21 03:13:38\n",
            "loss: 0.6500, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-05-21 03:13:38\n",
            "loss: 0.6757, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:13:38\n",
            "loss: 0.6760, acc: 0.7126\n",
            "E2E-ABSA >>> 2022-05-21 03:13:38\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:38\n",
            "loss: 0.6760, acc: 0.7009\n",
            "E2E-ABSA >>> 2022-05-21 03:13:39\n",
            "loss: 0.6717, acc: 0.7112\n",
            "E2E-ABSA >>> 2022-05-21 03:13:39\n",
            "loss: 0.6694, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-05-21 03:13:39\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.747\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:39\n",
            "loss: 0.6432, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-05-21 03:13:39\n",
            "loss: 0.6683, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-05-21 03:13:40\n",
            "loss: 0.6598, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-05-21 03:13:40\n",
            "loss: 0.6605, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-05-21 03:13:40\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:40\n",
            "loss: 0.6545, acc: 0.7479\n",
            "E2E-ABSA >>> 2022-05-21 03:13:40\n",
            "loss: 0.6783, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-05-21 03:13:41\n",
            "loss: 0.6664, acc: 0.7229\n",
            "E2E-ABSA >>> 2022-05-21 03:13:41\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:41\n",
            "loss: 0.6590, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-05-21 03:13:41\n",
            "loss: 0.6409, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-05-21 03:13:41\n",
            "loss: 0.6573, acc: 0.7253\n",
            "E2E-ABSA >>> 2022-05-21 03:13:42\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:42\n",
            "loss: 0.6284, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:13:42\n",
            "loss: 0.6203, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-05-21 03:13:42\n",
            "loss: 0.6281, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-05-21 03:13:43\n",
            "loss: 0.6385, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-05-21 03:13:43\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:43\n",
            "loss: 0.6463, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-05-21 03:13:43\n",
            "loss: 0.6462, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-05-21 03:13:43\n",
            "loss: 0.6446, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-05-21 03:13:44\n",
            ">>> val_acc: 0.7518, val_precision: 0.7518 val_recall: 0.7518, val_f1: 0.7518\n",
            ">> saved: state_dict/memnet_twitter_val_f1_0.7518\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:44\n",
            "loss: 0.6045, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:13:44\n",
            "loss: 0.6044, acc: 0.7445\n",
            "E2E-ABSA >>> 2022-05-21 03:13:44\n",
            "loss: 0.6326, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:13:44\n",
            "loss: 0.6226, acc: 0.7380\n",
            "E2E-ABSA >>> 2022-05-21 03:13:45\n",
            ">>> val_acc: 0.7518, val_precision: 0.7518 val_recall: 0.7518, val_f1: 0.7518\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:45\n",
            "loss: 0.6457, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-05-21 03:13:45\n",
            "loss: 0.6274, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-05-21 03:13:45\n",
            "loss: 0.6204, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-05-21 03:13:46\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:46\n",
            "loss: 0.5794, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:13:46\n",
            "loss: 0.6098, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:13:46\n",
            "loss: 0.6291, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-05-21 03:13:46\n",
            "loss: 0.6134, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:13:46\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:47\n",
            "loss: 0.6637, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:13:47\n",
            "loss: 0.6290, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-05-21 03:13:47\n",
            "loss: 0.6192, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-05-21 03:13:47\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:47\n",
            "loss: 0.6197, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:13:48\n",
            "loss: 0.5893, acc: 0.7615\n",
            "E2E-ABSA >>> 2022-05-21 03:13:48\n",
            "loss: 0.6124, acc: 0.7417\n",
            "E2E-ABSA >>> 2022-05-21 03:13:48\n",
            "loss: 0.6120, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-05-21 03:13:48\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:49\n",
            "loss: 0.6527, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:13:49\n",
            "loss: 0.6252, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 03:13:49\n",
            "loss: 0.6126, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-05-21 03:13:49\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:49\n",
            "loss: 0.5933, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 03:13:50\n",
            "loss: 0.5933, acc: 0.7609\n",
            "E2E-ABSA >>> 2022-05-21 03:13:50\n",
            "loss: 0.6003, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:13:50\n",
            "loss: 0.6058, acc: 0.7481\n",
            "E2E-ABSA >>> 2022-05-21 03:13:50\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:50\n",
            "loss: 0.6075, acc: 0.7596\n",
            "E2E-ABSA >>> 2022-05-21 03:13:51\n",
            "loss: 0.6016, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-05-21 03:13:51\n",
            "loss: 0.6004, acc: 0.7551\n",
            "E2E-ABSA >>> 2022-05-21 03:13:51\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:51\n",
            "loss: 0.6506, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:13:51\n",
            "loss: 0.5998, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-05-21 03:13:52\n",
            "loss: 0.5825, acc: 0.7543\n",
            "E2E-ABSA >>> 2022-05-21 03:13:52\n",
            "loss: 0.5956, acc: 0.7445\n",
            "E2E-ABSA >>> 2022-05-21 03:13:52\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:52\n",
            "loss: 0.5818, acc: 0.7701\n",
            "E2E-ABSA >>> 2022-05-21 03:13:52\n",
            "loss: 0.5816, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-05-21 03:13:53\n",
            "loss: 0.5996, acc: 0.7479\n",
            "E2E-ABSA >>> 2022-05-21 03:13:53\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:53\n",
            "loss: 0.5609, acc: 0.7991\n",
            "E2E-ABSA >>> 2022-05-21 03:13:53\n",
            "loss: 0.5919, acc: 0.7585\n",
            "E2E-ABSA >>> 2022-05-21 03:13:53\n",
            "loss: 0.5958, acc: 0.7525\n",
            "E2E-ABSA >>> 2022-05-21 03:13:54\n",
            "loss: 0.5891, acc: 0.7506\n",
            "E2E-ABSA >>> 2022-05-21 03:13:54\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:54\n",
            "loss: 0.5729, acc: 0.7583\n",
            "E2E-ABSA >>> 2022-05-21 03:13:54\n",
            "loss: 0.5673, acc: 0.7615\n",
            "E2E-ABSA >>> 2022-05-21 03:13:54\n",
            "loss: 0.5791, acc: 0.7542\n",
            "E2E-ABSA >>> 2022-05-21 03:13:55\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:55\n",
            "loss: 0.5530, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-05-21 03:13:55\n",
            "loss: 0.6086, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-05-21 03:13:55\n",
            "loss: 0.5912, acc: 0.7516\n",
            "E2E-ABSA >>> 2022-05-21 03:13:56\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:56\n",
            "loss: 0.8526, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 03:13:56\n",
            "loss: 0.5818, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-05-21 03:13:56\n",
            "loss: 0.5978, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-05-21 03:13:56\n",
            "loss: 0.5740, acc: 0.7534\n",
            "E2E-ABSA >>> 2022-05-21 03:13:57\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:57\n",
            "loss: 0.5718, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:13:57\n",
            "loss: 0.5770, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-05-21 03:13:57\n",
            "loss: 0.5729, acc: 0.7580\n",
            "E2E-ABSA >>> 2022-05-21 03:13:57\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:57\n",
            "loss: 0.5435, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:13:58\n",
            "loss: 0.5809, acc: 0.7445\n",
            "E2E-ABSA >>> 2022-05-21 03:13:58\n",
            "loss: 0.5830, acc: 0.7510\n",
            "E2E-ABSA >>> 2022-05-21 03:13:58\n",
            "loss: 0.5697, acc: 0.7646\n",
            "E2E-ABSA >>> 2022-05-21 03:13:58\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:59\n",
            "loss: 0.5656, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 03:13:59\n",
            "loss: 0.5932, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:13:59\n",
            "loss: 0.5784, acc: 0.7523\n",
            "E2E-ABSA >>> 2022-05-21 03:13:59\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:13:59\n",
            "loss: 0.6033, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 03:14:00\n",
            "loss: 0.5596, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-05-21 03:14:00\n",
            "loss: 0.5754, acc: 0.7576\n",
            "E2E-ABSA >>> 2022-05-21 03:14:00\n",
            "loss: 0.5715, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-05-21 03:14:00\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:00\n",
            "loss: 0.5830, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-05-21 03:14:01\n",
            "loss: 0.5715, acc: 0.7560\n",
            "E2E-ABSA >>> 2022-05-21 03:14:01\n",
            "loss: 0.5624, acc: 0.7607\n",
            "E2E-ABSA >>> 2022-05-21 03:14:01\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:01\n",
            "loss: 0.5636, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:14:01\n",
            "loss: 0.5602, acc: 0.7467\n",
            "E2E-ABSA >>> 2022-05-21 03:14:02\n",
            "loss: 0.5712, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-05-21 03:14:02\n",
            "loss: 0.5572, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-05-21 03:14:02\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:02\n",
            "loss: 0.5400, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:14:02\n",
            "loss: 0.5533, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-05-21 03:14:03\n",
            "loss: 0.5558, acc: 0.7671\n",
            "E2E-ABSA >>> 2022-05-21 03:14:03\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:03\n",
            "loss: 0.5412, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-05-21 03:14:03\n",
            "loss: 0.5390, acc: 0.7844\n",
            "E2E-ABSA >>> 2022-05-21 03:14:03\n",
            "loss: 0.5547, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-05-21 03:14:04\n",
            "loss: 0.5536, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:14:04\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:04\n",
            "loss: 0.5201, acc: 0.7788\n",
            "E2E-ABSA >>> 2022-05-21 03:14:04\n",
            "loss: 0.5396, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:14:04\n",
            "loss: 0.5382, acc: 0.7711\n",
            "E2E-ABSA >>> 2022-05-21 03:14:05\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:05\n",
            "loss: 0.5457, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:14:05\n",
            "loss: 0.5525, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-05-21 03:14:05\n",
            "loss: 0.5609, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-05-21 03:14:05\n",
            "loss: 0.5512, acc: 0.7665\n",
            "E2E-ABSA >>> 2022-05-21 03:14:06\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:06\n",
            "loss: 0.5544, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-05-21 03:14:06\n",
            "loss: 0.5357, acc: 0.7834\n",
            "E2E-ABSA >>> 2022-05-21 03:14:06\n",
            "loss: 0.5464, acc: 0.7685\n",
            "E2E-ABSA >>> 2022-05-21 03:14:07\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:07\n",
            "loss: 0.5370, acc: 0.7902\n",
            "E2E-ABSA >>> 2022-05-21 03:14:07\n",
            "loss: 0.5561, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-05-21 03:14:07\n",
            "loss: 0.5480, acc: 0.7652\n",
            "E2E-ABSA >>> 2022-05-21 03:14:07\n",
            "loss: 0.5428, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-05-21 03:14:08\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:08\n",
            "loss: 0.5345, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:14:08\n",
            "loss: 0.5424, acc: 0.7615\n",
            "E2E-ABSA >>> 2022-05-21 03:14:08\n",
            "loss: 0.5383, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-05-21 03:14:08\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:09\n",
            "loss: 0.5983, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:14:09\n",
            "loss: 0.5408, acc: 0.7690\n",
            "E2E-ABSA >>> 2022-05-21 03:14:09\n",
            "loss: 0.5365, acc: 0.7706\n",
            "E2E-ABSA >>> 2022-05-21 03:14:09\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:09\n",
            "loss: 0.4582, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-05-21 03:14:10\n",
            "loss: 0.5279, acc: 0.7832\n",
            "E2E-ABSA >>> 2022-05-21 03:14:10\n",
            "loss: 0.5287, acc: 0.7802\n",
            "E2E-ABSA >>> 2022-05-21 03:14:10\n",
            "loss: 0.5335, acc: 0.7751\n",
            "E2E-ABSA >>> 2022-05-21 03:14:10\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:11\n",
            "loss: 0.5731, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-05-21 03:14:11\n",
            "loss: 0.5417, acc: 0.7826\n",
            "E2E-ABSA >>> 2022-05-21 03:14:11\n",
            "loss: 0.5376, acc: 0.7821\n",
            "E2E-ABSA >>> 2022-05-21 03:14:11\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:11\n",
            "loss: 0.4585, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-05-21 03:14:12\n",
            "loss: 0.5424, acc: 0.7629\n",
            "E2E-ABSA >>> 2022-05-21 03:14:12\n",
            "loss: 0.5405, acc: 0.7607\n",
            "E2E-ABSA >>> 2022-05-21 03:14:12\n",
            "loss: 0.5372, acc: 0.7673\n",
            "E2E-ABSA >>> 2022-05-21 03:14:12\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:12\n",
            "loss: 0.5129, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-05-21 03:14:13\n",
            "loss: 0.5238, acc: 0.7863\n",
            "E2E-ABSA >>> 2022-05-21 03:14:13\n",
            "loss: 0.5316, acc: 0.7797\n",
            "E2E-ABSA >>> 2022-05-21 03:14:13\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:13\n",
            "loss: 0.4196, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-05-21 03:14:13\n",
            "loss: 0.5448, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-05-21 03:14:14\n",
            "loss: 0.5325, acc: 0.7699\n",
            "E2E-ABSA >>> 2022-05-21 03:14:14\n",
            "loss: 0.5317, acc: 0.7754\n",
            "E2E-ABSA >>> 2022-05-21 03:14:14\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:14\n",
            "loss: 0.4747, acc: 0.8011\n",
            "E2E-ABSA >>> 2022-05-21 03:14:14\n",
            "loss: 0.5023, acc: 0.7825\n",
            "E2E-ABSA >>> 2022-05-21 03:14:15\n",
            "loss: 0.5334, acc: 0.7691\n",
            "E2E-ABSA >>> 2022-05-21 03:14:15\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:15\n",
            "loss: 0.5663, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:14:15\n",
            "loss: 0.5259, acc: 0.7747\n",
            "E2E-ABSA >>> 2022-05-21 03:14:15\n",
            "loss: 0.5181, acc: 0.7803\n",
            "E2E-ABSA >>> 2022-05-21 03:14:16\n",
            "loss: 0.5320, acc: 0.7723\n",
            "E2E-ABSA >>> 2022-05-21 03:14:16\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:16\n",
            "loss: 0.5114, acc: 0.7865\n",
            "E2E-ABSA >>> 2022-05-21 03:14:16\n",
            "loss: 0.5010, acc: 0.7870\n",
            "E2E-ABSA >>> 2022-05-21 03:14:17\n",
            "loss: 0.5163, acc: 0.7805\n",
            "E2E-ABSA >>> 2022-05-21 03:14:17\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:17\n",
            "loss: 0.4504, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-05-21 03:14:17\n",
            "loss: 0.5031, acc: 0.7953\n",
            "E2E-ABSA >>> 2022-05-21 03:14:17\n",
            "loss: 0.5183, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-05-21 03:14:18\n",
            "loss: 0.5167, acc: 0.7806\n",
            "E2E-ABSA >>> 2022-05-21 03:14:18\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:18\n",
            "loss: 0.4968, acc: 0.7957\n",
            "E2E-ABSA >>> 2022-05-21 03:14:18\n",
            "loss: 0.5256, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-05-21 03:14:18\n",
            "loss: 0.5215, acc: 0.7762\n",
            "E2E-ABSA >>> 2022-05-21 03:14:19\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:19\n",
            "loss: 0.5320, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:14:19\n",
            "loss: 0.5138, acc: 0.7768\n",
            "E2E-ABSA >>> 2022-05-21 03:14:19\n",
            "loss: 0.5053, acc: 0.7769\n",
            "E2E-ABSA >>> 2022-05-21 03:14:19\n",
            "loss: 0.5189, acc: 0.7800\n",
            "E2E-ABSA >>> 2022-05-21 03:14:20\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:20\n",
            "loss: 0.5029, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-05-21 03:14:20\n",
            "loss: 0.5012, acc: 0.7909\n",
            "E2E-ABSA >>> 2022-05-21 03:14:20\n",
            "loss: 0.5239, acc: 0.7706\n",
            "E2E-ABSA >>> 2022-05-21 03:14:20\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:21\n",
            "loss: 0.4790, acc: 0.7946\n",
            "E2E-ABSA >>> 2022-05-21 03:14:21\n",
            "loss: 0.5097, acc: 0.7827\n",
            "E2E-ABSA >>> 2022-05-21 03:14:21\n",
            "loss: 0.5019, acc: 0.7863\n",
            "E2E-ABSA >>> 2022-05-21 03:14:21\n",
            "loss: 0.5120, acc: 0.7800\n",
            "E2E-ABSA >>> 2022-05-21 03:14:21\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:22\n",
            "loss: 0.5025, acc: 0.8042\n",
            "E2E-ABSA >>> 2022-05-21 03:14:22\n",
            "loss: 0.4882, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-05-21 03:14:22\n",
            "loss: 0.5111, acc: 0.7868\n",
            "E2E-ABSA >>> 2022-05-21 03:14:22\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:22\n",
            "loss: 0.4558, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-05-21 03:14:23\n",
            "loss: 0.4929, acc: 0.7853\n",
            "E2E-ABSA >>> 2022-05-21 03:14:23\n",
            "loss: 0.4976, acc: 0.7804\n",
            "E2E-ABSA >>> 2022-05-21 03:14:23\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:23\n",
            "loss: 0.6182, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:14:23\n",
            "loss: 0.5232, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-05-21 03:14:24\n",
            "loss: 0.5055, acc: 0.7903\n",
            "E2E-ABSA >>> 2022-05-21 03:14:24\n",
            "loss: 0.5159, acc: 0.7819\n",
            "E2E-ABSA >>> 2022-05-21 03:14:24\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:24\n",
            "loss: 0.5393, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-05-21 03:14:24\n",
            "loss: 0.5374, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-05-21 03:14:25\n",
            "loss: 0.5104, acc: 0.7837\n",
            "E2E-ABSA >>> 2022-05-21 03:14:25\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:25\n",
            "loss: 0.5872, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:14:25\n",
            "loss: 0.4943, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-05-21 03:14:26\n",
            "loss: 0.5176, acc: 0.7754\n",
            "E2E-ABSA >>> 2022-05-21 03:14:26\n",
            "loss: 0.5047, acc: 0.7826\n",
            "E2E-ABSA >>> 2022-05-21 03:14:26\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:26\n",
            "loss: 0.5397, acc: 0.7781\n",
            "E2E-ABSA >>> 2022-05-21 03:14:26\n",
            "loss: 0.5111, acc: 0.7900\n",
            "E2E-ABSA >>> 2022-05-21 03:14:27\n",
            "loss: 0.4944, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-05-21 03:14:27\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:27\n",
            "loss: 0.4575, acc: 0.8542\n",
            "E2E-ABSA >>> 2022-05-21 03:14:27\n",
            "loss: 0.5091, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 03:14:27\n",
            "loss: 0.4903, acc: 0.7992\n",
            "E2E-ABSA >>> 2022-05-21 03:14:28\n",
            "loss: 0.4971, acc: 0.7923\n",
            "E2E-ABSA >>> 2022-05-21 03:14:28\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:28\n",
            "loss: 0.5726, acc: 0.7415\n",
            "E2E-ABSA >>> 2022-05-21 03:14:28\n",
            "loss: 0.4980, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-05-21 03:14:28\n",
            "loss: 0.4943, acc: 0.7881\n",
            "E2E-ABSA >>> 2022-05-21 03:14:29\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:14:29\n",
            "loss: 0.4644, acc: 0.8359\n",
            "E2E-ABSA >>> 2022-05-21 03:14:29\n",
            "loss: 0.4944, acc: 0.7862\n",
            "E2E-ABSA >>> 2022-05-21 03:14:29\n",
            "loss: 0.4981, acc: 0.7923\n",
            "E2E-ABSA >>> 2022-05-21 03:14:29\n",
            "loss: 0.4936, acc: 0.7934\n",
            "E2E-ABSA >>> 2022-05-21 03:14:30\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            "E2E-ABSA >>> 2022-05-21 03:14:30\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7518, val_precision: 0.7518 val_recall: 0.7518, val_f1: 0.7518\n",
            "you can download the best model from state_dict/memnet_twitter_val_f1_0.7518\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.7518, test_precision: 0.7518, test_recall: 0.7518, test_f1: 0.7518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后: Training **Twitter** dataset on model(**memnet**)"
      ],
      "metadata": {
        "id": "s-n19o_2mkKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset twitter_know --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxT7MPn-mkYB",
        "outputId": "350e9873-8cf8-4d81-d1a1-2904209918c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1664.\n",
            "> testing dataset count: 419.\n",
            "cuda memory allocated: 16163840\n",
            "> n_trainable_params: 161803, n_nontrainable_params: 3878400\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f25f4c01b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:03\n",
            "loss: 0.9977, acc: 0.4437\n",
            "E2E-ABSA >>> 2022-05-21 03:15:03\n",
            "loss: 0.9533, acc: 0.5396\n",
            "E2E-ABSA >>> 2022-05-21 03:15:03\n",
            "loss: 0.9222, acc: 0.5785\n",
            "E2E-ABSA >>> 2022-05-21 03:15:03\n",
            ">>> val_acc: 0.6444, val_precision: 0.6444 val_recall: 0.6444, val_f1: 0.6444\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6444\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:03\n",
            "loss: 0.8222, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 03:15:04\n",
            "loss: 0.8407, acc: 0.6549\n",
            "E2E-ABSA >>> 2022-05-21 03:15:04\n",
            "loss: 0.8323, acc: 0.6538\n",
            "E2E-ABSA >>> 2022-05-21 03:15:04\n",
            ">>> val_acc: 0.6539, val_precision: 0.6539 val_recall: 0.6539, val_f1: 0.6539\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6539\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:04\n",
            "loss: 0.9323, acc: 0.5312\n",
            "E2E-ABSA >>> 2022-05-21 03:15:05\n",
            "loss: 0.8084, acc: 0.6348\n",
            "E2E-ABSA >>> 2022-05-21 03:15:05\n",
            "loss: 0.8239, acc: 0.6522\n",
            "E2E-ABSA >>> 2022-05-21 03:15:05\n",
            "loss: 0.8231, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-05-21 03:15:05\n",
            ">>> val_acc: 0.6587, val_precision: 0.6587 val_recall: 0.6587, val_f1: 0.6587\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6587\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:05\n",
            "loss: 0.7480, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:15:06\n",
            "loss: 0.7801, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 03:15:06\n",
            "loss: 0.7943, acc: 0.6675\n",
            "E2E-ABSA >>> 2022-05-21 03:15:06\n",
            ">>> val_acc: 0.6683, val_precision: 0.6683 val_recall: 0.6683, val_f1: 0.6683\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6683\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:06\n",
            "loss: 0.7716, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:15:06\n",
            "loss: 0.7847, acc: 0.6673\n",
            "E2E-ABSA >>> 2022-05-21 03:15:07\n",
            "loss: 0.7844, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-05-21 03:15:07\n",
            "loss: 0.7885, acc: 0.6682\n",
            "E2E-ABSA >>> 2022-05-21 03:15:07\n",
            ">>> val_acc: 0.6659, val_precision: 0.6659 val_recall: 0.6659, val_f1: 0.6659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:07\n",
            "loss: 0.7576, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 03:15:07\n",
            "loss: 0.7802, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-05-21 03:15:08\n",
            "loss: 0.7850, acc: 0.6664\n",
            "E2E-ABSA >>> 2022-05-21 03:15:08\n",
            ">>> val_acc: 0.6683, val_precision: 0.6683 val_recall: 0.6683, val_f1: 0.6683\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:08\n",
            "loss: 0.7607, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:15:08\n",
            "loss: 0.7831, acc: 0.6580\n",
            "E2E-ABSA >>> 2022-05-21 03:15:09\n",
            "loss: 0.7824, acc: 0.6686\n",
            "E2E-ABSA >>> 2022-05-21 03:15:09\n",
            "loss: 0.7749, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-21 03:15:09\n",
            ">>> val_acc: 0.6635, val_precision: 0.6635 val_recall: 0.6635, val_f1: 0.6635\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:09\n",
            "loss: 0.7054, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-05-21 03:15:09\n",
            "loss: 0.7699, acc: 0.6683\n",
            "E2E-ABSA >>> 2022-05-21 03:15:10\n",
            "loss: 0.7731, acc: 0.6723\n",
            "E2E-ABSA >>> 2022-05-21 03:15:10\n",
            ">>> val_acc: 0.6754, val_precision: 0.6754 val_recall: 0.6754, val_f1: 0.6754\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6754\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:10\n",
            "loss: 0.7466, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:15:10\n",
            "loss: 0.7207, acc: 0.7072\n",
            "E2E-ABSA >>> 2022-05-21 03:15:11\n",
            "loss: 0.7580, acc: 0.6765\n",
            "E2E-ABSA >>> 2022-05-21 03:15:11\n",
            "loss: 0.7533, acc: 0.6805\n",
            "E2E-ABSA >>> 2022-05-21 03:15:11\n",
            ">>> val_acc: 0.6754, val_precision: 0.6754 val_recall: 0.6754, val_f1: 0.6754\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:11\n",
            "loss: 0.7266, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 03:15:11\n",
            "loss: 0.7442, acc: 0.6806\n",
            "E2E-ABSA >>> 2022-05-21 03:15:12\n",
            "loss: 0.7484, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:15:12\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:12\n",
            "loss: 0.7796, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 03:15:12\n",
            "loss: 0.7659, acc: 0.6656\n",
            "E2E-ABSA >>> 2022-05-21 03:15:12\n",
            "loss: 0.7543, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-05-21 03:15:13\n",
            "loss: 0.7509, acc: 0.6769\n",
            "E2E-ABSA >>> 2022-05-21 03:15:13\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:13\n",
            "loss: 0.7253, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-05-21 03:15:13\n",
            "loss: 0.7285, acc: 0.6886\n",
            "E2E-ABSA >>> 2022-05-21 03:15:14\n",
            "loss: 0.7494, acc: 0.6795\n",
            "E2E-ABSA >>> 2022-05-21 03:15:14\n",
            ">>> val_acc: 0.6802, val_precision: 0.6802 val_recall: 0.6802, val_f1: 0.6802\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6802\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:14\n",
            "loss: 0.7347, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:15:14\n",
            "loss: 0.7032, acc: 0.6905\n",
            "E2E-ABSA >>> 2022-05-21 03:15:14\n",
            "loss: 0.7303, acc: 0.6780\n",
            "E2E-ABSA >>> 2022-05-21 03:15:15\n",
            "loss: 0.7385, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:15:15\n",
            ">>> val_acc: 0.6826, val_precision: 0.6826 val_recall: 0.6826, val_f1: 0.6826\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6826\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:15\n",
            "loss: 0.7545, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-05-21 03:15:15\n",
            "loss: 0.7355, acc: 0.6832\n",
            "E2E-ABSA >>> 2022-05-21 03:15:15\n",
            "loss: 0.7324, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-21 03:15:16\n",
            ">>> val_acc: 0.6826, val_precision: 0.6826 val_recall: 0.6826, val_f1: 0.6826\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:16\n",
            "loss: 0.7063, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 03:15:16\n",
            "loss: 0.7259, acc: 0.6932\n",
            "E2E-ABSA >>> 2022-05-21 03:15:16\n",
            "loss: 0.7273, acc: 0.6892\n",
            "E2E-ABSA >>> 2022-05-21 03:15:16\n",
            "loss: 0.7264, acc: 0.6905\n",
            "E2E-ABSA >>> 2022-05-21 03:15:17\n",
            ">>> val_acc: 0.6993, val_precision: 0.6993 val_recall: 0.6993, val_f1: 0.6993\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.6993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:17\n",
            "loss: 0.7367, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:15:17\n",
            "loss: 0.7499, acc: 0.6792\n",
            "E2E-ABSA >>> 2022-05-21 03:15:17\n",
            "loss: 0.7351, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-21 03:15:18\n",
            ">>> val_acc: 0.6993, val_precision: 0.6993 val_recall: 0.6993, val_f1: 0.6993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:18\n",
            "loss: 0.7244, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:15:18\n",
            "loss: 0.7144, acc: 0.6957\n",
            "E2E-ABSA >>> 2022-05-21 03:15:18\n",
            "loss: 0.7245, acc: 0.6957\n",
            "E2E-ABSA >>> 2022-05-21 03:15:18\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:19\n",
            "loss: 0.7256, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 03:15:19\n",
            "loss: 0.6891, acc: 0.7051\n",
            "E2E-ABSA >>> 2022-05-21 03:15:19\n",
            "loss: 0.6976, acc: 0.7006\n",
            "E2E-ABSA >>> 2022-05-21 03:15:19\n",
            "loss: 0.7072, acc: 0.6970\n",
            "E2E-ABSA >>> 2022-05-21 03:15:19\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:20\n",
            "loss: 0.7290, acc: 0.6840\n",
            "E2E-ABSA >>> 2022-05-21 03:15:20\n",
            "loss: 0.7233, acc: 0.6940\n",
            "E2E-ABSA >>> 2022-05-21 03:15:20\n",
            "loss: 0.7155, acc: 0.6963\n",
            "E2E-ABSA >>> 2022-05-21 03:15:20\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:20\n",
            "loss: 0.7276, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-05-21 03:15:21\n",
            "loss: 0.6692, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-05-21 03:15:21\n",
            "loss: 0.6978, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:15:21\n",
            "loss: 0.6976, acc: 0.7088\n",
            "E2E-ABSA >>> 2022-05-21 03:15:21\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:21\n",
            "loss: 0.7027, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-05-21 03:15:22\n",
            "loss: 0.6879, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-05-21 03:15:22\n",
            "loss: 0.6864, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-05-21 03:15:22\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:22\n",
            "loss: 0.6664, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:15:23\n",
            "loss: 0.6729, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-21 03:15:23\n",
            "loss: 0.7007, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-05-21 03:15:23\n",
            "loss: 0.6871, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-05-21 03:15:23\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:23\n",
            "loss: 0.7461, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-05-21 03:15:24\n",
            "loss: 0.7095, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-05-21 03:15:24\n",
            "loss: 0.6985, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-05-21 03:15:24\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:24\n",
            "loss: 0.6887, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:15:24\n",
            "loss: 0.6604, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-05-21 03:15:25\n",
            "loss: 0.6813, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-05-21 03:15:25\n",
            "loss: 0.6873, acc: 0.7085\n",
            "E2E-ABSA >>> 2022-05-21 03:15:25\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:25\n",
            "loss: 0.7076, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:15:25\n",
            "loss: 0.6910, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-05-21 03:15:26\n",
            "loss: 0.6859, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-05-21 03:15:26\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:26\n",
            "loss: 0.7029, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-05-21 03:15:26\n",
            "loss: 0.6687, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-05-21 03:15:27\n",
            "loss: 0.6800, acc: 0.7152\n",
            "E2E-ABSA >>> 2022-05-21 03:15:27\n",
            "loss: 0.6824, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-05-21 03:15:27\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:27\n",
            "loss: 0.6971, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-05-21 03:15:27\n",
            "loss: 0.6837, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-05-21 03:15:28\n",
            "loss: 0.6801, acc: 0.7202\n",
            "E2E-ABSA >>> 2022-05-21 03:15:28\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:28\n",
            "loss: 0.7260, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:15:28\n",
            "loss: 0.6643, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:15:28\n",
            "loss: 0.6575, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-05-21 03:15:29\n",
            "loss: 0.6709, acc: 0.7194\n",
            "E2E-ABSA >>> 2022-05-21 03:15:29\n",
            ">>> val_acc: 0.7160, val_precision: 0.7160 val_recall: 0.7160, val_f1: 0.7160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:29\n",
            "loss: 0.6641, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-05-21 03:15:29\n",
            "loss: 0.6574, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-05-21 03:15:30\n",
            "loss: 0.6790, acc: 0.7202\n",
            "E2E-ABSA >>> 2022-05-21 03:15:30\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:30\n",
            "loss: 0.6370, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-05-21 03:15:30\n",
            "loss: 0.6734, acc: 0.7145\n",
            "E2E-ABSA >>> 2022-05-21 03:15:30\n",
            "loss: 0.6742, acc: 0.7179\n",
            "E2E-ABSA >>> 2022-05-21 03:15:31\n",
            "loss: 0.6660, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-05-21 03:15:31\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:31\n",
            "loss: 0.6526, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:15:31\n",
            "loss: 0.6497, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-05-21 03:15:31\n",
            "loss: 0.6580, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-05-21 03:15:32\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:32\n",
            "loss: 0.6458, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:15:32\n",
            "loss: 0.6870, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:15:32\n",
            "loss: 0.6676, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-05-21 03:15:33\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:33\n",
            "loss: 0.8583, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 03:15:33\n",
            "loss: 0.6602, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-05-21 03:15:33\n",
            "loss: 0.6770, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-05-21 03:15:33\n",
            "loss: 0.6515, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-05-21 03:15:34\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:34\n",
            "loss: 0.6531, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-05-21 03:15:34\n",
            "loss: 0.6640, acc: 0.7174\n",
            "E2E-ABSA >>> 2022-05-21 03:15:34\n",
            "loss: 0.6533, acc: 0.7276\n",
            "E2E-ABSA >>> 2022-05-21 03:15:35\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:35\n",
            "loss: 0.5882, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-05-21 03:15:35\n",
            "loss: 0.6513, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-05-21 03:15:35\n",
            "loss: 0.6563, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-05-21 03:15:35\n",
            "loss: 0.6494, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-05-21 03:15:36\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:36\n",
            "loss: 0.6478, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:15:36\n",
            "loss: 0.6754, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-05-21 03:15:36\n",
            "loss: 0.6596, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-05-21 03:15:36\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:37\n",
            "loss: 0.7187, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:15:37\n",
            "loss: 0.6449, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-05-21 03:15:37\n",
            "loss: 0.6554, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-05-21 03:15:37\n",
            "loss: 0.6478, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-05-21 03:15:37\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:38\n",
            "loss: 0.6615, acc: 0.7102\n",
            "E2E-ABSA >>> 2022-05-21 03:15:38\n",
            "loss: 0.6654, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-05-21 03:15:38\n",
            "loss: 0.6451, acc: 0.7340\n",
            "E2E-ABSA >>> 2022-05-21 03:15:38\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:38\n",
            "loss: 0.6694, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:15:39\n",
            "loss: 0.6490, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-05-21 03:15:39\n",
            "loss: 0.6499, acc: 0.7353\n",
            "E2E-ABSA >>> 2022-05-21 03:15:39\n",
            "loss: 0.6376, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-05-21 03:15:39\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:39\n",
            "loss: 0.6262, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-05-21 03:15:40\n",
            "loss: 0.6303, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-05-21 03:15:40\n",
            "loss: 0.6301, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-05-21 03:15:40\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:40\n",
            "loss: 0.6001, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:15:41\n",
            "loss: 0.6177, acc: 0.7609\n",
            "E2E-ABSA >>> 2022-05-21 03:15:41\n",
            "loss: 0.6281, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-05-21 03:15:41\n",
            "loss: 0.6314, acc: 0.7394\n",
            "E2E-ABSA >>> 2022-05-21 03:15:41\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:41\n",
            "loss: 0.6027, acc: 0.7548\n",
            "E2E-ABSA >>> 2022-05-21 03:15:42\n",
            "loss: 0.6147, acc: 0.7467\n",
            "E2E-ABSA >>> 2022-05-21 03:15:42\n",
            "loss: 0.6189, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:15:42\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:42\n",
            "loss: 0.6101, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:15:42\n",
            "loss: 0.6260, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:15:43\n",
            "loss: 0.6393, acc: 0.7335\n",
            "E2E-ABSA >>> 2022-05-21 03:15:43\n",
            "loss: 0.6287, acc: 0.7347\n",
            "E2E-ABSA >>> 2022-05-21 03:15:43\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:43\n",
            "loss: 0.6296, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-05-21 03:15:43\n",
            "loss: 0.6187, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-05-21 03:15:44\n",
            "loss: 0.6283, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-05-21 03:15:44\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:44\n",
            "loss: 0.6163, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-05-21 03:15:44\n",
            "loss: 0.6387, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-05-21 03:15:44\n",
            "loss: 0.6291, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-05-21 03:15:45\n",
            "loss: 0.6227, acc: 0.7404\n",
            "E2E-ABSA >>> 2022-05-21 03:15:45\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:45\n",
            "loss: 0.5984, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 03:15:45\n",
            "loss: 0.6190, acc: 0.7510\n",
            "E2E-ABSA >>> 2022-05-21 03:15:46\n",
            "loss: 0.6179, acc: 0.7486\n",
            "E2E-ABSA >>> 2022-05-21 03:15:46\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:46\n",
            "loss: 0.6924, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-05-21 03:15:46\n",
            "loss: 0.6302, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-05-21 03:15:46\n",
            "loss: 0.6186, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-05-21 03:15:47\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:47\n",
            "loss: 0.5606, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 03:15:47\n",
            "loss: 0.6225, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:15:47\n",
            "loss: 0.6084, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-05-21 03:15:47\n",
            "loss: 0.6125, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-05-21 03:15:48\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:48\n",
            "loss: 0.6591, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:15:48\n",
            "loss: 0.6165, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-05-21 03:15:48\n",
            "loss: 0.6100, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-05-21 03:15:49\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:49\n",
            "loss: 0.5539, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:15:49\n",
            "loss: 0.6129, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-05-21 03:15:49\n",
            "loss: 0.6072, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-05-21 03:15:49\n",
            "loss: 0.6125, acc: 0.7460\n",
            "E2E-ABSA >>> 2022-05-21 03:15:50\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.747\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:50\n",
            "loss: 0.5962, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-05-21 03:15:50\n",
            "loss: 0.6020, acc: 0.7525\n",
            "E2E-ABSA >>> 2022-05-21 03:15:50\n",
            "loss: 0.6104, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-05-21 03:15:51\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:51\n",
            "loss: 0.5011, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:15:51\n",
            "loss: 0.6130, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-05-21 03:15:51\n",
            "loss: 0.6103, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-05-21 03:15:51\n",
            "loss: 0.6085, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-05-21 03:15:51\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:52\n",
            "loss: 0.5442, acc: 0.7898\n",
            "E2E-ABSA >>> 2022-05-21 03:15:52\n",
            "loss: 0.5771, acc: 0.7680\n",
            "E2E-ABSA >>> 2022-05-21 03:15:52\n",
            "loss: 0.6112, acc: 0.7470\n",
            "E2E-ABSA >>> 2022-05-21 03:15:52\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:52\n",
            "loss: 0.6528, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-05-21 03:15:53\n",
            "loss: 0.6053, acc: 0.7599\n",
            "E2E-ABSA >>> 2022-05-21 03:15:53\n",
            "loss: 0.5946, acc: 0.7629\n",
            "E2E-ABSA >>> 2022-05-21 03:15:53\n",
            "loss: 0.6063, acc: 0.7532\n",
            "E2E-ABSA >>> 2022-05-21 03:15:53\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:53\n",
            "loss: 0.5720, acc: 0.7630\n",
            "E2E-ABSA >>> 2022-05-21 03:15:54\n",
            "loss: 0.5844, acc: 0.7593\n",
            "E2E-ABSA >>> 2022-05-21 03:15:54\n",
            "loss: 0.5950, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:15:54\n",
            ">>> val_acc: 0.7518, val_precision: 0.7518 val_recall: 0.7518, val_f1: 0.7518\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7518\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:54\n",
            "loss: 0.5286, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-05-21 03:15:55\n",
            "loss: 0.5701, acc: 0.7719\n",
            "E2E-ABSA >>> 2022-05-21 03:15:55\n",
            "loss: 0.5866, acc: 0.7723\n",
            "E2E-ABSA >>> 2022-05-21 03:15:55\n",
            "loss: 0.5920, acc: 0.7606\n",
            "E2E-ABSA >>> 2022-05-21 03:15:55\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:55\n",
            "loss: 0.5788, acc: 0.7620\n",
            "E2E-ABSA >>> 2022-05-21 03:15:56\n",
            "loss: 0.6017, acc: 0.7433\n",
            "E2E-ABSA >>> 2022-05-21 03:15:56\n",
            "loss: 0.5978, acc: 0.7485\n",
            "E2E-ABSA >>> 2022-05-21 03:15:56\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:56\n",
            "loss: 0.6273, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:15:56\n",
            "loss: 0.5768, acc: 0.7560\n",
            "E2E-ABSA >>> 2022-05-21 03:15:57\n",
            "loss: 0.5732, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 03:15:57\n",
            "loss: 0.5968, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-05-21 03:15:57\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:57\n",
            "loss: 0.5568, acc: 0.7790\n",
            "E2E-ABSA >>> 2022-05-21 03:15:57\n",
            "loss: 0.5747, acc: 0.7694\n",
            "E2E-ABSA >>> 2022-05-21 03:15:58\n",
            "loss: 0.6008, acc: 0.7493\n",
            "E2E-ABSA >>> 2022-05-21 03:15:58\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:58\n",
            "loss: 0.5307, acc: 0.7768\n",
            "E2E-ABSA >>> 2022-05-21 03:15:58\n",
            "loss: 0.5762, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-05-21 03:15:59\n",
            "loss: 0.5741, acc: 0.7669\n",
            "E2E-ABSA >>> 2022-05-21 03:15:59\n",
            "loss: 0.5918, acc: 0.7572\n",
            "E2E-ABSA >>> 2022-05-21 03:15:59\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:15:59\n",
            "loss: 0.5778, acc: 0.7833\n",
            "E2E-ABSA >>> 2022-05-21 03:15:59\n",
            "loss: 0.5692, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-05-21 03:16:00\n",
            "loss: 0.5920, acc: 0.7618\n",
            "E2E-ABSA >>> 2022-05-21 03:16:00\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:00\n",
            "loss: 0.5751, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-05-21 03:16:00\n",
            "loss: 0.5720, acc: 0.7649\n",
            "E2E-ABSA >>> 2022-05-21 03:16:00\n",
            "loss: 0.5803, acc: 0.7582\n",
            "E2E-ABSA >>> 2022-05-21 03:16:01\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:01\n",
            "loss: 0.7117, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:16:01\n",
            "loss: 0.6032, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-05-21 03:16:01\n",
            "loss: 0.5839, acc: 0.7661\n",
            "E2E-ABSA >>> 2022-05-21 03:16:02\n",
            "loss: 0.5937, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-05-21 03:16:02\n",
            ">>> val_acc: 0.7518, val_precision: 0.7518 val_recall: 0.7518, val_f1: 0.7518\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:02\n",
            "loss: 0.6171, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:16:02\n",
            "loss: 0.6130, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-05-21 03:16:02\n",
            "loss: 0.5893, acc: 0.7572\n",
            "E2E-ABSA >>> 2022-05-21 03:16:03\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:03\n",
            "loss: 0.6051, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:16:03\n",
            "loss: 0.5639, acc: 0.7555\n",
            "E2E-ABSA >>> 2022-05-21 03:16:03\n",
            "loss: 0.6019, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-05-21 03:16:03\n",
            "loss: 0.5828, acc: 0.7606\n",
            "E2E-ABSA >>> 2022-05-21 03:16:04\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:04\n",
            "loss: 0.6076, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:16:04\n",
            "loss: 0.5910, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-05-21 03:16:04\n",
            "loss: 0.5747, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 03:16:05\n",
            ">>> val_acc: 0.7255, val_precision: 0.7255 val_recall: 0.7255, val_f1: 0.7255\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:05\n",
            "loss: 0.5733, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:16:05\n",
            "loss: 0.5875, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-05-21 03:16:05\n",
            "loss: 0.5770, acc: 0.7689\n",
            "E2E-ABSA >>> 2022-05-21 03:16:05\n",
            "loss: 0.5751, acc: 0.7669\n",
            "E2E-ABSA >>> 2022-05-21 03:16:05\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:06\n",
            "loss: 0.6661, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-05-21 03:16:06\n",
            "loss: 0.5824, acc: 0.7608\n",
            "E2E-ABSA >>> 2022-05-21 03:16:06\n",
            "loss: 0.5733, acc: 0.7675\n",
            "E2E-ABSA >>> 2022-05-21 03:16:06\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:06\n",
            "loss: 0.5187, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 03:16:07\n",
            "loss: 0.5711, acc: 0.7632\n",
            "E2E-ABSA >>> 2022-05-21 03:16:07\n",
            "loss: 0.5765, acc: 0.7675\n",
            "E2E-ABSA >>> 2022-05-21 03:16:07\n",
            "loss: 0.5716, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-05-21 03:16:07\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:07\n",
            "loss: 0.5974, acc: 0.7318\n",
            "E2E-ABSA >>> 2022-05-21 03:16:08\n",
            "loss: 0.6075, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-05-21 03:16:08\n",
            "loss: 0.5814, acc: 0.7515\n",
            "E2E-ABSA >>> 2022-05-21 03:16:08\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:08\n",
            "loss: 0.5325, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 03:16:09\n",
            "loss: 0.5996, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:16:09\n",
            "loss: 0.5629, acc: 0.7696\n",
            "E2E-ABSA >>> 2022-05-21 03:16:09\n",
            "loss: 0.5699, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:16:09\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:09\n",
            "loss: 0.5762, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:16:10\n",
            "loss: 0.5724, acc: 0.7533\n",
            "E2E-ABSA >>> 2022-05-21 03:16:10\n",
            "loss: 0.5796, acc: 0.7565\n",
            "E2E-ABSA >>> 2022-05-21 03:16:10\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">> saved: state_dict/memnet_twitter_know_val_f1_0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:10\n",
            "loss: 0.5957, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-05-21 03:16:10\n",
            "loss: 0.5484, acc: 0.7723\n",
            "E2E-ABSA >>> 2022-05-21 03:16:11\n",
            "loss: 0.5859, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-05-21 03:16:11\n",
            "loss: 0.5697, acc: 0.7659\n",
            "E2E-ABSA >>> 2022-05-21 03:16:11\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:11\n",
            "loss: 0.5619, acc: 0.7790\n",
            "E2E-ABSA >>> 2022-05-21 03:16:11\n",
            "loss: 0.5615, acc: 0.7694\n",
            "E2E-ABSA >>> 2022-05-21 03:16:12\n",
            "loss: 0.5595, acc: 0.7692\n",
            "E2E-ABSA >>> 2022-05-21 03:16:12\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:12\n",
            "loss: 0.5218, acc: 0.7991\n",
            "E2E-ABSA >>> 2022-05-21 03:16:12\n",
            "loss: 0.5534, acc: 0.7798\n",
            "E2E-ABSA >>> 2022-05-21 03:16:13\n",
            "loss: 0.5574, acc: 0.7736\n",
            "E2E-ABSA >>> 2022-05-21 03:16:13\n",
            "loss: 0.5644, acc: 0.7680\n",
            "E2E-ABSA >>> 2022-05-21 03:16:13\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:13\n",
            "loss: 0.5393, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:16:13\n",
            "loss: 0.5647, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 03:16:14\n",
            "loss: 0.5634, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-05-21 03:16:14\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:14\n",
            "loss: 0.5994, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:16:14\n",
            "loss: 0.5904, acc: 0.7514\n",
            "E2E-ABSA >>> 2022-05-21 03:16:14\n",
            "loss: 0.5807, acc: 0.7533\n",
            "E2E-ABSA >>> 2022-05-21 03:16:15\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:15\n",
            "loss: 0.5879, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:16:15\n",
            "loss: 0.5631, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:16:15\n",
            "loss: 0.5650, acc: 0.7702\n",
            "E2E-ABSA >>> 2022-05-21 03:16:16\n",
            "loss: 0.5637, acc: 0.7683\n",
            "E2E-ABSA >>> 2022-05-21 03:16:16\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:16\n",
            "loss: 0.5317, acc: 0.7847\n",
            "E2E-ABSA >>> 2022-05-21 03:16:16\n",
            "loss: 0.5478, acc: 0.7826\n",
            "E2E-ABSA >>> 2022-05-21 03:16:16\n",
            "loss: 0.5533, acc: 0.7732\n",
            "E2E-ABSA >>> 2022-05-21 03:16:17\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:17\n",
            "loss: 0.6880, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:16:17\n",
            "loss: 0.5953, acc: 0.7463\n",
            "E2E-ABSA >>> 2022-05-21 03:16:17\n",
            "loss: 0.5633, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:16:18\n",
            "loss: 0.5573, acc: 0.7693\n",
            "E2E-ABSA >>> 2022-05-21 03:16:18\n",
            ">>> val_acc: 0.7494, val_precision: 0.7494 val_recall: 0.7494, val_f1: 0.7494\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:18\n",
            "loss: 0.6205, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:16:18\n",
            "loss: 0.5753, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-05-21 03:16:18\n",
            "loss: 0.5534, acc: 0.7711\n",
            "E2E-ABSA >>> 2022-05-21 03:16:19\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:19\n",
            "loss: 0.6653, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:16:19\n",
            "loss: 0.5486, acc: 0.7778\n",
            "E2E-ABSA >>> 2022-05-21 03:16:19\n",
            "loss: 0.5630, acc: 0.7680\n",
            "E2E-ABSA >>> 2022-05-21 03:16:19\n",
            "loss: 0.5519, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-05-21 03:16:20\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:20\n",
            "loss: 0.5749, acc: 0.7756\n",
            "E2E-ABSA >>> 2022-05-21 03:16:20\n",
            "loss: 0.5477, acc: 0.7825\n",
            "E2E-ABSA >>> 2022-05-21 03:16:20\n",
            "loss: 0.5497, acc: 0.7790\n",
            "E2E-ABSA >>> 2022-05-21 03:16:20\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:21\n",
            "loss: 0.5488, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-05-21 03:16:21\n",
            "loss: 0.5935, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-05-21 03:16:21\n",
            "loss: 0.5642, acc: 0.7601\n",
            "E2E-ABSA >>> 2022-05-21 03:16:21\n",
            "loss: 0.5557, acc: 0.7685\n",
            "E2E-ABSA >>> 2022-05-21 03:16:21\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:22\n",
            "loss: 0.5329, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:16:22\n",
            "loss: 0.5601, acc: 0.7512\n",
            "E2E-ABSA >>> 2022-05-21 03:16:22\n",
            "loss: 0.5488, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-05-21 03:16:22\n",
            ">>> val_acc: 0.7518, val_precision: 0.7518 val_recall: 0.7518, val_f1: 0.7518\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:22\n",
            "loss: 0.5238, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-05-21 03:16:23\n",
            "loss: 0.5262, acc: 0.7828\n",
            "E2E-ABSA >>> 2022-05-21 03:16:23\n",
            "loss: 0.5440, acc: 0.7714\n",
            "E2E-ABSA >>> 2022-05-21 03:16:23\n",
            "loss: 0.5511, acc: 0.7706\n",
            "E2E-ABSA >>> 2022-05-21 03:16:23\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:23\n",
            "loss: 0.5086, acc: 0.8005\n",
            "E2E-ABSA >>> 2022-05-21 03:16:24\n",
            "loss: 0.5436, acc: 0.7701\n",
            "E2E-ABSA >>> 2022-05-21 03:16:24\n",
            "loss: 0.5474, acc: 0.7762\n",
            "E2E-ABSA >>> 2022-05-21 03:16:24\n",
            ">>> val_acc: 0.7446, val_precision: 0.7446 val_recall: 0.7446, val_f1: 0.7446\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:24\n",
            "loss: 0.5102, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 03:16:25\n",
            "loss: 0.5390, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:16:25\n",
            "loss: 0.5413, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-05-21 03:16:25\n",
            "loss: 0.5454, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:16:25\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:25\n",
            "loss: 0.5591, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-05-21 03:16:26\n",
            "loss: 0.5510, acc: 0.7726\n",
            "E2E-ABSA >>> 2022-05-21 03:16:26\n",
            "loss: 0.5489, acc: 0.7749\n",
            "E2E-ABSA >>> 2022-05-21 03:16:26\n",
            ">>> val_acc: 0.7518, val_precision: 0.7518 val_recall: 0.7518, val_f1: 0.7518\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:26\n",
            "loss: 0.5490, acc: 0.7902\n",
            "E2E-ABSA >>> 2022-05-21 03:16:26\n",
            "loss: 0.5468, acc: 0.7855\n",
            "E2E-ABSA >>> 2022-05-21 03:16:27\n",
            "loss: 0.5346, acc: 0.7872\n",
            "E2E-ABSA >>> 2022-05-21 03:16:27\n",
            "loss: 0.5435, acc: 0.7794\n",
            "E2E-ABSA >>> 2022-05-21 03:16:27\n",
            ">>> val_acc: 0.7470, val_precision: 0.7470 val_recall: 0.7470, val_f1: 0.7470\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:27\n",
            "loss: 0.5263, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-05-21 03:16:28\n",
            "loss: 0.5253, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-05-21 03:16:28\n",
            "loss: 0.5366, acc: 0.7736\n",
            "E2E-ABSA >>> 2022-05-21 03:16:28\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:28\n",
            "loss: 0.5195, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-05-21 03:16:28\n",
            "loss: 0.5256, acc: 0.7785\n",
            "E2E-ABSA >>> 2022-05-21 03:16:29\n",
            "loss: 0.5374, acc: 0.7821\n",
            "E2E-ABSA >>> 2022-05-21 03:16:29\n",
            ">>> val_acc: 0.7375, val_precision: 0.7375 val_recall: 0.7375, val_f1: 0.7375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:29\n",
            "loss: 0.5132, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:16:29\n",
            "loss: 0.5686, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-05-21 03:16:29\n",
            "loss: 0.5396, acc: 0.7762\n",
            "E2E-ABSA >>> 2022-05-21 03:16:30\n",
            "loss: 0.5297, acc: 0.7819\n",
            "E2E-ABSA >>> 2022-05-21 03:16:30\n",
            ">>> val_acc: 0.7303, val_precision: 0.7303 val_recall: 0.7303, val_f1: 0.7303\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:30\n",
            "loss: 0.6031, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-05-21 03:16:30\n",
            "loss: 0.5734, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-05-21 03:16:30\n",
            "loss: 0.5514, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-05-21 03:16:31\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:31\n",
            "loss: 0.5902, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 03:16:31\n",
            "loss: 0.5305, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-05-21 03:16:31\n",
            "loss: 0.5305, acc: 0.7793\n",
            "E2E-ABSA >>> 2022-05-21 03:16:32\n",
            "loss: 0.5424, acc: 0.7753\n",
            "E2E-ABSA >>> 2022-05-21 03:16:32\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:32\n",
            "loss: 0.5286, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 03:16:32\n",
            "loss: 0.5077, acc: 0.7900\n",
            "E2E-ABSA >>> 2022-05-21 03:16:32\n",
            "loss: 0.5259, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-05-21 03:16:33\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:33\n",
            "loss: 0.5276, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:16:33\n",
            "loss: 0.5205, acc: 0.7778\n",
            "E2E-ABSA >>> 2022-05-21 03:16:33\n",
            "loss: 0.5327, acc: 0.7784\n",
            "E2E-ABSA >>> 2022-05-21 03:16:33\n",
            "loss: 0.5324, acc: 0.7793\n",
            "E2E-ABSA >>> 2022-05-21 03:16:34\n",
            ">>> val_acc: 0.7208, val_precision: 0.7208 val_recall: 0.7208, val_f1: 0.7208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:34\n",
            "loss: 0.5174, acc: 0.7898\n",
            "E2E-ABSA >>> 2022-05-21 03:16:34\n",
            "loss: 0.5238, acc: 0.7788\n",
            "E2E-ABSA >>> 2022-05-21 03:16:34\n",
            "loss: 0.5266, acc: 0.7790\n",
            "E2E-ABSA >>> 2022-05-21 03:16:35\n",
            ">>> val_acc: 0.7399, val_precision: 0.7399 val_recall: 0.7399, val_f1: 0.7399\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:35\n",
            "loss: 0.5081, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 03:16:35\n",
            "loss: 0.5352, acc: 0.7664\n",
            "E2E-ABSA >>> 2022-05-21 03:16:35\n",
            "loss: 0.5143, acc: 0.7803\n",
            "E2E-ABSA >>> 2022-05-21 03:16:35\n",
            "loss: 0.5357, acc: 0.7698\n",
            "E2E-ABSA >>> 2022-05-21 03:16:36\n",
            ">>> val_acc: 0.7279, val_precision: 0.7279 val_recall: 0.7279, val_f1: 0.7279\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-05-21 03:16:36\n",
            "loss: 0.5019, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 03:16:36\n",
            "loss: 0.5186, acc: 0.7859\n",
            "E2E-ABSA >>> 2022-05-21 03:16:36\n",
            "loss: 0.5242, acc: 0.7865\n",
            "E2E-ABSA >>> 2022-05-21 03:16:36\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            "you can download the best model from state_dict/memnet_twitter_know_val_f1_0.7542\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.7542, test_precision: 0.7542, test_recall: 0.7542, test_f1: 0.7542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Twitter** dataset on model(**cabasc**)"
      ],
      "metadata": {
        "id": "UKxsWM1VnCfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset twitter --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wn9OI5LFnCwD",
        "outputId": "f8da5474-bbd4-4aa1-88bd-62f3be48cac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1154.\n",
            "> testing dataset count: 310.\n",
            "cuda memory allocated: 13017600\n",
            "> n_trainable_params: 1065405, n_nontrainable_params: 2188000\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f9cc20d7b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:25\n",
            "loss: 1.3107, acc: 0.1792\n",
            "E2E-ABSA >>> 2022-05-21 03:22:26\n",
            "loss: 1.2671, acc: 0.1927\n",
            "E2E-ABSA >>> 2022-05-21 03:22:26\n",
            ">>> val_acc: 0.2355, val_precision: 0.2355 val_recall: 0.2355, val_f1: 0.2355\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.2355\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:27\n",
            "loss: 1.1371, acc: 0.2794\n",
            "E2E-ABSA >>> 2022-05-21 03:22:27\n",
            "loss: 1.1179, acc: 0.3205\n",
            "E2E-ABSA >>> 2022-05-21 03:22:28\n",
            ">>> val_acc: 0.5097, val_precision: 0.5097 val_recall: 0.5097, val_f1: 0.5097\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.5097\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:29\n",
            "loss: 1.0189, acc: 0.5000\n",
            "E2E-ABSA >>> 2022-05-21 03:22:29\n",
            "loss: 1.0172, acc: 0.5221\n",
            "E2E-ABSA >>> 2022-05-21 03:22:30\n",
            "loss: 0.9854, acc: 0.5820\n",
            "E2E-ABSA >>> 2022-05-21 03:22:31\n",
            ">>> val_acc: 0.6516, val_precision: 0.6516 val_recall: 0.6516, val_f1: 0.6516\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6516\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:31\n",
            "loss: 0.9283, acc: 0.6161\n",
            "E2E-ABSA >>> 2022-05-21 03:22:32\n",
            "loss: 0.9262, acc: 0.6140\n",
            "E2E-ABSA >>> 2022-05-21 03:22:33\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:33\n",
            "loss: 0.9017, acc: 0.6016\n",
            "E2E-ABSA >>> 2022-05-21 03:22:34\n",
            "loss: 0.8675, acc: 0.6382\n",
            "E2E-ABSA >>> 2022-05-21 03:22:34\n",
            "loss: 0.8641, acc: 0.6425\n",
            "E2E-ABSA >>> 2022-05-21 03:22:35\n",
            ">>> val_acc: 0.6581, val_precision: 0.6581 val_recall: 0.6581, val_f1: 0.6581\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:35\n",
            "loss: 0.8457, acc: 0.6525\n",
            "E2E-ABSA >>> 2022-05-21 03:22:36\n",
            "loss: 0.8468, acc: 0.6409\n",
            "E2E-ABSA >>> 2022-05-21 03:22:37\n",
            ">>> val_acc: 0.6548, val_precision: 0.6548 val_recall: 0.6548, val_f1: 0.6548\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:37\n",
            "loss: 0.8320, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-05-21 03:22:38\n",
            "loss: 0.8224, acc: 0.6369\n",
            "E2E-ABSA >>> 2022-05-21 03:22:39\n",
            "loss: 0.8211, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-05-21 03:22:39\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:40\n",
            "loss: 0.8095, acc: 0.6315\n",
            "E2E-ABSA >>> 2022-05-21 03:22:40\n",
            "loss: 0.8136, acc: 0.6462\n",
            "E2E-ABSA >>> 2022-05-21 03:22:41\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:41\n",
            "loss: 0.8359, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-21 03:22:42\n",
            "loss: 0.8144, acc: 0.6386\n",
            "E2E-ABSA >>> 2022-05-21 03:22:43\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:43\n",
            "loss: 0.6935, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:22:44\n",
            "loss: 0.8157, acc: 0.6534\n",
            "E2E-ABSA >>> 2022-05-21 03:22:45\n",
            "loss: 0.7935, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-05-21 03:22:45\n",
            ">>> val_acc: 0.6613, val_precision: 0.6613 val_recall: 0.6613, val_f1: 0.6613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:46\n",
            "loss: 0.7600, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:22:46\n",
            "loss: 0.7987, acc: 0.6587\n",
            "E2E-ABSA >>> 2022-05-21 03:22:47\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:48\n",
            "loss: 0.7719, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-05-21 03:22:48\n",
            "loss: 0.7835, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-05-21 03:22:49\n",
            "loss: 0.7890, acc: 0.6614\n",
            "E2E-ABSA >>> 2022-05-21 03:22:49\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.671\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:50\n",
            "loss: 0.7588, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:22:51\n",
            "loss: 0.7756, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 03:22:52\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:52\n",
            "loss: 0.7367, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-05-21 03:22:53\n",
            "loss: 0.8035, acc: 0.6418\n",
            "E2E-ABSA >>> 2022-05-21 03:22:53\n",
            "loss: 0.7707, acc: 0.6699\n",
            "E2E-ABSA >>> 2022-05-21 03:22:54\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:54\n",
            "loss: 0.7757, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:22:55\n",
            "loss: 0.7652, acc: 0.6659\n",
            "E2E-ABSA >>> 2022-05-21 03:22:56\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:56\n",
            "loss: 0.7325, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-05-21 03:22:57\n",
            "loss: 0.7545, acc: 0.6819\n",
            "E2E-ABSA >>> 2022-05-21 03:22:58\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 03:22:58\n",
            "loss: 0.6479, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:22:59\n",
            "loss: 0.7747, acc: 0.6680\n",
            "E2E-ABSA >>> 2022-05-21 03:22:59\n",
            "loss: 0.7611, acc: 0.6784\n",
            "E2E-ABSA >>> 2022-05-21 03:23:00\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:00\n",
            "loss: 0.7591, acc: 0.6809\n",
            "E2E-ABSA >>> 2022-05-21 03:23:01\n",
            "loss: 0.7276, acc: 0.6888\n",
            "E2E-ABSA >>> 2022-05-21 03:23:02\n",
            ">>> val_acc: 0.6645, val_precision: 0.6645 val_recall: 0.6645, val_f1: 0.6645\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:02\n",
            "loss: 0.7969, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:23:03\n",
            "loss: 0.7401, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 03:23:04\n",
            "loss: 0.7497, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-05-21 03:23:04\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:05\n",
            "loss: 0.7152, acc: 0.6984\n",
            "E2E-ABSA >>> 2022-05-21 03:23:06\n",
            "loss: 0.7337, acc: 0.6887\n",
            "E2E-ABSA >>> 2022-05-21 03:23:06\n",
            ">>> val_acc: 0.6742, val_precision: 0.6742 val_recall: 0.6742, val_f1: 0.6742\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6742\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:07\n",
            "loss: 0.7989, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-05-21 03:23:07\n",
            "loss: 0.7265, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-05-21 03:23:08\n",
            "loss: 0.7426, acc: 0.6848\n",
            "E2E-ABSA >>> 2022-05-21 03:23:08\n",
            ">>> val_acc: 0.6710, val_precision: 0.6710 val_recall: 0.6710, val_f1: 0.6710\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:09\n",
            "loss: 0.6951, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-05-21 03:23:10\n",
            "loss: 0.7241, acc: 0.6897\n",
            "E2E-ABSA >>> 2022-05-21 03:23:11\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:11\n",
            "loss: 0.7845, acc: 0.6741\n",
            "E2E-ABSA >>> 2022-05-21 03:23:12\n",
            "loss: 0.7389, acc: 0.6932\n",
            "E2E-ABSA >>> 2022-05-21 03:23:13\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:13\n",
            "loss: 0.7349, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 03:23:13\n",
            "loss: 0.7353, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:23:14\n",
            "loss: 0.7284, acc: 0.6926\n",
            "E2E-ABSA >>> 2022-05-21 03:23:15\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:15\n",
            "loss: 0.7501, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 03:23:16\n",
            "loss: 0.7209, acc: 0.6966\n",
            "E2E-ABSA >>> 2022-05-21 03:23:17\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:17\n",
            "loss: 0.6454, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-05-21 03:23:18\n",
            "loss: 0.7214, acc: 0.6839\n",
            "E2E-ABSA >>> 2022-05-21 03:23:18\n",
            "loss: 0.7171, acc: 0.6971\n",
            "E2E-ABSA >>> 2022-05-21 03:23:19\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:20\n",
            "loss: 0.6747, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:23:20\n",
            "loss: 0.7139, acc: 0.7007\n",
            "E2E-ABSA >>> 2022-05-21 03:23:21\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:21\n",
            "loss: 0.6965, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-05-21 03:23:22\n",
            "loss: 0.6989, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-05-21 03:23:23\n",
            "loss: 0.7067, acc: 0.7047\n",
            "E2E-ABSA >>> 2022-05-21 03:23:23\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:24\n",
            "loss: 0.7042, acc: 0.7067\n",
            "E2E-ABSA >>> 2022-05-21 03:23:24\n",
            "loss: 0.7042, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:23:25\n",
            ">>> val_acc: 0.6774, val_precision: 0.6774 val_recall: 0.6774, val_f1: 0.6774\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:26\n",
            "loss: 0.6340, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-05-21 03:23:26\n",
            "loss: 0.7008, acc: 0.7078\n",
            "E2E-ABSA >>> 2022-05-21 03:23:27\n",
            "loss: 0.7069, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-05-21 03:23:27\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:28\n",
            "loss: 0.7323, acc: 0.6854\n",
            "E2E-ABSA >>> 2022-05-21 03:23:29\n",
            "loss: 0.7200, acc: 0.6948\n",
            "E2E-ABSA >>> 2022-05-21 03:23:29\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:30\n",
            "loss: 0.7060, acc: 0.7169\n",
            "E2E-ABSA >>> 2022-05-21 03:23:31\n",
            "loss: 0.7226, acc: 0.6955\n",
            "E2E-ABSA >>> 2022-05-21 03:23:32\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:32\n",
            "loss: 0.6638, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:23:32\n",
            "loss: 0.7399, acc: 0.6691\n",
            "E2E-ABSA >>> 2022-05-21 03:23:33\n",
            "loss: 0.6985, acc: 0.7080\n",
            "E2E-ABSA >>> 2022-05-21 03:23:34\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:34\n",
            "loss: 0.7126, acc: 0.6935\n",
            "E2E-ABSA >>> 2022-05-21 03:23:35\n",
            "loss: 0.6997, acc: 0.7047\n",
            "E2E-ABSA >>> 2022-05-21 03:23:36\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:36\n",
            "loss: 0.6693, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:23:37\n",
            "loss: 0.6767, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:23:37\n",
            "loss: 0.6856, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-05-21 03:23:38\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:38\n",
            "loss: 0.6628, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-05-21 03:23:39\n",
            "loss: 0.6691, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-05-21 03:23:40\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:40\n",
            "loss: 0.6803, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:23:41\n",
            "loss: 0.7096, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-05-21 03:23:42\n",
            "loss: 0.6907, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-05-21 03:23:42\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:43\n",
            "loss: 0.6760, acc: 0.7112\n",
            "E2E-ABSA >>> 2022-05-21 03:23:43\n",
            "loss: 0.6765, acc: 0.7140\n",
            "E2E-ABSA >>> 2022-05-21 03:23:44\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:45\n",
            "loss: 0.6936, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-05-21 03:23:45\n",
            "loss: 0.6839, acc: 0.7052\n",
            "E2E-ABSA >>> 2022-05-21 03:23:46\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:46\n",
            "loss: 0.6877, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 03:23:47\n",
            "loss: 0.6773, acc: 0.7121\n",
            "E2E-ABSA >>> 2022-05-21 03:23:48\n",
            "loss: 0.6834, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-05-21 03:23:48\n",
            ">>> val_acc: 0.6806, val_precision: 0.6806 val_recall: 0.6806, val_f1: 0.6806\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:49\n",
            "loss: 0.7098, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:23:50\n",
            "loss: 0.6758, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-05-21 03:23:50\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:51\n",
            "loss: 0.6740, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:23:51\n",
            "loss: 0.6803, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-05-21 03:23:52\n",
            "loss: 0.6777, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-05-21 03:23:52\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:53\n",
            "loss: 0.6901, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:23:54\n",
            "loss: 0.6817, acc: 0.7130\n",
            "E2E-ABSA >>> 2022-05-21 03:23:55\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:55\n",
            "loss: 0.7139, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-05-21 03:23:56\n",
            "loss: 0.6994, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-05-21 03:23:56\n",
            "loss: 0.6748, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-05-21 03:23:57\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:57\n",
            "loss: 0.7020, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:23:58\n",
            "loss: 0.6775, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-05-21 03:23:59\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 03:23:59\n",
            "loss: 0.6839, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-05-21 03:24:00\n",
            "loss: 0.6597, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-05-21 03:24:01\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:01\n",
            "loss: 0.8739, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-05-21 03:24:02\n",
            "loss: 0.6681, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-05-21 03:24:02\n",
            "loss: 0.6663, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:24:03\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:04\n",
            "loss: 0.7000, acc: 0.6941\n",
            "E2E-ABSA >>> 2022-05-21 03:24:04\n",
            "loss: 0.6693, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-05-21 03:24:05\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:05\n",
            "loss: 0.5625, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-05-21 03:24:06\n",
            "loss: 0.6608, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-05-21 03:24:07\n",
            "loss: 0.6685, acc: 0.7169\n",
            "E2E-ABSA >>> 2022-05-21 03:24:07\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:08\n",
            "loss: 0.7254, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:24:09\n",
            "loss: 0.6700, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-05-21 03:24:09\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:10\n",
            "loss: 0.5903, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-05-21 03:24:10\n",
            "loss: 0.6436, acc: 0.7234\n",
            "E2E-ABSA >>> 2022-05-21 03:24:11\n",
            "loss: 0.6671, acc: 0.7152\n",
            "E2E-ABSA >>> 2022-05-21 03:24:11\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:12\n",
            "loss: 0.6840, acc: 0.6921\n",
            "E2E-ABSA >>> 2022-05-21 03:24:13\n",
            "loss: 0.6681, acc: 0.7138\n",
            "E2E-ABSA >>> 2022-05-21 03:24:14\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:14\n",
            "loss: 0.6523, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:24:15\n",
            "loss: 0.6664, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-05-21 03:24:16\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:16\n",
            "loss: 0.6275, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:24:16\n",
            "loss: 0.6594, acc: 0.7278\n",
            "E2E-ABSA >>> 2022-05-21 03:24:17\n",
            "loss: 0.6587, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-05-21 03:24:18\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:18\n",
            "loss: 0.6331, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-05-21 03:24:19\n",
            "loss: 0.6532, acc: 0.7253\n",
            "E2E-ABSA >>> 2022-05-21 03:24:20\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:20\n",
            "loss: 0.7010, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-05-21 03:24:21\n",
            "loss: 0.6357, acc: 0.7446\n",
            "E2E-ABSA >>> 2022-05-21 03:24:21\n",
            "loss: 0.6569, acc: 0.7279\n",
            "E2E-ABSA >>> 2022-05-21 03:24:22\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:22\n",
            "loss: 0.6337, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-05-21 03:24:23\n",
            "loss: 0.6545, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-05-21 03:24:24\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:24\n",
            "loss: 0.6457, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-05-21 03:24:25\n",
            "loss: 0.6597, acc: 0.7179\n",
            "E2E-ABSA >>> 2022-05-21 03:24:26\n",
            "loss: 0.6511, acc: 0.7228\n",
            "E2E-ABSA >>> 2022-05-21 03:24:26\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:27\n",
            "loss: 0.6617, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-05-21 03:24:28\n",
            "loss: 0.6614, acc: 0.7199\n",
            "E2E-ABSA >>> 2022-05-21 03:24:28\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:29\n",
            "loss: 0.6424, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-05-21 03:24:29\n",
            "loss: 0.6363, acc: 0.7340\n",
            "E2E-ABSA >>> 2022-05-21 03:24:30\n",
            "loss: 0.6525, acc: 0.7201\n",
            "E2E-ABSA >>> 2022-05-21 03:24:30\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:31\n",
            "loss: 0.6420, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-05-21 03:24:32\n",
            "loss: 0.6504, acc: 0.7229\n",
            "E2E-ABSA >>> 2022-05-21 03:24:33\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:33\n",
            "loss: 0.6453, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-05-21 03:24:34\n",
            "loss: 0.6407, acc: 0.7247\n",
            "E2E-ABSA >>> 2022-05-21 03:24:35\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:35\n",
            "loss: 0.5736, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:24:36\n",
            "loss: 0.6468, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-05-21 03:24:36\n",
            "loss: 0.6498, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-05-21 03:24:37\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.7\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:37\n",
            "loss: 0.6836, acc: 0.6815\n",
            "E2E-ABSA >>> 2022-05-21 03:24:38\n",
            "loss: 0.6463, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-05-21 03:24:39\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:39\n",
            "loss: 0.7648, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-05-21 03:24:40\n",
            "loss: 0.6661, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-05-21 03:24:41\n",
            "loss: 0.6446, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-05-21 03:24:41\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:42\n",
            "loss: 0.6259, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:24:42\n",
            "loss: 0.6463, acc: 0.7182\n",
            "E2E-ABSA >>> 2022-05-21 03:24:43\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:43\n",
            "loss: 0.5884, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:24:44\n",
            "loss: 0.6316, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:24:45\n",
            "loss: 0.6432, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-05-21 03:24:45\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:46\n",
            "loss: 0.6352, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-05-21 03:24:47\n",
            "loss: 0.6560, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-05-21 03:24:47\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:48\n",
            "loss: 0.6885, acc: 0.6914\n",
            "E2E-ABSA >>> 2022-05-21 03:24:48\n",
            "loss: 0.6284, acc: 0.7296\n",
            "E2E-ABSA >>> 2022-05-21 03:24:49\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">> saved: state_dict/cabasc_twitter_val_f1_0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:50\n",
            "loss: 0.8371, acc: 0.5833\n",
            "E2E-ABSA >>> 2022-05-21 03:24:50\n",
            "loss: 0.6291, acc: 0.7178\n",
            "E2E-ABSA >>> 2022-05-21 03:24:51\n",
            "loss: 0.6356, acc: 0.7331\n",
            "E2E-ABSA >>> 2022-05-21 03:24:52\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:52\n",
            "loss: 0.5471, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 03:24:53\n",
            "loss: 0.6113, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 03:24:54\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:54\n",
            "loss: 0.6678, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-05-21 03:24:55\n",
            "loss: 0.6398, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-05-21 03:24:55\n",
            "loss: 0.6355, acc: 0.7351\n",
            "E2E-ABSA >>> 2022-05-21 03:24:56\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:56\n",
            "loss: 0.6056, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 03:24:57\n",
            "loss: 0.6327, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-05-21 03:24:58\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-05-21 03:24:58\n",
            "loss: 0.6063, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:24:59\n",
            "loss: 0.6324, acc: 0.7393\n",
            "E2E-ABSA >>> 2022-05-21 03:25:00\n",
            "loss: 0.6330, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-05-21 03:25:00\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:01\n",
            "loss: 0.6425, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-05-21 03:25:01\n",
            "loss: 0.6352, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-05-21 03:25:02\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:02\n",
            "loss: 0.6386, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:25:03\n",
            "loss: 0.6442, acc: 0.7236\n",
            "E2E-ABSA >>> 2022-05-21 03:25:04\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:04\n",
            "loss: 0.4925, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-05-21 03:25:05\n",
            "loss: 0.6218, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:25:06\n",
            "loss: 0.6273, acc: 0.7369\n",
            "E2E-ABSA >>> 2022-05-21 03:25:06\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:07\n",
            "loss: 0.6491, acc: 0.7007\n",
            "E2E-ABSA >>> 2022-05-21 03:25:08\n",
            "loss: 0.6161, acc: 0.7347\n",
            "E2E-ABSA >>> 2022-05-21 03:25:08\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:09\n",
            "loss: 0.5472, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-05-21 03:25:09\n",
            "loss: 0.6074, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-05-21 03:25:10\n",
            "loss: 0.6212, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-05-21 03:25:11\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:11\n",
            "loss: 0.6018, acc: 0.7527\n",
            "E2E-ABSA >>> 2022-05-21 03:25:12\n",
            "loss: 0.6205, acc: 0.7370\n",
            "E2E-ABSA >>> 2022-05-21 03:25:13\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:13\n",
            "loss: 0.6158, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-05-21 03:25:14\n",
            "loss: 0.6272, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:25:14\n",
            "loss: 0.6222, acc: 0.7366\n",
            "E2E-ABSA >>> 2022-05-21 03:25:15\n",
            ">>> val_acc: 0.6871, val_precision: 0.6871 val_recall: 0.6871, val_f1: 0.6871\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:16\n",
            "loss: 0.6692, acc: 0.7130\n",
            "E2E-ABSA >>> 2022-05-21 03:25:16\n",
            "loss: 0.6448, acc: 0.7237\n",
            "E2E-ABSA >>> 2022-05-21 03:25:17\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:17\n",
            "loss: 0.5766, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-05-21 03:25:18\n",
            "loss: 0.5984, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:25:19\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:19\n",
            "loss: 0.5068, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-21 03:25:20\n",
            "loss: 0.6514, acc: 0.7198\n",
            "E2E-ABSA >>> 2022-05-21 03:25:21\n",
            "loss: 0.6326, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-05-21 03:25:21\n",
            ">>> val_acc: 0.6968, val_precision: 0.6968 val_recall: 0.6968, val_f1: 0.6968\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:22\n",
            "loss: 0.6610, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:25:22\n",
            "loss: 0.6253, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-05-21 03:25:23\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:23\n",
            "loss: 0.5091, acc: 0.8250\n",
            "E2E-ABSA >>> 2022-05-21 03:25:24\n",
            "loss: 0.5812, acc: 0.7714\n",
            "E2E-ABSA >>> 2022-05-21 03:25:25\n",
            "loss: 0.6222, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-05-21 03:25:25\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:26\n",
            "loss: 0.6285, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-05-21 03:25:27\n",
            "loss: 0.6231, acc: 0.7356\n",
            "E2E-ABSA >>> 2022-05-21 03:25:28\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:28\n",
            "loss: 0.6641, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-05-21 03:25:28\n",
            "loss: 0.5946, acc: 0.7612\n",
            "E2E-ABSA >>> 2022-05-21 03:25:29\n",
            "loss: 0.6153, acc: 0.7382\n",
            "E2E-ABSA >>> 2022-05-21 03:25:30\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:30\n",
            "loss: 0.6184, acc: 0.7524\n",
            "E2E-ABSA >>> 2022-05-21 03:25:31\n",
            "loss: 0.6256, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-05-21 03:25:32\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:32\n",
            "loss: 0.6250, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-05-21 03:25:33\n",
            "loss: 0.6060, acc: 0.7529\n",
            "E2E-ABSA >>> 2022-05-21 03:25:33\n",
            "loss: 0.6168, acc: 0.7444\n",
            "E2E-ABSA >>> 2022-05-21 03:25:34\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:35\n",
            "loss: 0.6087, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-05-21 03:25:35\n",
            "loss: 0.6160, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-05-21 03:25:36\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:36\n",
            "loss: 0.5950, acc: 0.7463\n",
            "E2E-ABSA >>> 2022-05-21 03:25:37\n",
            "loss: 0.6034, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:25:38\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:38\n",
            "loss: 0.5745, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-05-21 03:25:39\n",
            "loss: 0.6314, acc: 0.7335\n",
            "E2E-ABSA >>> 2022-05-21 03:25:40\n",
            "loss: 0.6212, acc: 0.7334\n",
            "E2E-ABSA >>> 2022-05-21 03:25:40\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:41\n",
            "loss: 0.6072, acc: 0.7470\n",
            "E2E-ABSA >>> 2022-05-21 03:25:41\n",
            "loss: 0.6134, acc: 0.7463\n",
            "E2E-ABSA >>> 2022-05-21 03:25:42\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:42\n",
            "loss: 0.5873, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:25:43\n",
            "loss: 0.5810, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:25:44\n",
            "loss: 0.6096, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-05-21 03:25:44\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:45\n",
            "loss: 0.6006, acc: 0.7450\n",
            "E2E-ABSA >>> 2022-05-21 03:25:46\n",
            "loss: 0.6176, acc: 0.7341\n",
            "E2E-ABSA >>> 2022-05-21 03:25:47\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:47\n",
            "loss: 0.5342, acc: 0.7865\n",
            "E2E-ABSA >>> 2022-05-21 03:25:48\n",
            "loss: 0.6098, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:25:48\n",
            "loss: 0.6101, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-05-21 03:25:49\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:50\n",
            "loss: 0.6232, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-05-21 03:25:50\n",
            "loss: 0.6174, acc: 0.7362\n",
            "E2E-ABSA >>> 2022-05-21 03:25:51\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:51\n",
            "loss: 0.6302, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-05-21 03:25:52\n",
            "loss: 0.6039, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-05-21 03:25:53\n",
            ">>> val_acc: 0.6903, val_precision: 0.6903 val_recall: 0.6903, val_f1: 0.6903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-05-21 03:25:53\n",
            "loss: 0.6325, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:25:54\n",
            "loss: 0.5804, acc: 0.7595\n",
            "E2E-ABSA >>> 2022-05-21 03:25:55\n",
            "loss: 0.6026, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-05-21 03:25:55\n",
            ">>> val_acc: 0.6935, val_precision: 0.6935 val_recall: 0.6935, val_f1: 0.6935\n",
            "you can download the best model from state_dict/cabasc_twitter_val_f1_0.7032\n",
            ">>> test_acc: 0.7032, test_precision: 0.7032, test_recall: 0.7032, test_f1: 0.7032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后: Training **Twitter** dataset on model(**cabasc**)"
      ],
      "metadata": {
        "id": "3eOyBSFOnC9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset twitter_know --embed_dim 200 --patience 50 --log_step 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtVmB55KnDKW",
        "outputId": "3078e7c9-71a6-4a31-8c70-4a971661f7a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_twitter_embedding_matrix.dat       models\n",
            "200_twitter_know_embedding_matrix.dat  __pycache__\n",
            "datasets\t\t\t       README.md\n",
            "data_utils.py\t\t\t       requirements.txt\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.twitter.27B.200d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1245.\n",
            "> testing dataset count: 332.\n",
            "cuda memory allocated: 19779072\n",
            "> n_trainable_params: 1065405, n_nontrainable_params: 3878400\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f7c43913b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 30\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 50\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-21 03:26:31\n",
            "loss: 1.2914, acc: 0.1833\n",
            "E2E-ABSA >>> 2022-05-21 03:26:33\n",
            "loss: 1.2386, acc: 0.2125\n",
            "E2E-ABSA >>> 2022-05-21 03:26:35\n",
            ">>> val_acc: 0.2861, val_precision: 0.2861 val_recall: 0.2861, val_f1: 0.2861\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.2861\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-21 03:26:36\n",
            "loss: 1.0956, acc: 0.3385\n",
            "E2E-ABSA >>> 2022-05-21 03:26:37\n",
            "loss: 1.0714, acc: 0.4122\n",
            "E2E-ABSA >>> 2022-05-21 03:26:39\n",
            "loss: 1.0432, acc: 0.4818\n",
            "E2E-ABSA >>> 2022-05-21 03:26:40\n",
            ">>> val_acc: 0.6114, val_precision: 0.6114 val_recall: 0.6114, val_f1: 0.6114\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6114\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-21 03:26:42\n",
            "loss: 0.9585, acc: 0.6302\n",
            "E2E-ABSA >>> 2022-05-21 03:26:43\n",
            "loss: 0.9424, acc: 0.6331\n",
            "E2E-ABSA >>> 2022-05-21 03:26:45\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-21 03:26:46\n",
            "loss: 0.8947, acc: 0.6146\n",
            "E2E-ABSA >>> 2022-05-21 03:26:47\n",
            "loss: 0.8668, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-05-21 03:26:49\n",
            "loss: 0.8719, acc: 0.6553\n",
            "E2E-ABSA >>> 2022-05-21 03:26:51\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-21 03:26:52\n",
            "loss: 0.8690, acc: 0.6389\n",
            "E2E-ABSA >>> 2022-05-21 03:26:53\n",
            "loss: 0.8601, acc: 0.6458\n",
            "E2E-ABSA >>> 2022-05-21 03:26:55\n",
            "loss: 0.8418, acc: 0.6530\n",
            "E2E-ABSA >>> 2022-05-21 03:26:56\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-21 03:26:58\n",
            "loss: 0.8201, acc: 0.6583\n",
            "E2E-ABSA >>> 2022-05-21 03:26:59\n",
            "loss: 0.8333, acc: 0.6510\n",
            "E2E-ABSA >>> 2022-05-21 03:27:01\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:02\n",
            "loss: 0.7814, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-05-21 03:27:03\n",
            "loss: 0.8000, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-05-21 03:27:05\n",
            "loss: 0.8167, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:27:06\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:08\n",
            "loss: 0.7875, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-05-21 03:27:09\n",
            "loss: 0.7997, acc: 0.6597\n",
            "E2E-ABSA >>> 2022-05-21 03:27:12\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:12\n",
            "loss: 0.8029, acc: 0.6458\n",
            "E2E-ABSA >>> 2022-05-21 03:27:14\n",
            "loss: 0.7789, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 03:27:15\n",
            "loss: 0.7879, acc: 0.6648\n",
            "E2E-ABSA >>> 2022-05-21 03:27:17\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:18\n",
            "loss: 0.7887, acc: 0.6597\n",
            "E2E-ABSA >>> 2022-05-21 03:27:20\n",
            "loss: 0.8173, acc: 0.6432\n",
            "E2E-ABSA >>> 2022-05-21 03:27:21\n",
            "loss: 0.8003, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:27:22\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:24\n",
            "loss: 0.7705, acc: 0.6854\n",
            "E2E-ABSA >>> 2022-05-21 03:27:25\n",
            "loss: 0.7778, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-05-21 03:27:27\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:28\n",
            "loss: 0.8360, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-05-21 03:27:30\n",
            "loss: 0.8093, acc: 0.6488\n",
            "E2E-ABSA >>> 2022-05-21 03:27:31\n",
            "loss: 0.7910, acc: 0.6597\n",
            "E2E-ABSA >>> 2022-05-21 03:27:32\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:34\n",
            "loss: 0.7547, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 03:27:35\n",
            "loss: 0.7745, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-05-21 03:27:38\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:38\n",
            "loss: 0.7021, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:27:40\n",
            "loss: 0.7712, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-05-21 03:27:41\n",
            "loss: 0.7895, acc: 0.6553\n",
            "E2E-ABSA >>> 2022-05-21 03:27:43\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:44\n",
            "loss: 0.7960, acc: 0.6528\n",
            "E2E-ABSA >>> 2022-05-21 03:27:45\n",
            "loss: 0.7961, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:27:47\n",
            "loss: 0.7817, acc: 0.6594\n",
            "E2E-ABSA >>> 2022-05-21 03:27:48\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:50\n",
            "loss: 0.7750, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-05-21 03:27:51\n",
            "loss: 0.7745, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-05-21 03:27:53\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-21 03:27:54\n",
            "loss: 0.8300, acc: 0.6302\n",
            "E2E-ABSA >>> 2022-05-21 03:27:55\n",
            "loss: 0.7629, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:27:57\n",
            "loss: 0.7698, acc: 0.6658\n",
            "E2E-ABSA >>> 2022-05-21 03:27:58\n",
            ">>> val_acc: 0.6747, val_precision: 0.6747 val_recall: 0.6747, val_f1: 0.6747\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6747\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:00\n",
            "loss: 0.7565, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-05-21 03:28:01\n",
            "loss: 0.7689, acc: 0.6620\n",
            "E2E-ABSA >>> 2022-05-21 03:28:04\n",
            ">>> val_acc: 0.6747, val_precision: 0.6747 val_recall: 0.6747, val_f1: 0.6747\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:04\n",
            "loss: 0.7324, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 03:28:06\n",
            "loss: 0.7562, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 03:28:07\n",
            "loss: 0.7692, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-05-21 03:28:09\n",
            ">>> val_acc: 0.6777, val_precision: 0.6777 val_recall: 0.6777, val_f1: 0.6777\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6777\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:10\n",
            "loss: 0.8000, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 03:28:11\n",
            "loss: 0.7769, acc: 0.6680\n",
            "E2E-ABSA >>> 2022-05-21 03:28:13\n",
            "loss: 0.7600, acc: 0.6795\n",
            "E2E-ABSA >>> 2022-05-21 03:28:14\n",
            ">>> val_acc: 0.6747, val_precision: 0.6747 val_recall: 0.6747, val_f1: 0.6747\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:16\n",
            "loss: 0.7444, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:28:17\n",
            "loss: 0.7536, acc: 0.6885\n",
            "E2E-ABSA >>> 2022-05-21 03:28:19\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:20\n",
            "loss: 0.7403, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:28:22\n",
            "loss: 0.7555, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-21 03:28:23\n",
            "loss: 0.7467, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:28:24\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:26\n",
            "loss: 0.7181, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-05-21 03:28:27\n",
            "loss: 0.7579, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-05-21 03:28:30\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:30\n",
            "loss: 0.7267, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:28:32\n",
            "loss: 0.7861, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 03:28:33\n",
            "loss: 0.7492, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:28:35\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:36\n",
            "loss: 0.7104, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-05-21 03:28:37\n",
            "loss: 0.7394, acc: 0.6901\n",
            "E2E-ABSA >>> 2022-05-21 03:28:39\n",
            "loss: 0.7374, acc: 0.6924\n",
            "E2E-ABSA >>> 2022-05-21 03:28:40\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:42\n",
            "loss: 0.7319, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-05-21 03:28:43\n",
            "loss: 0.7212, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:28:45\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:46\n",
            "loss: 0.7717, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:28:47\n",
            "loss: 0.7161, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-05-21 03:28:49\n",
            "loss: 0.7187, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:28:50\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:52\n",
            "loss: 0.7321, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 03:28:53\n",
            "loss: 0.7214, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:28:55\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-21 03:28:56\n",
            "loss: 0.7721, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-21 03:28:57\n",
            "loss: 0.7052, acc: 0.6997\n",
            "E2E-ABSA >>> 2022-05-21 03:28:59\n",
            "loss: 0.7059, acc: 0.7055\n",
            "E2E-ABSA >>> 2022-05-21 03:29:01\n",
            ">>> val_acc: 0.6807, val_precision: 0.6807 val_recall: 0.6807, val_f1: 0.6807\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6807\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:02\n",
            "loss: 0.6956, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-05-21 03:29:03\n",
            "loss: 0.7063, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-05-21 03:29:05\n",
            "loss: 0.7208, acc: 0.6980\n",
            "E2E-ABSA >>> 2022-05-21 03:29:06\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:08\n",
            "loss: 0.7186, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:29:09\n",
            "loss: 0.7228, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-05-21 03:29:11\n",
            ">>> val_acc: 0.6837, val_precision: 0.6837 val_recall: 0.6837, val_f1: 0.6837\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:12\n",
            "loss: 0.6928, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:29:13\n",
            "loss: 0.7151, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:29:15\n",
            "loss: 0.7245, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-05-21 03:29:16\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:18\n",
            "loss: 0.6838, acc: 0.7370\n",
            "E2E-ABSA >>> 2022-05-21 03:29:19\n",
            "loss: 0.6953, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-05-21 03:29:22\n",
            ">>> val_acc: 0.6867, val_precision: 0.6867 val_recall: 0.6867, val_f1: 0.6867\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:22\n",
            "loss: 0.6753, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:29:24\n",
            "loss: 0.7361, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-05-21 03:29:25\n",
            "loss: 0.7096, acc: 0.7064\n",
            "E2E-ABSA >>> 2022-05-21 03:29:27\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:28\n",
            "loss: 0.6936, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-05-21 03:29:29\n",
            "loss: 0.7155, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:29:31\n",
            "loss: 0.7083, acc: 0.7060\n",
            "E2E-ABSA >>> 2022-05-21 03:29:32\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:34\n",
            "loss: 0.7090, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:29:35\n",
            "loss: 0.7081, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-05-21 03:29:37\n",
            ">>> val_acc: 0.6898, val_precision: 0.6898 val_recall: 0.6898, val_f1: 0.6898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:38\n",
            "loss: 0.6158, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 03:29:40\n",
            "loss: 0.6886, acc: 0.7217\n",
            "E2E-ABSA >>> 2022-05-21 03:29:41\n",
            "loss: 0.7050, acc: 0.7066\n",
            "E2E-ABSA >>> 2022-05-21 03:29:42\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:44\n",
            "loss: 0.6741, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-05-21 03:29:45\n",
            "loss: 0.6871, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-05-21 03:29:48\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:48\n",
            "loss: 0.6991, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:29:50\n",
            "loss: 0.7030, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:29:51\n",
            "loss: 0.6976, acc: 0.7055\n",
            "E2E-ABSA >>> 2022-05-21 03:29:53\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-21 03:29:54\n",
            "loss: 0.7003, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:29:55\n",
            "loss: 0.7197, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-05-21 03:29:57\n",
            "loss: 0.6992, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-05-21 03:29:58\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:00\n",
            "loss: 0.7203, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-05-21 03:30:03\n",
            ">>> val_acc: 0.6958, val_precision: 0.6958 val_recall: 0.6958, val_f1: 0.6958\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6958\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:04\n",
            "loss: 0.7071, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-21 03:30:06\n",
            "loss: 0.6828, acc: 0.7217\n",
            "E2E-ABSA >>> 2022-05-21 03:30:07\n",
            "loss: 0.6988, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:30:08\n",
            ">>> val_acc: 0.6988, val_precision: 0.6988 val_recall: 0.6988, val_f1: 0.6988\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.6988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:10\n",
            "loss: 0.7082, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-05-21 03:30:11\n",
            "loss: 0.7106, acc: 0.7002\n",
            "E2E-ABSA >>> 2022-05-21 03:30:14\n",
            ">>> val_acc: 0.6988, val_precision: 0.6988 val_recall: 0.6988, val_f1: 0.6988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:14\n",
            "loss: 0.5804, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-05-21 03:30:16\n",
            "loss: 0.6708, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-05-21 03:30:17\n",
            "loss: 0.6952, acc: 0.7064\n",
            "E2E-ABSA >>> 2022-05-21 03:30:19\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:20\n",
            "loss: 0.7062, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:30:22\n",
            "loss: 0.7045, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:30:23\n",
            "loss: 0.6913, acc: 0.7084\n",
            "E2E-ABSA >>> 2022-05-21 03:30:24\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:26\n",
            "loss: 0.6936, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:30:27\n",
            "loss: 0.7024, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-05-21 03:30:29\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:30\n",
            "loss: 0.6611, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:30:32\n",
            "loss: 0.6758, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:30:33\n",
            "loss: 0.6915, acc: 0.7092\n",
            "E2E-ABSA >>> 2022-05-21 03:30:34\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:36\n",
            "loss: 0.6654, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-05-21 03:30:37\n",
            "loss: 0.6863, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:30:40\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:40\n",
            "loss: 0.7606, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 03:30:42\n",
            "loss: 0.6677, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-05-21 03:30:43\n",
            "loss: 0.6836, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-05-21 03:30:45\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:46\n",
            "loss: 0.6688, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-05-21 03:30:48\n",
            "loss: 0.6684, acc: 0.7331\n",
            "E2E-ABSA >>> 2022-05-21 03:30:49\n",
            "loss: 0.6846, acc: 0.7100\n",
            "E2E-ABSA >>> 2022-05-21 03:30:50\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:52\n",
            "loss: 0.6853, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-05-21 03:30:53\n",
            "loss: 0.6879, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-05-21 03:30:55\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-21 03:30:56\n",
            "loss: 0.6605, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:30:58\n",
            "loss: 0.6812, acc: 0.7217\n",
            "E2E-ABSA >>> 2022-05-21 03:30:59\n",
            "loss: 0.6815, acc: 0.7127\n",
            "E2E-ABSA >>> 2022-05-21 03:31:00\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:02\n",
            "loss: 0.7401, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:31:03\n",
            "loss: 0.7042, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-05-21 03:31:06\n",
            ">>> val_acc: 0.7018, val_precision: 0.7018 val_recall: 0.7018, val_f1: 0.7018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:06\n",
            "loss: 0.6893, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:31:08\n",
            "loss: 0.6533, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-05-21 03:31:09\n",
            "loss: 0.6703, acc: 0.7197\n",
            "E2E-ABSA >>> 2022-05-21 03:31:11\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:12\n",
            "loss: 0.7000, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-05-21 03:31:13\n",
            "loss: 0.6907, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:31:15\n",
            "loss: 0.6779, acc: 0.7133\n",
            "E2E-ABSA >>> 2022-05-21 03:31:16\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:18\n",
            "loss: 0.6693, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-05-21 03:31:19\n",
            "loss: 0.6842, acc: 0.7073\n",
            "E2E-ABSA >>> 2022-05-21 03:31:21\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:22\n",
            "loss: 0.7032, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-05-21 03:31:23\n",
            "loss: 0.6558, acc: 0.7247\n",
            "E2E-ABSA >>> 2022-05-21 03:31:25\n",
            "loss: 0.6784, acc: 0.7101\n",
            "E2E-ABSA >>> 2022-05-21 03:31:26\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:28\n",
            "loss: 0.7093, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:31:29\n",
            "loss: 0.6732, acc: 0.7072\n",
            "E2E-ABSA >>> 2022-05-21 03:31:31\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:32\n",
            "loss: 0.6868, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-21 03:31:33\n",
            "loss: 0.6838, acc: 0.7066\n",
            "E2E-ABSA >>> 2022-05-21 03:31:35\n",
            "loss: 0.6764, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-05-21 03:31:36\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:37\n",
            "loss: 0.7254, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-21 03:31:39\n",
            "loss: 0.6735, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-05-21 03:31:41\n",
            "loss: 0.6720, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-05-21 03:31:42\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:43\n",
            "loss: 0.6484, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-05-21 03:31:45\n",
            "loss: 0.6771, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:31:47\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:48\n",
            "loss: 0.6989, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-21 03:31:49\n",
            "loss: 0.6560, acc: 0.7158\n",
            "E2E-ABSA >>> 2022-05-21 03:31:51\n",
            "loss: 0.6694, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-05-21 03:31:52\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:53\n",
            "loss: 0.6440, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-21 03:31:55\n",
            "loss: 0.6621, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-05-21 03:31:57\n",
            ">>> val_acc: 0.7048, val_precision: 0.7048 val_recall: 0.7048, val_f1: 0.7048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-05-21 03:31:57\n",
            "loss: 0.7116, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-05-21 03:31:59\n",
            "loss: 0.7113, acc: 0.6806\n",
            "E2E-ABSA >>> 2022-05-21 03:32:01\n",
            "loss: 0.6757, acc: 0.7064\n",
            "E2E-ABSA >>> 2022-05-21 03:32:02\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:03\n",
            "loss: 0.6650, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-05-21 03:32:05\n",
            "loss: 0.6871, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-05-21 03:32:07\n",
            "loss: 0.6666, acc: 0.7181\n",
            "E2E-ABSA >>> 2022-05-21 03:32:07\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:09\n",
            "loss: 0.6623, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-05-21 03:32:11\n",
            "loss: 0.6643, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-05-21 03:32:13\n",
            ">>> val_acc: 0.7108, val_precision: 0.7108 val_recall: 0.7108, val_f1: 0.7108\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.7108\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:13\n",
            "loss: 0.6655, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-21 03:32:15\n",
            "loss: 0.6710, acc: 0.7158\n",
            "E2E-ABSA >>> 2022-05-21 03:32:17\n",
            "loss: 0.6630, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-05-21 03:32:18\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:19\n",
            "loss: 0.6088, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-05-21 03:32:21\n",
            "loss: 0.6466, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 03:32:23\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:23\n",
            "loss: 0.6573, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:32:25\n",
            "loss: 0.6476, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:32:27\n",
            "loss: 0.6530, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-05-21 03:32:28\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:29\n",
            "loss: 0.6608, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:32:31\n",
            "loss: 0.6778, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-05-21 03:32:32\n",
            "loss: 0.6612, acc: 0.7213\n",
            "E2E-ABSA >>> 2022-05-21 03:32:33\n",
            ">>> val_acc: 0.7078, val_precision: 0.7078 val_recall: 0.7078, val_f1: 0.7078\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:35\n",
            "loss: 0.6270, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:32:37\n",
            "loss: 0.6661, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-05-21 03:32:38\n",
            ">>> val_acc: 0.7108, val_precision: 0.7108 val_recall: 0.7108, val_f1: 0.7108\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:39\n",
            "loss: 0.6556, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:32:41\n",
            "loss: 0.6572, acc: 0.7158\n",
            "E2E-ABSA >>> 2022-05-21 03:32:43\n",
            "loss: 0.6579, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-05-21 03:32:44\n",
            ">>> val_acc: 0.7108, val_precision: 0.7108 val_recall: 0.7108, val_f1: 0.7108\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:45\n",
            "loss: 0.6997, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-05-21 03:32:47\n",
            "loss: 0.6673, acc: 0.7234\n",
            "E2E-ABSA >>> 2022-05-21 03:32:49\n",
            ">>> val_acc: 0.7139, val_precision: 0.7139 val_recall: 0.7139, val_f1: 0.7139\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.7139\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:49\n",
            "loss: 0.5676, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-05-21 03:32:51\n",
            "loss: 0.6125, acc: 0.7587\n",
            "E2E-ABSA >>> 2022-05-21 03:32:53\n",
            "loss: 0.6443, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-05-21 03:32:54\n",
            ">>> val_acc: 0.7108, val_precision: 0.7108 val_recall: 0.7108, val_f1: 0.7108\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-05-21 03:32:55\n",
            "loss: 0.6365, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:32:57\n",
            "loss: 0.6518, acc: 0.7331\n",
            "E2E-ABSA >>> 2022-05-21 03:32:58\n",
            "loss: 0.6563, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-05-21 03:32:59\n",
            ">>> val_acc: 0.7108, val_precision: 0.7108 val_recall: 0.7108, val_f1: 0.7108\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:01\n",
            "loss: 0.7025, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-05-21 03:33:03\n",
            "loss: 0.6782, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-05-21 03:33:04\n",
            ">>> val_acc: 0.7139, val_precision: 0.7139 val_recall: 0.7139, val_f1: 0.7139\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:05\n",
            "loss: 0.6419, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-05-21 03:33:07\n",
            "loss: 0.6405, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-05-21 03:33:08\n",
            "loss: 0.6481, acc: 0.7300\n",
            "E2E-ABSA >>> 2022-05-21 03:33:10\n",
            ">>> val_acc: 0.7169, val_precision: 0.7169 val_recall: 0.7169, val_f1: 0.7169\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.7169\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:11\n",
            "loss: 0.7126, acc: 0.6901\n",
            "E2E-ABSA >>> 2022-05-21 03:33:13\n",
            "loss: 0.6745, acc: 0.7141\n",
            "E2E-ABSA >>> 2022-05-21 03:33:15\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:15\n",
            "loss: 0.5808, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-05-21 03:33:17\n",
            "loss: 0.6185, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-05-21 03:33:18\n",
            "loss: 0.6547, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-05-21 03:33:20\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:21\n",
            "loss: 0.6467, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-05-21 03:33:23\n",
            "loss: 0.6623, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-05-21 03:33:24\n",
            "loss: 0.6506, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-05-21 03:33:25\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:27\n",
            "loss: 0.6484, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-21 03:33:28\n",
            "loss: 0.6404, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-05-21 03:33:30\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:31\n",
            "loss: 0.6935, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-21 03:33:32\n",
            "loss: 0.6490, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-05-21 03:33:34\n",
            "loss: 0.6476, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-05-21 03:33:35\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:37\n",
            "loss: 0.6746, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-21 03:33:38\n",
            "loss: 0.6375, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-05-21 03:33:40\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:41\n",
            "loss: 0.7893, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-21 03:33:42\n",
            "loss: 0.6425, acc: 0.7413\n",
            "E2E-ABSA >>> 2022-05-21 03:33:44\n",
            "loss: 0.6395, acc: 0.7348\n",
            "E2E-ABSA >>> 2022-05-21 03:33:46\n",
            ">>> val_acc: 0.7169, val_precision: 0.7169 val_recall: 0.7169, val_f1: 0.7169\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:47\n",
            "loss: 0.6111, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-05-21 03:33:48\n",
            "loss: 0.6244, acc: 0.7357\n",
            "E2E-ABSA >>> 2022-05-21 03:33:50\n",
            "loss: 0.6462, acc: 0.7301\n",
            "E2E-ABSA >>> 2022-05-21 03:33:51\n",
            ">>> val_acc: 0.7139, val_precision: 0.7139 val_recall: 0.7139, val_f1: 0.7139\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:52\n",
            "loss: 0.6851, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:33:54\n",
            "loss: 0.6470, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-05-21 03:33:56\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-05-21 03:33:56\n",
            "loss: 0.6728, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:33:58\n",
            "loss: 0.6471, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-05-21 03:34:00\n",
            "loss: 0.6472, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-05-21 03:34:01\n",
            ">>> val_acc: 0.7259, val_precision: 0.7259 val_recall: 0.7259, val_f1: 0.7259\n",
            ">> saved: state_dict/cabasc_twitter_know_val_f1_0.7259\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:02\n",
            "loss: 0.6342, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-05-21 03:34:04\n",
            "loss: 0.6427, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-05-21 03:34:06\n",
            ">>> val_acc: 0.7139, val_precision: 0.7139 val_recall: 0.7139, val_f1: 0.7139\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:06\n",
            "loss: 0.5489, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-05-21 03:34:08\n",
            "loss: 0.6405, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-05-21 03:34:10\n",
            "loss: 0.6316, acc: 0.7491\n",
            "E2E-ABSA >>> 2022-05-21 03:34:11\n",
            ">>> val_acc: 0.7169, val_precision: 0.7169 val_recall: 0.7169, val_f1: 0.7169\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:12\n",
            "loss: 0.6347, acc: 0.7569\n",
            "E2E-ABSA >>> 2022-05-21 03:34:14\n",
            "loss: 0.6360, acc: 0.7487\n",
            "E2E-ABSA >>> 2022-05-21 03:34:15\n",
            "loss: 0.6414, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-05-21 03:34:16\n",
            ">>> val_acc: 0.7139, val_precision: 0.7139 val_recall: 0.7139, val_f1: 0.7139\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:18\n",
            "loss: 0.6556, acc: 0.7167\n",
            "E2E-ABSA >>> 2022-05-21 03:34:20\n",
            "loss: 0.6415, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-05-21 03:34:21\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:22\n",
            "loss: 0.6224, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-21 03:34:24\n",
            "loss: 0.6265, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-05-21 03:34:25\n",
            "loss: 0.6394, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-05-21 03:34:26\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:28\n",
            "loss: 0.6748, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:34:29\n",
            "loss: 0.6421, acc: 0.7269\n",
            "E2E-ABSA >>> 2022-05-21 03:34:32\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:32\n",
            "loss: 0.6797, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:34:33\n",
            "loss: 0.6475, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-05-21 03:34:35\n",
            "loss: 0.6425, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-05-21 03:34:37\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:38\n",
            "loss: 0.6325, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-05-21 03:34:39\n",
            "loss: 0.6448, acc: 0.7318\n",
            "E2E-ABSA >>> 2022-05-21 03:34:41\n",
            "loss: 0.6364, acc: 0.7382\n",
            "E2E-ABSA >>> 2022-05-21 03:34:42\n",
            ">>> val_acc: 0.7199, val_precision: 0.7199 val_recall: 0.7199, val_f1: 0.7199\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:43\n",
            "loss: 0.6725, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-05-21 03:34:45\n",
            "loss: 0.6412, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-05-21 03:34:47\n",
            ">>> val_acc: 0.7259, val_precision: 0.7259 val_recall: 0.7259, val_f1: 0.7259\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:48\n",
            "loss: 0.6570, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-05-21 03:34:49\n",
            "loss: 0.6468, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-05-21 03:34:51\n",
            "loss: 0.6323, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-05-21 03:34:52\n",
            ">>> val_acc: 0.7259, val_precision: 0.7259 val_recall: 0.7259, val_f1: 0.7259\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:53\n",
            "loss: 0.6453, acc: 0.7474\n",
            "E2E-ABSA >>> 2022-05-21 03:34:55\n",
            "loss: 0.6405, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:34:57\n",
            ">>> val_acc: 0.7169, val_precision: 0.7169 val_recall: 0.7169, val_f1: 0.7169\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-05-21 03:34:58\n",
            "loss: 0.5660, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-05-21 03:34:59\n",
            "loss: 0.6252, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-21 03:35:01\n",
            "loss: 0.6170, acc: 0.7481\n",
            "E2E-ABSA >>> 2022-05-21 03:35:02\n",
            ">>> val_acc: 0.7229, val_precision: 0.7229 val_recall: 0.7229, val_f1: 0.7229\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-05-21 03:35:03\n",
            "loss: 0.6434, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-05-21 03:35:05\n",
            "loss: 0.6298, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-05-21 03:35:07\n",
            "loss: 0.6320, acc: 0.7398\n",
            "E2E-ABSA >>> 2022-05-21 03:35:07\n",
            ">>> val_acc: 0.7229, val_precision: 0.7229 val_recall: 0.7229, val_f1: 0.7229\n",
            "you can download the best model from state_dict/cabasc_twitter_know_val_f1_0.7259\n",
            ">>> test_acc: 0.7259, test_precision: 0.7259, test_recall: 0.7259, test_f1: 0.7259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 针对SemEval201X以及acl2014data的notebook上实验\n",
        "Training **SemEval2014** dataset on model(**LSTM**)"
      ],
      "metadata": {
        "id": "zkHL4QNxR_tQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset SemEval2014 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhR3OCr8SACA",
        "outputId": "b570588b-76f1-4c0a-b507-21842d55d610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_SemEval2014_embedding_matrix.dat   models\n",
            "200_twitter_embedding_matrix.dat       __pycache__\n",
            "200_twitter_know_embedding_matrix.dat  README.md\n",
            "datasets\t\t\t       requirements.txt\n",
            "data_utils.py\t\t\t       SemEval2014_tokenizer.dat\n",
            "deberta_abas.ipynb\t\t       state_dict\n",
            "glove_embeddings\t\t       train.py\n",
            "infer_example.py\t\t       twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t       twitter_tokenizer.dat\n",
            "加载Bert...\n",
            "loading tokenizer: SemEval2014_tokenizer.dat\n",
            "加载预训练向量...\n",
            "loading embedding_matrix: 200_SemEval2014_embedding_matrix.dat\n",
            "预训练向量加载完毕.\n",
            "['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> n_trainable_params: 603303, n_nontrainable_params: 1333200\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f28cf53c9e0>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cpu\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 07:07:39\n",
            "loss: 1.0772, acc: 0.4012\n",
            "E2E-ABSA >>> 2022-05-20 07:07:48\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.577\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 07:07:48\n",
            "loss: 1.0263, acc: 0.5104\n",
            "E2E-ABSA >>> 2022-05-20 07:07:59\n",
            "loss: 0.9756, acc: 0.5584\n",
            "E2E-ABSA >>> 2022-05-20 07:08:08\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 07:08:09\n",
            "loss: 0.8996, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-05-20 07:08:19\n",
            "loss: 0.9520, acc: 0.5670\n",
            "E2E-ABSA >>> 2022-05-20 07:08:28\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 07:08:30\n",
            "loss: 0.8937, acc: 0.6285\n",
            "E2E-ABSA >>> 2022-05-20 07:08:43\n",
            "loss: 0.9382, acc: 0.5699\n",
            "E2E-ABSA >>> 2022-05-20 07:08:51\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 07:08:53\n",
            "loss: 0.9657, acc: 0.5234\n",
            "E2E-ABSA >>> 2022-05-20 07:09:03\n",
            "loss: 0.9306, acc: 0.5680\n",
            "E2E-ABSA >>> 2022-05-20 07:09:10\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 07:09:13\n",
            "loss: 0.8707, acc: 0.5958\n",
            "E2E-ABSA >>> 2022-05-20 07:09:23\n",
            "loss: 0.9013, acc: 0.5846\n",
            "E2E-ABSA >>> 2022-05-20 07:09:29\n",
            ">>> val_acc: 0.5891, val_precision: 0.5891 val_recall: 0.5891, val_f1: 0.5891\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.5891\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 07:09:33\n",
            "loss: 0.8909, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-05-20 07:09:43\n",
            "loss: 0.8777, acc: 0.6002\n",
            "E2E-ABSA >>> 2022-05-20 07:09:49\n",
            ">>> val_acc: 0.6314, val_precision: 0.6314 val_recall: 0.6314, val_f1: 0.6314\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.6314\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 07:09:53\n",
            "loss: 0.8721, acc: 0.6012\n",
            "E2E-ABSA >>> 2022-05-20 07:10:03\n",
            "loss: 0.8313, acc: 0.6351\n",
            "E2E-ABSA >>> 2022-05-20 07:10:08\n",
            ">>> val_acc: 0.6495, val_precision: 0.6495 val_recall: 0.6495, val_f1: 0.6495\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.6495\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 07:10:13\n",
            "loss: 0.8000, acc: 0.6354\n",
            "E2E-ABSA >>> 2022-05-20 07:10:23\n",
            "loss: 0.8018, acc: 0.6486\n",
            "E2E-ABSA >>> 2022-05-20 07:10:28\n",
            ">>> val_acc: 0.6526, val_precision: 0.6526 val_recall: 0.6526, val_f1: 0.6526\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.6526\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 07:10:33\n",
            "loss: 0.7777, acc: 0.6655\n",
            "E2E-ABSA >>> 2022-05-20 07:10:43\n",
            "loss: 0.7941, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-05-20 07:10:47\n",
            ">>> val_acc: 0.6344, val_precision: 0.6344 val_recall: 0.6344, val_f1: 0.6344\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 07:10:53\n",
            "loss: 0.8264, acc: 0.6135\n",
            "E2E-ABSA >>> 2022-05-20 07:11:02\n",
            "loss: 0.7931, acc: 0.6477\n",
            "E2E-ABSA >>> 2022-05-20 07:11:06\n",
            ">>> val_acc: 0.6375, val_precision: 0.6375 val_recall: 0.6375, val_f1: 0.6375\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 07:11:12\n",
            "loss: 0.7867, acc: 0.6534\n",
            "E2E-ABSA >>> 2022-05-20 07:11:21\n",
            "loss: 0.7798, acc: 0.6570\n",
            "E2E-ABSA >>> 2022-05-20 07:11:25\n",
            ">>> val_acc: 0.6465, val_precision: 0.6465 val_recall: 0.6465, val_f1: 0.6465\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 07:11:31\n",
            "loss: 0.7787, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-20 07:11:41\n",
            "loss: 0.7751, acc: 0.6599\n",
            "E2E-ABSA >>> 2022-05-20 07:11:43\n",
            ">>> val_acc: 0.6465, val_precision: 0.6465 val_recall: 0.6465, val_f1: 0.6465\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 07:11:50\n",
            "loss: 0.7580, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-05-20 07:12:00\n",
            "loss: 0.7664, acc: 0.6643\n",
            "E2E-ABSA >>> 2022-05-20 07:12:02\n",
            ">>> val_acc: 0.6586, val_precision: 0.6586 val_recall: 0.6586, val_f1: 0.6586\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.6586\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 07:12:10\n",
            "loss: 0.7817, acc: 0.6555\n",
            "E2E-ABSA >>> 2022-05-20 07:12:20\n",
            "loss: 0.7599, acc: 0.6661\n",
            "E2E-ABSA >>> 2022-05-20 07:12:21\n",
            ">>> val_acc: 0.6647, val_precision: 0.6647 val_recall: 0.6647, val_f1: 0.6647\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.6647\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 07:12:30\n",
            "loss: 0.7580, acc: 0.6708\n",
            "E2E-ABSA >>> 2022-05-20 07:12:39\n",
            "loss: 0.7588, acc: 0.6658\n",
            "E2E-ABSA >>> 2022-05-20 07:12:40\n",
            ">>> val_acc: 0.6647, val_precision: 0.6647 val_recall: 0.6647, val_f1: 0.6647\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 07:12:49\n",
            "loss: 0.7281, acc: 0.6849\n",
            "E2E-ABSA >>> 2022-05-20 07:12:59\n",
            ">>> val_acc: 0.6647, val_precision: 0.6647 val_recall: 0.6647, val_f1: 0.6647\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 07:12:59\n",
            "loss: 0.6897, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-20 07:13:10\n",
            "loss: 0.7675, acc: 0.6544\n",
            "E2E-ABSA >>> 2022-05-20 07:13:19\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 07:13:20\n",
            "loss: 0.6819, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-20 07:13:30\n",
            "loss: 0.7494, acc: 0.6655\n",
            "E2E-ABSA >>> 2022-05-20 07:13:38\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 07:13:39\n",
            "loss: 0.7439, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-20 07:13:49\n",
            "loss: 0.7401, acc: 0.6738\n",
            "E2E-ABSA >>> 2022-05-20 07:13:56\n",
            ">>> val_acc: 0.6798, val_precision: 0.6798 val_recall: 0.6798, val_f1: 0.6798\n",
            ">> saved: state_dict/lstm_SemEval2014_val_f1_0.6798\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 07:13:58\n",
            "loss: 0.7123, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-05-20 07:14:08\n",
            "loss: 0.7393, acc: 0.6906\n",
            "E2E-ABSA >>> 2022-05-20 07:14:15\n",
            ">>> val_acc: 0.6798, val_precision: 0.6798 val_recall: 0.6798, val_f1: 0.6798\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 07:14:17\n",
            "loss: 0.7729, acc: 0.6490\n",
            "E2E-ABSA >>> 2022-05-20 07:14:26\n",
            "loss: 0.7439, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-05-20 07:14:33\n",
            ">>> val_acc: 0.6737, val_precision: 0.6737 val_recall: 0.6737, val_f1: 0.6737\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 07:14:36\n",
            "loss: 0.7504, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-05-20 07:14:45\n",
            "loss: 0.7521, acc: 0.6738\n",
            "E2E-ABSA >>> 2022-05-20 07:14:52\n",
            ">>> val_acc: 0.6767, val_precision: 0.6767 val_recall: 0.6767, val_f1: 0.6767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 07:14:55\n",
            "loss: 0.7651, acc: 0.6645\n",
            "E2E-ABSA >>> 2022-05-20 07:15:05\n",
            "loss: 0.7347, acc: 0.6803\n",
            "E2E-ABSA >>> 2022-05-20 07:15:11\n",
            ">>> val_acc: 0.6798, val_precision: 0.6798 val_recall: 0.6798, val_f1: 0.6798\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 07:15:15\n",
            "loss: 0.7597, acc: 0.6477\n",
            "E2E-ABSA >>> 2022-05-20 07:15:25\n",
            "loss: 0.7464, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-20 07:15:30\n",
            ">>> val_acc: 0.6677, val_precision: 0.6677 val_recall: 0.6677, val_f1: 0.6677\n",
            ">>> early stop.\n",
            "E2E-ABSA >>> 2022-05-20 07:15:30\n",
            ">>> test_acc: 0.6798, test_precision: 0.6798, test_recall: 0.6798, test_f1: 0.6798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **SemEval2014** dataset on model(**LSTM**)"
      ],
      "metadata": {
        "id": "aLdGitf1R-1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset SemEval2014_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa4Li_RLR1Uv",
        "outputId": "3e16a377-f874-4ed6-8665-d0a06a507f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "200_SemEval2014_embedding_matrix.dat\t   models\n",
            "200_SemEval2014_know_embedding_matrix.dat  __pycache__\n",
            "200_twitter_embedding_matrix.dat\t   README.md\n",
            "200_twitter_know_embedding_matrix.dat\t   requirements.txt\n",
            "datasets\t\t\t\t   SemEval2014_know_tokenizer.dat\n",
            "data_utils.py\t\t\t\t   SemEval2014_tokenizer.dat\n",
            "deberta_abas.ipynb\t\t\t   state_dict\n",
            "glove_embeddings\t\t\t   train.py\n",
            "infer_example.py\t\t\t   twitter_know_tokenizer.dat\n",
            "layers\t\t\t\t\t   twitter_tokenizer.dat\n",
            "加载Bert...\n",
            "loading tokenizer: SemEval2014_know_tokenizer.dat\n",
            "加载预训练向量...\n",
            "loading embedding_matrix: 200_SemEval2014_know_embedding_matrix.dat\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 3104.\n",
            "> testing dataset count: 331.\n",
            "> n_trainable_params: 603303, n_nontrainable_params: 2154200\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fc5cda559e0>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 200\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cpu\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 08:11:25\n",
            "loss: 1.0825, acc: 0.3738\n",
            "AAA 0.5770392749244713 (0.5770392749244713, 0.5770392749244713, 0.5770392749244713, None)\n",
            "E2E-ABSA >>> 2022-05-20 08:12:03\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">> saved: state_dict/lstm_SemEval2014_know_val_f1_0.577\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 08:12:05\n",
            "loss: 0.9986, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-05-20 08:12:43\n",
            "loss: 0.9726, acc: 0.5708\n",
            "AAA 0.5770392749244713 (0.5770392749244713, 0.5770392749244713, 0.5770392749244713, None)\n",
            "E2E-ABSA >>> 2022-05-20 08:13:19\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 08:13:23\n",
            "loss: 0.9685, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-05-20 08:14:01\n",
            "loss: 0.9585, acc: 0.5714\n",
            "AAA 0.5770392749244713 (0.5770392749244713, 0.5770392749244713, 0.5770392749244713, None)\n",
            "E2E-ABSA >>> 2022-05-20 08:14:34\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 08:14:41\n",
            "loss: 0.9598, acc: 0.5660\n",
            "E2E-ABSA >>> 2022-05-20 08:15:20\n",
            "loss: 0.9653, acc: 0.5609\n",
            "AAA 0.5770392749244713 (0.5770392749244713, 0.5770392749244713, 0.5770392749244713, None)\n",
            "E2E-ABSA >>> 2022-05-20 08:15:51\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 08:16:00\n",
            "loss: 0.9420, acc: 0.5573\n",
            "E2E-ABSA >>> 2022-05-20 08:16:39\n",
            "loss: 0.9448, acc: 0.5660\n",
            "AAA 0.5770392749244713 (0.5770392749244713, 0.5770392749244713, 0.5770392749244713, None)\n",
            "E2E-ABSA >>> 2022-05-20 08:17:08\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 08:17:20\n",
            "loss: 0.9545, acc: 0.5583\n",
            "E2E-ABSA >>> 2022-05-20 08:18:00\n",
            "loss: 0.9545, acc: 0.5591\n",
            "AAA 0.5770392749244713 (0.5770392749244713, 0.5770392749244713, 0.5770392749244713, None)\n",
            "E2E-ABSA >>> 2022-05-20 08:18:25\n",
            ">>> val_acc: 0.5770, val_precision: 0.5770 val_recall: 0.5770, val_f1: 0.5770\n",
            ">>> early stop.\n",
            "E2E-ABSA >>> 2022-05-20 08:18:25\n",
            "AAA 0.5770392749244713 (0.5770392749244713, 0.5770392749244713, 0.5770392749244713, None)\n",
            ">>> test_acc: 0.5770, test_precision: 0.5770, test_recall: 0.5770, test_f1: 0.5770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train.py embed_size == 300 则需要下载\n",
        "!wget https://huggingface.co/stanfordnlp/glove/resolve/main/glove.42B.300d.zip #Common Crawl数据集,不区分大小写\n",
        "!unzip glove.42B.300d.zip -d /content/DictionaryFused-E2E-ABSA/glove_embeddings"
      ],
      "metadata": {
        "id": "xlLB1-mkg6uI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38d4d27-4139-4826-a7e7-13cc63b09963"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-17 13:51:09--  https://huggingface.co/stanfordnlp/glove/resolve/main/glove.42B.300d.zip\n",
            "Resolving huggingface.co (huggingface.co)... 52.6.16.131, 52.202.207.64, 2600:1f18:147f:e850:db35:e0c7:187b:c770, ...\n",
            "Connecting to huggingface.co (huggingface.co)|52.6.16.131|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/stanfordnlp/glove/357baac33090f645e71e253b3295ee1b767c98a0336e9a1d99c77e9e33b43c4a?response-content-disposition=attachment%3B%20filename%3D%22glove.42B.300d.zip%22 [following]\n",
            "--2022-08-17 13:51:10--  https://cdn-lfs.huggingface.co/stanfordnlp/glove/357baac33090f645e71e253b3295ee1b767c98a0336e9a1d99c77e9e33b43c4a?response-content-disposition=attachment%3B%20filename%3D%22glove.42B.300d.zip%22\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.227.254.33, 13.227.254.123, 13.227.254.47, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.227.254.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1877802108 (1.7G) [application/zip]\n",
            "Saving to: ‘glove.42B.300d.zip’\n",
            "\n",
            "glove.42B.300d.zip  100%[===================>]   1.75G  16.4MB/s    in 1m 54s  \n",
            "\n",
            "2022-08-17 13:53:05 (15.6 MB/s) - ‘glove.42B.300d.zip’ saved [1877802108/1877802108]\n",
            "\n",
            "Archive:  glove.42B.300d.zip\n",
            "  inflating: /content/DictionaryFused-E2E-ABSA/glove_embeddings/glove.42B.300d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**TDLSTM**)\n",
        "lstm\n",
        "tdlstm  \n",
        "tclstm  \n",
        "ataelstm  \n",
        "ian \n",
        "memnet  \n",
        "cabasc "
      ],
      "metadata": {
        "id": "hxg2FWc_E5Io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset SemEval2014 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWXPBea4E5PZ",
        "outputId": "73bc3ed6-ca60-4b80-caea-a66d0b5593fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 2905.\n",
            "> testing dataset count: 311.\n",
            "cuda memory allocated: 15916032\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 2532000\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1a7af6cb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:38\n",
            "loss: 1.1178, acc: 0.3706\n",
            "E2E-ABSA >>> 2022-08-17 13:57:39\n",
            ">>> val_acc: 0.5820, val_precision: 0.5820 val_recall: 0.5820, val_f1: 0.5820\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.582\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:39\n",
            "loss: 0.9751, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 13:57:40\n",
            "loss: 0.9490, acc: 0.5699\n",
            "E2E-ABSA >>> 2022-08-17 13:57:41\n",
            ">>> val_acc: 0.6045, val_precision: 0.6045 val_recall: 0.6045, val_f1: 0.6045\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.6045\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:41\n",
            "loss: 0.9520, acc: 0.5816\n",
            "E2E-ABSA >>> 2022-08-17 13:57:42\n",
            "loss: 0.9204, acc: 0.5841\n",
            "E2E-ABSA >>> 2022-08-17 13:57:42\n",
            ">>> val_acc: 0.6495, val_precision: 0.6495 val_recall: 0.6495, val_f1: 0.6495\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.6495\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:43\n",
            "loss: 0.9082, acc: 0.6053\n",
            "E2E-ABSA >>> 2022-08-17 13:57:43\n",
            "loss: 0.9061, acc: 0.6031\n",
            "E2E-ABSA >>> 2022-08-17 13:57:44\n",
            ">>> val_acc: 0.6334, val_precision: 0.6334 val_recall: 0.6334, val_f1: 0.6334\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:44\n",
            "loss: 0.8892, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 13:57:45\n",
            "loss: 0.8919, acc: 0.6054\n",
            "E2E-ABSA >>> 2022-08-17 13:57:45\n",
            ">>> val_acc: 0.6527, val_precision: 0.6527 val_recall: 0.6527, val_f1: 0.6527\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.6527\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:46\n",
            "loss: 0.8715, acc: 0.6278\n",
            "E2E-ABSA >>> 2022-08-17 13:57:47\n",
            ">>> val_acc: 0.6495, val_precision: 0.6495 val_recall: 0.6495, val_f1: 0.6495\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:47\n",
            "loss: 0.8863, acc: 0.6016\n",
            "E2E-ABSA >>> 2022-08-17 13:57:48\n",
            "loss: 0.8800, acc: 0.6169\n",
            "E2E-ABSA >>> 2022-08-17 13:57:48\n",
            ">>> val_acc: 0.6527, val_precision: 0.6527 val_recall: 0.6527, val_f1: 0.6527\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:48\n",
            "loss: 0.8364, acc: 0.6418\n",
            "E2E-ABSA >>> 2022-08-17 13:57:49\n",
            "loss: 0.8530, acc: 0.6295\n",
            "E2E-ABSA >>> 2022-08-17 13:57:50\n",
            ">>> val_acc: 0.6527, val_precision: 0.6527 val_recall: 0.6527, val_f1: 0.6527\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:50\n",
            "loss: 0.8424, acc: 0.6193\n",
            "E2E-ABSA >>> 2022-08-17 13:57:51\n",
            "loss: 0.8406, acc: 0.6324\n",
            "E2E-ABSA >>> 2022-08-17 13:57:51\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:52\n",
            "loss: 0.7951, acc: 0.6643\n",
            "E2E-ABSA >>> 2022-08-17 13:57:53\n",
            "loss: 0.8179, acc: 0.6462\n",
            "E2E-ABSA >>> 2022-08-17 13:57:53\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:53\n",
            "loss: 0.7992, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 13:57:54\n",
            "loss: 0.7971, acc: 0.6604\n",
            "E2E-ABSA >>> 2022-08-17 13:57:54\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:55\n",
            "loss: 0.7708, acc: 0.6690\n",
            "E2E-ABSA >>> 2022-08-17 13:57:56\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:56\n",
            "loss: 0.7696, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 13:57:57\n",
            "loss: 0.7532, acc: 0.6783\n",
            "E2E-ABSA >>> 2022-08-17 13:57:57\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:58\n",
            "loss: 0.7611, acc: 0.6599\n",
            "E2E-ABSA >>> 2022-08-17 13:57:58\n",
            "loss: 0.7388, acc: 0.6856\n",
            "E2E-ABSA >>> 2022-08-17 13:57:59\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 13:57:59\n",
            "loss: 0.7245, acc: 0.6995\n",
            "E2E-ABSA >>> 2022-08-17 13:58:00\n",
            "loss: 0.7298, acc: 0.6883\n",
            "E2E-ABSA >>> 2022-08-17 13:58:00\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:01\n",
            "loss: 0.7116, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 13:58:02\n",
            "loss: 0.7192, acc: 0.6949\n",
            "E2E-ABSA >>> 2022-08-17 13:58:02\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:03\n",
            "loss: 0.7016, acc: 0.7024\n",
            "E2E-ABSA >>> 2022-08-17 13:58:03\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:04\n",
            "loss: 0.7570, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-08-17 13:58:04\n",
            "loss: 0.6994, acc: 0.7099\n",
            "E2E-ABSA >>> 2022-08-17 13:58:05\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:05\n",
            "loss: 0.7311, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-08-17 13:58:06\n",
            "loss: 0.7022, acc: 0.6996\n",
            "E2E-ABSA >>> 2022-08-17 13:58:07\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:07\n",
            "loss: 0.6790, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 13:58:08\n",
            "loss: 0.7033, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 13:58:08\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:09\n",
            "loss: 0.6793, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 13:58:09\n",
            "loss: 0.6826, acc: 0.7105\n",
            "E2E-ABSA >>> 2022-08-17 13:58:10\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:10\n",
            "loss: 0.6932, acc: 0.7003\n",
            "E2E-ABSA >>> 2022-08-17 13:58:11\n",
            "loss: 0.6896, acc: 0.7012\n",
            "E2E-ABSA >>> 2022-08-17 13:58:11\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:12\n",
            "loss: 0.6967, acc: 0.6908\n",
            "E2E-ABSA >>> 2022-08-17 13:58:13\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:13\n",
            "loss: 0.6987, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 13:58:14\n",
            "loss: 0.6942, acc: 0.7023\n",
            "E2E-ABSA >>> 2022-08-17 13:58:14\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:14\n",
            "loss: 0.6694, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-08-17 13:58:15\n",
            "loss: 0.6883, acc: 0.7060\n",
            "E2E-ABSA >>> 2022-08-17 13:58:16\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:16\n",
            "loss: 0.6719, acc: 0.7050\n",
            "E2E-ABSA >>> 2022-08-17 13:58:17\n",
            "loss: 0.6791, acc: 0.7067\n",
            "E2E-ABSA >>> 2022-08-17 13:58:17\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:18\n",
            "loss: 0.6605, acc: 0.7279\n",
            "E2E-ABSA >>> 2022-08-17 13:58:19\n",
            "loss: 0.6736, acc: 0.7124\n",
            "E2E-ABSA >>> 2022-08-17 13:58:19\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:20\n",
            "loss: 0.6756, acc: 0.7100\n",
            "E2E-ABSA >>> 2022-08-17 13:58:20\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:20\n",
            "loss: 0.7070, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 13:58:21\n",
            "loss: 0.6508, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 13:58:22\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:22\n",
            "loss: 0.6893, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-08-17 13:58:23\n",
            "loss: 0.6728, acc: 0.7126\n",
            "E2E-ABSA >>> 2022-08-17 13:58:23\n",
            ">>> val_acc: 0.7203, val_precision: 0.7203 val_recall: 0.7203, val_f1: 0.7203\n",
            ">> saved: state_dict/td_lstm_SemEval2014_val_f1_0.7203\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:24\n",
            "loss: 0.6458, acc: 0.7172\n",
            "E2E-ABSA >>> 2022-08-17 13:58:25\n",
            "loss: 0.6597, acc: 0.7143\n",
            "E2E-ABSA >>> 2022-08-17 13:58:25\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:26\n",
            "loss: 0.6375, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 13:58:26\n",
            "loss: 0.6574, acc: 0.7239\n",
            "E2E-ABSA >>> 2022-08-17 13:58:27\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:27\n",
            "loss: 0.6588, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-08-17 13:58:28\n",
            "loss: 0.6566, acc: 0.7195\n",
            "E2E-ABSA >>> 2022-08-17 13:58:28\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:29\n",
            "loss: 0.6650, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-08-17 13:58:30\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:30\n",
            "loss: 0.5992, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 13:58:30\n",
            "loss: 0.6537, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 13:58:31\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:31\n",
            "loss: 0.6699, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 13:58:32\n",
            "loss: 0.6338, acc: 0.7346\n",
            "E2E-ABSA >>> 2022-08-17 13:58:33\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:33\n",
            "loss: 0.6444, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 13:58:34\n",
            "loss: 0.6492, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 13:58:34\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:35\n",
            "loss: 0.6323, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 13:58:35\n",
            "loss: 0.6445, acc: 0.7300\n",
            "E2E-ABSA >>> 2022-08-17 13:58:35\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:36\n",
            "loss: 0.6175, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-08-17 13:58:37\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:37\n",
            "loss: 0.6256, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 13:58:38\n",
            "loss: 0.6431, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 13:58:39\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:39\n",
            "loss: 0.6879, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 13:58:39\n",
            "loss: 0.6487, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 13:58:40\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:40\n",
            "loss: 0.6375, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 13:58:41\n",
            "loss: 0.6265, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-08-17 13:58:42\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:42\n",
            "loss: 0.6246, acc: 0.7467\n",
            "E2E-ABSA >>> 2022-08-17 13:58:43\n",
            "loss: 0.6325, acc: 0.7304\n",
            "E2E-ABSA >>> 2022-08-17 13:58:43\n",
            ">>> val_acc: 0.7170, val_precision: 0.7170 val_recall: 0.7170, val_f1: 0.7170\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:44\n",
            "loss: 0.6374, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-08-17 13:58:44\n",
            "loss: 0.6304, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-08-17 13:58:45\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:45\n",
            "loss: 0.6493, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 13:58:46\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:46\n",
            "loss: 0.6651, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 13:58:47\n",
            "loss: 0.6193, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 13:58:48\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:48\n",
            "loss: 0.5763, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 13:58:49\n",
            "loss: 0.6290, acc: 0.7314\n",
            "E2E-ABSA >>> 2022-08-17 13:58:49\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:49\n",
            "loss: 0.5855, acc: 0.7609\n",
            "E2E-ABSA >>> 2022-08-17 13:58:50\n",
            "loss: 0.6213, acc: 0.7410\n",
            "E2E-ABSA >>> 2022-08-17 13:58:51\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:51\n",
            "loss: 0.5964, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 13:58:52\n",
            "loss: 0.6132, acc: 0.7401\n",
            "E2E-ABSA >>> 2022-08-17 13:58:52\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:53\n",
            "loss: 0.6241, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-08-17 13:58:54\n",
            "loss: 0.6181, acc: 0.7380\n",
            "E2E-ABSA >>> 2022-08-17 13:58:54\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:54\n",
            "loss: 0.5997, acc: 0.7512\n",
            "E2E-ABSA >>> 2022-08-17 13:58:55\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:55\n",
            "loss: 0.6539, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 13:58:56\n",
            "loss: 0.6058, acc: 0.7495\n",
            "E2E-ABSA >>> 2022-08-17 13:58:57\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:57\n",
            "loss: 0.6129, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-08-17 13:58:58\n",
            "loss: 0.6082, acc: 0.7509\n",
            "E2E-ABSA >>> 2022-08-17 13:58:58\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 13:58:59\n",
            "loss: 0.6208, acc: 0.7477\n",
            "E2E-ABSA >>> 2022-08-17 13:58:59\n",
            "loss: 0.6134, acc: 0.7447\n",
            "E2E-ABSA >>> 2022-08-17 13:59:00\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 13:59:00\n",
            "loss: 0.5869, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-08-17 13:59:01\n",
            "loss: 0.6091, acc: 0.7427\n",
            "E2E-ABSA >>> 2022-08-17 13:59:01\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 13:59:02\n",
            "loss: 0.5969, acc: 0.7556\n",
            "E2E-ABSA >>> 2022-08-17 13:59:03\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 13:59:03\n",
            "loss: 0.5640, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 13:59:03\n",
            "loss: 0.5892, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-08-17 13:59:04\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 13:59:04\n",
            "loss: 0.5571, acc: 0.7764\n",
            "E2E-ABSA >>> 2022-08-17 13:59:05\n",
            "loss: 0.6024, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-08-17 13:59:06\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 13:59:06\n",
            "loss: 0.6159, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 13:59:07\n",
            "loss: 0.5955, acc: 0.7587\n",
            "E2E-ABSA >>> 2022-08-17 13:59:07\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 13:59:08\n",
            "loss: 0.5904, acc: 0.7450\n",
            "E2E-ABSA >>> 2022-08-17 13:59:08\n",
            "loss: 0.5967, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-08-17 13:59:09\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            "E2E-ABSA >>> 2022-08-17 13:59:09\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7203, val_precision: 0.7203 val_recall: 0.7203, val_f1: 0.7203\n",
            "you can download the best model from state_dict/td_lstm_SemEval2014_val_f1_0.7203\n",
            ">>> test_acc: 0.7203, test_precision: 0.7203, test_recall: 0.7203, test_f1: 0.7203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2014** dataset on model(**TDLSTM**)"
      ],
      "metadata": {
        "id": "Uqmjs0O_E5V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset SemEval2014_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_Ip6UjOE5cL",
        "outputId": "35d2b48f-7df9-4d28-ddf3-b2c652ba428b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 2913.\n",
            "> testing dataset count: 311.\n",
            "cuda memory allocated: 22564864\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 3969000\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f4e6ddffb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:25\n",
            "loss: 1.1115, acc: 0.3925\n",
            "E2E-ABSA >>> 2022-08-17 14:00:26\n",
            ">>> val_acc: 0.5627, val_precision: 0.5627 val_recall: 0.5627, val_f1: 0.5627\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.5627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:27\n",
            "loss: 0.9597, acc: 0.5735\n",
            "E2E-ABSA >>> 2022-08-17 14:00:28\n",
            "loss: 0.9386, acc: 0.5700\n",
            "E2E-ABSA >>> 2022-08-17 14:00:28\n",
            ">>> val_acc: 0.5820, val_precision: 0.5820 val_recall: 0.5820, val_f1: 0.5820\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.582\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:29\n",
            "loss: 0.9420, acc: 0.5772\n",
            "E2E-ABSA >>> 2022-08-17 14:00:30\n",
            "loss: 0.9289, acc: 0.5774\n",
            "E2E-ABSA >>> 2022-08-17 14:00:30\n",
            ">>> val_acc: 0.5949, val_precision: 0.5949 val_recall: 0.5949, val_f1: 0.5949\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.5949\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:31\n",
            "loss: 0.9202, acc: 0.5821\n",
            "E2E-ABSA >>> 2022-08-17 14:00:32\n",
            "loss: 0.9050, acc: 0.5902\n",
            "E2E-ABSA >>> 2022-08-17 14:00:32\n",
            ">>> val_acc: 0.6495, val_precision: 0.6495 val_recall: 0.6495, val_f1: 0.6495\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.6495\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:33\n",
            "loss: 0.9010, acc: 0.5974\n",
            "E2E-ABSA >>> 2022-08-17 14:00:34\n",
            "loss: 0.8960, acc: 0.6049\n",
            "E2E-ABSA >>> 2022-08-17 14:00:34\n",
            ">>> val_acc: 0.6334, val_precision: 0.6334 val_recall: 0.6334, val_f1: 0.6334\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:35\n",
            "loss: 0.8792, acc: 0.6191\n",
            "E2E-ABSA >>> 2022-08-17 14:00:36\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:36\n",
            "loss: 0.8630, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 14:00:37\n",
            "loss: 0.8735, acc: 0.6152\n",
            "E2E-ABSA >>> 2022-08-17 14:00:38\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:39\n",
            "loss: 0.7724, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 14:00:40\n",
            "loss: 0.8485, acc: 0.6297\n",
            "E2E-ABSA >>> 2022-08-17 14:00:40\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:41\n",
            "loss: 0.8357, acc: 0.6476\n",
            "E2E-ABSA >>> 2022-08-17 14:00:42\n",
            "loss: 0.8378, acc: 0.6443\n",
            "E2E-ABSA >>> 2022-08-17 14:00:42\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:43\n",
            "loss: 0.8466, acc: 0.6321\n",
            "E2E-ABSA >>> 2022-08-17 14:00:44\n",
            "loss: 0.8393, acc: 0.6377\n",
            "E2E-ABSA >>> 2022-08-17 14:00:44\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:45\n",
            "loss: 0.8225, acc: 0.6420\n",
            "E2E-ABSA >>> 2022-08-17 14:00:46\n",
            "loss: 0.8179, acc: 0.6441\n",
            "E2E-ABSA >>> 2022-08-17 14:00:46\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:47\n",
            "loss: 0.8154, acc: 0.6473\n",
            "E2E-ABSA >>> 2022-08-17 14:00:48\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:48\n",
            "loss: 1.0317, acc: 0.5312\n",
            "E2E-ABSA >>> 2022-08-17 14:00:49\n",
            "loss: 0.7985, acc: 0.6460\n",
            "E2E-ABSA >>> 2022-08-17 14:00:50\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:50\n",
            "loss: 0.7691, acc: 0.6548\n",
            "E2E-ABSA >>> 2022-08-17 14:00:51\n",
            "loss: 0.7756, acc: 0.6674\n",
            "E2E-ABSA >>> 2022-08-17 14:00:52\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:53\n",
            "loss: 0.7737, acc: 0.6678\n",
            "E2E-ABSA >>> 2022-08-17 14:00:54\n",
            "loss: 0.7612, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-08-17 14:00:54\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:55\n",
            "loss: 0.7442, acc: 0.6705\n",
            "E2E-ABSA >>> 2022-08-17 14:00:56\n",
            "loss: 0.7432, acc: 0.6802\n",
            "E2E-ABSA >>> 2022-08-17 14:00:56\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:57\n",
            "loss: 0.7433, acc: 0.6762\n",
            "E2E-ABSA >>> 2022-08-17 14:00:58\n",
            "loss: 0.7402, acc: 0.6835\n",
            "E2E-ABSA >>> 2022-08-17 14:00:58\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:00:59\n",
            "loss: 0.7248, acc: 0.6882\n",
            "E2E-ABSA >>> 2022-08-17 14:01:00\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:00\n",
            "loss: 0.7256, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 14:01:01\n",
            "loss: 0.7088, acc: 0.6969\n",
            "E2E-ABSA >>> 2022-08-17 14:01:02\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:02\n",
            "loss: 0.7185, acc: 0.6766\n",
            "E2E-ABSA >>> 2022-08-17 14:01:03\n",
            "loss: 0.7239, acc: 0.6839\n",
            "E2E-ABSA >>> 2022-08-17 14:01:04\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:04\n",
            "loss: 0.7026, acc: 0.6984\n",
            "E2E-ABSA >>> 2022-08-17 14:01:05\n",
            "loss: 0.7007, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-08-17 14:01:06\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:06\n",
            "loss: 0.7032, acc: 0.6831\n",
            "E2E-ABSA >>> 2022-08-17 14:01:07\n",
            "loss: 0.7090, acc: 0.6955\n",
            "E2E-ABSA >>> 2022-08-17 14:01:08\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:09\n",
            "loss: 0.6864, acc: 0.7002\n",
            "E2E-ABSA >>> 2022-08-17 14:01:10\n",
            "loss: 0.6978, acc: 0.7004\n",
            "E2E-ABSA >>> 2022-08-17 14:01:10\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:11\n",
            "loss: 0.7133, acc: 0.6909\n",
            "E2E-ABSA >>> 2022-08-17 14:01:12\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:12\n",
            "loss: 0.6636, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 14:01:13\n",
            "loss: 0.6777, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 14:01:14\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:14\n",
            "loss: 0.7327, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 14:01:15\n",
            "loss: 0.6911, acc: 0.6995\n",
            "E2E-ABSA >>> 2022-08-17 14:01:16\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:16\n",
            "loss: 0.7200, acc: 0.6801\n",
            "E2E-ABSA >>> 2022-08-17 14:01:17\n",
            "loss: 0.6886, acc: 0.6998\n",
            "E2E-ABSA >>> 2022-08-17 14:01:17\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:18\n",
            "loss: 0.6901, acc: 0.7013\n",
            "E2E-ABSA >>> 2022-08-17 14:01:19\n",
            "loss: 0.6769, acc: 0.7099\n",
            "E2E-ABSA >>> 2022-08-17 14:01:19\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:20\n",
            "loss: 0.6700, acc: 0.7138\n",
            "E2E-ABSA >>> 2022-08-17 14:01:21\n",
            "loss: 0.6758, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-08-17 14:01:21\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:22\n",
            "loss: 0.6733, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-08-17 14:01:23\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:23\n",
            "loss: 0.7231, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 14:01:24\n",
            "loss: 0.6650, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-08-17 14:01:25\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:26\n",
            "loss: 0.6653, acc: 0.7037\n",
            "E2E-ABSA >>> 2022-08-17 14:01:27\n",
            "loss: 0.6712, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-08-17 14:01:27\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:28\n",
            "loss: 0.6491, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 14:01:29\n",
            "loss: 0.6632, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-08-17 14:01:29\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:30\n",
            "loss: 0.6837, acc: 0.7172\n",
            "E2E-ABSA >>> 2022-08-17 14:01:31\n",
            "loss: 0.6645, acc: 0.7178\n",
            "E2E-ABSA >>> 2022-08-17 14:01:31\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:32\n",
            "loss: 0.6348, acc: 0.7300\n",
            "E2E-ABSA >>> 2022-08-17 14:01:33\n",
            "loss: 0.6560, acc: 0.7202\n",
            "E2E-ABSA >>> 2022-08-17 14:01:33\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:34\n",
            "loss: 0.6563, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 14:01:35\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:35\n",
            "loss: 0.7101, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 14:01:36\n",
            "loss: 0.6529, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 14:01:37\n",
            ">>> val_acc: 0.7138, val_precision: 0.7138 val_recall: 0.7138, val_f1: 0.7138\n",
            ">> saved: state_dict/td_lstm_SemEval2014_know_val_f1_0.7138\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:37\n",
            "loss: 0.6348, acc: 0.7263\n",
            "E2E-ABSA >>> 2022-08-17 14:01:38\n",
            "loss: 0.6432, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-08-17 14:01:39\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:39\n",
            "loss: 0.6721, acc: 0.7079\n",
            "E2E-ABSA >>> 2022-08-17 14:01:40\n",
            "loss: 0.6479, acc: 0.7158\n",
            "E2E-ABSA >>> 2022-08-17 14:01:41\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:42\n",
            "loss: 0.6537, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-08-17 14:01:43\n",
            "loss: 0.6412, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-08-17 14:01:43\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:44\n",
            "loss: 0.6315, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 14:01:45\n",
            "loss: 0.6411, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 14:01:45\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:46\n",
            "loss: 0.6553, acc: 0.7165\n",
            "E2E-ABSA >>> 2022-08-17 14:01:47\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:47\n",
            "loss: 0.6083, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 14:01:48\n",
            "loss: 0.6397, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 14:01:49\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:49\n",
            "loss: 0.6087, acc: 0.7601\n",
            "E2E-ABSA >>> 2022-08-17 14:01:50\n",
            "loss: 0.6269, acc: 0.7343\n",
            "E2E-ABSA >>> 2022-08-17 14:01:51\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:51\n",
            "loss: 0.6106, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:01:52\n",
            "loss: 0.6336, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-08-17 14:01:53\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:53\n",
            "loss: 0.6493, acc: 0.7192\n",
            "E2E-ABSA >>> 2022-08-17 14:01:54\n",
            "loss: 0.6332, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 14:01:54\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:55\n",
            "loss: 0.6184, acc: 0.7439\n",
            "E2E-ABSA >>> 2022-08-17 14:01:56\n",
            "loss: 0.6278, acc: 0.7363\n",
            "E2E-ABSA >>> 2022-08-17 14:01:56\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:57\n",
            "loss: 0.6378, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 14:01:58\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:01:59\n",
            "loss: 0.6474, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 14:02:00\n",
            "loss: 0.6210, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-08-17 14:02:00\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:01\n",
            "loss: 0.5708, acc: 0.7557\n",
            "E2E-ABSA >>> 2022-08-17 14:02:02\n",
            "loss: 0.6201, acc: 0.7359\n",
            "E2E-ABSA >>> 2022-08-17 14:02:02\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:03\n",
            "loss: 0.6283, acc: 0.7288\n",
            "E2E-ABSA >>> 2022-08-17 14:02:04\n",
            "loss: 0.6205, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 14:02:04\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:05\n",
            "loss: 0.6075, acc: 0.7416\n",
            "E2E-ABSA >>> 2022-08-17 14:02:06\n",
            "loss: 0.6197, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 14:02:06\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:07\n",
            "loss: 0.6034, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-08-17 14:02:08\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:08\n",
            "loss: 0.6677, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:02:09\n",
            "loss: 0.5947, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:02:10\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:10\n",
            "loss: 0.5862, acc: 0.7743\n",
            "E2E-ABSA >>> 2022-08-17 14:02:11\n",
            "loss: 0.6161, acc: 0.7447\n",
            "E2E-ABSA >>> 2022-08-17 14:02:12\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:12\n",
            "loss: 0.6330, acc: 0.7357\n",
            "E2E-ABSA >>> 2022-08-17 14:02:13\n",
            "loss: 0.6065, acc: 0.7463\n",
            "E2E-ABSA >>> 2022-08-17 14:02:14\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:14\n",
            "loss: 0.6437, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-08-17 14:02:15\n",
            "loss: 0.6185, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-08-17 14:02:16\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:17\n",
            "loss: 0.6073, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 14:02:18\n",
            "loss: 0.6076, acc: 0.7430\n",
            "E2E-ABSA >>> 2022-08-17 14:02:18\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:19\n",
            "loss: 0.6049, acc: 0.7464\n",
            "E2E-ABSA >>> 2022-08-17 14:02:20\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:20\n",
            "loss: 0.5776, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 14:02:21\n",
            "loss: 0.6078, acc: 0.7397\n",
            "E2E-ABSA >>> 2022-08-17 14:02:22\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:22\n",
            "loss: 0.6007, acc: 0.7469\n",
            "E2E-ABSA >>> 2022-08-17 14:02:23\n",
            "loss: 0.6006, acc: 0.7443\n",
            "E2E-ABSA >>> 2022-08-17 14:02:24\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:24\n",
            "loss: 0.6057, acc: 0.7449\n",
            "E2E-ABSA >>> 2022-08-17 14:02:25\n",
            "loss: 0.5984, acc: 0.7495\n",
            "E2E-ABSA >>> 2022-08-17 14:02:26\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:26\n",
            "loss: 0.5872, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:02:27\n",
            "loss: 0.5990, acc: 0.7459\n",
            "E2E-ABSA >>> 2022-08-17 14:02:27\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:28\n",
            "loss: 0.5838, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-08-17 14:02:29\n",
            "loss: 0.6011, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-08-17 14:02:29\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:30\n",
            "loss: 0.6016, acc: 0.7564\n",
            "E2E-ABSA >>> 2022-08-17 14:02:31\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:31\n",
            "loss: 0.4782, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:02:32\n",
            "loss: 0.6055, acc: 0.7488\n",
            "E2E-ABSA >>> 2022-08-17 14:02:33\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 14:02:34\n",
            "loss: 0.5873, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-08-17 14:02:35\n",
            "loss: 0.5761, acc: 0.7541\n",
            "E2E-ABSA >>> 2022-08-17 14:02:35\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            "E2E-ABSA >>> 2022-08-17 14:02:35\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7138, val_precision: 0.7138 val_recall: 0.7138, val_f1: 0.7138\n",
            "you can download the best model from state_dict/td_lstm_SemEval2014_know_val_f1_0.7138\n",
            ">>> test_acc: 0.7138, test_precision: 0.7138, test_recall: 0.7138, test_f1: 0.7138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**TCLSTM**)"
      ],
      "metadata": {
        "id": "ymd8SJ1tE_jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset SemEval2014 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP0-hBY8E_pb",
        "outputId": "c3645e60-ef35-41a1-bad0-a5de46c57e8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 2905.\n",
            "> testing dataset count: 311.\n",
            "cuda memory allocated: 18796544\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 2532000\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f57bde0eb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:15\n",
            "loss: 0.9819, acc: 0.5281\n",
            "E2E-ABSA >>> 2022-08-17 14:03:16\n",
            ">>> val_acc: 0.5981, val_precision: 0.5981 val_recall: 0.5981, val_f1: 0.5981\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.5981\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:16\n",
            "loss: 0.9501, acc: 0.5764\n",
            "E2E-ABSA >>> 2022-08-17 14:03:17\n",
            "loss: 0.9373, acc: 0.5757\n",
            "E2E-ABSA >>> 2022-08-17 14:03:18\n",
            ">>> val_acc: 0.6334, val_precision: 0.6334 val_recall: 0.6334, val_f1: 0.6334\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.6334\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:18\n",
            "loss: 0.9062, acc: 0.6111\n",
            "E2E-ABSA >>> 2022-08-17 14:03:19\n",
            "loss: 0.9095, acc: 0.6011\n",
            "E2E-ABSA >>> 2022-08-17 14:03:19\n",
            ">>> val_acc: 0.6463, val_precision: 0.6463 val_recall: 0.6463, val_f1: 0.6463\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.6463\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:20\n",
            "loss: 0.8822, acc: 0.5995\n",
            "E2E-ABSA >>> 2022-08-17 14:03:21\n",
            "loss: 0.8850, acc: 0.6035\n",
            "E2E-ABSA >>> 2022-08-17 14:03:21\n",
            ">>> val_acc: 0.6527, val_precision: 0.6527 val_recall: 0.6527, val_f1: 0.6527\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.6527\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:22\n",
            "loss: 0.8494, acc: 0.6241\n",
            "E2E-ABSA >>> 2022-08-17 14:03:23\n",
            "loss: 0.8675, acc: 0.6123\n",
            "E2E-ABSA >>> 2022-08-17 14:03:23\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:24\n",
            "loss: 0.8608, acc: 0.6264\n",
            "E2E-ABSA >>> 2022-08-17 14:03:24\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:25\n",
            "loss: 0.8729, acc: 0.6328\n",
            "E2E-ABSA >>> 2022-08-17 14:03:25\n",
            "loss: 0.8504, acc: 0.6262\n",
            "E2E-ABSA >>> 2022-08-17 14:03:26\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:26\n",
            "loss: 0.8646, acc: 0.6130\n",
            "E2E-ABSA >>> 2022-08-17 14:03:27\n",
            "loss: 0.8397, acc: 0.6364\n",
            "E2E-ABSA >>> 2022-08-17 14:03:28\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:28\n",
            "loss: 0.7966, acc: 0.6548\n",
            "E2E-ABSA >>> 2022-08-17 14:03:29\n",
            "loss: 0.8197, acc: 0.6497\n",
            "E2E-ABSA >>> 2022-08-17 14:03:29\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:30\n",
            "loss: 0.7946, acc: 0.6724\n",
            "E2E-ABSA >>> 2022-08-17 14:03:31\n",
            "loss: 0.8122, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-08-17 14:03:31\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:32\n",
            "loss: 0.7984, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-08-17 14:03:33\n",
            "loss: 0.7953, acc: 0.6642\n",
            "E2E-ABSA >>> 2022-08-17 14:03:33\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:33\n",
            "loss: 0.8023, acc: 0.6550\n",
            "E2E-ABSA >>> 2022-08-17 14:03:34\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:34\n",
            "loss: 0.7525, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 14:03:35\n",
            "loss: 0.7535, acc: 0.6864\n",
            "E2E-ABSA >>> 2022-08-17 14:03:36\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:36\n",
            "loss: 0.7635, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-08-17 14:03:37\n",
            "loss: 0.7691, acc: 0.6628\n",
            "E2E-ABSA >>> 2022-08-17 14:03:37\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:38\n",
            "loss: 0.7495, acc: 0.6887\n",
            "E2E-ABSA >>> 2022-08-17 14:03:39\n",
            "loss: 0.7463, acc: 0.6842\n",
            "E2E-ABSA >>> 2022-08-17 14:03:39\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:40\n",
            "loss: 0.7198, acc: 0.6839\n",
            "E2E-ABSA >>> 2022-08-17 14:03:41\n",
            "loss: 0.7287, acc: 0.6908\n",
            "E2E-ABSA >>> 2022-08-17 14:03:41\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:41\n",
            "loss: 0.6798, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-08-17 14:03:42\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_val_f1_0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:42\n",
            "loss: 0.6981, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-08-17 14:03:43\n",
            "loss: 0.7112, acc: 0.6952\n",
            "E2E-ABSA >>> 2022-08-17 14:03:44\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:44\n",
            "loss: 0.6795, acc: 0.7370\n",
            "E2E-ABSA >>> 2022-08-17 14:03:45\n",
            "loss: 0.6902, acc: 0.7092\n",
            "E2E-ABSA >>> 2022-08-17 14:03:46\n",
            ">>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:46\n",
            "loss: 0.6459, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 14:03:47\n",
            "loss: 0.6753, acc: 0.7179\n",
            "E2E-ABSA >>> 2022-08-17 14:03:47\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:48\n",
            "loss: 0.6579, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 14:03:48\n",
            "loss: 0.6635, acc: 0.7223\n",
            "E2E-ABSA >>> 2022-08-17 14:03:49\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:49\n",
            "loss: 0.6549, acc: 0.7099\n",
            "E2E-ABSA >>> 2022-08-17 14:03:50\n",
            "loss: 0.6560, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 14:03:50\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:51\n",
            "loss: 0.6272, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 14:03:52\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:52\n",
            "loss: 0.6654, acc: 0.7098\n",
            "E2E-ABSA >>> 2022-08-17 14:03:53\n",
            "loss: 0.6463, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 14:03:54\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:54\n",
            "loss: 0.6265, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 14:03:55\n",
            "loss: 0.6260, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 14:03:55\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:56\n",
            "loss: 0.5859, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 14:03:56\n",
            "loss: 0.6190, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-08-17 14:03:57\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:57\n",
            "loss: 0.6116, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-08-17 14:03:58\n",
            "loss: 0.6123, acc: 0.7437\n",
            "E2E-ABSA >>> 2022-08-17 14:03:58\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:03:59\n",
            "loss: 0.6175, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 14:04:00\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:00\n",
            "loss: 0.4162, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 14:04:01\n",
            "loss: 0.6096, acc: 0.7506\n",
            "E2E-ABSA >>> 2022-08-17 14:04:02\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:02\n",
            "loss: 0.6102, acc: 0.7642\n",
            "E2E-ABSA >>> 2022-08-17 14:04:03\n",
            "loss: 0.5970, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 14:04:03\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:04\n",
            "loss: 0.5610, acc: 0.7766\n",
            "E2E-ABSA >>> 2022-08-17 14:04:04\n",
            "loss: 0.5825, acc: 0.7571\n",
            "E2E-ABSA >>> 2022-08-17 14:04:05\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:05\n",
            "loss: 0.5677, acc: 0.7683\n",
            "E2E-ABSA >>> 2022-08-17 14:04:06\n",
            "loss: 0.5778, acc: 0.7579\n",
            "E2E-ABSA >>> 2022-08-17 14:04:06\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:07\n",
            "loss: 0.5711, acc: 0.7681\n",
            "E2E-ABSA >>> 2022-08-17 14:04:08\n",
            "loss: 0.5784, acc: 0.7603\n",
            "E2E-ABSA >>> 2022-08-17 14:04:08\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:09\n",
            "loss: 0.5629, acc: 0.7653\n",
            "E2E-ABSA >>> 2022-08-17 14:04:10\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:10\n",
            "loss: 0.4617, acc: 0.8073\n",
            "E2E-ABSA >>> 2022-08-17 14:04:11\n",
            "loss: 0.5460, acc: 0.7751\n",
            "E2E-ABSA >>> 2022-08-17 14:04:11\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:11\n",
            "loss: 0.5825, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 14:04:12\n",
            "loss: 0.5469, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 14:04:13\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:13\n",
            "loss: 0.5250, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:04:14\n",
            "loss: 0.5512, acc: 0.7715\n",
            "E2E-ABSA >>> 2022-08-17 14:04:14\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:15\n",
            "loss: 0.5317, acc: 0.7964\n",
            "E2E-ABSA >>> 2022-08-17 14:04:16\n",
            "loss: 0.5479, acc: 0.7782\n",
            "E2E-ABSA >>> 2022-08-17 14:04:16\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:17\n",
            "loss: 0.5315, acc: 0.7865\n",
            "E2E-ABSA >>> 2022-08-17 14:04:18\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:18\n",
            "loss: 0.4614, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:04:19\n",
            "loss: 0.5286, acc: 0.7898\n",
            "E2E-ABSA >>> 2022-08-17 14:04:19\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:19\n",
            "loss: 0.5514, acc: 0.7594\n",
            "E2E-ABSA >>> 2022-08-17 14:04:20\n",
            "loss: 0.5210, acc: 0.7844\n",
            "E2E-ABSA >>> 2022-08-17 14:04:21\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:21\n",
            "loss: 0.5520, acc: 0.7878\n",
            "E2E-ABSA >>> 2022-08-17 14:04:22\n",
            "loss: 0.5340, acc: 0.7808\n",
            "E2E-ABSA >>> 2022-08-17 14:04:22\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:23\n",
            "loss: 0.5448, acc: 0.7768\n",
            "E2E-ABSA >>> 2022-08-17 14:04:24\n",
            "loss: 0.5259, acc: 0.7853\n",
            "E2E-ABSA >>> 2022-08-17 14:04:24\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:25\n",
            "loss: 0.5182, acc: 0.7965\n",
            "E2E-ABSA >>> 2022-08-17 14:04:25\n",
            "loss: 0.5300, acc: 0.7834\n",
            "E2E-ABSA >>> 2022-08-17 14:04:26\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:26\n",
            "loss: 0.4980, acc: 0.8010\n",
            "E2E-ABSA >>> 2022-08-17 14:04:27\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:27\n",
            "loss: 0.5510, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 14:04:28\n",
            "loss: 0.5100, acc: 0.8034\n",
            "E2E-ABSA >>> 2022-08-17 14:04:29\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:04:29\n",
            "loss: 0.4563, acc: 0.8103\n",
            "E2E-ABSA >>> 2022-08-17 14:04:30\n",
            "loss: 0.5163, acc: 0.7935\n",
            "E2E-ABSA >>> 2022-08-17 14:04:30\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            "E2E-ABSA >>> 2022-08-17 14:04:30\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7042, val_precision: 0.7042 val_recall: 0.7042, val_f1: 0.7042\n",
            "you can download the best model from state_dict/tc_lstm_SemEval2014_val_f1_0.7042\n",
            ">>> test_acc: 0.7042, test_precision: 0.7042, test_recall: 0.7042, test_f1: 0.7042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2014** dataset on model(**TCLSTM**)"
      ],
      "metadata": {
        "id": "wA8T_VonE_6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset SemEval2014_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czQaYUYdFABu",
        "outputId": "76d80aa6-8fb3-4ec0-acea-2d8db2bd8d23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 2913.\n",
            "> testing dataset count: 311.\n",
            "cuda memory allocated: 25445376\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 3969000\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f76b0b25b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:10\n",
            "loss: 0.9893, acc: 0.5300\n",
            "E2E-ABSA >>> 2022-08-17 14:05:11\n",
            ">>> val_acc: 0.6174, val_precision: 0.6174 val_recall: 0.6174, val_f1: 0.6174\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6174\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:11\n",
            "loss: 0.9605, acc: 0.5772\n",
            "E2E-ABSA >>> 2022-08-17 14:05:12\n",
            "loss: 0.9187, acc: 0.5978\n",
            "E2E-ABSA >>> 2022-08-17 14:05:13\n",
            ">>> val_acc: 0.6302, val_precision: 0.6302 val_recall: 0.6302, val_f1: 0.6302\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6302\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:14\n",
            "loss: 0.8797, acc: 0.6232\n",
            "E2E-ABSA >>> 2022-08-17 14:05:15\n",
            "loss: 0.8941, acc: 0.6031\n",
            "E2E-ABSA >>> 2022-08-17 14:05:15\n",
            ">>> val_acc: 0.6367, val_precision: 0.6367 val_recall: 0.6367, val_f1: 0.6367\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6367\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:16\n",
            "loss: 0.8860, acc: 0.6213\n",
            "E2E-ABSA >>> 2022-08-17 14:05:17\n",
            "loss: 0.8832, acc: 0.6076\n",
            "E2E-ABSA >>> 2022-08-17 14:05:17\n",
            ">>> val_acc: 0.6495, val_precision: 0.6495 val_recall: 0.6495, val_f1: 0.6495\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6495\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:18\n",
            "loss: 0.8817, acc: 0.6048\n",
            "E2E-ABSA >>> 2022-08-17 14:05:19\n",
            "loss: 0.8660, acc: 0.6105\n",
            "E2E-ABSA >>> 2022-08-17 14:05:20\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:21\n",
            "loss: 0.8349, acc: 0.6419\n",
            "E2E-ABSA >>> 2022-08-17 14:05:22\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:22\n",
            "loss: 0.8162, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 14:05:23\n",
            "loss: 0.8464, acc: 0.6330\n",
            "E2E-ABSA >>> 2022-08-17 14:05:24\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:24\n",
            "loss: 0.8598, acc: 0.6382\n",
            "E2E-ABSA >>> 2022-08-17 14:05:25\n",
            "loss: 0.8358, acc: 0.6355\n",
            "E2E-ABSA >>> 2022-08-17 14:05:26\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:27\n",
            "loss: 0.8248, acc: 0.6545\n",
            "E2E-ABSA >>> 2022-08-17 14:05:28\n",
            "loss: 0.8293, acc: 0.6415\n",
            "E2E-ABSA >>> 2022-08-17 14:05:28\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:29\n",
            "loss: 0.7814, acc: 0.6651\n",
            "E2E-ABSA >>> 2022-08-17 14:05:30\n",
            "loss: 0.8065, acc: 0.6560\n",
            "E2E-ABSA >>> 2022-08-17 14:05:30\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:31\n",
            "loss: 0.8070, acc: 0.6607\n",
            "E2E-ABSA >>> 2022-08-17 14:05:32\n",
            "loss: 0.7968, acc: 0.6592\n",
            "E2E-ABSA >>> 2022-08-17 14:05:32\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:33\n",
            "loss: 0.7925, acc: 0.6624\n",
            "E2E-ABSA >>> 2022-08-17 14:05:35\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:35\n",
            "loss: 0.6654, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:05:36\n",
            "loss: 0.7928, acc: 0.6683\n",
            "E2E-ABSA >>> 2022-08-17 14:05:37\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:37\n",
            "loss: 0.7611, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-08-17 14:05:38\n",
            "loss: 0.7530, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-08-17 14:05:39\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:39\n",
            "loss: 0.7476, acc: 0.7056\n",
            "E2E-ABSA >>> 2022-08-17 14:05:40\n",
            "loss: 0.7530, acc: 0.6852\n",
            "E2E-ABSA >>> 2022-08-17 14:05:41\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:42\n",
            "loss: 0.7424, acc: 0.6795\n",
            "E2E-ABSA >>> 2022-08-17 14:05:43\n",
            "loss: 0.7411, acc: 0.6851\n",
            "E2E-ABSA >>> 2022-08-17 14:05:43\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:44\n",
            "loss: 0.7254, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 14:05:45\n",
            "loss: 0.7370, acc: 0.6871\n",
            "E2E-ABSA >>> 2022-08-17 14:05:45\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:46\n",
            "loss: 0.7107, acc: 0.7044\n",
            "E2E-ABSA >>> 2022-08-17 14:05:47\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:47\n",
            "loss: 0.6753, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:05:49\n",
            "loss: 0.7159, acc: 0.6940\n",
            "E2E-ABSA >>> 2022-08-17 14:05:50\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">> saved: state_dict/tc_lstm_SemEval2014_know_val_f1_0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:50\n",
            "loss: 0.6736, acc: 0.7201\n",
            "E2E-ABSA >>> 2022-08-17 14:05:51\n",
            "loss: 0.7001, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-08-17 14:05:52\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:52\n",
            "loss: 0.6920, acc: 0.7078\n",
            "E2E-ABSA >>> 2022-08-17 14:05:53\n",
            "loss: 0.7050, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-08-17 14:05:54\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:54\n",
            "loss: 0.7006, acc: 0.6963\n",
            "E2E-ABSA >>> 2022-08-17 14:05:56\n",
            "loss: 0.6793, acc: 0.7114\n",
            "E2E-ABSA >>> 2022-08-17 14:05:56\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:57\n",
            "loss: 0.6757, acc: 0.7154\n",
            "E2E-ABSA >>> 2022-08-17 14:05:58\n",
            "loss: 0.6755, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-08-17 14:05:58\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:05:59\n",
            "loss: 0.6773, acc: 0.7198\n",
            "E2E-ABSA >>> 2022-08-17 14:06:00\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:00\n",
            "loss: 0.6598, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:06:01\n",
            "loss: 0.6306, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-08-17 14:06:02\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:03\n",
            "loss: 0.6401, acc: 0.7275\n",
            "E2E-ABSA >>> 2022-08-17 14:06:04\n",
            "loss: 0.6469, acc: 0.7335\n",
            "E2E-ABSA >>> 2022-08-17 14:06:04\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:05\n",
            "loss: 0.6732, acc: 0.7202\n",
            "E2E-ABSA >>> 2022-08-17 14:06:06\n",
            "loss: 0.6457, acc: 0.7368\n",
            "E2E-ABSA >>> 2022-08-17 14:06:07\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:07\n",
            "loss: 0.6506, acc: 0.7235\n",
            "E2E-ABSA >>> 2022-08-17 14:06:08\n",
            "loss: 0.6359, acc: 0.7355\n",
            "E2E-ABSA >>> 2022-08-17 14:06:09\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:10\n",
            "loss: 0.6369, acc: 0.7262\n",
            "E2E-ABSA >>> 2022-08-17 14:06:11\n",
            "loss: 0.6322, acc: 0.7372\n",
            "E2E-ABSA >>> 2022-08-17 14:06:11\n",
            ">>> val_acc: 0.6527, val_precision: 0.6527 val_recall: 0.6527, val_f1: 0.6527\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:12\n",
            "loss: 0.6084, acc: 0.7466\n",
            "E2E-ABSA >>> 2022-08-17 14:06:13\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:13\n",
            "loss: 0.5467, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 14:06:14\n",
            "loss: 0.5936, acc: 0.7591\n",
            "E2E-ABSA >>> 2022-08-17 14:06:15\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:15\n",
            "loss: 0.6051, acc: 0.7407\n",
            "E2E-ABSA >>> 2022-08-17 14:06:16\n",
            "loss: 0.5975, acc: 0.7544\n",
            "E2E-ABSA >>> 2022-08-17 14:06:17\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:18\n",
            "loss: 0.6129, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-08-17 14:06:19\n",
            "loss: 0.5986, acc: 0.7565\n",
            "E2E-ABSA >>> 2022-08-17 14:06:19\n",
            ">>> val_acc: 0.6559, val_precision: 0.6559 val_recall: 0.6559, val_f1: 0.6559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:20\n",
            "loss: 0.5967, acc: 0.7592\n",
            "E2E-ABSA >>> 2022-08-17 14:06:21\n",
            "loss: 0.6038, acc: 0.7566\n",
            "E2E-ABSA >>> 2022-08-17 14:06:21\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:22\n",
            "loss: 0.6203, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-08-17 14:06:23\n",
            "loss: 0.6003, acc: 0.7532\n",
            "E2E-ABSA >>> 2022-08-17 14:06:23\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:24\n",
            "loss: 0.5903, acc: 0.7586\n",
            "E2E-ABSA >>> 2022-08-17 14:06:25\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:26\n",
            "loss: 0.5891, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-08-17 14:06:27\n",
            "loss: 0.5890, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 14:06:28\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:28\n",
            "loss: 0.5902, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 14:06:29\n",
            "loss: 0.5784, acc: 0.7553\n",
            "E2E-ABSA >>> 2022-08-17 14:06:30\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:30\n",
            "loss: 0.5683, acc: 0.7717\n",
            "E2E-ABSA >>> 2022-08-17 14:06:31\n",
            "loss: 0.5605, acc: 0.7723\n",
            "E2E-ABSA >>> 2022-08-17 14:06:32\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:33\n",
            "loss: 0.5716, acc: 0.7669\n",
            "E2E-ABSA >>> 2022-08-17 14:06:34\n",
            "loss: 0.5770, acc: 0.7665\n",
            "E2E-ABSA >>> 2022-08-17 14:06:34\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:35\n",
            "loss: 0.5959, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 14:06:36\n",
            "loss: 0.5715, acc: 0.7674\n",
            "E2E-ABSA >>> 2022-08-17 14:06:36\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:37\n",
            "loss: 0.5669, acc: 0.7693\n",
            "E2E-ABSA >>> 2022-08-17 14:06:38\n",
            ">>> val_acc: 0.6367, val_precision: 0.6367 val_recall: 0.6367, val_f1: 0.6367\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:38\n",
            "loss: 0.5793, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 14:06:39\n",
            "loss: 0.5629, acc: 0.7686\n",
            "E2E-ABSA >>> 2022-08-17 14:06:40\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:41\n",
            "loss: 0.5501, acc: 0.7823\n",
            "E2E-ABSA >>> 2022-08-17 14:06:42\n",
            "loss: 0.5518, acc: 0.7762\n",
            "E2E-ABSA >>> 2022-08-17 14:06:42\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:43\n",
            "loss: 0.5441, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 14:06:44\n",
            "loss: 0.5476, acc: 0.7838\n",
            "E2E-ABSA >>> 2022-08-17 14:06:44\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:45\n",
            "loss: 0.5541, acc: 0.7798\n",
            "E2E-ABSA >>> 2022-08-17 14:06:46\n",
            "loss: 0.5444, acc: 0.7822\n",
            "E2E-ABSA >>> 2022-08-17 14:06:47\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:47\n",
            "loss: 0.5425, acc: 0.7744\n",
            "E2E-ABSA >>> 2022-08-17 14:06:49\n",
            "loss: 0.5444, acc: 0.7809\n",
            "E2E-ABSA >>> 2022-08-17 14:06:49\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:50\n",
            "loss: 0.5277, acc: 0.7879\n",
            "E2E-ABSA >>> 2022-08-17 14:06:51\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:06:51\n",
            "loss: 0.5178, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:06:52\n",
            "loss: 0.5315, acc: 0.7845\n",
            "E2E-ABSA >>> 2022-08-17 14:06:53\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            "E2E-ABSA >>> 2022-08-17 14:06:53\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            "you can download the best model from state_dict/tc_lstm_SemEval2014_know_val_f1_0.6913\n",
            ">>> test_acc: 0.6913, test_precision: 0.6913, test_recall: 0.6913, test_f1: 0.6913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**ATAELSTM**)"
      ],
      "metadata": {
        "id": "e8dpwIuFE5lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset SemEval2014 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekr9z3hEE5r5",
        "outputId": "d0730aa1-02d8-4f59-951d-803d32ceb1be"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 3091.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 20986880\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 2532000\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7efd388adb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:40\n",
            "loss: 1.0430, acc: 0.4344\n",
            "E2E-ABSA >>> 2022-08-17 14:07:41\n",
            ">>> val_acc: 0.5897, val_precision: 0.5897 val_recall: 0.5897, val_f1: 0.5897\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_val_f1_0.5897\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:41\n",
            "loss: 0.9564, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 14:07:42\n",
            "loss: 0.9547, acc: 0.5631\n",
            "E2E-ABSA >>> 2022-08-17 14:07:43\n",
            ">>> val_acc: 0.6170, val_precision: 0.6170 val_recall: 0.6170, val_f1: 0.6170\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_val_f1_0.617\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:43\n",
            "loss: 0.8944, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 14:07:44\n",
            "loss: 0.8966, acc: 0.6021\n",
            "E2E-ABSA >>> 2022-08-17 14:07:45\n",
            ">>> val_acc: 0.6505, val_precision: 0.6505 val_recall: 0.6505, val_f1: 0.6505\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_val_f1_0.6505\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:45\n",
            "loss: 0.8767, acc: 0.6181\n",
            "E2E-ABSA >>> 2022-08-17 14:07:46\n",
            "loss: 0.8748, acc: 0.6144\n",
            "E2E-ABSA >>> 2022-08-17 14:07:47\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_val_f1_0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:47\n",
            "loss: 0.8660, acc: 0.6354\n",
            "E2E-ABSA >>> 2022-08-17 14:07:48\n",
            "loss: 0.8393, acc: 0.6351\n",
            "E2E-ABSA >>> 2022-08-17 14:07:48\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_val_f1_0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:49\n",
            "loss: 0.8168, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-08-17 14:07:50\n",
            "loss: 0.8056, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-08-17 14:07:50\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_val_f1_0.696\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:51\n",
            "loss: 0.7674, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 14:07:52\n",
            "loss: 0.7816, acc: 0.6659\n",
            "E2E-ABSA >>> 2022-08-17 14:07:52\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:53\n",
            "loss: 0.7182, acc: 0.7158\n",
            "E2E-ABSA >>> 2022-08-17 14:07:54\n",
            "loss: 0.7517, acc: 0.6827\n",
            "E2E-ABSA >>> 2022-08-17 14:07:54\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:55\n",
            "loss: 0.7463, acc: 0.6680\n",
            "E2E-ABSA >>> 2022-08-17 14:07:55\n",
            "loss: 0.7414, acc: 0.6744\n",
            "E2E-ABSA >>> 2022-08-17 14:07:56\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:56\n",
            "loss: 0.7222, acc: 0.6898\n",
            "E2E-ABSA >>> 2022-08-17 14:07:57\n",
            "loss: 0.7144, acc: 0.6944\n",
            "E2E-ABSA >>> 2022-08-17 14:07:58\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:07:58\n",
            "loss: 0.7299, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 14:07:59\n",
            "loss: 0.7101, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 14:08:00\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:00\n",
            "loss: 0.6901, acc: 0.6932\n",
            "E2E-ABSA >>> 2022-08-17 14:08:01\n",
            "loss: 0.6942, acc: 0.7044\n",
            "E2E-ABSA >>> 2022-08-17 14:08:01\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:02\n",
            "loss: 0.6434, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 14:08:03\n",
            "loss: 0.6867, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-08-17 14:08:03\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:04\n",
            "loss: 0.6561, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:08:05\n",
            "loss: 0.6627, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 14:08:05\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:06\n",
            "loss: 0.6625, acc: 0.7202\n",
            "E2E-ABSA >>> 2022-08-17 14:08:07\n",
            "loss: 0.6632, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:08:07\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:08\n",
            "loss: 0.6444, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 14:08:09\n",
            "loss: 0.6537, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 14:08:09\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:10\n",
            "loss: 0.6316, acc: 0.7337\n",
            "E2E-ABSA >>> 2022-08-17 14:08:11\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:11\n",
            "loss: 0.6806, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:08:12\n",
            "loss: 0.6289, acc: 0.7414\n",
            "E2E-ABSA >>> 2022-08-17 14:08:14\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:14\n",
            "loss: 0.6313, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:08:16\n",
            "loss: 0.6199, acc: 0.7390\n",
            "E2E-ABSA >>> 2022-08-17 14:08:18\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:19\n",
            "loss: 0.5997, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 14:08:20\n",
            "loss: 0.6341, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 14:08:21\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:21\n",
            "loss: 0.6128, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 14:08:22\n",
            "loss: 0.6075, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 14:08:22\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:23\n",
            "loss: 0.5804, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:08:24\n",
            "loss: 0.6133, acc: 0.7436\n",
            "E2E-ABSA >>> 2022-08-17 14:08:24\n",
            ">>> val_acc: 0.6565, val_precision: 0.6565 val_recall: 0.6565, val_f1: 0.6565\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:25\n",
            "loss: 0.6214, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-08-17 14:08:25\n",
            "loss: 0.6100, acc: 0.7481\n",
            "E2E-ABSA >>> 2022-08-17 14:08:26\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:26\n",
            "loss: 0.6208, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-08-17 14:08:27\n",
            "loss: 0.5942, acc: 0.7541\n",
            "E2E-ABSA >>> 2022-08-17 14:08:28\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:28\n",
            "loss: 0.5900, acc: 0.7713\n",
            "E2E-ABSA >>> 2022-08-17 14:08:29\n",
            "loss: 0.5844, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-08-17 14:08:30\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:31\n",
            "loss: 0.5923, acc: 0.7571\n",
            "E2E-ABSA >>> 2022-08-17 14:08:32\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:32\n",
            "loss: 0.5710, acc: 0.7779\n",
            "E2E-ABSA >>> 2022-08-17 14:08:33\n",
            "loss: 0.5826, acc: 0.7636\n",
            "E2E-ABSA >>> 2022-08-17 14:08:34\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:34\n",
            "loss: 0.5835, acc: 0.7560\n",
            "E2E-ABSA >>> 2022-08-17 14:08:35\n",
            "loss: 0.5797, acc: 0.7623\n",
            "E2E-ABSA >>> 2022-08-17 14:08:35\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:36\n",
            "loss: 0.5812, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-08-17 14:08:37\n",
            "loss: 0.5625, acc: 0.7697\n",
            "E2E-ABSA >>> 2022-08-17 14:08:37\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:38\n",
            "loss: 0.5490, acc: 0.7846\n",
            "E2E-ABSA >>> 2022-08-17 14:08:39\n",
            "loss: 0.5592, acc: 0.7769\n",
            "E2E-ABSA >>> 2022-08-17 14:08:39\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:40\n",
            "loss: 0.5620, acc: 0.7789\n",
            "E2E-ABSA >>> 2022-08-17 14:08:41\n",
            "loss: 0.5589, acc: 0.7767\n",
            "E2E-ABSA >>> 2022-08-17 14:08:41\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:42\n",
            "loss: 0.5454, acc: 0.7747\n",
            "E2E-ABSA >>> 2022-08-17 14:08:43\n",
            "loss: 0.5528, acc: 0.7732\n",
            "E2E-ABSA >>> 2022-08-17 14:08:43\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:44\n",
            "loss: 0.5337, acc: 0.7751\n",
            "E2E-ABSA >>> 2022-08-17 14:08:45\n",
            "loss: 0.5486, acc: 0.7744\n",
            "E2E-ABSA >>> 2022-08-17 14:08:45\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:46\n",
            "loss: 0.5563, acc: 0.7698\n",
            "E2E-ABSA >>> 2022-08-17 14:08:46\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:46\n",
            "loss: 0.4741, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:08:47\n",
            "loss: 0.5405, acc: 0.7758\n",
            "E2E-ABSA >>> 2022-08-17 14:08:48\n",
            ">>> val_acc: 0.7052, val_precision: 0.7052 val_recall: 0.7052, val_f1: 0.7052\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_val_f1_0.7052\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:48\n",
            "loss: 0.4897, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 14:08:49\n",
            "loss: 0.5435, acc: 0.7795\n",
            "E2E-ABSA >>> 2022-08-17 14:08:50\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:50\n",
            "loss: 0.5143, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-08-17 14:08:51\n",
            "loss: 0.5244, acc: 0.7877\n",
            "E2E-ABSA >>> 2022-08-17 14:08:52\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:52\n",
            "loss: 0.5171, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:08:53\n",
            "loss: 0.5269, acc: 0.7864\n",
            "E2E-ABSA >>> 2022-08-17 14:08:54\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:54\n",
            "loss: 0.4852, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 14:08:55\n",
            "loss: 0.5256, acc: 0.7861\n",
            "E2E-ABSA >>> 2022-08-17 14:08:56\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:56\n",
            "loss: 0.5154, acc: 0.8051\n",
            "E2E-ABSA >>> 2022-08-17 14:08:57\n",
            "loss: 0.5182, acc: 0.7985\n",
            "E2E-ABSA >>> 2022-08-17 14:08:57\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:08:58\n",
            "loss: 0.5334, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 14:08:59\n",
            "loss: 0.5115, acc: 0.7879\n",
            "E2E-ABSA >>> 2022-08-17 14:08:59\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:00\n",
            "loss: 0.4965, acc: 0.7962\n",
            "E2E-ABSA >>> 2022-08-17 14:09:01\n",
            "loss: 0.5191, acc: 0.7851\n",
            "E2E-ABSA >>> 2022-08-17 14:09:01\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:02\n",
            "loss: 0.5166, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-08-17 14:09:02\n",
            "loss: 0.5124, acc: 0.7932\n",
            "E2E-ABSA >>> 2022-08-17 14:09:03\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:03\n",
            "loss: 0.4866, acc: 0.7996\n",
            "E2E-ABSA >>> 2022-08-17 14:09:04\n",
            "loss: 0.4960, acc: 0.7983\n",
            "E2E-ABSA >>> 2022-08-17 14:09:05\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:05\n",
            "loss: 0.4726, acc: 0.8145\n",
            "E2E-ABSA >>> 2022-08-17 14:09:06\n",
            "loss: 0.5046, acc: 0.7988\n",
            "E2E-ABSA >>> 2022-08-17 14:09:07\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:07\n",
            "loss: 0.4995, acc: 0.8080\n",
            "E2E-ABSA >>> 2022-08-17 14:09:08\n",
            "loss: 0.4956, acc: 0.8085\n",
            "E2E-ABSA >>> 2022-08-17 14:09:08\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:09\n",
            "loss: 0.4907, acc: 0.8059\n",
            "E2E-ABSA >>> 2022-08-17 14:09:10\n",
            "loss: 0.4923, acc: 0.8029\n",
            "E2E-ABSA >>> 2022-08-17 14:09:10\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:11\n",
            "loss: 0.4713, acc: 0.8194\n",
            "E2E-ABSA >>> 2022-08-17 14:09:12\n",
            "loss: 0.4787, acc: 0.8163\n",
            "E2E-ABSA >>> 2022-08-17 14:09:12\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:13\n",
            "loss: 0.4702, acc: 0.8168\n",
            "E2E-ABSA >>> 2022-08-17 14:09:14\n",
            "loss: 0.4780, acc: 0.8105\n",
            "E2E-ABSA >>> 2022-08-17 14:09:14\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:15\n",
            "loss: 0.4969, acc: 0.7979\n",
            "E2E-ABSA >>> 2022-08-17 14:09:16\n",
            "loss: 0.4801, acc: 0.8094\n",
            "E2E-ABSA >>> 2022-08-17 14:09:16\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:17\n",
            "loss: 0.4715, acc: 0.8200\n",
            "E2E-ABSA >>> 2022-08-17 14:09:17\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:18\n",
            "loss: 0.4577, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-08-17 14:09:18\n",
            "loss: 0.4778, acc: 0.8154\n",
            "E2E-ABSA >>> 2022-08-17 14:09:19\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:19\n",
            "loss: 0.3979, acc: 0.8490\n",
            "E2E-ABSA >>> 2022-08-17 14:09:20\n",
            "loss: 0.4578, acc: 0.8214\n",
            "E2E-ABSA >>> 2022-08-17 14:09:21\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:21\n",
            "loss: 0.4294, acc: 0.8472\n",
            "E2E-ABSA >>> 2022-08-17 14:09:22\n",
            "loss: 0.4541, acc: 0.8257\n",
            "E2E-ABSA >>> 2022-08-17 14:09:23\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:23\n",
            "loss: 0.4313, acc: 0.8411\n",
            "E2E-ABSA >>> 2022-08-17 14:09:24\n",
            "loss: 0.4573, acc: 0.8170\n",
            "E2E-ABSA >>> 2022-08-17 14:09:25\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:25\n",
            "loss: 0.4583, acc: 0.8396\n",
            "E2E-ABSA >>> 2022-08-17 14:09:26\n",
            "loss: 0.4614, acc: 0.8255\n",
            "E2E-ABSA >>> 2022-08-17 14:09:27\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:27\n",
            "loss: 0.4821, acc: 0.8090\n",
            "E2E-ABSA >>> 2022-08-17 14:09:28\n",
            "loss: 0.4381, acc: 0.8217\n",
            "E2E-ABSA >>> 2022-08-17 14:09:28\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:29\n",
            "loss: 0.4285, acc: 0.8393\n",
            "E2E-ABSA >>> 2022-08-17 14:09:30\n",
            "loss: 0.4473, acc: 0.8248\n",
            "E2E-ABSA >>> 2022-08-17 14:09:30\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:31\n",
            "loss: 0.4286, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-08-17 14:09:32\n",
            "loss: 0.4493, acc: 0.8269\n",
            "E2E-ABSA >>> 2022-08-17 14:09:32\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:33\n",
            "loss: 0.4311, acc: 0.8252\n",
            "E2E-ABSA >>> 2022-08-17 14:09:33\n",
            "loss: 0.4428, acc: 0.8308\n",
            "E2E-ABSA >>> 2022-08-17 14:09:34\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:35\n",
            "loss: 0.4092, acc: 0.8552\n",
            "E2E-ABSA >>> 2022-08-17 14:09:35\n",
            "loss: 0.4340, acc: 0.8355\n",
            "E2E-ABSA >>> 2022-08-17 14:09:36\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:36\n",
            "loss: 0.4178, acc: 0.8409\n",
            "E2E-ABSA >>> 2022-08-17 14:09:37\n",
            "loss: 0.4252, acc: 0.8404\n",
            "E2E-ABSA >>> 2022-08-17 14:09:38\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:38\n",
            "loss: 0.4000, acc: 0.8377\n",
            "E2E-ABSA >>> 2022-08-17 14:09:39\n",
            "loss: 0.4200, acc: 0.8318\n",
            "E2E-ABSA >>> 2022-08-17 14:09:39\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:40\n",
            "loss: 0.4277, acc: 0.8309\n",
            "E2E-ABSA >>> 2022-08-17 14:09:41\n",
            "loss: 0.4256, acc: 0.8318\n",
            "E2E-ABSA >>> 2022-08-17 14:09:41\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 14:09:42\n",
            "loss: 0.4305, acc: 0.8318\n",
            "E2E-ABSA >>> 2022-08-17 14:09:43\n",
            "loss: 0.4217, acc: 0.8339\n",
            "E2E-ABSA >>> 2022-08-17 14:09:43\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            "E2E-ABSA >>> 2022-08-17 14:09:43\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7052, val_precision: 0.7052 val_recall: 0.7052, val_f1: 0.7052\n",
            "you can download the best model from state_dict/atae_lstm_SemEval2014_val_f1_0.7052\n",
            ">>> test_acc: 0.7052, test_precision: 0.7052, test_recall: 0.7052, test_f1: 0.7052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2014** dataset on model(**ATAELSTM**)"
      ],
      "metadata": {
        "id": "s-iO7yDyFA7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset SemEval2014_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLcZcCTBFBEL",
        "outputId": "19e718c3-1ff1-4882-860d-9af581c3f067"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 3093.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 26883072\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 3969000\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7faf58a7fb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:22\n",
            "loss: 1.0268, acc: 0.4662\n",
            "E2E-ABSA >>> 2022-08-17 14:10:24\n",
            ">>> val_acc: 0.5897, val_precision: 0.5897 val_recall: 0.5897, val_f1: 0.5897\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.5897\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:24\n",
            "loss: 0.9253, acc: 0.5417\n",
            "E2E-ABSA >>> 2022-08-17 14:10:25\n",
            "loss: 0.9468, acc: 0.5837\n",
            "E2E-ABSA >>> 2022-08-17 14:10:26\n",
            ">>> val_acc: 0.6079, val_precision: 0.6079 val_recall: 0.6079, val_f1: 0.6079\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6079\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:26\n",
            "loss: 0.9647, acc: 0.5885\n",
            "E2E-ABSA >>> 2022-08-17 14:10:27\n",
            "loss: 0.9158, acc: 0.5932\n",
            "E2E-ABSA >>> 2022-08-17 14:10:28\n",
            ">>> val_acc: 0.6353, val_precision: 0.6353 val_recall: 0.6353, val_f1: 0.6353\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6353\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:29\n",
            "loss: 0.8986, acc: 0.6007\n",
            "E2E-ABSA >>> 2022-08-17 14:10:30\n",
            "loss: 0.8928, acc: 0.6107\n",
            "E2E-ABSA >>> 2022-08-17 14:10:31\n",
            ">>> val_acc: 0.6444, val_precision: 0.6444 val_recall: 0.6444, val_f1: 0.6444\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6444\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:31\n",
            "loss: 0.8767, acc: 0.6328\n",
            "E2E-ABSA >>> 2022-08-17 14:10:32\n",
            "loss: 0.8866, acc: 0.6144\n",
            "E2E-ABSA >>> 2022-08-17 14:10:33\n",
            ">>> val_acc: 0.6535, val_precision: 0.6535 val_recall: 0.6535, val_f1: 0.6535\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6535\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:33\n",
            "loss: 0.9246, acc: 0.5917\n",
            "E2E-ABSA >>> 2022-08-17 14:10:35\n",
            "loss: 0.8890, acc: 0.6053\n",
            "E2E-ABSA >>> 2022-08-17 14:10:35\n",
            ">>> val_acc: 0.6596, val_precision: 0.6596 val_recall: 0.6596, val_f1: 0.6596\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6596\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:36\n",
            "loss: 0.8796, acc: 0.6128\n",
            "E2E-ABSA >>> 2022-08-17 14:10:37\n",
            "loss: 0.8690, acc: 0.6158\n",
            "E2E-ABSA >>> 2022-08-17 14:10:38\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:38\n",
            "loss: 0.8629, acc: 0.6339\n",
            "E2E-ABSA >>> 2022-08-17 14:10:39\n",
            "loss: 0.8592, acc: 0.6325\n",
            "E2E-ABSA >>> 2022-08-17 14:10:40\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:41\n",
            "loss: 0.8239, acc: 0.6471\n",
            "E2E-ABSA >>> 2022-08-17 14:10:42\n",
            "loss: 0.8350, acc: 0.6334\n",
            "E2E-ABSA >>> 2022-08-17 14:10:42\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:43\n",
            "loss: 0.7827, acc: 0.6690\n",
            "E2E-ABSA >>> 2022-08-17 14:10:44\n",
            "loss: 0.7824, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-08-17 14:10:45\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:45\n",
            "loss: 0.7588, acc: 0.6844\n",
            "E2E-ABSA >>> 2022-08-17 14:10:47\n",
            "loss: 0.7414, acc: 0.6902\n",
            "E2E-ABSA >>> 2022-08-17 14:10:47\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:48\n",
            "loss: 0.7333, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-08-17 14:10:49\n",
            "loss: 0.7287, acc: 0.6849\n",
            "E2E-ABSA >>> 2022-08-17 14:10:49\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:50\n",
            "loss: 0.7110, acc: 0.7040\n",
            "E2E-ABSA >>> 2022-08-17 14:10:51\n",
            "loss: 0.7135, acc: 0.6940\n",
            "E2E-ABSA >>> 2022-08-17 14:10:52\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:53\n",
            "loss: 0.6991, acc: 0.6963\n",
            "E2E-ABSA >>> 2022-08-17 14:10:54\n",
            "loss: 0.6928, acc: 0.7051\n",
            "E2E-ABSA >>> 2022-08-17 14:10:54\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:55\n",
            "loss: 0.6808, acc: 0.7024\n",
            "E2E-ABSA >>> 2022-08-17 14:10:56\n",
            "loss: 0.6812, acc: 0.7065\n",
            "E2E-ABSA >>> 2022-08-17 14:10:56\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:10:57\n",
            "loss: 0.6756, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 14:10:59\n",
            "loss: 0.6733, acc: 0.7079\n",
            "E2E-ABSA >>> 2022-08-17 14:10:59\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:00\n",
            "loss: 0.6637, acc: 0.7142\n",
            "E2E-ABSA >>> 2022-08-17 14:11:01\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:01\n",
            "loss: 0.6199, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 14:11:02\n",
            "loss: 0.6636, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-08-17 14:11:03\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:03\n",
            "loss: 0.7286, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:11:05\n",
            "loss: 0.6627, acc: 0.7193\n",
            "E2E-ABSA >>> 2022-08-17 14:11:06\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:06\n",
            "loss: 0.6045, acc: 0.7455\n",
            "E2E-ABSA >>> 2022-08-17 14:11:07\n",
            "loss: 0.6338, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-08-17 14:11:08\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:08\n",
            "loss: 0.5786, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 14:11:09\n",
            "loss: 0.6321, acc: 0.7307\n",
            "E2E-ABSA >>> 2022-08-17 14:11:10\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:11\n",
            "loss: 0.6366, acc: 0.7548\n",
            "E2E-ABSA >>> 2022-08-17 14:11:12\n",
            "loss: 0.6392, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 14:11:13\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:13\n",
            "loss: 0.6223, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 14:11:14\n",
            "loss: 0.6239, acc: 0.7391\n",
            "E2E-ABSA >>> 2022-08-17 14:11:15\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.693\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:15\n",
            "loss: 0.5966, acc: 0.7763\n",
            "E2E-ABSA >>> 2022-08-17 14:11:16\n",
            "loss: 0.6130, acc: 0.7378\n",
            "E2E-ABSA >>> 2022-08-17 14:11:17\n",
            ">>> val_acc: 0.6596, val_precision: 0.6596 val_recall: 0.6596, val_f1: 0.6596\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:18\n",
            "loss: 0.6271, acc: 0.7443\n",
            "E2E-ABSA >>> 2022-08-17 14:11:19\n",
            "loss: 0.6066, acc: 0.7509\n",
            "E2E-ABSA >>> 2022-08-17 14:11:19\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:20\n",
            "loss: 0.5985, acc: 0.7525\n",
            "E2E-ABSA >>> 2022-08-17 14:11:21\n",
            "loss: 0.5961, acc: 0.7529\n",
            "E2E-ABSA >>> 2022-08-17 14:11:22\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:22\n",
            "loss: 0.6055, acc: 0.7511\n",
            "E2E-ABSA >>> 2022-08-17 14:11:24\n",
            "loss: 0.5865, acc: 0.7592\n",
            "E2E-ABSA >>> 2022-08-17 14:11:24\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:25\n",
            "loss: 0.6190, acc: 0.7510\n",
            "E2E-ABSA >>> 2022-08-17 14:11:26\n",
            "loss: 0.5955, acc: 0.7612\n",
            "E2E-ABSA >>> 2022-08-17 14:11:26\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:27\n",
            "loss: 0.5877, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:11:28\n",
            "loss: 0.5920, acc: 0.7526\n",
            "E2E-ABSA >>> 2022-08-17 14:11:29\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:30\n",
            "loss: 0.5777, acc: 0.7635\n",
            "E2E-ABSA >>> 2022-08-17 14:11:31\n",
            "loss: 0.5784, acc: 0.7615\n",
            "E2E-ABSA >>> 2022-08-17 14:11:31\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:32\n",
            "loss: 0.5761, acc: 0.7680\n",
            "E2E-ABSA >>> 2022-08-17 14:11:33\n",
            "loss: 0.5760, acc: 0.7674\n",
            "E2E-ABSA >>> 2022-08-17 14:11:33\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:34\n",
            "loss: 0.5731, acc: 0.7580\n",
            "E2E-ABSA >>> 2022-08-17 14:11:35\n",
            "loss: 0.5735, acc: 0.7651\n",
            "E2E-ABSA >>> 2022-08-17 14:11:36\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:37\n",
            "loss: 0.5588, acc: 0.7806\n",
            "E2E-ABSA >>> 2022-08-17 14:11:38\n",
            "loss: 0.5683, acc: 0.7686\n",
            "E2E-ABSA >>> 2022-08-17 14:11:38\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:39\n",
            "loss: 0.5692, acc: 0.7647\n",
            "E2E-ABSA >>> 2022-08-17 14:11:40\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:40\n",
            "loss: 0.6138, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 14:11:41\n",
            "loss: 0.5648, acc: 0.7704\n",
            "E2E-ABSA >>> 2022-08-17 14:11:42\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:43\n",
            "loss: 0.5812, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 14:11:44\n",
            "loss: 0.5555, acc: 0.7756\n",
            "E2E-ABSA >>> 2022-08-17 14:11:45\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:45\n",
            "loss: 0.5624, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-08-17 14:11:46\n",
            "loss: 0.5566, acc: 0.7705\n",
            "E2E-ABSA >>> 2022-08-17 14:11:47\n",
            ">>> val_acc: 0.6505, val_precision: 0.6505 val_recall: 0.6505, val_f1: 0.6505\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:47\n",
            "loss: 0.4997, acc: 0.8040\n",
            "E2E-ABSA >>> 2022-08-17 14:11:49\n",
            "loss: 0.5498, acc: 0.7782\n",
            "E2E-ABSA >>> 2022-08-17 14:11:49\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:50\n",
            "loss: 0.5359, acc: 0.7790\n",
            "E2E-ABSA >>> 2022-08-17 14:11:51\n",
            "loss: 0.5409, acc: 0.7803\n",
            "E2E-ABSA >>> 2022-08-17 14:11:52\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:52\n",
            "loss: 0.5252, acc: 0.7904\n",
            "E2E-ABSA >>> 2022-08-17 14:11:53\n",
            "loss: 0.5427, acc: 0.7859\n",
            "E2E-ABSA >>> 2022-08-17 14:11:54\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:54\n",
            "loss: 0.4870, acc: 0.8078\n",
            "E2E-ABSA >>> 2022-08-17 14:11:56\n",
            "loss: 0.5091, acc: 0.8004\n",
            "E2E-ABSA >>> 2022-08-17 14:11:56\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:57\n",
            "loss: 0.5213, acc: 0.7826\n",
            "E2E-ABSA >>> 2022-08-17 14:11:58\n",
            "loss: 0.5259, acc: 0.7890\n",
            "E2E-ABSA >>> 2022-08-17 14:11:59\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:11:59\n",
            "loss: 0.5117, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 14:12:00\n",
            "loss: 0.5177, acc: 0.7940\n",
            "E2E-ABSA >>> 2022-08-17 14:12:01\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:02\n",
            "loss: 0.5127, acc: 0.7866\n",
            "E2E-ABSA >>> 2022-08-17 14:12:03\n",
            "loss: 0.5167, acc: 0.7963\n",
            "E2E-ABSA >>> 2022-08-17 14:12:03\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:04\n",
            "loss: 0.4950, acc: 0.8047\n",
            "E2E-ABSA >>> 2022-08-17 14:12:05\n",
            "loss: 0.5080, acc: 0.7950\n",
            "E2E-ABSA >>> 2022-08-17 14:12:05\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:06\n",
            "loss: 0.5050, acc: 0.7982\n",
            "E2E-ABSA >>> 2022-08-17 14:12:07\n",
            "loss: 0.5063, acc: 0.7949\n",
            "E2E-ABSA >>> 2022-08-17 14:12:08\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:09\n",
            "loss: 0.5083, acc: 0.8100\n",
            "E2E-ABSA >>> 2022-08-17 14:12:10\n",
            "loss: 0.4959, acc: 0.8079\n",
            "E2E-ABSA >>> 2022-08-17 14:12:10\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:11\n",
            "loss: 0.5003, acc: 0.8034\n",
            "E2E-ABSA >>> 2022-08-17 14:12:12\n",
            "loss: 0.4925, acc: 0.8108\n",
            "E2E-ABSA >>> 2022-08-17 14:12:12\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:13\n",
            "loss: 0.4792, acc: 0.8182\n",
            "E2E-ABSA >>> 2022-08-17 14:12:14\n",
            "loss: 0.4930, acc: 0.8078\n",
            "E2E-ABSA >>> 2022-08-17 14:12:15\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:16\n",
            "loss: 0.4853, acc: 0.8152\n",
            "E2E-ABSA >>> 2022-08-17 14:12:17\n",
            "loss: 0.4873, acc: 0.8083\n",
            "E2E-ABSA >>> 2022-08-17 14:12:17\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:18\n",
            "loss: 0.4746, acc: 0.8113\n",
            "E2E-ABSA >>> 2022-08-17 14:12:19\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:19\n",
            "loss: 0.5008, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:12:20\n",
            "loss: 0.4787, acc: 0.8101\n",
            "E2E-ABSA >>> 2022-08-17 14:12:22\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:22\n",
            "loss: 0.4472, acc: 0.8542\n",
            "E2E-ABSA >>> 2022-08-17 14:12:23\n",
            "loss: 0.4947, acc: 0.8114\n",
            "E2E-ABSA >>> 2022-08-17 14:12:24\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:24\n",
            "loss: 0.4772, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:12:25\n",
            "loss: 0.4542, acc: 0.8189\n",
            "E2E-ABSA >>> 2022-08-17 14:12:26\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:26\n",
            "loss: 0.4910, acc: 0.8177\n",
            "E2E-ABSA >>> 2022-08-17 14:12:28\n",
            "loss: 0.4744, acc: 0.8170\n",
            "E2E-ABSA >>> 2022-08-17 14:12:28\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:29\n",
            "loss: 0.4581, acc: 0.8104\n",
            "E2E-ABSA >>> 2022-08-17 14:12:30\n",
            "loss: 0.4571, acc: 0.8236\n",
            "E2E-ABSA >>> 2022-08-17 14:12:31\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:31\n",
            "loss: 0.4519, acc: 0.8108\n",
            "E2E-ABSA >>> 2022-08-17 14:12:32\n",
            "loss: 0.4537, acc: 0.8231\n",
            "E2E-ABSA >>> 2022-08-17 14:12:33\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:33\n",
            "loss: 0.4800, acc: 0.8065\n",
            "E2E-ABSA >>> 2022-08-17 14:12:35\n",
            "loss: 0.4508, acc: 0.8217\n",
            "E2E-ABSA >>> 2022-08-17 14:12:35\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:36\n",
            "loss: 0.4354, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-08-17 14:12:37\n",
            "loss: 0.4478, acc: 0.8201\n",
            "E2E-ABSA >>> 2022-08-17 14:12:38\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:38\n",
            "loss: 0.4343, acc: 0.8426\n",
            "E2E-ABSA >>> 2022-08-17 14:12:39\n",
            "loss: 0.4500, acc: 0.8210\n",
            "E2E-ABSA >>> 2022-08-17 14:12:40\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:41\n",
            "loss: 0.4496, acc: 0.8208\n",
            "E2E-ABSA >>> 2022-08-17 14:12:42\n",
            "loss: 0.4385, acc: 0.8297\n",
            "E2E-ABSA >>> 2022-08-17 14:12:42\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:43\n",
            "loss: 0.3997, acc: 0.8390\n",
            "E2E-ABSA >>> 2022-08-17 14:12:44\n",
            "loss: 0.4483, acc: 0.8230\n",
            "E2E-ABSA >>> 2022-08-17 14:12:45\n",
            ">>> val_acc: 0.7052, val_precision: 0.7052 val_recall: 0.7052, val_f1: 0.7052\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.7052\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:45\n",
            "loss: 0.4230, acc: 0.8316\n",
            "E2E-ABSA >>> 2022-08-17 14:12:47\n",
            "loss: 0.4307, acc: 0.8267\n",
            "E2E-ABSA >>> 2022-08-17 14:12:47\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:48\n",
            "loss: 0.4317, acc: 0.8309\n",
            "E2E-ABSA >>> 2022-08-17 14:12:49\n",
            "loss: 0.4294, acc: 0.8287\n",
            "E2E-ABSA >>> 2022-08-17 14:12:49\n",
            ">>> val_acc: 0.7021, val_precision: 0.7021 val_recall: 0.7021, val_f1: 0.7021\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:50\n",
            "loss: 0.4279, acc: 0.8393\n",
            "E2E-ABSA >>> 2022-08-17 14:12:51\n",
            "loss: 0.4278, acc: 0.8325\n",
            "E2E-ABSA >>> 2022-08-17 14:12:52\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:53\n",
            "loss: 0.4095, acc: 0.8424\n",
            "E2E-ABSA >>> 2022-08-17 14:12:54\n",
            "loss: 0.4206, acc: 0.8355\n",
            "E2E-ABSA >>> 2022-08-17 14:12:54\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:55\n",
            "loss: 0.4043, acc: 0.8320\n",
            "E2E-ABSA >>> 2022-08-17 14:12:56\n",
            ">>> val_acc: 0.7082, val_precision: 0.7082 val_recall: 0.7082, val_f1: 0.7082\n",
            ">> saved: state_dict/atae_lstm_SemEval2014_know_val_f1_0.7082\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:56\n",
            "loss: 0.6003, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:12:57\n",
            "loss: 0.4025, acc: 0.8456\n",
            "E2E-ABSA >>> 2022-08-17 14:12:58\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 14:12:59\n",
            "loss: 0.4278, acc: 0.8516\n",
            "E2E-ABSA >>> 2022-08-17 14:13:00\n",
            "loss: 0.4059, acc: 0.8403\n",
            "E2E-ABSA >>> 2022-08-17 14:13:01\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:01\n",
            "loss: 0.4821, acc: 0.7902\n",
            "E2E-ABSA >>> 2022-08-17 14:13:02\n",
            "loss: 0.4164, acc: 0.8355\n",
            "E2E-ABSA >>> 2022-08-17 14:13:03\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:03\n",
            "loss: 0.3390, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 14:13:04\n",
            "loss: 0.4092, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 14:13:05\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:06\n",
            "loss: 0.4023, acc: 0.8510\n",
            "E2E-ABSA >>> 2022-08-17 14:13:07\n",
            "loss: 0.4027, acc: 0.8442\n",
            "E2E-ABSA >>> 2022-08-17 14:13:08\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:08\n",
            "loss: 0.3939, acc: 0.8418\n",
            "E2E-ABSA >>> 2022-08-17 14:13:09\n",
            "loss: 0.3912, acc: 0.8480\n",
            "E2E-ABSA >>> 2022-08-17 14:13:10\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:10\n",
            "loss: 0.4254, acc: 0.8339\n",
            "E2E-ABSA >>> 2022-08-17 14:13:11\n",
            "loss: 0.3872, acc: 0.8501\n",
            "E2E-ABSA >>> 2022-08-17 14:13:12\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:13\n",
            "loss: 0.3943, acc: 0.8480\n",
            "E2E-ABSA >>> 2022-08-17 14:13:14\n",
            "loss: 0.3907, acc: 0.8490\n",
            "E2E-ABSA >>> 2022-08-17 14:13:14\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:15\n",
            "loss: 0.3787, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 14:13:16\n",
            "loss: 0.3804, acc: 0.8496\n",
            "E2E-ABSA >>> 2022-08-17 14:13:17\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:17\n",
            "loss: 0.3720, acc: 0.8460\n",
            "E2E-ABSA >>> 2022-08-17 14:13:19\n",
            "loss: 0.3827, acc: 0.8538\n",
            "E2E-ABSA >>> 2022-08-17 14:13:19\n",
            ">>> val_acc: 0.6596, val_precision: 0.6596 val_recall: 0.6596, val_f1: 0.6596\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:20\n",
            "loss: 0.3725, acc: 0.8538\n",
            "E2E-ABSA >>> 2022-08-17 14:13:21\n",
            "loss: 0.3807, acc: 0.8549\n",
            "E2E-ABSA >>> 2022-08-17 14:13:21\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:22\n",
            "loss: 0.3985, acc: 0.8465\n",
            "E2E-ABSA >>> 2022-08-17 14:13:23\n",
            "loss: 0.3969, acc: 0.8486\n",
            "E2E-ABSA >>> 2022-08-17 14:13:24\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:24\n",
            "loss: 0.3523, acc: 0.8767\n",
            "E2E-ABSA >>> 2022-08-17 14:13:26\n",
            "loss: 0.3732, acc: 0.8588\n",
            "E2E-ABSA >>> 2022-08-17 14:13:26\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:27\n",
            "loss: 0.4387, acc: 0.8211\n",
            "E2E-ABSA >>> 2022-08-17 14:13:28\n",
            "loss: 0.4344, acc: 0.8222\n",
            "E2E-ABSA >>> 2022-08-17 14:13:28\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:29\n",
            "loss: 0.3853, acc: 0.8488\n",
            "E2E-ABSA >>> 2022-08-17 14:13:30\n",
            "loss: 0.3916, acc: 0.8515\n",
            "E2E-ABSA >>> 2022-08-17 14:13:31\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:32\n",
            "loss: 0.3843, acc: 0.8539\n",
            "E2E-ABSA >>> 2022-08-17 14:13:33\n",
            "loss: 0.3809, acc: 0.8555\n",
            "E2E-ABSA >>> 2022-08-17 14:13:33\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:34\n",
            "loss: 0.3768, acc: 0.8482\n",
            "E2E-ABSA >>> 2022-08-17 14:13:35\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:35\n",
            "loss: 0.2675, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 14:13:36\n",
            "loss: 0.3735, acc: 0.8600\n",
            "E2E-ABSA >>> 2022-08-17 14:13:37\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:38\n",
            "loss: 0.3384, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 14:13:39\n",
            "loss: 0.3610, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 14:13:40\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:40\n",
            "loss: 0.3564, acc: 0.8555\n",
            "E2E-ABSA >>> 2022-08-17 14:13:41\n",
            "loss: 0.3612, acc: 0.8658\n",
            "E2E-ABSA >>> 2022-08-17 14:13:42\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:42\n",
            "loss: 0.3652, acc: 0.8636\n",
            "E2E-ABSA >>> 2022-08-17 14:13:44\n",
            "loss: 0.3552, acc: 0.8627\n",
            "E2E-ABSA >>> 2022-08-17 14:13:45\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:45\n",
            "loss: 0.3545, acc: 0.8772\n",
            "E2E-ABSA >>> 2022-08-17 14:13:46\n",
            "loss: 0.3656, acc: 0.8589\n",
            "E2E-ABSA >>> 2022-08-17 14:13:47\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:47\n",
            "loss: 0.3679, acc: 0.8566\n",
            "E2E-ABSA >>> 2022-08-17 14:13:48\n",
            "loss: 0.3777, acc: 0.8498\n",
            "E2E-ABSA >>> 2022-08-17 14:13:49\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:50\n",
            "loss: 0.3485, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 14:13:51\n",
            "loss: 0.3520, acc: 0.8607\n",
            "E2E-ABSA >>> 2022-08-17 14:13:52\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:52\n",
            "loss: 0.3461, acc: 0.8736\n",
            "E2E-ABSA >>> 2022-08-17 14:13:53\n",
            "loss: 0.3312, acc: 0.8754\n",
            "E2E-ABSA >>> 2022-08-17 14:13:54\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:55\n",
            "loss: 0.3692, acc: 0.8714\n",
            "E2E-ABSA >>> 2022-08-17 14:13:56\n",
            "loss: 0.3513, acc: 0.8655\n",
            "E2E-ABSA >>> 2022-08-17 14:13:56\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:57\n",
            "loss: 0.3345, acc: 0.8869\n",
            "E2E-ABSA >>> 2022-08-17 14:13:58\n",
            "loss: 0.3394, acc: 0.8695\n",
            "E2E-ABSA >>> 2022-08-17 14:13:59\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-08-17 14:13:59\n",
            "loss: 0.3466, acc: 0.8633\n",
            "E2E-ABSA >>> 2022-08-17 14:14:00\n",
            "loss: 0.3318, acc: 0.8712\n",
            "E2E-ABSA >>> 2022-08-17 14:14:01\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-08-17 14:14:02\n",
            "loss: 0.3143, acc: 0.8804\n",
            "E2E-ABSA >>> 2022-08-17 14:14:03\n",
            "loss: 0.3634, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 14:14:03\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-08-17 14:14:04\n",
            "loss: 0.3520, acc: 0.8651\n",
            "E2E-ABSA >>> 2022-08-17 14:14:05\n",
            "loss: 0.3441, acc: 0.8683\n",
            "E2E-ABSA >>> 2022-08-17 14:14:05\n",
            ">>> val_acc: 0.7052, val_precision: 0.7052 val_recall: 0.7052, val_f1: 0.7052\n",
            "E2E-ABSA >>> 2022-08-17 14:14:05\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7082, val_precision: 0.7082 val_recall: 0.7082, val_f1: 0.7082\n",
            "you can download the best model from state_dict/atae_lstm_SemEval2014_know_val_f1_0.7082\n",
            ">>> test_acc: 0.7082, test_precision: 0.7082, test_recall: 0.7082, test_f1: 0.7082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "6MKU_eMkFBLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset SemEval2014 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT1CZd9KFBSB",
        "outputId": "5da547db-cb15-4632-f22e-dff537c45d7b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 3091.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 18808832\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 2532000\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f6ef81feb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:14:45\n",
            "loss: 0.9976, acc: 0.5687\n",
            "E2E-ABSA >>> 2022-08-17 14:14:46\n",
            ">>> val_acc: 0.5745, val_precision: 0.5745 val_recall: 0.5745, val_f1: 0.5745\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.5745\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:14:46\n",
            "loss: 1.0205, acc: 0.5000\n",
            "E2E-ABSA >>> 2022-08-17 14:14:48\n",
            "loss: 0.9690, acc: 0.5637\n",
            "E2E-ABSA >>> 2022-08-17 14:14:49\n",
            ">>> val_acc: 0.5745, val_precision: 0.5745 val_recall: 0.5745, val_f1: 0.5745\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:14:49\n",
            "loss: 0.9554, acc: 0.5365\n",
            "E2E-ABSA >>> 2022-08-17 14:14:50\n",
            "loss: 0.9560, acc: 0.5586\n",
            "E2E-ABSA >>> 2022-08-17 14:14:51\n",
            ">>> val_acc: 0.5745, val_precision: 0.5745 val_recall: 0.5745, val_f1: 0.5745\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:14:52\n",
            "loss: 0.9325, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 14:14:53\n",
            "loss: 0.9247, acc: 0.5773\n",
            "E2E-ABSA >>> 2022-08-17 14:14:54\n",
            ">>> val_acc: 0.5866, val_precision: 0.5866 val_recall: 0.5866, val_f1: 0.5866\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.5866\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:14:54\n",
            "loss: 0.9018, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:14:55\n",
            "loss: 0.8983, acc: 0.5832\n",
            "E2E-ABSA >>> 2022-08-17 14:14:56\n",
            ">>> val_acc: 0.6049, val_precision: 0.6049 val_recall: 0.6049, val_f1: 0.6049\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.6049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:14:57\n",
            "loss: 0.8745, acc: 0.5917\n",
            "E2E-ABSA >>> 2022-08-17 14:14:58\n",
            "loss: 0.8524, acc: 0.6183\n",
            "E2E-ABSA >>> 2022-08-17 14:14:59\n",
            ">>> val_acc: 0.6444, val_precision: 0.6444 val_recall: 0.6444, val_f1: 0.6444\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.6444\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:00\n",
            "loss: 0.8045, acc: 0.6319\n",
            "E2E-ABSA >>> 2022-08-17 14:15:01\n",
            "loss: 0.7822, acc: 0.6595\n",
            "E2E-ABSA >>> 2022-08-17 14:15:02\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:02\n",
            "loss: 0.8152, acc: 0.6473\n",
            "E2E-ABSA >>> 2022-08-17 14:15:03\n",
            "loss: 0.7721, acc: 0.6554\n",
            "E2E-ABSA >>> 2022-08-17 14:15:04\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:05\n",
            "loss: 0.7914, acc: 0.6289\n",
            "E2E-ABSA >>> 2022-08-17 14:15:06\n",
            "loss: 0.7536, acc: 0.6571\n",
            "E2E-ABSA >>> 2022-08-17 14:15:07\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:07\n",
            "loss: 0.7516, acc: 0.6759\n",
            "E2E-ABSA >>> 2022-08-17 14:15:09\n",
            "loss: 0.7454, acc: 0.6725\n",
            "E2E-ABSA >>> 2022-08-17 14:15:09\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:10\n",
            "loss: 0.7302, acc: 0.6833\n",
            "E2E-ABSA >>> 2022-08-17 14:15:11\n",
            "loss: 0.7318, acc: 0.6762\n",
            "E2E-ABSA >>> 2022-08-17 14:15:12\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:12\n",
            "loss: 0.7228, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-08-17 14:15:14\n",
            "loss: 0.7287, acc: 0.6755\n",
            "E2E-ABSA >>> 2022-08-17 14:15:14\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.693\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:15\n",
            "loss: 0.7080, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 14:15:16\n",
            "loss: 0.7181, acc: 0.6871\n",
            "E2E-ABSA >>> 2022-08-17 14:15:17\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:18\n",
            "loss: 0.7178, acc: 0.6803\n",
            "E2E-ABSA >>> 2022-08-17 14:15:19\n",
            "loss: 0.7140, acc: 0.6836\n",
            "E2E-ABSA >>> 2022-08-17 14:15:19\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:20\n",
            "loss: 0.7002, acc: 0.6882\n",
            "E2E-ABSA >>> 2022-08-17 14:15:21\n",
            "loss: 0.7115, acc: 0.6844\n",
            "E2E-ABSA >>> 2022-08-17 14:15:22\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.696\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:23\n",
            "loss: 0.7031, acc: 0.6826\n",
            "E2E-ABSA >>> 2022-08-17 14:15:24\n",
            "loss: 0.7069, acc: 0.6882\n",
            "E2E-ABSA >>> 2022-08-17 14:15:24\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:25\n",
            "loss: 0.6874, acc: 0.7012\n",
            "E2E-ABSA >>> 2022-08-17 14:15:27\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:27\n",
            "loss: 0.9057, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 14:15:28\n",
            "loss: 0.7066, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-08-17 14:15:29\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:29\n",
            "loss: 0.6972, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 14:15:30\n",
            "loss: 0.7114, acc: 0.6892\n",
            "E2E-ABSA >>> 2022-08-17 14:15:32\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:32\n",
            "loss: 0.6739, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 14:15:33\n",
            "loss: 0.6989, acc: 0.6941\n",
            "E2E-ABSA >>> 2022-08-17 14:15:34\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:34\n",
            "loss: 0.6603, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 14:15:36\n",
            "loss: 0.6791, acc: 0.7052\n",
            "E2E-ABSA >>> 2022-08-17 14:15:37\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:37\n",
            "loss: 0.6761, acc: 0.6995\n",
            "E2E-ABSA >>> 2022-08-17 14:15:38\n",
            "loss: 0.6975, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-08-17 14:15:39\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:40\n",
            "loss: 0.6639, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 14:15:41\n",
            "loss: 0.6986, acc: 0.7022\n",
            "E2E-ABSA >>> 2022-08-17 14:15:42\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:42\n",
            "loss: 0.6576, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 14:15:43\n",
            "loss: 0.6803, acc: 0.7061\n",
            "E2E-ABSA >>> 2022-08-17 14:15:44\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:45\n",
            "loss: 0.6781, acc: 0.6918\n",
            "E2E-ABSA >>> 2022-08-17 14:15:46\n",
            "loss: 0.6764, acc: 0.7001\n",
            "E2E-ABSA >>> 2022-08-17 14:15:47\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:47\n",
            "loss: 0.6771, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 14:15:49\n",
            "loss: 0.6619, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-08-17 14:15:49\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:50\n",
            "loss: 0.6777, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 14:15:51\n",
            "loss: 0.6810, acc: 0.7023\n",
            "E2E-ABSA >>> 2022-08-17 14:15:52\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:52\n",
            "loss: 0.6774, acc: 0.6956\n",
            "E2E-ABSA >>> 2022-08-17 14:15:54\n",
            "loss: 0.6723, acc: 0.7041\n",
            "E2E-ABSA >>> 2022-08-17 14:15:54\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:55\n",
            "loss: 0.6586, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 14:15:56\n",
            "loss: 0.6608, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-08-17 14:15:57\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:15:58\n",
            "loss: 0.6900, acc: 0.6985\n",
            "E2E-ABSA >>> 2022-08-17 14:15:59\n",
            "loss: 0.6710, acc: 0.7087\n",
            "E2E-ABSA >>> 2022-08-17 14:15:59\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:00\n",
            "loss: 0.6818, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 14:16:01\n",
            "loss: 0.6694, acc: 0.7111\n",
            "E2E-ABSA >>> 2022-08-17 14:16:02\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:03\n",
            "loss: 0.6747, acc: 0.7144\n",
            "E2E-ABSA >>> 2022-08-17 14:16:04\n",
            "loss: 0.6650, acc: 0.7134\n",
            "E2E-ABSA >>> 2022-08-17 14:16:04\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:05\n",
            "loss: 0.6605, acc: 0.7289\n",
            "E2E-ABSA >>> 2022-08-17 14:16:06\n",
            "loss: 0.6572, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 14:16:07\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:08\n",
            "loss: 0.6582, acc: 0.7194\n",
            "E2E-ABSA >>> 2022-08-17 14:16:09\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:09\n",
            "loss: 0.5505, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:16:10\n",
            "loss: 0.6426, acc: 0.7302\n",
            "E2E-ABSA >>> 2022-08-17 14:16:12\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:12\n",
            "loss: 0.6946, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 14:16:13\n",
            "loss: 0.6546, acc: 0.7239\n",
            "E2E-ABSA >>> 2022-08-17 14:16:14\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:14\n",
            "loss: 0.6511, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 14:16:15\n",
            "loss: 0.6465, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-08-17 14:16:17\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:17\n",
            "loss: 0.6735, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-08-17 14:16:18\n",
            "loss: 0.6398, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-08-17 14:16:19\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:19\n",
            "loss: 0.6459, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-08-17 14:16:21\n",
            "loss: 0.6300, acc: 0.7393\n",
            "E2E-ABSA >>> 2022-08-17 14:16:22\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:22\n",
            "loss: 0.6500, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:16:23\n",
            "loss: 0.6425, acc: 0.7355\n",
            "E2E-ABSA >>> 2022-08-17 14:16:24\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:25\n",
            "loss: 0.6100, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-08-17 14:16:26\n",
            "loss: 0.6216, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 14:16:27\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:27\n",
            "loss: 0.6158, acc: 0.7527\n",
            "E2E-ABSA >>> 2022-08-17 14:16:28\n",
            "loss: 0.6392, acc: 0.7402\n",
            "E2E-ABSA >>> 2022-08-17 14:16:29\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:30\n",
            "loss: 0.6099, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-08-17 14:16:31\n",
            "loss: 0.6233, acc: 0.7434\n",
            "E2E-ABSA >>> 2022-08-17 14:16:32\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:32\n",
            "loss: 0.6408, acc: 0.7263\n",
            "E2E-ABSA >>> 2022-08-17 14:16:33\n",
            "loss: 0.6361, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 14:16:34\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:35\n",
            "loss: 0.6377, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 14:16:36\n",
            "loss: 0.6297, acc: 0.7405\n",
            "E2E-ABSA >>> 2022-08-17 14:16:36\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:37\n",
            "loss: 0.6258, acc: 0.7393\n",
            "E2E-ABSA >>> 2022-08-17 14:16:39\n",
            "loss: 0.6231, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-08-17 14:16:39\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:40\n",
            "loss: 0.6262, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-08-17 14:16:41\n",
            "loss: 0.6285, acc: 0.7386\n",
            "E2E-ABSA >>> 2022-08-17 14:16:41\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:43\n",
            "loss: 0.6172, acc: 0.7348\n",
            "E2E-ABSA >>> 2022-08-17 14:16:44\n",
            "loss: 0.6279, acc: 0.7380\n",
            "E2E-ABSA >>> 2022-08-17 14:16:44\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:45\n",
            "loss: 0.6444, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-08-17 14:16:46\n",
            "loss: 0.6245, acc: 0.7400\n",
            "E2E-ABSA >>> 2022-08-17 14:16:46\n",
            ">>> val_acc: 0.7021, val_precision: 0.7021 val_recall: 0.7021, val_f1: 0.7021\n",
            ">> saved: state_dict/ian_SemEval2014_val_f1_0.7021\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:48\n",
            "loss: 0.6142, acc: 0.7453\n",
            "E2E-ABSA >>> 2022-08-17 14:16:49\n",
            "loss: 0.6208, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-08-17 14:16:49\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:50\n",
            "loss: 0.6302, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-08-17 14:16:52\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:52\n",
            "loss: 0.7079, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 14:16:53\n",
            "loss: 0.6163, acc: 0.7423\n",
            "E2E-ABSA >>> 2022-08-17 14:16:54\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:54\n",
            "loss: 0.6149, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:16:55\n",
            "loss: 0.6146, acc: 0.7394\n",
            "E2E-ABSA >>> 2022-08-17 14:16:56\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:57\n",
            "loss: 0.6340, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:16:58\n",
            "loss: 0.6253, acc: 0.7357\n",
            "E2E-ABSA >>> 2022-08-17 14:16:59\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:16:59\n",
            "loss: 0.6599, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 14:17:00\n",
            "loss: 0.5988, acc: 0.7555\n",
            "E2E-ABSA >>> 2022-08-17 14:17:01\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:02\n",
            "loss: 0.5668, acc: 0.7667\n",
            "E2E-ABSA >>> 2022-08-17 14:17:03\n",
            "loss: 0.5911, acc: 0.7543\n",
            "E2E-ABSA >>> 2022-08-17 14:17:04\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:04\n",
            "loss: 0.6358, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 14:17:06\n",
            "loss: 0.6210, acc: 0.7436\n",
            "E2E-ABSA >>> 2022-08-17 14:17:06\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:07\n",
            "loss: 0.6412, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-08-17 14:17:08\n",
            "loss: 0.6216, acc: 0.7302\n",
            "E2E-ABSA >>> 2022-08-17 14:17:09\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:10\n",
            "loss: 0.6126, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-08-17 14:17:11\n",
            "loss: 0.6042, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-08-17 14:17:11\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:12\n",
            "loss: 0.6042, acc: 0.7407\n",
            "E2E-ABSA >>> 2022-08-17 14:17:13\n",
            "loss: 0.6123, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-08-17 14:17:14\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:15\n",
            "loss: 0.5920, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 14:17:16\n",
            "loss: 0.6052, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-08-17 14:17:16\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:17\n",
            "loss: 0.6014, acc: 0.7509\n",
            "E2E-ABSA >>> 2022-08-17 14:17:18\n",
            "loss: 0.6099, acc: 0.7489\n",
            "E2E-ABSA >>> 2022-08-17 14:17:19\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:20\n",
            "loss: 0.5972, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:17:21\n",
            "loss: 0.6098, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 14:17:21\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:22\n",
            "loss: 0.6172, acc: 0.7420\n",
            "E2E-ABSA >>> 2022-08-17 14:17:23\n",
            "loss: 0.6033, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-08-17 14:17:24\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:25\n",
            "loss: 0.6033, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 14:17:26\n",
            "loss: 0.6015, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-08-17 14:17:26\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:27\n",
            "loss: 0.6203, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-08-17 14:17:29\n",
            "loss: 0.6033, acc: 0.7457\n",
            "E2E-ABSA >>> 2022-08-17 14:17:29\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:30\n",
            "loss: 0.6208, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 14:17:31\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:31\n",
            "loss: 0.5395, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:17:33\n",
            "loss: 0.5790, acc: 0.7635\n",
            "E2E-ABSA >>> 2022-08-17 14:17:34\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:34\n",
            "loss: 0.6009, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:17:35\n",
            "loss: 0.5908, acc: 0.7477\n",
            "E2E-ABSA >>> 2022-08-17 14:17:36\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:36\n",
            "loss: 0.5562, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 14:17:38\n",
            "loss: 0.5910, acc: 0.7434\n",
            "E2E-ABSA >>> 2022-08-17 14:17:39\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:39\n",
            "loss: 0.6115, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 14:17:40\n",
            "loss: 0.5992, acc: 0.7495\n",
            "E2E-ABSA >>> 2022-08-17 14:17:41\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:42\n",
            "loss: 0.6193, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-08-17 14:17:43\n",
            "loss: 0.5902, acc: 0.7540\n",
            "E2E-ABSA >>> 2022-08-17 14:17:44\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:44\n",
            "loss: 0.6194, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 14:17:45\n",
            "loss: 0.5962, acc: 0.7495\n",
            "E2E-ABSA >>> 2022-08-17 14:17:46\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:47\n",
            "loss: 0.6548, acc: 0.7253\n",
            "E2E-ABSA >>> 2022-08-17 14:17:48\n",
            "loss: 0.6002, acc: 0.7477\n",
            "E2E-ABSA >>> 2022-08-17 14:17:49\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:49\n",
            "loss: 0.5966, acc: 0.7401\n",
            "E2E-ABSA >>> 2022-08-17 14:17:51\n",
            "loss: 0.5935, acc: 0.7556\n",
            "E2E-ABSA >>> 2022-08-17 14:17:51\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:52\n",
            "loss: 0.6235, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-08-17 14:17:53\n",
            "loss: 0.6062, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-08-17 14:17:54\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:54\n",
            "loss: 0.6177, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-08-17 14:17:56\n",
            "loss: 0.5903, acc: 0.7508\n",
            "E2E-ABSA >>> 2022-08-17 14:17:56\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:17:57\n",
            "loss: 0.5691, acc: 0.7732\n",
            "E2E-ABSA >>> 2022-08-17 14:17:58\n",
            "loss: 0.5855, acc: 0.7569\n",
            "E2E-ABSA >>> 2022-08-17 14:17:59\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:00\n",
            "loss: 0.6052, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-08-17 14:18:01\n",
            "loss: 0.5933, acc: 0.7481\n",
            "E2E-ABSA >>> 2022-08-17 14:18:01\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            "E2E-ABSA >>> 2022-08-17 14:18:01\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7021, val_precision: 0.7021 val_recall: 0.7021, val_f1: 0.7021\n",
            "you can download the best model from state_dict/ian_SemEval2014_val_f1_0.7021\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.7021, test_precision: 0.7021, test_recall: 0.7021, test_f1: 0.7021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2014** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "nqgXCCOoGtQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset SemEval2014_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9z-EMWpGtXk",
        "outputId": "168d961d-51c7-47b4-8f12-3c29c2a4c9ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 3093.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 25457664\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 3969000\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f5e37292b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:41\n",
            "loss: 1.0057, acc: 0.5575\n",
            "E2E-ABSA >>> 2022-08-17 14:18:42\n",
            ">>> val_acc: 0.5745, val_precision: 0.5745 val_recall: 0.5745, val_f1: 0.5745\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.5745\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:42\n",
            "loss: 0.9722, acc: 0.5729\n",
            "E2E-ABSA >>> 2022-08-17 14:18:44\n",
            "loss: 0.9855, acc: 0.5501\n",
            "E2E-ABSA >>> 2022-08-17 14:18:45\n",
            ">>> val_acc: 0.5745, val_precision: 0.5745 val_recall: 0.5745, val_f1: 0.5745\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:45\n",
            "loss: 0.9607, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 14:18:47\n",
            "loss: 0.9528, acc: 0.5714\n",
            "E2E-ABSA >>> 2022-08-17 14:18:48\n",
            ">>> val_acc: 0.5775, val_precision: 0.5775 val_recall: 0.5775, val_f1: 0.5775\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.5775\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:48\n",
            "loss: 0.9378, acc: 0.5694\n",
            "E2E-ABSA >>> 2022-08-17 14:18:49\n",
            "loss: 0.9543, acc: 0.5614\n",
            "E2E-ABSA >>> 2022-08-17 14:18:51\n",
            ">>> val_acc: 0.5775, val_precision: 0.5775 val_recall: 0.5775, val_f1: 0.5775\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:51\n",
            "loss: 0.9542, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 14:18:52\n",
            "loss: 0.9441, acc: 0.5691\n",
            "E2E-ABSA >>> 2022-08-17 14:18:53\n",
            ">>> val_acc: 0.5836, val_precision: 0.5836 val_recall: 0.5836, val_f1: 0.5836\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.5836\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:54\n",
            "loss: 0.9273, acc: 0.5813\n",
            "E2E-ABSA >>> 2022-08-17 14:18:55\n",
            "loss: 0.9297, acc: 0.5769\n",
            "E2E-ABSA >>> 2022-08-17 14:18:56\n",
            ">>> val_acc: 0.5805, val_precision: 0.5805 val_recall: 0.5805, val_f1: 0.5805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:18:57\n",
            "loss: 0.9497, acc: 0.5677\n",
            "E2E-ABSA >>> 2022-08-17 14:18:58\n",
            "loss: 0.9289, acc: 0.5818\n",
            "E2E-ABSA >>> 2022-08-17 14:18:59\n",
            ">>> val_acc: 0.5866, val_precision: 0.5866 val_recall: 0.5866, val_f1: 0.5866\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.5866\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:00\n",
            "loss: 0.9441, acc: 0.5670\n",
            "E2E-ABSA >>> 2022-08-17 14:19:01\n",
            "loss: 0.9405, acc: 0.5717\n",
            "E2E-ABSA >>> 2022-08-17 14:19:02\n",
            ">>> val_acc: 0.5866, val_precision: 0.5866 val_recall: 0.5866, val_f1: 0.5866\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:03\n",
            "loss: 0.9214, acc: 0.5742\n",
            "E2E-ABSA >>> 2022-08-17 14:19:04\n",
            "loss: 0.9250, acc: 0.5823\n",
            "E2E-ABSA >>> 2022-08-17 14:19:05\n",
            ">>> val_acc: 0.5927, val_precision: 0.5927 val_recall: 0.5927, val_f1: 0.5927\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.5927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:06\n",
            "loss: 0.9194, acc: 0.5984\n",
            "E2E-ABSA >>> 2022-08-17 14:19:07\n",
            "loss: 0.9320, acc: 0.5844\n",
            "E2E-ABSA >>> 2022-08-17 14:19:08\n",
            ">>> val_acc: 0.5927, val_precision: 0.5927 val_recall: 0.5927, val_f1: 0.5927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:09\n",
            "loss: 0.9299, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:19:10\n",
            "loss: 0.9257, acc: 0.5891\n",
            "E2E-ABSA >>> 2022-08-17 14:19:11\n",
            ">>> val_acc: 0.5897, val_precision: 0.5897 val_recall: 0.5897, val_f1: 0.5897\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:11\n",
            "loss: 0.9369, acc: 0.5824\n",
            "E2E-ABSA >>> 2022-08-17 14:19:13\n",
            "loss: 0.9274, acc: 0.5866\n",
            "E2E-ABSA >>> 2022-08-17 14:19:13\n",
            ">>> val_acc: 0.5957, val_precision: 0.5957 val_recall: 0.5957, val_f1: 0.5957\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.5957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:14\n",
            "loss: 0.9221, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:19:16\n",
            "loss: 0.9205, acc: 0.5901\n",
            "E2E-ABSA >>> 2022-08-17 14:19:16\n",
            ">>> val_acc: 0.5927, val_precision: 0.5927 val_recall: 0.5927, val_f1: 0.5927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:17\n",
            "loss: 0.9094, acc: 0.6034\n",
            "E2E-ABSA >>> 2022-08-17 14:19:19\n",
            "loss: 0.9223, acc: 0.5930\n",
            "E2E-ABSA >>> 2022-08-17 14:19:19\n",
            ">>> val_acc: 0.6049, val_precision: 0.6049 val_recall: 0.6049, val_f1: 0.6049\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.6049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:20\n",
            "loss: 0.9146, acc: 0.5900\n",
            "E2E-ABSA >>> 2022-08-17 14:19:22\n",
            "loss: 0.9193, acc: 0.5934\n",
            "E2E-ABSA >>> 2022-08-17 14:19:22\n",
            ">>> val_acc: 0.6049, val_precision: 0.6049 val_recall: 0.6049, val_f1: 0.6049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:23\n",
            "loss: 0.9216, acc: 0.5868\n",
            "E2E-ABSA >>> 2022-08-17 14:19:24\n",
            "loss: 0.9219, acc: 0.5908\n",
            "E2E-ABSA >>> 2022-08-17 14:19:25\n",
            ">>> val_acc: 0.6018, val_precision: 0.6018 val_recall: 0.6018, val_f1: 0.6018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:26\n",
            "loss: 0.9140, acc: 0.5957\n",
            "E2E-ABSA >>> 2022-08-17 14:19:27\n",
            ">>> val_acc: 0.6018, val_precision: 0.6018 val_recall: 0.6018, val_f1: 0.6018\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:27\n",
            "loss: 0.8479, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 14:19:29\n",
            "loss: 0.9308, acc: 0.5839\n",
            "E2E-ABSA >>> 2022-08-17 14:19:30\n",
            ">>> val_acc: 0.5988, val_precision: 0.5988 val_recall: 0.5988, val_f1: 0.5988\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:30\n",
            "loss: 0.9525, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 14:19:32\n",
            "loss: 0.9226, acc: 0.5972\n",
            "E2E-ABSA >>> 2022-08-17 14:19:33\n",
            ">>> val_acc: 0.6049, val_precision: 0.6049 val_recall: 0.6049, val_f1: 0.6049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:33\n",
            "loss: 0.8556, acc: 0.6473\n",
            "E2E-ABSA >>> 2022-08-17 14:19:34\n",
            "loss: 0.9152, acc: 0.6014\n",
            "E2E-ABSA >>> 2022-08-17 14:19:36\n",
            ">>> val_acc: 0.6140, val_precision: 0.6140 val_recall: 0.6140, val_f1: 0.6140\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.614\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:36\n",
            "loss: 0.9089, acc: 0.6062\n",
            "E2E-ABSA >>> 2022-08-17 14:19:37\n",
            "loss: 0.9054, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 14:19:38\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:39\n",
            "loss: 0.9191, acc: 0.5865\n",
            "E2E-ABSA >>> 2022-08-17 14:19:40\n",
            "loss: 0.9261, acc: 0.5942\n",
            "E2E-ABSA >>> 2022-08-17 14:19:41\n",
            ">>> val_acc: 0.6049, val_precision: 0.6049 val_recall: 0.6049, val_f1: 0.6049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:42\n",
            "loss: 0.9388, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 14:19:43\n",
            "loss: 0.9178, acc: 0.5942\n",
            "E2E-ABSA >>> 2022-08-17 14:19:44\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:45\n",
            "loss: 0.9043, acc: 0.6003\n",
            "E2E-ABSA >>> 2022-08-17 14:19:46\n",
            "loss: 0.9041, acc: 0.6069\n",
            "E2E-ABSA >>> 2022-08-17 14:19:47\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:47\n",
            "loss: 0.9109, acc: 0.5966\n",
            "E2E-ABSA >>> 2022-08-17 14:19:49\n",
            "loss: 0.9123, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:19:50\n",
            ">>> val_acc: 0.6140, val_precision: 0.6140 val_recall: 0.6140, val_f1: 0.6140\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:50\n",
            "loss: 0.9234, acc: 0.5950\n",
            "E2E-ABSA >>> 2022-08-17 14:19:52\n",
            "loss: 0.9187, acc: 0.5933\n",
            "E2E-ABSA >>> 2022-08-17 14:19:52\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:53\n",
            "loss: 0.8884, acc: 0.6228\n",
            "E2E-ABSA >>> 2022-08-17 14:19:55\n",
            "loss: 0.9127, acc: 0.5982\n",
            "E2E-ABSA >>> 2022-08-17 14:19:55\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:56\n",
            "loss: 0.8951, acc: 0.6119\n",
            "E2E-ABSA >>> 2022-08-17 14:19:57\n",
            "loss: 0.9096, acc: 0.6022\n",
            "E2E-ABSA >>> 2022-08-17 14:19:58\n",
            ">>> val_acc: 0.6322, val_precision: 0.6322 val_recall: 0.6322, val_f1: 0.6322\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.6322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:19:59\n",
            "loss: 0.8987, acc: 0.6121\n",
            "E2E-ABSA >>> 2022-08-17 14:20:00\n",
            "loss: 0.9069, acc: 0.6031\n",
            "E2E-ABSA >>> 2022-08-17 14:20:01\n",
            ">>> val_acc: 0.6322, val_precision: 0.6322 val_recall: 0.6322, val_f1: 0.6322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:02\n",
            "loss: 0.8985, acc: 0.6115\n",
            "E2E-ABSA >>> 2022-08-17 14:20:03\n",
            "loss: 0.9069, acc: 0.6070\n",
            "E2E-ABSA >>> 2022-08-17 14:20:04\n",
            ">>> val_acc: 0.6383, val_precision: 0.6383 val_recall: 0.6383, val_f1: 0.6383\n",
            ">> saved: state_dict/ian_SemEval2014_know_val_f1_0.6383\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:05\n",
            "loss: 0.9141, acc: 0.5984\n",
            "E2E-ABSA >>> 2022-08-17 14:20:06\n",
            "loss: 0.9108, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 14:20:06\n",
            ">>> val_acc: 0.6170, val_precision: 0.6170 val_recall: 0.6170, val_f1: 0.6170\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:08\n",
            "loss: 0.9143, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:20:09\n",
            "loss: 0.9121, acc: 0.5995\n",
            "E2E-ABSA >>> 2022-08-17 14:20:09\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:10\n",
            "loss: 0.9243, acc: 0.5931\n",
            "E2E-ABSA >>> 2022-08-17 14:20:12\n",
            "loss: 0.9131, acc: 0.6035\n",
            "E2E-ABSA >>> 2022-08-17 14:20:12\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:13\n",
            "loss: 0.9188, acc: 0.5893\n",
            "E2E-ABSA >>> 2022-08-17 14:20:15\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:15\n",
            "loss: 0.9243, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 14:20:16\n",
            "loss: 0.9312, acc: 0.5847\n",
            "E2E-ABSA >>> 2022-08-17 14:20:18\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:18\n",
            "loss: 0.9836, acc: 0.5563\n",
            "E2E-ABSA >>> 2022-08-17 14:20:19\n",
            "loss: 0.9266, acc: 0.5920\n",
            "E2E-ABSA >>> 2022-08-17 14:20:20\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:21\n",
            "loss: 0.9937, acc: 0.5234\n",
            "E2E-ABSA >>> 2022-08-17 14:20:22\n",
            "loss: 0.9321, acc: 0.5867\n",
            "E2E-ABSA >>> 2022-08-17 14:20:23\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:23\n",
            "loss: 0.8778, acc: 0.6307\n",
            "E2E-ABSA >>> 2022-08-17 14:20:25\n",
            "loss: 0.9037, acc: 0.6025\n",
            "E2E-ABSA >>> 2022-08-17 14:20:26\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:26\n",
            "loss: 0.9066, acc: 0.5960\n",
            "E2E-ABSA >>> 2022-08-17 14:20:28\n",
            "loss: 0.9094, acc: 0.5991\n",
            "E2E-ABSA >>> 2022-08-17 14:20:29\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:29\n",
            "loss: 0.8724, acc: 0.6140\n",
            "E2E-ABSA >>> 2022-08-17 14:20:31\n",
            "loss: 0.8972, acc: 0.6040\n",
            "E2E-ABSA >>> 2022-08-17 14:20:32\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:32\n",
            "loss: 0.8892, acc: 0.6297\n",
            "E2E-ABSA >>> 2022-08-17 14:20:33\n",
            "loss: 0.9046, acc: 0.6040\n",
            "E2E-ABSA >>> 2022-08-17 14:20:34\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:35\n",
            "loss: 0.9240, acc: 0.5897\n",
            "E2E-ABSA >>> 2022-08-17 14:20:36\n",
            "loss: 0.9086, acc: 0.5989\n",
            "E2E-ABSA >>> 2022-08-17 14:20:37\n",
            ">>> val_acc: 0.6261, val_precision: 0.6261 val_recall: 0.6261, val_f1: 0.6261\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:38\n",
            "loss: 0.9030, acc: 0.6142\n",
            "E2E-ABSA >>> 2022-08-17 14:20:39\n",
            "loss: 0.9112, acc: 0.5999\n",
            "E2E-ABSA >>> 2022-08-17 14:20:40\n",
            ">>> val_acc: 0.6261, val_precision: 0.6261 val_recall: 0.6261, val_f1: 0.6261\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:41\n",
            "loss: 0.8827, acc: 0.6315\n",
            "E2E-ABSA >>> 2022-08-17 14:20:42\n",
            "loss: 0.9054, acc: 0.6072\n",
            "E2E-ABSA >>> 2022-08-17 14:20:43\n",
            ">>> val_acc: 0.6353, val_precision: 0.6353 val_recall: 0.6353, val_f1: 0.6353\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:44\n",
            "loss: 0.9104, acc: 0.6045\n",
            "E2E-ABSA >>> 2022-08-17 14:20:45\n",
            "loss: 0.9016, acc: 0.6082\n",
            "E2E-ABSA >>> 2022-08-17 14:20:45\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:46\n",
            "loss: 0.9166, acc: 0.5964\n",
            "E2E-ABSA >>> 2022-08-17 14:20:48\n",
            "loss: 0.9044, acc: 0.6062\n",
            "E2E-ABSA >>> 2022-08-17 14:20:48\n",
            ">>> val_acc: 0.6261, val_precision: 0.6261 val_recall: 0.6261, val_f1: 0.6261\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:49\n",
            "loss: 0.9039, acc: 0.5995\n",
            "E2E-ABSA >>> 2022-08-17 14:20:51\n",
            "loss: 0.9059, acc: 0.6030\n",
            "E2E-ABSA >>> 2022-08-17 14:20:51\n",
            ">>> val_acc: 0.6322, val_precision: 0.6322 val_recall: 0.6322, val_f1: 0.6322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:52\n",
            "loss: 0.9101, acc: 0.6067\n",
            "E2E-ABSA >>> 2022-08-17 14:20:54\n",
            "loss: 0.9077, acc: 0.6041\n",
            "E2E-ABSA >>> 2022-08-17 14:20:54\n",
            ">>> val_acc: 0.6261, val_precision: 0.6261 val_recall: 0.6261, val_f1: 0.6261\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:55\n",
            "loss: 0.9157, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:20:56\n",
            "loss: 0.9054, acc: 0.6037\n",
            "E2E-ABSA >>> 2022-08-17 14:20:57\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:20:58\n",
            "loss: 0.9159, acc: 0.5918\n",
            "E2E-ABSA >>> 2022-08-17 14:20:59\n",
            "loss: 0.9055, acc: 0.6010\n",
            "E2E-ABSA >>> 2022-08-17 14:20:59\n",
            ">>> val_acc: 0.6170, val_precision: 0.6170 val_recall: 0.6170, val_f1: 0.6170\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:01\n",
            "loss: 0.9043, acc: 0.6031\n",
            "E2E-ABSA >>> 2022-08-17 14:21:02\n",
            ">>> val_acc: 0.6170, val_precision: 0.6170 val_recall: 0.6170, val_f1: 0.6170\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:02\n",
            "loss: 0.9482, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:21:04\n",
            "loss: 0.9118, acc: 0.5973\n",
            "E2E-ABSA >>> 2022-08-17 14:21:05\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:05\n",
            "loss: 0.8517, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 14:21:06\n",
            "loss: 0.9176, acc: 0.5965\n",
            "E2E-ABSA >>> 2022-08-17 14:21:08\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:08\n",
            "loss: 0.8728, acc: 0.6319\n",
            "E2E-ABSA >>> 2022-08-17 14:21:09\n",
            "loss: 0.8904, acc: 0.6123\n",
            "E2E-ABSA >>> 2022-08-17 14:21:10\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:11\n",
            "loss: 0.9161, acc: 0.6120\n",
            "E2E-ABSA >>> 2022-08-17 14:21:12\n",
            "loss: 0.9031, acc: 0.6084\n",
            "E2E-ABSA >>> 2022-08-17 14:21:13\n",
            ">>> val_acc: 0.6292, val_precision: 0.6292 val_recall: 0.6292, val_f1: 0.6292\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:14\n",
            "loss: 0.9159, acc: 0.5958\n",
            "E2E-ABSA >>> 2022-08-17 14:21:15\n",
            "loss: 0.9040, acc: 0.6034\n",
            "E2E-ABSA >>> 2022-08-17 14:21:16\n",
            ">>> val_acc: 0.6170, val_precision: 0.6170 val_recall: 0.6170, val_f1: 0.6170\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:17\n",
            "loss: 0.9138, acc: 0.5903\n",
            "E2E-ABSA >>> 2022-08-17 14:21:18\n",
            "loss: 0.9056, acc: 0.6052\n",
            "E2E-ABSA >>> 2022-08-17 14:21:19\n",
            ">>> val_acc: 0.6170, val_precision: 0.6170 val_recall: 0.6170, val_f1: 0.6170\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:19\n",
            "loss: 0.9040, acc: 0.5997\n",
            "E2E-ABSA >>> 2022-08-17 14:21:21\n",
            "loss: 0.8929, acc: 0.6166\n",
            "E2E-ABSA >>> 2022-08-17 14:21:22\n",
            ">>> val_acc: 0.6322, val_precision: 0.6322 val_recall: 0.6322, val_f1: 0.6322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:22\n",
            "loss: 0.9091, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 14:21:24\n",
            "loss: 0.8930, acc: 0.6136\n",
            "E2E-ABSA >>> 2022-08-17 14:21:24\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:21:25\n",
            "loss: 0.8901, acc: 0.6204\n",
            "E2E-ABSA >>> 2022-08-17 14:21:27\n",
            "loss: 0.9001, acc: 0.6063\n",
            "E2E-ABSA >>> 2022-08-17 14:21:27\n",
            ">>> val_acc: 0.6201, val_precision: 0.6201 val_recall: 0.6201, val_f1: 0.6201\n",
            "E2E-ABSA >>> 2022-08-17 14:21:27\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6383, val_precision: 0.6383 val_recall: 0.6383, val_f1: 0.6383\n",
            "you can download the best model from state_dict/ian_SemEval2014_know_val_f1_0.6383\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.6383, test_precision: 0.6383, test_recall: 0.6383, test_f1: 0.6383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**MEMNET**)"
      ],
      "metadata": {
        "id": "k8sj0wudGtdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset SemEval2014 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMa6BOjMGtj4",
        "outputId": "00349346-f265-478d-e8ca-7784814a7763"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 3091.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 11583488\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 2532000\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f567a859b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:07\n",
            "loss: 1.0615, acc: 0.4919\n",
            "E2E-ABSA >>> 2022-08-17 14:22:07\n",
            ">>> val_acc: 0.5927, val_precision: 0.5927 val_recall: 0.5927, val_f1: 0.5927\n",
            ">> saved: state_dict/memnet_SemEval2014_val_f1_0.5927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:07\n",
            "loss: 0.9896, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:22:08\n",
            "loss: 0.8993, acc: 0.6108\n",
            "E2E-ABSA >>> 2022-08-17 14:22:09\n",
            ">>> val_acc: 0.6444, val_precision: 0.6444 val_recall: 0.6444, val_f1: 0.6444\n",
            ">> saved: state_dict/memnet_SemEval2014_val_f1_0.6444\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:09\n",
            "loss: 0.8767, acc: 0.5990\n",
            "E2E-ABSA >>> 2022-08-17 14:22:10\n",
            "loss: 0.8590, acc: 0.6317\n",
            "E2E-ABSA >>> 2022-08-17 14:22:10\n",
            ">>> val_acc: 0.6413, val_precision: 0.6413 val_recall: 0.6413, val_f1: 0.6413\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:10\n",
            "loss: 0.8704, acc: 0.6111\n",
            "E2E-ABSA >>> 2022-08-17 14:22:11\n",
            "loss: 0.8164, acc: 0.6510\n",
            "E2E-ABSA >>> 2022-08-17 14:22:12\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">> saved: state_dict/memnet_SemEval2014_val_f1_0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:12\n",
            "loss: 0.8223, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:22:13\n",
            "loss: 0.7931, acc: 0.6603\n",
            "E2E-ABSA >>> 2022-08-17 14:22:13\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">> saved: state_dict/memnet_SemEval2014_val_f1_0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:13\n",
            "loss: 0.7955, acc: 0.6479\n",
            "E2E-ABSA >>> 2022-08-17 14:22:14\n",
            "loss: 0.7652, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 14:22:15\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">> saved: state_dict/memnet_SemEval2014_val_f1_0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:15\n",
            "loss: 0.7532, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-08-17 14:22:16\n",
            "loss: 0.7465, acc: 0.6765\n",
            "E2E-ABSA >>> 2022-08-17 14:22:16\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:17\n",
            "loss: 0.7083, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 14:22:17\n",
            "loss: 0.7250, acc: 0.6849\n",
            "E2E-ABSA >>> 2022-08-17 14:22:18\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">> saved: state_dict/memnet_SemEval2014_val_f1_0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:18\n",
            "loss: 0.6888, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 14:22:19\n",
            "loss: 0.7054, acc: 0.6968\n",
            "E2E-ABSA >>> 2022-08-17 14:22:19\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:20\n",
            "loss: 0.7076, acc: 0.7095\n",
            "E2E-ABSA >>> 2022-08-17 14:22:20\n",
            "loss: 0.6976, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-08-17 14:22:21\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">> saved: state_dict/memnet_SemEval2014_val_f1_0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:21\n",
            "loss: 0.6921, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 14:22:22\n",
            "loss: 0.6807, acc: 0.7086\n",
            "E2E-ABSA >>> 2022-08-17 14:22:22\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:23\n",
            "loss: 0.6783, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 14:22:23\n",
            "loss: 0.6791, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 14:22:24\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:24\n",
            "loss: 0.6568, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-08-17 14:22:25\n",
            "loss: 0.6647, acc: 0.7140\n",
            "E2E-ABSA >>> 2022-08-17 14:22:25\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:26\n",
            "loss: 0.6510, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-08-17 14:22:26\n",
            "loss: 0.6609, acc: 0.7198\n",
            "E2E-ABSA >>> 2022-08-17 14:22:27\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:27\n",
            "loss: 0.6577, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 14:22:28\n",
            "loss: 0.6495, acc: 0.7218\n",
            "E2E-ABSA >>> 2022-08-17 14:22:28\n",
            ">>> val_acc: 0.6596, val_precision: 0.6596 val_recall: 0.6596, val_f1: 0.6596\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:29\n",
            "loss: 0.6450, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 14:22:29\n",
            "loss: 0.6480, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 14:22:30\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:30\n",
            "loss: 0.6271, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 14:22:31\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:31\n",
            "loss: 0.6290, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:22:32\n",
            "loss: 0.6347, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-08-17 14:22:32\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:33\n",
            "loss: 0.5824, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:22:33\n",
            "loss: 0.6140, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-08-17 14:22:34\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:34\n",
            "loss: 0.5620, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-08-17 14:22:35\n",
            "loss: 0.6192, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 14:22:35\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:36\n",
            "loss: 0.5564, acc: 0.7594\n",
            "E2E-ABSA >>> 2022-08-17 14:22:36\n",
            "loss: 0.6195, acc: 0.7359\n",
            "E2E-ABSA >>> 2022-08-17 14:22:37\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:37\n",
            "loss: 0.6402, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-08-17 14:22:38\n",
            "loss: 0.6269, acc: 0.7307\n",
            "E2E-ABSA >>> 2022-08-17 14:22:38\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:39\n",
            "loss: 0.6347, acc: 0.7207\n",
            "E2E-ABSA >>> 2022-08-17 14:22:39\n",
            "loss: 0.6162, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-08-17 14:22:40\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:40\n",
            "loss: 0.5834, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-08-17 14:22:41\n",
            "loss: 0.6056, acc: 0.7473\n",
            "E2E-ABSA >>> 2022-08-17 14:22:41\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:42\n",
            "loss: 0.5762, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-08-17 14:22:42\n",
            "loss: 0.5961, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 14:22:43\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:43\n",
            "loss: 0.6091, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:22:44\n",
            "loss: 0.5972, acc: 0.7508\n",
            "E2E-ABSA >>> 2022-08-17 14:22:44\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:45\n",
            "loss: 0.6084, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 14:22:45\n",
            "loss: 0.5950, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-08-17 14:22:46\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:46\n",
            "loss: 0.5773, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:22:47\n",
            "loss: 0.6010, acc: 0.7465\n",
            "E2E-ABSA >>> 2022-08-17 14:22:47\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:48\n",
            "loss: 0.5966, acc: 0.7537\n",
            "E2E-ABSA >>> 2022-08-17 14:22:48\n",
            "loss: 0.5916, acc: 0.7522\n",
            "E2E-ABSA >>> 2022-08-17 14:22:49\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:49\n",
            "loss: 0.5808, acc: 0.7584\n",
            "E2E-ABSA >>> 2022-08-17 14:22:50\n",
            "loss: 0.5913, acc: 0.7554\n",
            "E2E-ABSA >>> 2022-08-17 14:22:50\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:51\n",
            "loss: 0.6017, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 14:22:52\n",
            "loss: 0.5870, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-08-17 14:22:52\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:52\n",
            "loss: 0.5767, acc: 0.7573\n",
            "E2E-ABSA >>> 2022-08-17 14:22:53\n",
            "loss: 0.5874, acc: 0.7544\n",
            "E2E-ABSA >>> 2022-08-17 14:22:53\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:54\n",
            "loss: 0.5923, acc: 0.7486\n",
            "E2E-ABSA >>> 2022-08-17 14:22:55\n",
            "loss: 0.5847, acc: 0.7565\n",
            "E2E-ABSA >>> 2022-08-17 14:22:55\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:55\n",
            "loss: 0.5513, acc: 0.7685\n",
            "E2E-ABSA >>> 2022-08-17 14:22:56\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:56\n",
            "loss: 0.5965, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:22:57\n",
            "loss: 0.5872, acc: 0.7638\n",
            "E2E-ABSA >>> 2022-08-17 14:22:58\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:58\n",
            "loss: 0.6196, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 14:22:58\n",
            "loss: 0.5840, acc: 0.7511\n",
            "E2E-ABSA >>> 2022-08-17 14:22:59\n",
            ">>> val_acc: 0.6657, val_precision: 0.6657 val_recall: 0.6657, val_f1: 0.6657\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:22:59\n",
            "loss: 0.5393, acc: 0.7852\n",
            "E2E-ABSA >>> 2022-08-17 14:23:00\n",
            "loss: 0.5555, acc: 0.7742\n",
            "E2E-ABSA >>> 2022-08-17 14:23:01\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:01\n",
            "loss: 0.5688, acc: 0.7727\n",
            "E2E-ABSA >>> 2022-08-17 14:23:02\n",
            "loss: 0.5724, acc: 0.7592\n",
            "E2E-ABSA >>> 2022-08-17 14:23:02\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:02\n",
            "loss: 0.5885, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:23:03\n",
            "loss: 0.5640, acc: 0.7676\n",
            "E2E-ABSA >>> 2022-08-17 14:23:04\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:04\n",
            "loss: 0.5701, acc: 0.7684\n",
            "E2E-ABSA >>> 2022-08-17 14:23:05\n",
            "loss: 0.5618, acc: 0.7682\n",
            "E2E-ABSA >>> 2022-08-17 14:23:05\n",
            ">>> val_acc: 0.6687, val_precision: 0.6687 val_recall: 0.6687, val_f1: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 14:23:05\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            "you can download the best model from state_dict/memnet_SemEval2014_val_f1_0.6991\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.6991, test_precision: 0.6991, test_recall: 0.6991, test_f1: 0.6991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2014** dataset on model(**MEMNET**)"
      ],
      "metadata": {
        "id": "6X6qwUSxGtqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset SemEval2014_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_npoh6iGtxV",
        "outputId": "88b72e9e-3e6d-4c5b-cca3-d868a779ca00"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 3093.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 18232320\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 3969000\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f63aa529b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:44\n",
            "loss: 1.0438, acc: 0.5006\n",
            "E2E-ABSA >>> 2022-08-17 14:23:45\n",
            ">>> val_acc: 0.5897, val_precision: 0.5897 val_recall: 0.5897, val_f1: 0.5897\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.5897\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:45\n",
            "loss: 0.9579, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 14:23:46\n",
            "loss: 0.9481, acc: 0.5737\n",
            "E2E-ABSA >>> 2022-08-17 14:23:46\n",
            ">>> val_acc: 0.6079, val_precision: 0.6079 val_recall: 0.6079, val_f1: 0.6079\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.6079\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:46\n",
            "loss: 0.9045, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 14:23:47\n",
            "loss: 0.9128, acc: 0.5999\n",
            "E2E-ABSA >>> 2022-08-17 14:23:48\n",
            ">>> val_acc: 0.6049, val_precision: 0.6049 val_recall: 0.6049, val_f1: 0.6049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:48\n",
            "loss: 0.8713, acc: 0.6181\n",
            "E2E-ABSA >>> 2022-08-17 14:23:49\n",
            "loss: 0.8780, acc: 0.6282\n",
            "E2E-ABSA >>> 2022-08-17 14:23:49\n",
            ">>> val_acc: 0.6322, val_precision: 0.6322 val_recall: 0.6322, val_f1: 0.6322\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.6322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:50\n",
            "loss: 0.9204, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 14:23:50\n",
            "loss: 0.8777, acc: 0.6184\n",
            "E2E-ABSA >>> 2022-08-17 14:23:51\n",
            ">>> val_acc: 0.6322, val_precision: 0.6322 val_recall: 0.6322, val_f1: 0.6322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:51\n",
            "loss: 0.8079, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-08-17 14:23:52\n",
            "loss: 0.8447, acc: 0.6423\n",
            "E2E-ABSA >>> 2022-08-17 14:23:52\n",
            ">>> val_acc: 0.6444, val_precision: 0.6444 val_recall: 0.6444, val_f1: 0.6444\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.6444\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:53\n",
            "loss: 0.8484, acc: 0.6319\n",
            "E2E-ABSA >>> 2022-08-17 14:23:53\n",
            "loss: 0.8438, acc: 0.6360\n",
            "E2E-ABSA >>> 2022-08-17 14:23:54\n",
            ">>> val_acc: 0.6565, val_precision: 0.6565 val_recall: 0.6565, val_f1: 0.6565\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.6565\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:54\n",
            "loss: 0.8091, acc: 0.6637\n",
            "E2E-ABSA >>> 2022-08-17 14:23:55\n",
            "loss: 0.8284, acc: 0.6435\n",
            "E2E-ABSA >>> 2022-08-17 14:23:55\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:56\n",
            "loss: 0.7906, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 14:23:57\n",
            "loss: 0.8139, acc: 0.6524\n",
            "E2E-ABSA >>> 2022-08-17 14:23:57\n",
            ">>> val_acc: 0.6626, val_precision: 0.6626 val_recall: 0.6626, val_f1: 0.6626\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:57\n",
            "loss: 0.8027, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-08-17 14:23:58\n",
            "loss: 0.8067, acc: 0.6640\n",
            "E2E-ABSA >>> 2022-08-17 14:23:58\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:23:59\n",
            "loss: 0.7960, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-08-17 14:24:00\n",
            "loss: 0.7930, acc: 0.6594\n",
            "E2E-ABSA >>> 2022-08-17 14:24:00\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:01\n",
            "loss: 0.7802, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-08-17 14:24:01\n",
            "loss: 0.7702, acc: 0.6709\n",
            "E2E-ABSA >>> 2022-08-17 14:24:02\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.693\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:02\n",
            "loss: 0.7788, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-08-17 14:24:03\n",
            "loss: 0.7607, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 14:24:03\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:04\n",
            "loss: 0.7312, acc: 0.6947\n",
            "E2E-ABSA >>> 2022-08-17 14:24:04\n",
            "loss: 0.7462, acc: 0.6826\n",
            "E2E-ABSA >>> 2022-08-17 14:24:05\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:05\n",
            "loss: 0.7200, acc: 0.7016\n",
            "E2E-ABSA >>> 2022-08-17 14:24:06\n",
            "loss: 0.7313, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 14:24:06\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:07\n",
            "loss: 0.7065, acc: 0.7090\n",
            "E2E-ABSA >>> 2022-08-17 14:24:07\n",
            "loss: 0.7204, acc: 0.6944\n",
            "E2E-ABSA >>> 2022-08-17 14:24:08\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.696\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:08\n",
            "loss: 0.6989, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 14:24:09\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:09\n",
            "loss: 0.6943, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:24:10\n",
            "loss: 0.7278, acc: 0.6844\n",
            "E2E-ABSA >>> 2022-08-17 14:24:11\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:11\n",
            "loss: 0.6515, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:24:11\n",
            "loss: 0.7029, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 14:24:12\n",
            ">>> val_acc: 0.7082, val_precision: 0.7082 val_recall: 0.7082, val_f1: 0.7082\n",
            ">> saved: state_dict/memnet_SemEval2014_know_val_f1_0.7082\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:12\n",
            "loss: 0.6673, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-08-17 14:24:13\n",
            "loss: 0.6810, acc: 0.7149\n",
            "E2E-ABSA >>> 2022-08-17 14:24:14\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:14\n",
            "loss: 0.7021, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:24:14\n",
            "loss: 0.7004, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-08-17 14:24:15\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:15\n",
            "loss: 0.7031, acc: 0.7115\n",
            "E2E-ABSA >>> 2022-08-17 14:24:16\n",
            "loss: 0.6761, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-08-17 14:24:17\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:17\n",
            "loss: 0.7102, acc: 0.6836\n",
            "E2E-ABSA >>> 2022-08-17 14:24:18\n",
            "loss: 0.6702, acc: 0.7145\n",
            "E2E-ABSA >>> 2022-08-17 14:24:18\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:18\n",
            "loss: 0.6550, acc: 0.7138\n",
            "E2E-ABSA >>> 2022-08-17 14:24:19\n",
            "loss: 0.6774, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 14:24:20\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:20\n",
            "loss: 0.6665, acc: 0.7145\n",
            "E2E-ABSA >>> 2022-08-17 14:24:21\n",
            "loss: 0.6713, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:24:21\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:21\n",
            "loss: 0.6860, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 14:24:22\n",
            "loss: 0.6718, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 14:24:23\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:23\n",
            "loss: 0.6695, acc: 0.7154\n",
            "E2E-ABSA >>> 2022-08-17 14:24:24\n",
            "loss: 0.6590, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 14:24:24\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:24\n",
            "loss: 0.6473, acc: 0.7218\n",
            "E2E-ABSA >>> 2022-08-17 14:24:25\n",
            "loss: 0.6607, acc: 0.7234\n",
            "E2E-ABSA >>> 2022-08-17 14:24:25\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:26\n",
            "loss: 0.6732, acc: 0.7206\n",
            "E2E-ABSA >>> 2022-08-17 14:24:27\n",
            "loss: 0.6598, acc: 0.7288\n",
            "E2E-ABSA >>> 2022-08-17 14:24:27\n",
            ">>> val_acc: 0.6748, val_precision: 0.6748 val_recall: 0.6748, val_f1: 0.6748\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:28\n",
            "loss: 0.6479, acc: 0.7314\n",
            "E2E-ABSA >>> 2022-08-17 14:24:28\n",
            "loss: 0.6493, acc: 0.7270\n",
            "E2E-ABSA >>> 2022-08-17 14:24:28\n",
            ">>> val_acc: 0.6869, val_precision: 0.6869 val_recall: 0.6869, val_f1: 0.6869\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:29\n",
            "loss: 0.6307, acc: 0.7398\n",
            "E2E-ABSA >>> 2022-08-17 14:24:30\n",
            "loss: 0.6439, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-08-17 14:24:30\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:31\n",
            "loss: 0.6495, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 14:24:31\n",
            "loss: 0.6476, acc: 0.7251\n",
            "E2E-ABSA >>> 2022-08-17 14:24:31\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:32\n",
            "loss: 0.6430, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-08-17 14:24:33\n",
            "loss: 0.6426, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 14:24:33\n",
            ">>> val_acc: 0.6778, val_precision: 0.6778 val_recall: 0.6778, val_f1: 0.6778\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:34\n",
            "loss: 0.6279, acc: 0.7392\n",
            "E2E-ABSA >>> 2022-08-17 14:24:34\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:34\n",
            "loss: 0.5751, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:24:35\n",
            "loss: 0.6490, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 14:24:36\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:36\n",
            "loss: 0.7043, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 14:24:37\n",
            "loss: 0.6286, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 14:24:37\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:37\n",
            "loss: 0.6616, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 14:24:38\n",
            "loss: 0.6358, acc: 0.7349\n",
            "E2E-ABSA >>> 2022-08-17 14:24:39\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:39\n",
            "loss: 0.5827, acc: 0.7756\n",
            "E2E-ABSA >>> 2022-08-17 14:24:40\n",
            "loss: 0.6349, acc: 0.7403\n",
            "E2E-ABSA >>> 2022-08-17 14:24:40\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:40\n",
            "loss: 0.6353, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-08-17 14:24:41\n",
            "loss: 0.6234, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-08-17 14:24:42\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:42\n",
            "loss: 0.6257, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 14:24:43\n",
            "loss: 0.6324, acc: 0.7425\n",
            "E2E-ABSA >>> 2022-08-17 14:24:43\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:43\n",
            "loss: 0.5931, acc: 0.7547\n",
            "E2E-ABSA >>> 2022-08-17 14:24:44\n",
            "loss: 0.6107, acc: 0.7496\n",
            "E2E-ABSA >>> 2022-08-17 14:24:45\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:45\n",
            "loss: 0.5955, acc: 0.7391\n",
            "E2E-ABSA >>> 2022-08-17 14:24:46\n",
            "loss: 0.6108, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 14:24:46\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:47\n",
            "loss: 0.6354, acc: 0.7464\n",
            "E2E-ABSA >>> 2022-08-17 14:24:47\n",
            "loss: 0.6174, acc: 0.7414\n",
            "E2E-ABSA >>> 2022-08-17 14:24:48\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:48\n",
            "loss: 0.6199, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:24:49\n",
            "loss: 0.6104, acc: 0.7508\n",
            "E2E-ABSA >>> 2022-08-17 14:24:49\n",
            ">>> val_acc: 0.6930, val_precision: 0.6930 val_recall: 0.6930, val_f1: 0.6930\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:50\n",
            "loss: 0.5890, acc: 0.7490\n",
            "E2E-ABSA >>> 2022-08-17 14:24:50\n",
            "loss: 0.6006, acc: 0.7515\n",
            "E2E-ABSA >>> 2022-08-17 14:24:51\n",
            ">>> val_acc: 0.6839, val_precision: 0.6839 val_recall: 0.6839, val_f1: 0.6839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:51\n",
            "loss: 0.6228, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 14:24:52\n",
            "loss: 0.6093, acc: 0.7478\n",
            "E2E-ABSA >>> 2022-08-17 14:24:52\n",
            ">>> val_acc: 0.6900, val_precision: 0.6900 val_recall: 0.6900, val_f1: 0.6900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:53\n",
            "loss: 0.5854, acc: 0.7623\n",
            "E2E-ABSA >>> 2022-08-17 14:24:53\n",
            "loss: 0.6096, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-08-17 14:24:54\n",
            ">>> val_acc: 0.6991, val_precision: 0.6991 val_recall: 0.6991, val_f1: 0.6991\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:54\n",
            "loss: 0.6081, acc: 0.7508\n",
            "E2E-ABSA >>> 2022-08-17 14:24:55\n",
            "loss: 0.6024, acc: 0.7510\n",
            "E2E-ABSA >>> 2022-08-17 14:24:55\n",
            ">>> val_acc: 0.7021, val_precision: 0.7021 val_recall: 0.7021, val_f1: 0.7021\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:24:56\n",
            "loss: 0.5971, acc: 0.7571\n",
            "E2E-ABSA >>> 2022-08-17 14:24:56\n",
            "loss: 0.5996, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-08-17 14:24:56\n",
            ">>> val_acc: 0.6717, val_precision: 0.6717 val_recall: 0.6717, val_f1: 0.6717\n",
            "E2E-ABSA >>> 2022-08-17 14:24:56\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7082, val_precision: 0.7082 val_recall: 0.7082, val_f1: 0.7082\n",
            "you can download the best model from state_dict/memnet_SemEval2014_know_val_f1_0.7082\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.7082, test_precision: 0.7082, test_recall: 0.7082, test_f1: 0.7082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**CABASC**)"
      ],
      "metadata": {
        "id": "BBYejymZGuaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset SemEval2014 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FZMu8LcGugt",
        "outputId": "03528f98-abcd-4442-e89f-56a87cfde16f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 2905.\n",
            "> testing dataset count: 311.\n",
            "cuda memory allocated: 15917056\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 2532000\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f4495707b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:25:37\n",
            "loss: 1.0156, acc: 0.5119\n",
            "E2E-ABSA >>> 2022-08-17 14:25:39\n",
            ">>> val_acc: 0.5595, val_precision: 0.5595 val_recall: 0.5595, val_f1: 0.5595\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.5595\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:25:39\n",
            "loss: 0.9051, acc: 0.5868\n",
            "E2E-ABSA >>> 2022-08-17 14:25:42\n",
            "loss: 0.9043, acc: 0.5794\n",
            "E2E-ABSA >>> 2022-08-17 14:25:43\n",
            ">>> val_acc: 0.5884, val_precision: 0.5884 val_recall: 0.5884, val_f1: 0.5884\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.5884\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:25:44\n",
            "loss: 0.9031, acc: 0.5729\n",
            "E2E-ABSA >>> 2022-08-17 14:25:47\n",
            "loss: 0.8775, acc: 0.5988\n",
            "E2E-ABSA >>> 2022-08-17 14:25:48\n",
            ">>> val_acc: 0.6045, val_precision: 0.6045 val_recall: 0.6045, val_f1: 0.6045\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.6045\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:25:49\n",
            "loss: 0.8451, acc: 0.6007\n",
            "E2E-ABSA >>> 2022-08-17 14:25:52\n",
            "loss: 0.8414, acc: 0.6144\n",
            "E2E-ABSA >>> 2022-08-17 14:25:53\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:25:54\n",
            "loss: 0.8250, acc: 0.6311\n",
            "E2E-ABSA >>> 2022-08-17 14:25:56\n",
            "loss: 0.8280, acc: 0.6344\n",
            "E2E-ABSA >>> 2022-08-17 14:25:57\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:25:59\n",
            "loss: 0.8360, acc: 0.6306\n",
            "E2E-ABSA >>> 2022-08-17 14:26:01\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:02\n",
            "loss: 0.8889, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 14:26:04\n",
            "loss: 0.7956, acc: 0.6539\n",
            "E2E-ABSA >>> 2022-08-17 14:26:06\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:07\n",
            "loss: 0.8166, acc: 0.6538\n",
            "E2E-ABSA >>> 2022-08-17 14:26:09\n",
            "loss: 0.7900, acc: 0.6587\n",
            "E2E-ABSA >>> 2022-08-17 14:26:11\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:12\n",
            "loss: 0.7709, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-08-17 14:26:14\n",
            "loss: 0.7839, acc: 0.6593\n",
            "E2E-ABSA >>> 2022-08-17 14:26:15\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:17\n",
            "loss: 0.7607, acc: 0.6724\n",
            "E2E-ABSA >>> 2022-08-17 14:26:19\n",
            "loss: 0.7651, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-08-17 14:26:20\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:21\n",
            "loss: 0.7593, acc: 0.6883\n",
            "E2E-ABSA >>> 2022-08-17 14:26:24\n",
            "loss: 0.7601, acc: 0.6792\n",
            "E2E-ABSA >>> 2022-08-17 14:26:24\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:26\n",
            "loss: 0.7419, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-08-17 14:26:29\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:29\n",
            "loss: 0.7595, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 14:26:31\n",
            "loss: 0.7311, acc: 0.6961\n",
            "E2E-ABSA >>> 2022-08-17 14:26:33\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:34\n",
            "loss: 0.7546, acc: 0.6691\n",
            "E2E-ABSA >>> 2022-08-17 14:26:36\n",
            "loss: 0.7457, acc: 0.6800\n",
            "E2E-ABSA >>> 2022-08-17 14:26:38\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:39\n",
            "loss: 0.7194, acc: 0.6983\n",
            "E2E-ABSA >>> 2022-08-17 14:26:41\n",
            "loss: 0.7390, acc: 0.6871\n",
            "E2E-ABSA >>> 2022-08-17 14:26:42\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:44\n",
            "loss: 0.7087, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 14:26:46\n",
            "loss: 0.7323, acc: 0.6853\n",
            "E2E-ABSA >>> 2022-08-17 14:26:47\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:49\n",
            "loss: 0.7185, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 14:26:51\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:51\n",
            "loss: 0.7176, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:26:53\n",
            "loss: 0.7247, acc: 0.7087\n",
            "E2E-ABSA >>> 2022-08-17 14:26:56\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:26:56\n",
            "loss: 0.7097, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 14:26:58\n",
            "loss: 0.7181, acc: 0.6946\n",
            "E2E-ABSA >>> 2022-08-17 14:27:00\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:01\n",
            "loss: 0.7085, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 14:27:03\n",
            "loss: 0.7305, acc: 0.6919\n",
            "E2E-ABSA >>> 2022-08-17 14:27:05\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:06\n",
            "loss: 0.7348, acc: 0.6896\n",
            "E2E-ABSA >>> 2022-08-17 14:27:08\n",
            "loss: 0.7119, acc: 0.7004\n",
            "E2E-ABSA >>> 2022-08-17 14:27:09\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:11\n",
            "loss: 0.6917, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-08-17 14:27:13\n",
            "loss: 0.7127, acc: 0.7001\n",
            "E2E-ABSA >>> 2022-08-17 14:27:14\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:16\n",
            "loss: 0.7056, acc: 0.6934\n",
            "E2E-ABSA >>> 2022-08-17 14:27:18\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:18\n",
            "loss: 0.7445, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-08-17 14:27:21\n",
            "loss: 0.6974, acc: 0.7056\n",
            "E2E-ABSA >>> 2022-08-17 14:27:23\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:23\n",
            "loss: 0.7238, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 14:27:26\n",
            "loss: 0.7036, acc: 0.7008\n",
            "E2E-ABSA >>> 2022-08-17 14:27:27\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:28\n",
            "loss: 0.6789, acc: 0.7113\n",
            "E2E-ABSA >>> 2022-08-17 14:27:30\n",
            "loss: 0.6972, acc: 0.7046\n",
            "E2E-ABSA >>> 2022-08-17 14:27:31\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:33\n",
            "loss: 0.7105, acc: 0.6912\n",
            "E2E-ABSA >>> 2022-08-17 14:27:35\n",
            "loss: 0.6983, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 14:27:36\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:38\n",
            "loss: 0.7090, acc: 0.6955\n",
            "E2E-ABSA >>> 2022-08-17 14:27:40\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:41\n",
            "loss: 0.7367, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:27:43\n",
            "loss: 0.6845, acc: 0.7055\n",
            "E2E-ABSA >>> 2022-08-17 14:27:45\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:45\n",
            "loss: 0.6418, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-08-17 14:27:48\n",
            "loss: 0.6826, acc: 0.7136\n",
            "E2E-ABSA >>> 2022-08-17 14:27:49\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:50\n",
            "loss: 0.7082, acc: 0.6828\n",
            "E2E-ABSA >>> 2022-08-17 14:27:53\n",
            "loss: 0.6901, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-08-17 14:27:54\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:27:55\n",
            "loss: 0.6572, acc: 0.7295\n",
            "E2E-ABSA >>> 2022-08-17 14:27:57\n",
            "loss: 0.6833, acc: 0.7124\n",
            "E2E-ABSA >>> 2022-08-17 14:27:58\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:00\n",
            "loss: 0.6933, acc: 0.7015\n",
            "E2E-ABSA >>> 2022-08-17 14:28:02\n",
            "loss: 0.6896, acc: 0.7095\n",
            "E2E-ABSA >>> 2022-08-17 14:28:03\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:05\n",
            "loss: 0.6775, acc: 0.7201\n",
            "E2E-ABSA >>> 2022-08-17 14:28:07\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:08\n",
            "loss: 0.6570, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:28:10\n",
            "loss: 0.6917, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 14:28:12\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:13\n",
            "loss: 0.6445, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 14:28:15\n",
            "loss: 0.6787, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-08-17 14:28:16\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:18\n",
            "loss: 0.7024, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 14:28:20\n",
            "loss: 0.6840, acc: 0.7120\n",
            "E2E-ABSA >>> 2022-08-17 14:28:21\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:22\n",
            "loss: 0.6787, acc: 0.7169\n",
            "E2E-ABSA >>> 2022-08-17 14:28:25\n",
            "loss: 0.6766, acc: 0.7142\n",
            "E2E-ABSA >>> 2022-08-17 14:28:25\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:27\n",
            "loss: 0.6752, acc: 0.7210\n",
            "E2E-ABSA >>> 2022-08-17 14:28:30\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:30\n",
            "loss: 0.6231, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:28:32\n",
            "loss: 0.6761, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 14:28:34\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:35\n",
            "loss: 0.6634, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-08-17 14:28:37\n",
            "loss: 0.6735, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 14:28:39\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:40\n",
            "loss: 0.6762, acc: 0.7138\n",
            "E2E-ABSA >>> 2022-08-17 14:28:42\n",
            "loss: 0.6784, acc: 0.7124\n",
            "E2E-ABSA >>> 2022-08-17 14:28:43\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:45\n",
            "loss: 0.6797, acc: 0.7165\n",
            "E2E-ABSA >>> 2022-08-17 14:28:47\n",
            "loss: 0.6694, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-08-17 14:28:48\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:50\n",
            "loss: 0.6838, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-08-17 14:28:52\n",
            "loss: 0.6747, acc: 0.7112\n",
            "E2E-ABSA >>> 2022-08-17 14:28:52\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:54\n",
            "loss: 0.6843, acc: 0.7092\n",
            "E2E-ABSA >>> 2022-08-17 14:28:57\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:28:57\n",
            "loss: 0.6707, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 14:28:59\n",
            "loss: 0.6708, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 14:29:01\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:02\n",
            "loss: 0.6813, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 14:29:04\n",
            "loss: 0.6821, acc: 0.7139\n",
            "E2E-ABSA >>> 2022-08-17 14:29:06\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:07\n",
            "loss: 0.6811, acc: 0.7120\n",
            "E2E-ABSA >>> 2022-08-17 14:29:09\n",
            "loss: 0.6747, acc: 0.7162\n",
            "E2E-ABSA >>> 2022-08-17 14:29:10\n",
            ">>> val_acc: 0.7010, val_precision: 0.7010 val_recall: 0.7010, val_f1: 0.7010\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.701\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:12\n",
            "loss: 0.6897, acc: 0.6963\n",
            "E2E-ABSA >>> 2022-08-17 14:29:14\n",
            "loss: 0.6752, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-08-17 14:29:15\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:17\n",
            "loss: 0.6789, acc: 0.7035\n",
            "E2E-ABSA >>> 2022-08-17 14:29:19\n",
            "loss: 0.6688, acc: 0.7143\n",
            "E2E-ABSA >>> 2022-08-17 14:29:19\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:22\n",
            "loss: 0.6737, acc: 0.7075\n",
            "E2E-ABSA >>> 2022-08-17 14:29:24\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:24\n",
            "loss: 0.6686, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 14:29:27\n",
            "loss: 0.6628, acc: 0.7172\n",
            "E2E-ABSA >>> 2022-08-17 14:29:28\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:29\n",
            "loss: 0.6569, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 14:29:32\n",
            "loss: 0.6571, acc: 0.7252\n",
            "E2E-ABSA >>> 2022-08-17 14:29:33\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:34\n",
            "loss: 0.6460, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-08-17 14:29:36\n",
            "loss: 0.6606, acc: 0.7183\n",
            "E2E-ABSA >>> 2022-08-17 14:29:37\n",
            ">>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            ">> saved: state_dict/cabasc_SemEval2014_val_f1_0.7074\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:39\n",
            "loss: 0.6826, acc: 0.7101\n",
            "E2E-ABSA >>> 2022-08-17 14:29:41\n",
            "loss: 0.6667, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-08-17 14:29:42\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:44\n",
            "loss: 0.6800, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 14:29:47\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:47\n",
            "loss: 0.6193, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:29:49\n",
            "loss: 0.6574, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-08-17 14:29:51\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:52\n",
            "loss: 0.6688, acc: 0.6995\n",
            "E2E-ABSA >>> 2022-08-17 14:29:54\n",
            "loss: 0.6641, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-08-17 14:29:55\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 14:29:56\n",
            "loss: 0.6584, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 14:29:59\n",
            "loss: 0.6571, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 14:30:00\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:01\n",
            "loss: 0.6510, acc: 0.7056\n",
            "E2E-ABSA >>> 2022-08-17 14:30:04\n",
            "loss: 0.6600, acc: 0.7160\n",
            "E2E-ABSA >>> 2022-08-17 14:30:04\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:06\n",
            "loss: 0.6791, acc: 0.7133\n",
            "E2E-ABSA >>> 2022-08-17 14:30:08\n",
            "loss: 0.6620, acc: 0.7215\n",
            "E2E-ABSA >>> 2022-08-17 14:30:09\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:11\n",
            "loss: 0.6514, acc: 0.7194\n",
            "E2E-ABSA >>> 2022-08-17 14:30:13\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:14\n",
            "loss: 0.6551, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 14:30:16\n",
            "loss: 0.6602, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-08-17 14:30:18\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:19\n",
            "loss: 0.6024, acc: 0.7482\n",
            "E2E-ABSA >>> 2022-08-17 14:30:21\n",
            "loss: 0.6561, acc: 0.7211\n",
            "E2E-ABSA >>> 2022-08-17 14:30:22\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:23\n",
            "loss: 0.6710, acc: 0.7019\n",
            "E2E-ABSA >>> 2022-08-17 14:30:26\n",
            "loss: 0.6540, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 14:30:27\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:28\n",
            "loss: 0.6694, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 14:30:31\n",
            "loss: 0.6627, acc: 0.7191\n",
            "E2E-ABSA >>> 2022-08-17 14:30:31\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:33\n",
            "loss: 0.6625, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-08-17 14:30:36\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:36\n",
            "loss: 0.5956, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 14:30:38\n",
            "loss: 0.6493, acc: 0.7347\n",
            "E2E-ABSA >>> 2022-08-17 14:30:40\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:41\n",
            "loss: 0.6401, acc: 0.7474\n",
            "E2E-ABSA >>> 2022-08-17 14:30:43\n",
            "loss: 0.6693, acc: 0.7132\n",
            "E2E-ABSA >>> 2022-08-17 14:30:45\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:46\n",
            "loss: 0.6379, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:30:48\n",
            "loss: 0.6532, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-08-17 14:30:49\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:50\n",
            "loss: 0.6571, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 14:30:53\n",
            "loss: 0.6529, acc: 0.7238\n",
            "E2E-ABSA >>> 2022-08-17 14:30:54\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 14:30:55\n",
            "loss: 0.6629, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 14:30:58\n",
            "loss: 0.6547, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 14:30:58\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:00\n",
            "loss: 0.6391, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-08-17 14:31:03\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:03\n",
            "loss: 0.6250, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-08-17 14:31:05\n",
            "loss: 0.6526, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-08-17 14:31:07\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:08\n",
            "loss: 0.6505, acc: 0.7129\n",
            "E2E-ABSA >>> 2022-08-17 14:31:10\n",
            "loss: 0.6498, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 14:31:12\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:13\n",
            "loss: 0.6559, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 14:31:15\n",
            "loss: 0.6436, acc: 0.7296\n",
            "E2E-ABSA >>> 2022-08-17 14:31:16\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:18\n",
            "loss: 0.6592, acc: 0.7114\n",
            "E2E-ABSA >>> 2022-08-17 14:31:20\n",
            "loss: 0.6550, acc: 0.7184\n",
            "E2E-ABSA >>> 2022-08-17 14:31:21\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:22\n",
            "loss: 0.6246, acc: 0.7369\n",
            "E2E-ABSA >>> 2022-08-17 14:31:25\n",
            ">>> val_acc: 0.6881, val_precision: 0.6881 val_recall: 0.6881, val_f1: 0.6881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:25\n",
            "loss: 0.6254, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 14:31:27\n",
            "loss: 0.6461, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 14:31:29\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:30\n",
            "loss: 0.6019, acc: 0.7727\n",
            "E2E-ABSA >>> 2022-08-17 14:31:32\n",
            "loss: 0.6456, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 14:31:34\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:35\n",
            "loss: 0.6683, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-08-17 14:31:37\n",
            "loss: 0.6420, acc: 0.7268\n",
            "E2E-ABSA >>> 2022-08-17 14:31:38\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:40\n",
            "loss: 0.6527, acc: 0.7295\n",
            "E2E-ABSA >>> 2022-08-17 14:31:42\n",
            "loss: 0.6549, acc: 0.7278\n",
            "E2E-ABSA >>> 2022-08-17 14:31:43\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:45\n",
            "loss: 0.6334, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 14:31:47\n",
            "loss: 0.6486, acc: 0.7237\n",
            "E2E-ABSA >>> 2022-08-17 14:31:47\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 14:31:50\n",
            "loss: 0.6389, acc: 0.7380\n",
            "E2E-ABSA >>> 2022-08-17 14:31:52\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            "E2E-ABSA >>> 2022-08-17 14:31:52\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7074, val_precision: 0.7074 val_recall: 0.7074, val_f1: 0.7074\n",
            "you can download the best model from state_dict/cabasc_SemEval2014_val_f1_0.7074\n",
            ">>> test_acc: 0.7074, test_precision: 0.7074, test_recall: 0.7074, test_f1: 0.7074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2014** dataset on model(**CABASC**)"
      ],
      "metadata": {
        "id": "FDUTdFkUGumI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset SemEval2014_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX9Pa13uGusR",
        "outputId": "d4f0dcd4-eae6-47b8-f0d1-379f1ddcb095"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 2913.\n",
            "> testing dataset count: 311.\n",
            "cuda memory allocated: 22565888\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 3969000\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1239c3db00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:32:36\n",
            "loss: 1.0150, acc: 0.4913\n",
            "E2E-ABSA >>> 2022-08-17 14:32:41\n",
            ">>> val_acc: 0.5627, val_precision: 0.5627 val_recall: 0.5627, val_f1: 0.5627\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.5627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:32:42\n",
            "loss: 0.8730, acc: 0.6434\n",
            "E2E-ABSA >>> 2022-08-17 14:32:48\n",
            "loss: 0.9497, acc: 0.5636\n",
            "E2E-ABSA >>> 2022-08-17 14:32:52\n",
            ">>> val_acc: 0.5627, val_precision: 0.5627 val_recall: 0.5627, val_f1: 0.5627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:32:54\n",
            "loss: 0.9527, acc: 0.5460\n",
            "E2E-ABSA >>> 2022-08-17 14:32:59\n",
            "loss: 0.9324, acc: 0.5583\n",
            "E2E-ABSA >>> 2022-08-17 14:33:03\n",
            ">>> val_acc: 0.5852, val_precision: 0.5852 val_recall: 0.5852, val_f1: 0.5852\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.5852\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:33:06\n",
            "loss: 0.8696, acc: 0.5858\n",
            "E2E-ABSA >>> 2022-08-17 14:33:11\n",
            "loss: 0.8775, acc: 0.5960\n",
            "E2E-ABSA >>> 2022-08-17 14:33:14\n",
            ">>> val_acc: 0.6013, val_precision: 0.6013 val_recall: 0.6013, val_f1: 0.6013\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6013\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:33:17\n",
            "loss: 0.8555, acc: 0.6066\n",
            "E2E-ABSA >>> 2022-08-17 14:33:23\n",
            "loss: 0.8376, acc: 0.6224\n",
            "E2E-ABSA >>> 2022-08-17 14:33:24\n",
            ">>> val_acc: 0.6495, val_precision: 0.6495 val_recall: 0.6495, val_f1: 0.6495\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6495\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:33:29\n",
            "loss: 0.8202, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-08-17 14:33:35\n",
            ">>> val_acc: 0.6495, val_precision: 0.6495 val_recall: 0.6495, val_f1: 0.6495\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:33:35\n",
            "loss: 0.6366, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:33:41\n",
            "loss: 0.8089, acc: 0.6489\n",
            "E2E-ABSA >>> 2022-08-17 14:33:46\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:33:47\n",
            "loss: 0.7613, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 14:33:53\n",
            "loss: 0.8036, acc: 0.6492\n",
            "E2E-ABSA >>> 2022-08-17 14:33:57\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:33:59\n",
            "loss: 0.7978, acc: 0.6441\n",
            "E2E-ABSA >>> 2022-08-17 14:34:04\n",
            "loss: 0.7753, acc: 0.6636\n",
            "E2E-ABSA >>> 2022-08-17 14:34:08\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:34:11\n",
            "loss: 0.7788, acc: 0.6545\n",
            "E2E-ABSA >>> 2022-08-17 14:34:16\n",
            "loss: 0.7715, acc: 0.6585\n",
            "E2E-ABSA >>> 2022-08-17 14:34:19\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:34:22\n",
            "loss: 0.7636, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-08-17 14:34:28\n",
            "loss: 0.7675, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:34:29\n",
            ">>> val_acc: 0.6559, val_precision: 0.6559 val_recall: 0.6559, val_f1: 0.6559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:34:34\n",
            "loss: 0.7862, acc: 0.6408\n",
            "E2E-ABSA >>> 2022-08-17 14:34:40\n",
            ">>> val_acc: 0.6559, val_precision: 0.6559 val_recall: 0.6559, val_f1: 0.6559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:34:41\n",
            "loss: 0.7328, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 14:34:46\n",
            "loss: 0.7542, acc: 0.6581\n",
            "E2E-ABSA >>> 2022-08-17 14:34:51\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:34:52\n",
            "loss: 0.7190, acc: 0.6905\n",
            "E2E-ABSA >>> 2022-08-17 14:34:58\n",
            "loss: 0.7514, acc: 0.6803\n",
            "E2E-ABSA >>> 2022-08-17 14:35:02\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:35:04\n",
            "loss: 0.7386, acc: 0.6826\n",
            "E2E-ABSA >>> 2022-08-17 14:35:10\n",
            "loss: 0.7567, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 14:35:13\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:35:16\n",
            "loss: 0.7472, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 14:35:21\n",
            "loss: 0.7419, acc: 0.6742\n",
            "E2E-ABSA >>> 2022-08-17 14:35:24\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:35:27\n",
            "loss: 0.7443, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-08-17 14:35:33\n",
            "loss: 0.7439, acc: 0.6762\n",
            "E2E-ABSA >>> 2022-08-17 14:35:34\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:35:39\n",
            "loss: 0.7330, acc: 0.6763\n",
            "E2E-ABSA >>> 2022-08-17 14:35:45\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:35:45\n",
            "loss: 0.7545, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 14:35:51\n",
            "loss: 0.7290, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 14:35:56\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:35:57\n",
            "loss: 0.7472, acc: 0.6712\n",
            "E2E-ABSA >>> 2022-08-17 14:36:03\n",
            "loss: 0.7297, acc: 0.6794\n",
            "E2E-ABSA >>> 2022-08-17 14:36:07\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:36:09\n",
            "loss: 0.7368, acc: 0.6828\n",
            "E2E-ABSA >>> 2022-08-17 14:36:14\n",
            "loss: 0.7224, acc: 0.6857\n",
            "E2E-ABSA >>> 2022-08-17 14:36:17\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:36:21\n",
            "loss: 0.7184, acc: 0.6897\n",
            "E2E-ABSA >>> 2022-08-17 14:36:26\n",
            "loss: 0.7283, acc: 0.6891\n",
            "E2E-ABSA >>> 2022-08-17 14:36:28\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:36:32\n",
            "loss: 0.7100, acc: 0.6985\n",
            "E2E-ABSA >>> 2022-08-17 14:36:38\n",
            "loss: 0.7251, acc: 0.6879\n",
            "E2E-ABSA >>> 2022-08-17 14:36:39\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:36:44\n",
            "loss: 0.7154, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 14:36:50\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:36:50\n",
            "loss: 0.6222, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 14:36:56\n",
            "loss: 0.7197, acc: 0.6973\n",
            "E2E-ABSA >>> 2022-08-17 14:37:01\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:37:02\n",
            "loss: 0.6676, acc: 0.7275\n",
            "E2E-ABSA >>> 2022-08-17 14:37:08\n",
            "loss: 0.7291, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 14:37:12\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:37:14\n",
            "loss: 0.6943, acc: 0.7098\n",
            "E2E-ABSA >>> 2022-08-17 14:37:19\n",
            "loss: 0.7172, acc: 0.6950\n",
            "E2E-ABSA >>> 2022-08-17 14:37:22\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:37:26\n",
            "loss: 0.7173, acc: 0.6970\n",
            "E2E-ABSA >>> 2022-08-17 14:37:31\n",
            "loss: 0.7135, acc: 0.6961\n",
            "E2E-ABSA >>> 2022-08-17 14:37:33\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:37:37\n",
            "loss: 0.7117, acc: 0.6933\n",
            "E2E-ABSA >>> 2022-08-17 14:37:43\n",
            "loss: 0.7126, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-08-17 14:37:44\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:37:49\n",
            "loss: 0.6937, acc: 0.7077\n",
            "E2E-ABSA >>> 2022-08-17 14:37:55\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:37:56\n",
            "loss: 0.7504, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:38:01\n",
            "loss: 0.6923, acc: 0.7114\n",
            "E2E-ABSA >>> 2022-08-17 14:38:06\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:38:07\n",
            "loss: 0.6572, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-08-17 14:38:13\n",
            "loss: 0.6889, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 14:38:17\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:38:19\n",
            "loss: 0.7224, acc: 0.6861\n",
            "E2E-ABSA >>> 2022-08-17 14:38:25\n",
            "loss: 0.7054, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-08-17 14:38:28\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:38:31\n",
            "loss: 0.6823, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 14:38:36\n",
            "loss: 0.6998, acc: 0.7023\n",
            "E2E-ABSA >>> 2022-08-17 14:38:38\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:38:43\n",
            "loss: 0.6988, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 14:38:48\n",
            "loss: 0.6964, acc: 0.7061\n",
            "E2E-ABSA >>> 2022-08-17 14:38:49\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:38:55\n",
            "loss: 0.7020, acc: 0.7013\n",
            "E2E-ABSA >>> 2022-08-17 14:39:00\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:39:01\n",
            "loss: 0.7470, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 14:39:06\n",
            "loss: 0.6894, acc: 0.7081\n",
            "E2E-ABSA >>> 2022-08-17 14:39:11\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:39:13\n",
            "loss: 0.6797, acc: 0.7134\n",
            "E2E-ABSA >>> 2022-08-17 14:39:18\n",
            "loss: 0.6822, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-08-17 14:39:22\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:39:25\n",
            "loss: 0.6958, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 14:39:30\n",
            "loss: 0.7017, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 14:39:33\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:39:36\n",
            "loss: 0.6729, acc: 0.7113\n",
            "E2E-ABSA >>> 2022-08-17 14:39:42\n",
            "loss: 0.6908, acc: 0.7074\n",
            "E2E-ABSA >>> 2022-08-17 14:39:44\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:39:48\n",
            "loss: 0.7010, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 14:39:54\n",
            "loss: 0.6892, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 14:39:55\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:40:00\n",
            "loss: 0.7010, acc: 0.7062\n",
            "E2E-ABSA >>> 2022-08-17 14:40:06\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:40:06\n",
            "loss: 0.7379, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-08-17 14:40:12\n",
            "loss: 0.6745, acc: 0.7314\n",
            "E2E-ABSA >>> 2022-08-17 14:40:17\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:40:19\n",
            "loss: 0.6886, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 14:40:25\n",
            "loss: 0.6797, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-08-17 14:40:28\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:40:31\n",
            "loss: 0.6993, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 14:40:37\n",
            "loss: 0.6973, acc: 0.7078\n",
            "E2E-ABSA >>> 2022-08-17 14:40:40\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:40:43\n",
            "loss: 0.7034, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-08-17 14:40:49\n",
            "loss: 0.6810, acc: 0.7223\n",
            "E2E-ABSA >>> 2022-08-17 14:40:51\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:40:55\n",
            "loss: 0.6687, acc: 0.7210\n",
            "E2E-ABSA >>> 2022-08-17 14:41:01\n",
            "loss: 0.6801, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 14:41:02\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:41:07\n",
            "loss: 0.6870, acc: 0.7229\n",
            "E2E-ABSA >>> 2022-08-17 14:41:13\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:41:14\n",
            "loss: 0.7152, acc: 0.6836\n",
            "E2E-ABSA >>> 2022-08-17 14:41:19\n",
            "loss: 0.6919, acc: 0.7080\n",
            "E2E-ABSA >>> 2022-08-17 14:41:24\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:41:26\n",
            "loss: 0.6934, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-08-17 14:41:31\n",
            "loss: 0.6803, acc: 0.7143\n",
            "E2E-ABSA >>> 2022-08-17 14:41:35\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:41:37\n",
            "loss: 0.6562, acc: 0.7238\n",
            "E2E-ABSA >>> 2022-08-17 14:41:43\n",
            "loss: 0.6769, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 14:41:46\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:41:49\n",
            "loss: 0.6744, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-08-17 14:41:55\n",
            "loss: 0.6734, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-08-17 14:41:57\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:42:01\n",
            "loss: 0.6479, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-08-17 14:42:07\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:42:07\n",
            "loss: 0.7262, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:42:13\n",
            "loss: 0.6831, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 14:42:18\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 14:42:19\n",
            "loss: 0.6551, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 14:42:25\n",
            "loss: 0.6797, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 14:42:29\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 14:42:31\n",
            "loss: 0.6521, acc: 0.7482\n",
            "E2E-ABSA >>> 2022-08-17 14:42:36\n",
            "loss: 0.6598, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-08-17 14:42:40\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 14:42:43\n",
            "loss: 0.6716, acc: 0.7272\n",
            "E2E-ABSA >>> 2022-08-17 14:42:48\n",
            "loss: 0.6642, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 14:42:51\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 14:42:55\n",
            "loss: 0.6500, acc: 0.7337\n",
            "E2E-ABSA >>> 2022-08-17 14:43:00\n",
            "loss: 0.6665, acc: 0.7223\n",
            "E2E-ABSA >>> 2022-08-17 14:43:02\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 14:43:07\n",
            "loss: 0.6488, acc: 0.7420\n",
            "E2E-ABSA >>> 2022-08-17 14:43:13\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 14:43:13\n",
            "loss: 0.5837, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:43:18\n",
            "loss: 0.6551, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-08-17 14:43:24\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 14:43:25\n",
            "loss: 0.6641, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:43:30\n",
            "loss: 0.6600, acc: 0.7297\n",
            "E2E-ABSA >>> 2022-08-17 14:43:34\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 14:43:36\n",
            "loss: 0.6479, acc: 0.7331\n",
            "E2E-ABSA >>> 2022-08-17 14:43:42\n",
            "loss: 0.6649, acc: 0.7249\n",
            "E2E-ABSA >>> 2022-08-17 14:43:45\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 14:43:48\n",
            "loss: 0.6624, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-08-17 14:43:54\n",
            "loss: 0.6669, acc: 0.7196\n",
            "E2E-ABSA >>> 2022-08-17 14:43:56\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 14:44:00\n",
            "loss: 0.6436, acc: 0.7403\n",
            "E2E-ABSA >>> 2022-08-17 14:44:05\n",
            "loss: 0.6649, acc: 0.7255\n",
            "E2E-ABSA >>> 2022-08-17 14:44:07\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 14:44:12\n",
            "loss: 0.6622, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-08-17 14:44:18\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 14:44:18\n",
            "loss: 0.7211, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 14:44:23\n",
            "loss: 0.6575, acc: 0.7304\n",
            "E2E-ABSA >>> 2022-08-17 14:44:28\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 14:44:30\n",
            "loss: 0.6635, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-08-17 14:44:35\n",
            "loss: 0.6662, acc: 0.7193\n",
            "E2E-ABSA >>> 2022-08-17 14:44:39\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 14:44:41\n",
            "loss: 0.6585, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 14:44:47\n",
            "loss: 0.6499, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-08-17 14:44:50\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 14:44:53\n",
            "loss: 0.6893, acc: 0.7009\n",
            "E2E-ABSA >>> 2022-08-17 14:44:59\n",
            "loss: 0.6681, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 14:45:01\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 14:45:05\n",
            "loss: 0.6642, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 14:45:11\n",
            "loss: 0.6599, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-08-17 14:45:12\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 14:45:17\n",
            "loss: 0.6565, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 14:45:23\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 14:45:24\n",
            "loss: 0.6299, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-08-17 14:45:29\n",
            "loss: 0.6528, acc: 0.7278\n",
            "E2E-ABSA >>> 2022-08-17 14:45:35\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 14:45:36\n",
            "loss: 0.6480, acc: 0.7370\n",
            "E2E-ABSA >>> 2022-08-17 14:45:41\n",
            "loss: 0.6452, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 14:45:46\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 14:45:48\n",
            "loss: 0.6806, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 14:45:53\n",
            "loss: 0.6665, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 14:45:57\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 14:46:00\n",
            "loss: 0.6382, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 14:46:05\n",
            "loss: 0.6504, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 14:46:08\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 14:46:12\n",
            "loss: 0.6580, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 14:46:17\n",
            "loss: 0.6524, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 14:46:18\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 14:46:24\n",
            "loss: 0.6558, acc: 0.7378\n",
            "E2E-ABSA >>> 2022-08-17 14:46:29\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 14:46:30\n",
            "loss: 0.6904, acc: 0.6806\n",
            "E2E-ABSA >>> 2022-08-17 14:46:35\n",
            "loss: 0.6487, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 14:46:40\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 14:46:42\n",
            "loss: 0.6225, acc: 0.7380\n",
            "E2E-ABSA >>> 2022-08-17 14:46:47\n",
            "loss: 0.6514, acc: 0.7302\n",
            "E2E-ABSA >>> 2022-08-17 14:46:51\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 14:46:54\n",
            "loss: 0.6648, acc: 0.7297\n",
            "E2E-ABSA >>> 2022-08-17 14:46:59\n",
            "loss: 0.6523, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-08-17 14:47:02\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 14:47:05\n",
            "loss: 0.6507, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 14:47:11\n",
            "loss: 0.6541, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 14:47:13\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 14:47:17\n",
            "loss: 0.6520, acc: 0.7419\n",
            "E2E-ABSA >>> 2022-08-17 14:47:23\n",
            "loss: 0.6509, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 14:47:24\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 14:47:29\n",
            "loss: 0.6260, acc: 0.7540\n",
            "E2E-ABSA >>> 2022-08-17 14:47:35\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 14:47:35\n",
            "loss: 0.6473, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 14:47:41\n",
            "loss: 0.6415, acc: 0.7382\n",
            "E2E-ABSA >>> 2022-08-17 14:47:45\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">> saved: state_dict/cabasc_SemEval2014_know_val_f1_0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 14:47:47\n",
            "loss: 0.6104, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 14:47:52\n",
            "loss: 0.6460, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 14:47:57\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 14:47:59\n",
            "loss: 0.6361, acc: 0.7403\n",
            "E2E-ABSA >>> 2022-08-17 14:48:05\n",
            "loss: 0.6523, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 14:48:07\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 14:48:11\n",
            "loss: 0.6461, acc: 0.7258\n",
            "E2E-ABSA >>> 2022-08-17 14:48:16\n",
            "loss: 0.6498, acc: 0.7296\n",
            "E2E-ABSA >>> 2022-08-17 14:48:18\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 14:48:22\n",
            "loss: 0.6578, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 14:48:28\n",
            "loss: 0.6477, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-08-17 14:48:29\n",
            ">>> val_acc: 0.6945, val_precision: 0.6945 val_recall: 0.6945, val_f1: 0.6945\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 14:48:34\n",
            "loss: 0.6387, acc: 0.7324\n",
            "E2E-ABSA >>> 2022-08-17 14:48:40\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 14:48:41\n",
            "loss: 0.6597, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-08-17 14:48:46\n",
            "loss: 0.6488, acc: 0.7235\n",
            "E2E-ABSA >>> 2022-08-17 14:48:51\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 14:48:52\n",
            "loss: 0.6689, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 14:48:58\n",
            "loss: 0.6501, acc: 0.7293\n",
            "E2E-ABSA >>> 2022-08-17 14:49:01\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 14:49:04\n",
            "loss: 0.6384, acc: 0.7327\n",
            "E2E-ABSA >>> 2022-08-17 14:49:09\n",
            "loss: 0.6475, acc: 0.7389\n",
            "E2E-ABSA >>> 2022-08-17 14:49:12\n",
            ">>> val_acc: 0.6785, val_precision: 0.6785 val_recall: 0.6785, val_f1: 0.6785\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-08-17 14:49:16\n",
            "loss: 0.6470, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-08-17 14:49:21\n",
            "loss: 0.6507, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 14:49:23\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-08-17 14:49:27\n",
            "loss: 0.6645, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-08-17 14:49:33\n",
            "loss: 0.6462, acc: 0.7327\n",
            "E2E-ABSA >>> 2022-08-17 14:49:34\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-08-17 14:49:39\n",
            "loss: 0.6608, acc: 0.7136\n",
            "E2E-ABSA >>> 2022-08-17 14:49:44\n",
            ">>> val_acc: 0.6913, val_precision: 0.6913 val_recall: 0.6913, val_f1: 0.6913\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-08-17 14:49:45\n",
            "loss: 0.6029, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 14:49:51\n",
            "loss: 0.6432, acc: 0.7310\n",
            "E2E-ABSA >>> 2022-08-17 14:49:55\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-08-17 14:49:57\n",
            "loss: 0.6153, acc: 0.7559\n",
            "E2E-ABSA >>> 2022-08-17 14:50:03\n",
            "loss: 0.6333, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-08-17 14:50:06\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-08-17 14:50:09\n",
            "loss: 0.6392, acc: 0.7258\n",
            "E2E-ABSA >>> 2022-08-17 14:50:14\n",
            "loss: 0.6447, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 14:50:17\n",
            ">>> val_acc: 0.6849, val_precision: 0.6849 val_recall: 0.6849, val_f1: 0.6849\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-08-17 14:50:21\n",
            "loss: 0.6649, acc: 0.7235\n",
            "E2E-ABSA >>> 2022-08-17 14:50:26\n",
            "loss: 0.6468, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-08-17 14:50:28\n",
            ">>> val_acc: 0.6817, val_precision: 0.6817 val_recall: 0.6817, val_f1: 0.6817\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-08-17 14:50:33\n",
            "loss: 0.6490, acc: 0.7357\n",
            "E2E-ABSA >>> 2022-08-17 14:50:38\n",
            "loss: 0.6425, acc: 0.7346\n",
            "E2E-ABSA >>> 2022-08-17 14:50:39\n",
            ">>> val_acc: 0.6977, val_precision: 0.6977 val_recall: 0.6977, val_f1: 0.6977\n",
            "you can download the best model from state_dict/cabasc_SemEval2014_know_val_f1_0.6977\n",
            ">>> test_acc: 0.6977, test_precision: 0.6977, test_recall: 0.6977, test_f1: 0.6977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 针对SemEval201X以及acl2014data的notebook上实验"
      ],
      "metadata": {
        "id": "w9s7x6OCJLyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SemEval2015 数据集"
      ],
      "metadata": {
        "id": "3CLfC7UPIxRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lstm\n",
        "\n",
        "tdlstm  \n",
        "tclstm  \n",
        "ataelstm  \n",
        "ian \n",
        "memnet  \n",
        "cabasc \n"
      ],
      "metadata": {
        "id": "180yPbjWQJjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2015** dataset on model(**LSTM**)"
      ],
      "metadata": {
        "id": "uUBzJ-tXSxQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset SemEval2015 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKWiQSf_SxgA",
        "outputId": "839cc4d2-6116-4a49-f495-dda5b1d75f41"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 6156288\n",
            "> n_trainable_params: 723303, n_nontrainable_params: 815400\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fdf76ae9b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/lstm_SemEval2015_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:02\n",
            "loss: 0.6778, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-08-17 15:04:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:03\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:03\n",
            "loss: 0.7016, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:04:03\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:03\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:03\n",
            "loss: 0.5978, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:04:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:04\n",
            "loss: 0.6217, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:04:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:04\n",
            "loss: 0.5810, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:04:05\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:05\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:05\n",
            "loss: 0.6236, acc: 0.7622\n",
            "E2E-ABSA >>> 2022-08-17 15:04:05\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:05\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:06\n",
            "loss: 0.6172, acc: 0.7664\n",
            "E2E-ABSA >>> 2022-08-17 15:04:06\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:06\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:06\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:06\n",
            "loss: 0.5019, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:04:06\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:07\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:07\n",
            "loss: 0.5496, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 15:04:07\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:07\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:07\n",
            "loss: 0.5416, acc: 0.7885\n",
            "E2E-ABSA >>> 2022-08-17 15:04:07\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:08\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:08\n",
            "loss: 0.6165, acc: 0.7599\n",
            "E2E-ABSA >>> 2022-08-17 15:04:08\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:08\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:08\n",
            "loss: 0.5467, acc: 0.7850\n",
            "E2E-ABSA >>> 2022-08-17 15:04:09\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:09\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:09\n",
            "loss: 0.5528, acc: 0.7742\n",
            "E2E-ABSA >>> 2022-08-17 15:04:09\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:09\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:10\n",
            "loss: 0.5441, acc: 0.7703\n",
            "E2E-ABSA >>> 2022-08-17 15:04:10\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:10\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "E2E-ABSA >>> 2022-08-17 15:04:10\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "you can download the best model from state_dict/lstm_SemEval2015_val_f1_0.8293\n",
            ">>> test_acc: 0.8293, test_precision: 0.8293, test_recall: 0.8293, test_f1: 0.8293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2015** dataset on model(**LSTM**)"
      ],
      "metadata": {
        "id": "hUhN-KCtSx4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset SemEval2015_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeYin41YSyRY",
        "outputId": "55f1d57e-2ec0-45df-9d99-246381c86124"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 9065984\n",
            "> n_trainable_params: 723303, n_nontrainable_params: 1542900\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fad62d16b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:48\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/lstm_SemEval2015_know_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:48\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:48\n",
            "loss: 0.6758, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-08-17 15:04:49\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:49\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:49\n",
            "loss: 0.7064, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:04:49\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:50\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:50\n",
            "loss: 0.6150, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:04:50\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:50\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:50\n",
            "loss: 0.6295, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:04:51\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:51\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:51\n",
            "loss: 0.5952, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:04:51\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:52\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:52\n",
            "loss: 0.6383, acc: 0.7622\n",
            "E2E-ABSA >>> 2022-08-17 15:04:52\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:52\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:53\n",
            "loss: 0.6377, acc: 0.7664\n",
            "E2E-ABSA >>> 2022-08-17 15:04:53\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:53\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:53\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:53\n",
            "loss: 0.5517, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:04:54\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:54\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:54\n",
            "loss: 0.5908, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 15:04:54\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:55\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:55\n",
            "loss: 0.5612, acc: 0.7885\n",
            "E2E-ABSA >>> 2022-08-17 15:04:55\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:55\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:56\n",
            "loss: 0.6454, acc: 0.7599\n",
            "E2E-ABSA >>> 2022-08-17 15:04:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:56\n",
            "loss: 0.5853, acc: 0.7850\n",
            "E2E-ABSA >>> 2022-08-17 15:04:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:57\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:57\n",
            "loss: 0.6062, acc: 0.7722\n",
            "E2E-ABSA >>> 2022-08-17 15:04:57\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:58\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:58\n",
            "loss: 0.5971, acc: 0.7703\n",
            "E2E-ABSA >>> 2022-08-17 15:04:58\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:04:58\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "E2E-ABSA >>> 2022-08-17 15:04:58\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "you can download the best model from state_dict/lstm_SemEval2015_know_val_f1_0.8293\n",
            ">>> test_acc: 0.8293, test_precision: 0.8293, test_recall: 0.8293, test_f1: 0.8293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2015** dataset on model(**TDLSTM**)"
      ],
      "metadata": {
        "id": "UUvrSHJ7S0Ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset SemEval2015 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh-4ZG57QJrH",
        "outputId": "856c7aa5-2b2d-4171-cd19-a107bdf929f6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 655.\n",
            "> testing dataset count: 79.\n",
            "cuda memory allocated: 9049600\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 815400\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fcdfb36db00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:18\n",
            ">>> val_acc: 0.3418, val_precision: 0.3418 val_recall: 0.3418, val_f1: 0.3418\n",
            ">> saved: state_dict/td_lstm_SemEval2015_val_f1_0.3418\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:19\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">> saved: state_dict/td_lstm_SemEval2015_val_f1_0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:19\n",
            "loss: 0.8710, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 14:51:19\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:19\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:19\n",
            "loss: 0.7143, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-08-17 14:51:20\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:20\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:20\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:20\n",
            "loss: 0.6969, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-08-17 14:51:21\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:21\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:21\n",
            "loss: 0.6492, acc: 0.7560\n",
            "E2E-ABSA >>> 2022-08-17 14:51:21\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:22\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:22\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:22\n",
            "loss: 0.6695, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 14:51:22\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:22\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:23\n",
            "loss: 0.6339, acc: 0.7596\n",
            "E2E-ABSA >>> 2022-08-17 14:51:23\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:23\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:23\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:23\n",
            "loss: 0.6741, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:51:24\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:24\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:24\n",
            "loss: 0.6251, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 14:51:24\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:25\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:25\n",
            "loss: 0.5965, acc: 0.7644\n",
            "E2E-ABSA >>> 2022-08-17 14:51:25\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:25\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:26\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:26\n",
            "loss: 0.5881, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-08-17 14:51:26\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:26\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:27\n",
            "loss: 0.5905, acc: 0.7629\n",
            "E2E-ABSA >>> 2022-08-17 14:51:27\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:27\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:27\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:28\n",
            "loss: 0.5924, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-08-17 14:51:28\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:28\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:51:28\n",
            "loss: 0.5577, acc: 0.7802\n",
            "E2E-ABSA >>> 2022-08-17 14:51:28\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "E2E-ABSA >>> 2022-08-17 14:51:28\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "you can download the best model from state_dict/td_lstm_SemEval2015_val_f1_0.8354\n",
            ">>> test_acc: 0.8354, test_precision: 0.8354, test_recall: 0.8354, test_f1: 0.8354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 增加字典知识后：Training **SemEval2015** dataset on model(**TDLSTM**)"
      ],
      "metadata": {
        "id": "YrqsLvCCQJxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset SemEval2015_know --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiJ_pb78QJ3Z",
        "outputId": "49b7b7e7-5768-4c56-b44f-c230343d5307"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 658.\n",
            "> testing dataset count: 79.\n",
            "cuda memory allocated: 11959296\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 1542900\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f5feb169b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:07\n",
            ">>> val_acc: 0.3671, val_precision: 0.3671 val_recall: 0.3671, val_f1: 0.3671\n",
            ">> saved: state_dict/td_lstm_SemEval2015_know_val_f1_0.3671\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:07\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">> saved: state_dict/td_lstm_SemEval2015_know_val_f1_0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:07\n",
            "loss: 0.8719, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 14:52:08\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:08\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:09\n",
            "loss: 0.7152, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 14:52:09\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:09\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:10\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:10\n",
            "loss: 0.6359, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 14:52:10\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:10\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:11\n",
            "loss: 0.6153, acc: 0.7727\n",
            "E2E-ABSA >>> 2022-08-17 14:52:11\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:11\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:12\n",
            "loss: 0.6143, acc: 0.7648\n",
            "E2E-ABSA >>> 2022-08-17 14:52:12\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:12\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:13\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:13\n",
            "loss: 0.5852, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-08-17 14:52:13\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:13\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:14\n",
            "loss: 0.5951, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:52:14\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:14\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:15\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:15\n",
            "loss: 0.5661, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:52:15\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:16\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:16\n",
            "loss: 0.5802, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-08-17 14:52:16\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:16\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:17\n",
            "loss: 0.6055, acc: 0.7482\n",
            "E2E-ABSA >>> 2022-08-17 14:52:17\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:17\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:18\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:18\n",
            "loss: 0.5799, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 14:52:18\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:19\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:19\n",
            "loss: 0.5387, acc: 0.7943\n",
            "E2E-ABSA >>> 2022-08-17 14:52:19\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:20\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:20\n",
            "loss: 0.5672, acc: 0.7672\n",
            "E2E-ABSA >>> 2022-08-17 14:52:20\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:20\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "E2E-ABSA >>> 2022-08-17 14:52:20\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "you can download the best model from state_dict/td_lstm_SemEval2015_know_val_f1_0.8354\n",
            ">>> test_acc: 0.8354, test_precision: 0.8354, test_recall: 0.8354, test_f1: 0.8354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**TCLSTM**)"
      ],
      "metadata": {
        "id": "iGPePTY9QJ9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset SemEval2015 --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD9MO-JuQKDc",
        "outputId": "8033549d-060c-433e-a38e-4f7074c5793a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 655.\n",
            "> testing dataset count: 79.\n",
            "cuda memory allocated: 11930112\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 815400\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fbb9956bb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:58\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">> saved: state_dict/tc_lstm_SemEval2015_val_f1_0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:58\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">> saved: state_dict/tc_lstm_SemEval2015_val_f1_0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:59\n",
            "loss: 0.6797, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 14:52:59\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:59\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:52:59\n",
            "loss: 0.6601, acc: 0.7535\n",
            "E2E-ABSA >>> 2022-08-17 14:53:00\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:00\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:00\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:01\n",
            "loss: 0.6074, acc: 0.7885\n",
            "E2E-ABSA >>> 2022-08-17 14:53:01\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:01\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:02\n",
            "loss: 0.6123, acc: 0.7581\n",
            "E2E-ABSA >>> 2022-08-17 14:53:02\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:02\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:02\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:02\n",
            "loss: 0.5671, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:53:03\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:03\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:03\n",
            "loss: 0.5722, acc: 0.7716\n",
            "E2E-ABSA >>> 2022-08-17 14:53:03\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:04\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:04\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:04\n",
            "loss: 0.6651, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 14:53:04\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:05\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:05\n",
            "loss: 0.5618, acc: 0.7798\n",
            "E2E-ABSA >>> 2022-08-17 14:53:05\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:06\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:06\n",
            "loss: 0.5535, acc: 0.7821\n",
            "E2E-ABSA >>> 2022-08-17 14:53:06\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:06\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:07\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:07\n",
            "loss: 0.5335, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 14:53:07\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:07\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:08\n",
            "loss: 0.5353, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:53:08\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:08\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:08\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:08\n",
            "loss: 0.5237, acc: 0.7898\n",
            "E2E-ABSA >>> 2022-08-17 14:53:09\n",
            ">>> val_acc: 0.8481, val_precision: 0.8481 val_recall: 0.8481, val_f1: 0.8481\n",
            ">> saved: state_dict/tc_lstm_SemEval2015_val_f1_0.8481\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:09\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:09\n",
            "loss: 0.5330, acc: 0.7716\n",
            "E2E-ABSA >>> 2022-08-17 14:53:09\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:10\n",
            ">>> val_acc: 0.8481, val_precision: 0.8481 val_recall: 0.8481, val_f1: 0.8481\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:10\n",
            ">>> val_acc: 0.8481, val_precision: 0.8481 val_recall: 0.8481, val_f1: 0.8481\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:10\n",
            "loss: 0.4103, acc: 0.8646\n",
            "E2E-ABSA >>> 2022-08-17 14:53:10\n",
            ">>> val_acc: 0.8481, val_precision: 0.8481 val_recall: 0.8481, val_f1: 0.8481\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:11\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:11\n",
            "loss: 0.5314, acc: 0.7839\n",
            "E2E-ABSA >>> 2022-08-17 14:53:11\n",
            ">>> val_acc: 0.8481, val_precision: 0.8481 val_recall: 0.8481, val_f1: 0.8481\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:11\n",
            ">>> val_acc: 0.8481, val_precision: 0.8481 val_recall: 0.8481, val_f1: 0.8481\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:12\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:12\n",
            "loss: 0.6110, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 14:53:12\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:13\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:13\n",
            "loss: 0.4637, acc: 0.8059\n",
            "E2E-ABSA >>> 2022-08-17 14:53:13\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:13\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:14\n",
            "loss: 0.4879, acc: 0.7956\n",
            "E2E-ABSA >>> 2022-08-17 14:53:14\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:14\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:14\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:14\n",
            "loss: 0.4785, acc: 0.8170\n",
            "E2E-ABSA >>> 2022-08-17 14:53:15\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:15\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:15\n",
            "loss: 0.4667, acc: 0.8145\n",
            "E2E-ABSA >>> 2022-08-17 14:53:15\n",
            ">>> val_acc: 0.7975, val_precision: 0.7975 val_recall: 0.7975, val_f1: 0.7975\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:16\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:16\n",
            ">>> val_acc: 0.7848, val_precision: 0.7848 val_recall: 0.7848, val_f1: 0.7848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:16\n",
            "loss: 0.4011, acc: 0.8403\n",
            "E2E-ABSA >>> 2022-08-17 14:53:16\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:17\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:17\n",
            "loss: 0.4205, acc: 0.8449\n",
            "E2E-ABSA >>> 2022-08-17 14:53:17\n",
            ">>> val_acc: 0.7975, val_precision: 0.7975 val_recall: 0.7975, val_f1: 0.7975\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:17\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:18\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:18\n",
            "loss: 0.4483, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:53:18\n",
            ">>> val_acc: 0.7722, val_precision: 0.7722 val_recall: 0.7722, val_f1: 0.7722\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:18\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:19\n",
            "loss: 0.3902, acc: 0.8523\n",
            "E2E-ABSA >>> 2022-08-17 14:53:19\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:19\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            "E2E-ABSA >>> 2022-08-17 14:53:19\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8481, val_precision: 0.8481 val_recall: 0.8481, val_f1: 0.8481\n",
            "you can download the best model from state_dict/tc_lstm_SemEval2015_val_f1_0.8481\n",
            ">>> test_acc: 0.8481, test_precision: 0.8481, test_recall: 0.8481, test_f1: 0.8481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2015** dataset on model(**TCLSTM**)"
      ],
      "metadata": {
        "id": "6tHpJfEyQKJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset SemEval2015_know --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3wg6HobQKPG",
        "outputId": "197678df-7c84-4d1e-8bd6-54137264cbbd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 658.\n",
            "> testing dataset count: 79.\n",
            "cuda memory allocated: 14839808\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 1542900\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f96b89a7b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:58\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">> saved: state_dict/tc_lstm_SemEval2015_know_val_f1_0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:58\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:58\n",
            "loss: 0.6764, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-08-17 14:53:59\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:59\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:53:59\n",
            "loss: 0.6246, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 14:54:00\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:00\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:00\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:01\n",
            "loss: 0.6033, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 14:54:01\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:01\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:02\n",
            "loss: 0.6471, acc: 0.7386\n",
            "E2E-ABSA >>> 2022-08-17 14:54:02\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:02\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:03\n",
            "loss: 0.5801, acc: 0.7697\n",
            "E2E-ABSA >>> 2022-08-17 14:54:03\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:03\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:04\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:04\n",
            "loss: 0.6327, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 14:54:04\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:05\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:05\n",
            "loss: 0.5783, acc: 0.7701\n",
            "E2E-ABSA >>> 2022-08-17 14:54:05\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:06\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:06\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:06\n",
            "loss: 0.5290, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:54:07\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:07\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:07\n",
            "loss: 0.5148, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 14:54:08\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:08\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:08\n",
            "loss: 0.5447, acc: 0.7757\n",
            "E2E-ABSA >>> 2022-08-17 14:54:09\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:09\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:10\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:10\n",
            "loss: 0.5198, acc: 0.8047\n",
            "E2E-ABSA >>> 2022-08-17 14:54:10\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:11\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:11\n",
            "loss: 0.5131, acc: 0.7995\n",
            "E2E-ABSA >>> 2022-08-17 14:54:11\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:11\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:12\n",
            "loss: 0.5254, acc: 0.7828\n",
            "E2E-ABSA >>> 2022-08-17 14:54:12\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "E2E-ABSA >>> 2022-08-17 14:54:12\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "you can download the best model from state_dict/tc_lstm_SemEval2015_know_val_f1_0.8354\n",
            ">>> test_acc: 0.8354, test_precision: 0.8354, test_recall: 0.8354, test_f1: 0.8354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2015** dataset on model(**ATAELSTM**)"
      ],
      "metadata": {
        "id": "qQIhzcYeQKU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset SemEval2015 --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emy4KVv5QKbO",
        "outputId": "cec0adad-6a31-42ff-f0d8-ba275e6476d9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 13367808\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 815400\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f3ffe406b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:50\n",
            ">>> val_acc: 0.6707, val_precision: 0.6707 val_recall: 0.6707, val_f1: 0.6707\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_val_f1_0.6707\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:50\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_val_f1_0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:50\n",
            "loss: 0.7704, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 14:54:51\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:51\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:51\n",
            "loss: 0.6761, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 14:54:52\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:52\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:52\n",
            "loss: 0.7036, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-08-17 14:54:52\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:53\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:53\n",
            "loss: 0.6547, acc: 0.7630\n",
            "E2E-ABSA >>> 2022-08-17 14:54:53\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:54\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:54\n",
            "loss: 0.6121, acc: 0.7646\n",
            "E2E-ABSA >>> 2022-08-17 14:54:54\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:55\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:55\n",
            "loss: 0.5693, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:54:55\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:56\n",
            "loss: 0.6033, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-08-17 14:54:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:57\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:57\n",
            "loss: 0.7562, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 14:54:57\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:58\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:58\n",
            "loss: 0.5114, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:54:58\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:59\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:59\n",
            "loss: 0.5670, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:54:59\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:54:59\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:00\n",
            "loss: 0.4776, acc: 0.7993\n",
            "E2E-ABSA >>> 2022-08-17 14:55:00\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_val_f1_0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:00\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:01\n",
            "loss: 0.5357, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 14:55:01\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:01\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:01\n",
            "loss: 0.4890, acc: 0.7903\n",
            "E2E-ABSA >>> 2022-08-17 14:55:02\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:02\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:02\n",
            "loss: 0.5016, acc: 0.7990\n",
            "E2E-ABSA >>> 2022-08-17 14:55:02\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:03\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:03\n",
            "loss: 0.4886, acc: 0.7994\n",
            "E2E-ABSA >>> 2022-08-17 14:55:03\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:04\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:04\n",
            ">>> val_acc: 0.7805, val_precision: 0.7805 val_recall: 0.7805, val_f1: 0.7805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:04\n",
            "loss: 0.5279, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:55:05\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:05\n",
            ">>> val_acc: 0.7805, val_precision: 0.7805 val_recall: 0.7805, val_f1: 0.7805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:05\n",
            "loss: 0.4375, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 14:55:06\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:06\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:06\n",
            "loss: 0.4685, acc: 0.8170\n",
            "E2E-ABSA >>> 2022-08-17 14:55:06\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:07\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:07\n",
            "loss: 0.4287, acc: 0.8344\n",
            "E2E-ABSA >>> 2022-08-17 14:55:07\n",
            ">>> val_acc: 0.7805, val_precision: 0.7805 val_recall: 0.7805, val_f1: 0.7805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:08\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:08\n",
            "loss: 0.3865, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 14:55:08\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:09\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:09\n",
            "loss: 0.4138, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 14:55:09\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:09\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:10\n",
            "loss: 0.3847, acc: 0.8536\n",
            "E2E-ABSA >>> 2022-08-17 14:55:10\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:10\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:11\n",
            "loss: 0.3607, acc: 0.8580\n",
            "E2E-ABSA >>> 2022-08-17 14:55:11\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:11\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:12\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:12\n",
            "loss: 0.2234, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 14:55:12\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:13\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:13\n",
            "loss: 0.3676, acc: 0.8611\n",
            "E2E-ABSA >>> 2022-08-17 14:55:13\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            "E2E-ABSA >>> 2022-08-17 14:55:13\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            "you can download the best model from state_dict/atae_lstm_SemEval2015_val_f1_0.8415\n",
            ">>> test_acc: 0.8415, test_precision: 0.8415, test_recall: 0.8415, test_f1: 0.8415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2015** dataset on model(**ATAELSTM**)"
      ],
      "metadata": {
        "id": "8aeGtDrMQKf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset SemEval2015_know --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IPyME5kQKme",
        "outputId": "7f95b42b-231f-4290-abac-b676684f74c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 16277504\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 1542900\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f6a9d9e9b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:52\n",
            ">>> val_acc: 0.7805, val_precision: 0.7805 val_recall: 0.7805, val_f1: 0.7805\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_know_val_f1_0.7805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:52\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_know_val_f1_0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:52\n",
            "loss: 0.7704, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:55:53\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_know_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:53\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:54\n",
            "loss: 0.6877, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:55:54\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:55\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:55\n",
            "loss: 0.7016, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-08-17 14:55:55\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:56\n",
            "loss: 0.6562, acc: 0.7630\n",
            "E2E-ABSA >>> 2022-08-17 14:55:56\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:57\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:57\n",
            "loss: 0.6213, acc: 0.7667\n",
            "E2E-ABSA >>> 2022-08-17 14:55:57\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:58\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:58\n",
            "loss: 0.5887, acc: 0.7778\n",
            "E2E-ABSA >>> 2022-08-17 14:55:58\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:59\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 14:55:59\n",
            "loss: 0.6257, acc: 0.7515\n",
            "E2E-ABSA >>> 2022-08-17 14:56:00\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:00\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:01\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:01\n",
            "loss: 0.7613, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 14:56:01\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:02\n",
            "loss: 0.5447, acc: 0.8036\n",
            "E2E-ABSA >>> 2022-08-17 14:56:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:03\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:03\n",
            "loss: 0.6137, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-08-17 14:56:03\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:04\n",
            "loss: 0.5178, acc: 0.8026\n",
            "E2E-ABSA >>> 2022-08-17 14:56:05\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:05\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:05\n",
            "loss: 0.5910, acc: 0.7700\n",
            "E2E-ABSA >>> 2022-08-17 14:56:06\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:06\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:07\n",
            "loss: 0.5550, acc: 0.7762\n",
            "E2E-ABSA >>> 2022-08-17 14:56:07\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:07\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:08\n",
            "loss: 0.5681, acc: 0.7821\n",
            "E2E-ABSA >>> 2022-08-17 14:56:08\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:08\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:09\n",
            "loss: 0.5652, acc: 0.7820\n",
            "E2E-ABSA >>> 2022-08-17 14:56:09\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:10\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_know_val_f1_0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:10\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:10\n",
            "loss: 0.6686, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 14:56:11\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:11\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:11\n",
            "loss: 0.5003, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 14:56:12\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:12\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:13\n",
            "loss: 0.5458, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 14:56:13\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:13\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:14\n",
            "loss: 0.5079, acc: 0.8094\n",
            "E2E-ABSA >>> 2022-08-17 14:56:14\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:15\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:15\n",
            "loss: 0.4702, acc: 0.8173\n",
            "E2E-ABSA >>> 2022-08-17 14:56:15\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">> saved: state_dict/atae_lstm_SemEval2015_know_val_f1_0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:16\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:16\n",
            "loss: 0.5055, acc: 0.7871\n",
            "E2E-ABSA >>> 2022-08-17 14:56:16\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:17\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:17\n",
            "loss: 0.4683, acc: 0.8240\n",
            "E2E-ABSA >>> 2022-08-17 14:56:17\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:18\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:18\n",
            "loss: 0.4419, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:56:19\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:19\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:20\n",
            ">>> val_acc: 0.7805, val_precision: 0.7805 val_recall: 0.7805, val_f1: 0.7805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:20\n",
            "loss: 0.2963, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-08-17 14:56:20\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:21\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:21\n",
            "loss: 0.4524, acc: 0.8056\n",
            "E2E-ABSA >>> 2022-08-17 14:56:21\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:22\n",
            ">>> val_acc: 0.7805, val_precision: 0.7805 val_recall: 0.7805, val_f1: 0.7805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:22\n",
            "loss: 0.4107, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 14:56:22\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:23\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:23\n",
            "loss: 0.4070, acc: 0.8304\n",
            "E2E-ABSA >>> 2022-08-17 14:56:24\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:24\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:24\n",
            "loss: 0.3863, acc: 0.8403\n",
            "E2E-ABSA >>> 2022-08-17 14:56:25\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:25\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:26\n",
            "loss: 0.3512, acc: 0.8693\n",
            "E2E-ABSA >>> 2022-08-17 14:56:26\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:26\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:27\n",
            "loss: 0.3522, acc: 0.8526\n",
            "E2E-ABSA >>> 2022-08-17 14:56:27\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:27\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:28\n",
            "loss: 0.3632, acc: 0.8569\n",
            "E2E-ABSA >>> 2022-08-17 14:56:28\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:29\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:29\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:29\n",
            "loss: 0.4972, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 14:56:30\n",
            ">>> val_acc: 0.7805, val_precision: 0.7805 val_recall: 0.7805, val_f1: 0.7805\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:30\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:30\n",
            "loss: 0.2723, acc: 0.8875\n",
            "E2E-ABSA >>> 2022-08-17 14:56:31\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:31\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 14:56:32\n",
            "loss: 0.3198, acc: 0.8516\n",
            "E2E-ABSA >>> 2022-08-17 14:56:32\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            "E2E-ABSA >>> 2022-08-17 14:56:32\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            "you can download the best model from state_dict/atae_lstm_SemEval2015_know_val_f1_0.8537\n",
            ">>> test_acc: 0.8537, test_precision: 0.8537, test_recall: 0.8537, test_f1: 0.8537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2015** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "5vqws4Z-QKsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset SemEval2015 --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-yv9aO6QKya",
        "outputId": "fcd91c3a-7350-49f4-cb9c-48eb4b7ddf7f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 11942400\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 815400\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f7fdb67cb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:10\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/ian_SemEval2015_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:11\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:11\n",
            "loss: 0.7754, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 14:57:12\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:12\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:12\n",
            "loss: 0.6773, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 14:57:13\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:13\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:14\n",
            "loss: 0.6669, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 14:57:14\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:15\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:15\n",
            "loss: 0.6262, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 14:57:15\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:16\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:16\n",
            "loss: 0.5846, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-08-17 14:57:16\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:17\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:17\n",
            "loss: 0.5899, acc: 0.7847\n",
            "E2E-ABSA >>> 2022-08-17 14:57:18\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:18\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:19\n",
            "loss: 0.6219, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 14:57:19\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:19\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:20\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:20\n",
            "loss: 0.6519, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:57:21\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:21\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:21\n",
            "loss: 0.5332, acc: 0.8036\n",
            "E2E-ABSA >>> 2022-08-17 14:57:22\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:22\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:22\n",
            "loss: 0.5943, acc: 0.7404\n",
            "E2E-ABSA >>> 2022-08-17 14:57:23\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:24\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:24\n",
            "loss: 0.5206, acc: 0.7796\n",
            "E2E-ABSA >>> 2022-08-17 14:57:24\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:25\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:25\n",
            "loss: 0.5433, acc: 0.7700\n",
            "E2E-ABSA >>> 2022-08-17 14:57:25\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:26\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:26\n",
            "loss: 0.5228, acc: 0.7802\n",
            "E2E-ABSA >>> 2022-08-17 14:57:26\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:27\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:28\n",
            "loss: 0.5125, acc: 0.7889\n",
            "E2E-ABSA >>> 2022-08-17 14:57:28\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:57:28\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "E2E-ABSA >>> 2022-08-17 14:57:28\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "you can download the best model from state_dict/ian_SemEval2015_val_f1_0.8293\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.8293, test_precision: 0.8293, test_recall: 0.8293, test_f1: 0.8293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2015** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "8gQap8pBQK4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset SemEval2015_know --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDhddmi4QK-w",
        "outputId": "9f9ee3cd-cf2d-40fe-8667-6433be8be044"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 14852096\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 1542900\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fb2b47f4b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:07\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/ian_SemEval2015_know_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:08\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:08\n",
            "loss: 0.7347, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 14:58:09\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:09\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:09\n",
            "loss: 0.6780, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 14:58:10\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:11\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:11\n",
            "loss: 0.6680, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 14:58:11\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:12\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:12\n",
            "loss: 0.6412, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 14:58:13\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:13\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:14\n",
            "loss: 0.5990, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-08-17 14:58:14\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:15\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:15\n",
            "loss: 0.6104, acc: 0.7847\n",
            "E2E-ABSA >>> 2022-08-17 14:58:15\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:16\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:17\n",
            "loss: 0.6508, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 14:58:17\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:17\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:18\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:18\n",
            "loss: 0.6849, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:58:19\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:20\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:20\n",
            "loss: 0.5937, acc: 0.8036\n",
            "E2E-ABSA >>> 2022-08-17 14:58:20\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:21\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:21\n",
            "loss: 0.6707, acc: 0.7404\n",
            "E2E-ABSA >>> 2022-08-17 14:58:22\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:22\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:23\n",
            "loss: 0.5969, acc: 0.7796\n",
            "E2E-ABSA >>> 2022-08-17 14:58:23\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:24\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:24\n",
            "loss: 0.6355, acc: 0.7725\n",
            "E2E-ABSA >>> 2022-08-17 14:58:24\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:25\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:25\n",
            "loss: 0.6231, acc: 0.7722\n",
            "E2E-ABSA >>> 2022-08-17 14:58:26\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:26\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:27\n",
            "loss: 0.6359, acc: 0.7669\n",
            "E2E-ABSA >>> 2022-08-17 14:58:27\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 14:58:28\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "E2E-ABSA >>> 2022-08-17 14:58:28\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            "you can download the best model from state_dict/ian_SemEval2015_know_val_f1_0.8293\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.8293, test_precision: 0.8293, test_recall: 0.8293, test_f1: 0.8293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2015** dataset on model(**MEMNET**)"
      ],
      "metadata": {
        "id": "RBLz11cVQLEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset SemEval2015 --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCtm8sSBQLKx",
        "outputId": "813f8e10-329f-40d4-b7e7-5c8af4ad75a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 4717056\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 815400\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fce995c7b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:05\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">> saved: state_dict/memnet_SemEval2015_val_f1_0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:06\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:06\n",
            "loss: 0.6497, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 14:59:06\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/memnet_SemEval2015_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:06\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">> saved: state_dict/memnet_SemEval2015_val_f1_0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:06\n",
            "loss: 0.5805, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 14:59:07\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:07\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:07\n",
            "loss: 0.5840, acc: 0.7847\n",
            "E2E-ABSA >>> 2022-08-17 14:59:07\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">> saved: state_dict/memnet_SemEval2015_val_f1_0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:08\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:08\n",
            "loss: 0.5465, acc: 0.8047\n",
            "E2E-ABSA >>> 2022-08-17 14:59:08\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:08\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:09\n",
            "loss: 0.5569, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-08-17 14:59:09\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:09\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:09\n",
            "loss: 0.5934, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-08-17 14:59:10\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:10\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:10\n",
            "loss: 0.5410, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 14:59:10\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:11\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:11\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:11\n",
            "loss: 0.4685, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 14:59:11\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">> saved: state_dict/memnet_SemEval2015_val_f1_0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:12\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:12\n",
            "loss: 0.5119, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 14:59:12\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:13\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:13\n",
            "loss: 0.5106, acc: 0.7933\n",
            "E2E-ABSA >>> 2022-08-17 14:59:13\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:13\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:13\n",
            "loss: 0.4886, acc: 0.8158\n",
            "E2E-ABSA >>> 2022-08-17 14:59:14\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:14\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:14\n",
            "loss: 0.4528, acc: 0.8275\n",
            "E2E-ABSA >>> 2022-08-17 14:59:14\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:15\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:15\n",
            "loss: 0.4432, acc: 0.8266\n",
            "E2E-ABSA >>> 2022-08-17 14:59:15\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:15\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:16\n",
            "loss: 0.4432, acc: 0.8243\n",
            "E2E-ABSA >>> 2022-08-17 14:59:16\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:16\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:16\n",
            "loss: 0.4415, acc: 0.8183\n",
            "E2E-ABSA >>> 2022-08-17 14:59:16\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:17\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:17\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:17\n",
            "loss: 0.4823, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 14:59:18\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:18\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:18\n",
            "loss: 0.3868, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 14:59:18\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:19\n",
            ">>> val_acc: 0.8049, val_precision: 0.8049 val_recall: 0.8049, val_f1: 0.8049\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:19\n",
            "loss: 0.4068, acc: 0.8304\n",
            "E2E-ABSA >>> 2022-08-17 14:59:19\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:19\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:20\n",
            "loss: 0.4073, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 14:59:20\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:20\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:20\n",
            "loss: 0.3947, acc: 0.8365\n",
            "E2E-ABSA >>> 2022-08-17 14:59:20\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:21\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:21\n",
            "loss: 0.3495, acc: 0.8672\n",
            "E2E-ABSA >>> 2022-08-17 14:59:21\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:22\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:22\n",
            "loss: 0.3755, acc: 0.8553\n",
            "E2E-ABSA >>> 2022-08-17 14:59:22\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 14:59:22\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            "E2E-ABSA >>> 2022-08-17 14:59:22\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            "you can download the best model from state_dict/memnet_SemEval2015_val_f1_0.8659\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.8659, test_precision: 0.8659, test_recall: 0.8659, test_f1: 0.8659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2015** dataset on model(**MEMNET**)"
      ],
      "metadata": {
        "id": "7-nlDvPSQLQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset SemEval2015_know --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FKsYm7IQLWw",
        "outputId": "38b3e8a5-00eb-46d1-8102-264aecd59a34"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 7626752\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 1542900\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1a7fa4eb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:01\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">> saved: state_dict/memnet_SemEval2015_know_val_f1_0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:01\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:01\n",
            "loss: 0.6589, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:00:01\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/memnet_SemEval2015_know_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:02\n",
            "loss: 0.5934, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:00:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:03\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:03\n",
            "loss: 0.6092, acc: 0.7847\n",
            "E2E-ABSA >>> 2022-08-17 15:00:03\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:03\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:03\n",
            "loss: 0.5656, acc: 0.7995\n",
            "E2E-ABSA >>> 2022-08-17 15:00:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:04\n",
            "loss: 0.5838, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 15:00:04\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:05\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:05\n",
            "loss: 0.6309, acc: 0.7587\n",
            "E2E-ABSA >>> 2022-08-17 15:00:05\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:05\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:06\n",
            "loss: 0.5805, acc: 0.7872\n",
            "E2E-ABSA >>> 2022-08-17 15:00:06\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:06\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:07\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:07\n",
            "loss: 0.4734, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:00:07\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:07\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:07\n",
            "loss: 0.5642, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 15:00:08\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:08\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:08\n",
            "loss: 0.5859, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 15:00:08\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">> saved: state_dict/memnet_SemEval2015_know_val_f1_0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:09\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:09\n",
            "loss: 0.5447, acc: 0.7862\n",
            "E2E-ABSA >>> 2022-08-17 15:00:09\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:09\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:10\n",
            "loss: 0.5280, acc: 0.8025\n",
            "E2E-ABSA >>> 2022-08-17 15:00:10\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:10\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:10\n",
            "loss: 0.5288, acc: 0.7964\n",
            "E2E-ABSA >>> 2022-08-17 15:00:10\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:11\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">> saved: state_dict/memnet_SemEval2015_know_val_f1_0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:11\n",
            "loss: 0.5297, acc: 0.7905\n",
            "E2E-ABSA >>> 2022-08-17 15:00:11\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:12\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:12\n",
            "loss: 0.5388, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-08-17 15:00:12\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:12\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:13\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:13\n",
            "loss: 0.6420, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:00:13\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:13\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:13\n",
            "loss: 0.4891, acc: 0.8047\n",
            "E2E-ABSA >>> 2022-08-17 15:00:14\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:14\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:14\n",
            "loss: 0.5376, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 15:00:14\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:15\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:15\n",
            "loss: 0.5475, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:00:15\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:16\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:16\n",
            "loss: 0.5087, acc: 0.7885\n",
            "E2E-ABSA >>> 2022-08-17 15:00:16\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:16\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:17\n",
            "loss: 0.4655, acc: 0.8223\n",
            "E2E-ABSA >>> 2022-08-17 15:00:17\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:17\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:17\n",
            "loss: 0.5047, acc: 0.7928\n",
            "E2E-ABSA >>> 2022-08-17 15:00:17\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:18\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:18\n",
            "loss: 0.4951, acc: 0.7940\n",
            "E2E-ABSA >>> 2022-08-17 15:00:18\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:18\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:19\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:19\n",
            "loss: 0.3192, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 15:00:19\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:20\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:20\n",
            "loss: 0.5850, acc: 0.7569\n",
            "E2E-ABSA >>> 2022-08-17 15:00:20\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:20\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:20\n",
            "loss: 0.5271, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:00:21\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:21\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:21\n",
            "loss: 0.4342, acc: 0.8304\n",
            "E2E-ABSA >>> 2022-08-17 15:00:21\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:00:22\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            "E2E-ABSA >>> 2022-08-17 15:00:22\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            "you can download the best model from state_dict/memnet_SemEval2015_know_val_f1_0.8537\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.8537, test_precision: 0.8537, test_recall: 0.8537, test_f1: 0.8537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2015** dataset on model(**CABASC**)"
      ],
      "metadata": {
        "id": "srjWmKGjQLbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset SemEval2015 --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80lVaxryQLhL",
        "outputId": "0a96eb59-b622-46b8-fc42-a4a85d0907a4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 655.\n",
            "> testing dataset count: 79.\n",
            "cuda memory allocated: 9050624\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 815400\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f8dae460b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:01\n",
            ">>> val_acc: 0.7215, val_precision: 0.7215 val_recall: 0.7215, val_f1: 0.7215\n",
            ">> saved: state_dict/cabasc_SemEval2015_val_f1_0.7215\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:02\n",
            ">>> val_acc: 0.7975, val_precision: 0.7975 val_recall: 0.7975, val_f1: 0.7975\n",
            ">> saved: state_dict/cabasc_SemEval2015_val_f1_0.7975\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:02\n",
            "loss: 0.6956, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 15:01:03\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">> saved: state_dict/cabasc_SemEval2015_val_f1_0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:04\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">> saved: state_dict/cabasc_SemEval2015_val_f1_0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:04\n",
            "loss: 0.6402, acc: 0.7587\n",
            "E2E-ABSA >>> 2022-08-17 15:01:04\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:05\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:06\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:07\n",
            "loss: 0.6395, acc: 0.7404\n",
            "E2E-ABSA >>> 2022-08-17 15:01:07\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:08\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:09\n",
            "loss: 0.6060, acc: 0.7581\n",
            "E2E-ABSA >>> 2022-08-17 15:01:09\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:10\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:11\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:11\n",
            "loss: 0.6303, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 15:01:12\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:12\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:13\n",
            "loss: 0.5749, acc: 0.7668\n",
            "E2E-ABSA >>> 2022-08-17 15:01:13\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:14\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:15\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:15\n",
            "loss: 0.4753, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-08-17 15:01:16\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:17\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:17\n",
            "loss: 0.6097, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-08-17 15:01:18\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:19\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:20\n",
            "loss: 0.5691, acc: 0.7676\n",
            "E2E-ABSA >>> 2022-08-17 15:01:20\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:20\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:21\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:22\n",
            "loss: 0.5121, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:01:22\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:23\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:24\n",
            "loss: 0.5520, acc: 0.7739\n",
            "E2E-ABSA >>> 2022-08-17 15:01:24\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:25\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:26\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:26\n",
            "loss: 0.5043, acc: 0.8011\n",
            "E2E-ABSA >>> 2022-08-17 15:01:27\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:28\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:28\n",
            "loss: 0.5152, acc: 0.7780\n",
            "E2E-ABSA >>> 2022-08-17 15:01:29\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:30\n",
            ">>> val_acc: 0.8228, val_precision: 0.8228 val_recall: 0.8228, val_f1: 0.8228\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:01:30\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            "E2E-ABSA >>> 2022-08-17 15:01:30\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "you can download the best model from state_dict/cabasc_SemEval2015_val_f1_0.8354\n",
            ">>> test_acc: 0.8354, test_precision: 0.8354, test_recall: 0.8354, test_f1: 0.8354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2015** dataset on model(**CABASC**)"
      ],
      "metadata": {
        "id": "t1iodVnKQLma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset SemEval2015_know --embed_dim 300 --patience 30\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWVJe1ZwQLsS",
        "outputId": "369a5779-c47a-4d97-ca75-3f5affb39a00"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 658.\n",
            "> testing dataset count: 79.\n",
            "cuda memory allocated: 11960320\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 1542900\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7efe9c65db00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:11\n",
            ">>> val_acc: 0.8101, val_precision: 0.8101 val_recall: 0.8101, val_f1: 0.8101\n",
            ">> saved: state_dict/cabasc_SemEval2015_know_val_f1_0.8101\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:13\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">> saved: state_dict/cabasc_SemEval2015_know_val_f1_0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:14\n",
            "loss: 0.7118, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 15:02:15\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:18\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:19\n",
            "loss: 0.6531, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 15:02:20\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:22\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:25\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:25\n",
            "loss: 0.6686, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 15:02:27\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:29\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:30\n",
            "loss: 0.6345, acc: 0.7472\n",
            "E2E-ABSA >>> 2022-08-17 15:02:32\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:34\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:36\n",
            "loss: 0.6324, acc: 0.7566\n",
            "E2E-ABSA >>> 2022-08-17 15:02:36\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:39\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:41\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:42\n",
            "loss: 0.5840, acc: 0.7865\n",
            "E2E-ABSA >>> 2022-08-17 15:02:43\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:46\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:47\n",
            "loss: 0.5598, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 15:02:48\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:50\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:53\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:53\n",
            "loss: 0.5977, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:02:55\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:58\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:02:58\n",
            "loss: 0.6353, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-08-17 15:03:00\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:02\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:04\n",
            "loss: 0.6089, acc: 0.7482\n",
            "E2E-ABSA >>> 2022-08-17 15:03:05\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:07\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:10\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:10\n",
            "loss: 0.6096, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:03:12\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:14\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:16\n",
            "loss: 0.5605, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:03:17\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:19\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:21\n",
            "loss: 0.5688, acc: 0.7594\n",
            "E2E-ABSA >>> 2022-08-17 15:03:21\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:03:24\n",
            ">>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "E2E-ABSA >>> 2022-08-17 15:03:24\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8354, val_precision: 0.8354 val_recall: 0.8354, val_f1: 0.8354\n",
            "you can download the best model from state_dict/cabasc_SemEval2015_know_val_f1_0.8354\n",
            ">>> test_acc: 0.8354, test_precision: 0.8354, test_recall: 0.8354, test_f1: 0.8354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lstm\n",
        "tdlstm  \n",
        "tclstm  \n",
        "ataelstm  \n",
        "ian \n",
        "memnet  \n",
        "cabasc "
      ],
      "metadata": {
        "id": "K4teqx1kV8F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2016** dataset on model(**LSTM**)"
      ],
      "metadata": {
        "id": "18hY-lzCWOLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset SemEval2016 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efw95HhAV8MT",
        "outputId": "72ddd06b-a571-4987-9d49-bffae199dfaa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 7341568\n",
            "> n_trainable_params: 723303, n_nontrainable_params: 1111800\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f622af50b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:54\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:55\n",
            "loss: 0.8087, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:08:55\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:55\n",
            "loss: 0.7370, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-08-17 15:08:55\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:56\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:56\n",
            "loss: 0.7459, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 15:08:56\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:56\n",
            "loss: 0.6938, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 15:08:56\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:57\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:57\n",
            "loss: 0.6982, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:08:57\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:57\n",
            "loss: 0.6741, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:08:58\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:58\n",
            "loss: 0.6820, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-08-17 15:08:58\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:58\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:59\n",
            "loss: 0.6853, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 15:08:59\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:08:59\n",
            "loss: 0.6704, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 15:08:59\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:00\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:00\n",
            "loss: 0.6396, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 15:09:00\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:00\n",
            "loss: 0.6498, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 15:09:00\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:01\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:01\n",
            "loss: 0.6664, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:09:01\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:01\n",
            "loss: 0.6322, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 15:09:02\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:02\n",
            "loss: 0.6125, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-08-17 15:09:02\n",
            ">>> val_acc: 0.7203, val_precision: 0.7203 val_recall: 0.7203, val_f1: 0.7203\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:02\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:02\n",
            "loss: 0.5881, acc: 0.7542\n",
            "E2E-ABSA >>> 2022-08-17 15:09:03\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:03\n",
            "loss: 0.5435, acc: 0.7615\n",
            "E2E-ABSA >>> 2022-08-17 15:09:03\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:04\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:04\n",
            "loss: 0.5458, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:09:04\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:04\n",
            "loss: 0.5167, acc: 0.7887\n",
            "E2E-ABSA >>> 2022-08-17 15:09:04\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:05\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:05\n",
            "loss: 0.4004, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:09:05\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:05\n",
            "loss: 0.4965, acc: 0.8031\n",
            "E2E-ABSA >>> 2022-08-17 15:09:06\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:06\n",
            "loss: 0.4924, acc: 0.7950\n",
            "E2E-ABSA >>> 2022-08-17 15:09:06\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:06\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:06\n",
            "loss: 0.4641, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:09:07\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:07\n",
            "loss: 0.4765, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-08-17 15:09:07\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:07\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:08\n",
            "loss: 0.4212, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 15:09:08\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:08\n",
            "loss: 0.4533, acc: 0.8063\n",
            "E2E-ABSA >>> 2022-08-17 15:09:08\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:09\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:09\n",
            "loss: 0.3985, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:09:09\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:09\n",
            "loss: 0.4525, acc: 0.8172\n",
            "E2E-ABSA >>> 2022-08-17 15:09:09\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:10\n",
            "loss: 0.4568, acc: 0.8138\n",
            "E2E-ABSA >>> 2022-08-17 15:09:10\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:10\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:10\n",
            "loss: 0.4167, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-08-17 15:09:11\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:11\n",
            "loss: 0.4475, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:09:11\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:11\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:12\n",
            "loss: 0.4403, acc: 0.8219\n",
            "E2E-ABSA >>> 2022-08-17 15:09:12\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:12\n",
            "loss: 0.4436, acc: 0.8225\n",
            "E2E-ABSA >>> 2022-08-17 15:09:12\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:13\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:13\n",
            "loss: 0.4080, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:09:13\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:13\n",
            "loss: 0.4695, acc: 0.8094\n",
            "E2E-ABSA >>> 2022-08-17 15:09:13\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:14\n",
            "loss: 0.4342, acc: 0.8237\n",
            "E2E-ABSA >>> 2022-08-17 15:09:14\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:14\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:14\n",
            "loss: 0.4482, acc: 0.8146\n",
            "E2E-ABSA >>> 2022-08-17 15:09:15\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:15\n",
            "loss: 0.4188, acc: 0.8271\n",
            "E2E-ABSA >>> 2022-08-17 15:09:15\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:15\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:15\n",
            "loss: 0.4690, acc: 0.8219\n",
            "E2E-ABSA >>> 2022-08-17 15:09:16\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:16\n",
            "loss: 0.4179, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 15:09:16\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:16\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:17\n",
            "loss: 0.4045, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-08-17 15:09:17\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:17\n",
            "loss: 0.4140, acc: 0.8328\n",
            "E2E-ABSA >>> 2022-08-17 15:09:17\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:18\n",
            "loss: 0.4190, acc: 0.8318\n",
            "E2E-ABSA >>> 2022-08-17 15:09:18\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:18\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:18\n",
            "loss: 0.4297, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-08-17 15:09:18\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:19\n",
            "loss: 0.4129, acc: 0.8458\n",
            "E2E-ABSA >>> 2022-08-17 15:09:19\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:19\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:19\n",
            "loss: 0.3817, acc: 0.8531\n",
            "E2E-ABSA >>> 2022-08-17 15:09:20\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:20\n",
            "loss: 0.3971, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:09:20\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:20\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:20\n",
            "loss: 0.3697, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:09:21\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:21\n",
            "loss: 0.4170, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 15:09:21\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:22\n",
            "loss: 0.3972, acc: 0.8462\n",
            "E2E-ABSA >>> 2022-08-17 15:09:22\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:22\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:22\n",
            "loss: 0.4016, acc: 0.8417\n",
            "E2E-ABSA >>> 2022-08-17 15:09:22\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:23\n",
            "loss: 0.4037, acc: 0.8448\n",
            "E2E-ABSA >>> 2022-08-17 15:09:23\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:23\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:23\n",
            "loss: 0.3832, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 15:09:24\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:24\n",
            "loss: 0.3759, acc: 0.8588\n",
            "E2E-ABSA >>> 2022-08-17 15:09:24\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:24\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:24\n",
            "loss: 0.3298, acc: 0.9000\n",
            "E2E-ABSA >>> 2022-08-17 15:09:25\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:25\n",
            "loss: 0.3954, acc: 0.8578\n",
            "E2E-ABSA >>> 2022-08-17 15:09:25\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">> saved: state_dict/lstm_SemEval2016_val_f1_0.822\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:25\n",
            "loss: 0.3824, acc: 0.8570\n",
            "E2E-ABSA >>> 2022-08-17 15:09:26\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:26\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:26\n",
            "loss: 0.3676, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:09:26\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:27\n",
            "loss: 0.3816, acc: 0.8479\n",
            "E2E-ABSA >>> 2022-08-17 15:09:27\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:27\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:27\n",
            "loss: 0.4095, acc: 0.8406\n",
            "E2E-ABSA >>> 2022-08-17 15:09:27\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:28\n",
            "loss: 0.3757, acc: 0.8475\n",
            "E2E-ABSA >>> 2022-08-17 15:09:28\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:28\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:28\n",
            "loss: 0.4171, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:09:29\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:29\n",
            "loss: 0.3758, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:09:29\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:29\n",
            "loss: 0.3709, acc: 0.8543\n",
            "E2E-ABSA >>> 2022-08-17 15:09:29\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:30\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:30\n",
            "loss: 0.3458, acc: 0.8833\n",
            "E2E-ABSA >>> 2022-08-17 15:09:30\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:31\n",
            "loss: 0.3741, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:09:31\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:31\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:31\n",
            "loss: 0.3765, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:09:31\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:32\n",
            "loss: 0.3487, acc: 0.8662\n",
            "E2E-ABSA >>> 2022-08-17 15:09:32\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:32\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:32\n",
            "loss: 0.3221, acc: 0.9125\n",
            "E2E-ABSA >>> 2022-08-17 15:09:33\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:33\n",
            "loss: 0.3625, acc: 0.8578\n",
            "E2E-ABSA >>> 2022-08-17 15:09:33\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-08-17 15:09:33\n",
            "loss: 0.3592, acc: 0.8642\n",
            "E2E-ABSA >>> 2022-08-17 15:09:33\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            "you can download the best model from state_dict/lstm_SemEval2016_val_f1_0.822\n",
            ">>> test_acc: 0.8220, test_precision: 0.8220, test_recall: 0.8220, test_f1: 0.8220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2016** dataset on model(**LSTM**)"
      ],
      "metadata": {
        "id": "S6gVgYJUV8S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset SemEval2016_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuTuSo0bV8ZJ",
        "outputId": "56ce768c-c364-4cd7-e239-8adadb571cac"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 10843136\n",
            "> n_trainable_params: 723303, n_nontrainable_params: 1987200\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7efc4bdbcb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:12\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/lstm_SemEval2016_know_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:12\n",
            "loss: 0.8028, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:10:13\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:13\n",
            "loss: 0.7399, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-08-17 15:10:13\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:14\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:14\n",
            "loss: 0.7505, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 15:10:14\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:14\n",
            "loss: 0.7021, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 15:10:15\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:15\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:15\n",
            "loss: 0.7111, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:10:16\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:16\n",
            "loss: 0.6860, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:10:16\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:17\n",
            "loss: 0.7028, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-08-17 15:10:17\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:17\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:17\n",
            "loss: 0.7113, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 15:10:18\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:18\n",
            "loss: 0.7018, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 15:10:18\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:19\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:19\n",
            "loss: 0.6740, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 15:10:19\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:20\n",
            "loss: 0.6926, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 15:10:20\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:20\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:20\n",
            "loss: 0.7252, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:10:21\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:21\n",
            "loss: 0.7025, acc: 0.7172\n",
            "E2E-ABSA >>> 2022-08-17 15:10:21\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:22\n",
            "loss: 0.6900, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-08-17 15:10:22\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:22\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:23\n",
            "loss: 0.6969, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:10:23\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:23\n",
            "loss: 0.6778, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-08-17 15:10:23\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:24\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:24\n",
            "loss: 0.6893, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 15:10:24\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:25\n",
            "loss: 0.6536, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-08-17 15:10:25\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:25\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:25\n",
            "loss: 0.6288, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 15:10:26\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:26\n",
            "loss: 0.6757, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:10:26\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:27\n",
            "loss: 0.6807, acc: 0.7167\n",
            "E2E-ABSA >>> 2022-08-17 15:10:27\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:10:27\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            "E2E-ABSA >>> 2022-08-17 15:10:27\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            "you can download the best model from state_dict/lstm_SemEval2016_know_val_f1_0.7288\n",
            ">>> test_acc: 0.7288, test_precision: 0.7288, test_recall: 0.7288, test_f1: 0.7288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2016** dataset on model(**TDLSTM**)"
      ],
      "metadata": {
        "id": "N1PJnI2OV8e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset SemEval2016 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0E3TH95V8k3",
        "outputId": "c7ffa2b5-17b1-4af8-d229-b7b119a713c4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 989.\n",
            "> testing dataset count: 103.\n",
            "cuda memory allocated: 10234880\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 1111800\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f46e8d2db00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:07\n",
            ">>> val_acc: 0.6117, val_precision: 0.6117 val_recall: 0.6117, val_f1: 0.6117\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.6117\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:07\n",
            "loss: 0.9357, acc: 0.6809\n",
            "E2E-ABSA >>> 2022-08-17 15:11:07\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:08\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:08\n",
            "loss: 0.7488, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:11:08\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:09\n",
            "loss: 0.7206, acc: 0.7103\n",
            "E2E-ABSA >>> 2022-08-17 15:11:09\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:09\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:10\n",
            "loss: 0.7163, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 15:11:10\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:10\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:10\n",
            "loss: 0.7770, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:11:11\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:11\n",
            "loss: 0.6877, acc: 0.7009\n",
            "E2E-ABSA >>> 2022-08-17 15:11:11\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:12\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:12\n",
            "loss: 0.6709, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-08-17 15:11:13\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:13\n",
            "loss: 0.6548, acc: 0.7210\n",
            "E2E-ABSA >>> 2022-08-17 15:11:13\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:14\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:14\n",
            "loss: 0.6761, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 15:11:14\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:15\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:15\n",
            "loss: 0.6028, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 15:11:15\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:16\n",
            "loss: 0.6547, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 15:11:16\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:16\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:16\n",
            "loss: 0.6531, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 15:11:17\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:17\n",
            "loss: 0.6419, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 15:11:17\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:18\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:18\n",
            "loss: 0.6447, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-08-17 15:11:18\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:19\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:19\n",
            "loss: 0.6622, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 15:11:19\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:20\n",
            "loss: 0.6221, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:11:20\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:20\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:21\n",
            "loss: 0.5742, acc: 0.7668\n",
            "E2E-ABSA >>> 2022-08-17 15:11:21\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:21\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:21\n",
            "loss: 0.4775, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:11:22\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:22\n",
            "loss: 0.5680, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 15:11:22\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:23\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:23\n",
            "loss: 0.5430, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 15:11:23\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:24\n",
            "loss: 0.5792, acc: 0.7477\n",
            "E2E-ABSA >>> 2022-08-17 15:11:24\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:25\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:25\n",
            "loss: 0.4876, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-08-17 15:11:25\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:26\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:26\n",
            "loss: 0.5205, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-08-17 15:11:26\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:26\n",
            "loss: 0.5244, acc: 0.7798\n",
            "E2E-ABSA >>> 2022-08-17 15:11:27\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:27\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:27\n",
            "loss: 0.5544, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:11:28\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:28\n",
            "loss: 0.5032, acc: 0.7909\n",
            "E2E-ABSA >>> 2022-08-17 15:11:28\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:29\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:29\n",
            "loss: 0.5007, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-08-17 15:11:29\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:30\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:30\n",
            "loss: 0.5437, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:11:30\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:31\n",
            "loss: 0.4679, acc: 0.8060\n",
            "E2E-ABSA >>> 2022-08-17 15:11:31\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:31\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:31\n",
            "loss: 0.4583, acc: 0.8151\n",
            "E2E-ABSA >>> 2022-08-17 15:11:32\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:32\n",
            "loss: 0.4613, acc: 0.8150\n",
            "E2E-ABSA >>> 2022-08-17 15:11:32\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:33\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:33\n",
            "loss: 0.4712, acc: 0.8076\n",
            "E2E-ABSA >>> 2022-08-17 15:11:33\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:34\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:34\n",
            "loss: 0.4622, acc: 0.8304\n",
            "E2E-ABSA >>> 2022-08-17 15:11:34\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:35\n",
            "loss: 0.4334, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 15:11:35\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:35\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:36\n",
            "loss: 0.4216, acc: 0.8326\n",
            "E2E-ABSA >>> 2022-08-17 15:11:36\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:36\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:36\n",
            "loss: 0.3100, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:11:37\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:37\n",
            "loss: 0.4241, acc: 0.8423\n",
            "E2E-ABSA >>> 2022-08-17 15:11:37\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:38\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:38\n",
            "loss: 0.4194, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-08-17 15:11:38\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">> saved: state_dict/td_lstm_SemEval2016_val_f1_0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:39\n",
            "loss: 0.4108, acc: 0.8371\n",
            "E2E-ABSA >>> 2022-08-17 15:11:39\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:39\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:40\n",
            "loss: 0.3947, acc: 0.8477\n",
            "E2E-ABSA >>> 2022-08-17 15:11:40\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:40\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:41\n",
            "loss: 0.3715, acc: 0.8672\n",
            "E2E-ABSA >>> 2022-08-17 15:11:41\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:41\n",
            "loss: 0.3965, acc: 0.8601\n",
            "E2E-ABSA >>> 2022-08-17 15:11:42\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:42\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:42\n",
            "loss: 0.4231, acc: 0.8295\n",
            "E2E-ABSA >>> 2022-08-17 15:11:43\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:43\n",
            "loss: 0.3832, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:11:43\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:44\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:44\n",
            "loss: 0.3818, acc: 0.8576\n",
            "E2E-ABSA >>> 2022-08-17 15:11:44\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:45\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:45\n",
            "loss: 0.3972, acc: 0.8646\n",
            "E2E-ABSA >>> 2022-08-17 15:11:45\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:46\n",
            "loss: 0.3773, acc: 0.8612\n",
            "E2E-ABSA >>> 2022-08-17 15:11:46\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:46\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:46\n",
            "loss: 0.3663, acc: 0.8702\n",
            "E2E-ABSA >>> 2022-08-17 15:11:47\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:47\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:47\n",
            "loss: 0.3675, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:11:48\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:48\n",
            "loss: 0.3676, acc: 0.8578\n",
            "E2E-ABSA >>> 2022-08-17 15:11:48\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:49\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:49\n",
            "loss: 0.3701, acc: 0.8789\n",
            "E2E-ABSA >>> 2022-08-17 15:11:49\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:50\n",
            "loss: 0.3598, acc: 0.8704\n",
            "E2E-ABSA >>> 2022-08-17 15:11:50\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:50\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:51\n",
            "loss: 0.3794, acc: 0.8688\n",
            "E2E-ABSA >>> 2022-08-17 15:11:51\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:51\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:51\n",
            "loss: 0.3255, acc: 0.9271\n",
            "E2E-ABSA >>> 2022-08-17 15:11:52\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:52\n",
            "loss: 0.3627, acc: 0.8707\n",
            "E2E-ABSA >>> 2022-08-17 15:11:52\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:53\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:53\n",
            "loss: 0.3369, acc: 0.8781\n",
            "E2E-ABSA >>> 2022-08-17 15:11:53\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 15:11:54\n",
            "loss: 0.3474, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:11:54\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            "E2E-ABSA >>> 2022-08-17 15:11:54\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            "you can download the best model from state_dict/td_lstm_SemEval2016_val_f1_0.8155\n",
            ">>> test_acc: 0.8155, test_precision: 0.8155, test_recall: 0.8155, test_f1: 0.8155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2016** dataset on model(**TDLSTM**)"
      ],
      "metadata": {
        "id": "NQdcmH47V8qw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset SemEval2016_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sWG6_EbV8wq",
        "outputId": "b7ba5240-a48d-4bde-ba00-235d9c117faf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1006.\n",
            "> testing dataset count: 103.\n",
            "cuda memory allocated: 13736448\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 1987200\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fd01578cb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:33\n",
            ">>> val_acc: 0.6117, val_precision: 0.6117 val_recall: 0.6117, val_f1: 0.6117\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.6117\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:33\n",
            "loss: 0.9240, acc: 0.6976\n",
            "E2E-ABSA >>> 2022-08-17 15:12:33\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:34\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:34\n",
            "loss: 0.8075, acc: 0.6648\n",
            "E2E-ABSA >>> 2022-08-17 15:12:35\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:35\n",
            "loss: 0.7022, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:12:35\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:36\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:36\n",
            "loss: 0.6952, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-08-17 15:12:37\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:37\n",
            "loss: 0.6889, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 15:12:37\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:38\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:38\n",
            "loss: 0.6772, acc: 0.7178\n",
            "E2E-ABSA >>> 2022-08-17 15:12:39\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:39\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:39\n",
            "loss: 0.7061, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-08-17 15:12:40\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:40\n",
            "loss: 0.6526, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-08-17 15:12:41\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:41\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:41\n",
            "loss: 0.6774, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 15:12:42\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:42\n",
            "loss: 0.6556, acc: 0.7193\n",
            "E2E-ABSA >>> 2022-08-17 15:12:43\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:43\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:44\n",
            "loss: 0.6458, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-08-17 15:12:44\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:45\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:45\n",
            "loss: 0.6048, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:12:45\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:46\n",
            "loss: 0.6302, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-08-17 15:12:46\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:47\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:47\n",
            "loss: 0.5956, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:12:47\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:48\n",
            "loss: 0.6170, acc: 0.7439\n",
            "E2E-ABSA >>> 2022-08-17 15:12:48\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:49\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:49\n",
            "loss: 0.5857, acc: 0.7600\n",
            "E2E-ABSA >>> 2022-08-17 15:12:49\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:50\n",
            "loss: 0.5970, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-08-17 15:12:50\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:51\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:51\n",
            "loss: 0.5889, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-08-17 15:12:51\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:52\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:52\n",
            "loss: 0.5279, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:12:53\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:53\n",
            "loss: 0.5669, acc: 0.7593\n",
            "E2E-ABSA >>> 2022-08-17 15:12:53\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:54\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:54\n",
            "loss: 0.5089, acc: 0.7946\n",
            "E2E-ABSA >>> 2022-08-17 15:12:55\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:55\n",
            "loss: 0.5505, acc: 0.7705\n",
            "E2E-ABSA >>> 2022-08-17 15:12:55\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:56\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:56\n",
            "loss: 0.5902, acc: 0.7637\n",
            "E2E-ABSA >>> 2022-08-17 15:12:57\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:57\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:57\n",
            "loss: 0.6474, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:12:58\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:58\n",
            "loss: 0.5432, acc: 0.7776\n",
            "E2E-ABSA >>> 2022-08-17 15:12:59\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">> saved: state_dict/td_lstm_SemEval2016_know_val_f1_0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:12:59\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:00\n",
            "loss: 0.5334, acc: 0.7904\n",
            "E2E-ABSA >>> 2022-08-17 15:13:00\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:01\n",
            "loss: 0.5119, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-08-17 15:13:01\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:01\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:02\n",
            "loss: 0.5109, acc: 0.8013\n",
            "E2E-ABSA >>> 2022-08-17 15:13:02\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:03\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:03\n",
            "loss: 0.5402, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:13:03\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:04\n",
            "loss: 0.4987, acc: 0.8029\n",
            "E2E-ABSA >>> 2022-08-17 15:13:04\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:05\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:05\n",
            "loss: 0.4355, acc: 0.8365\n",
            "E2E-ABSA >>> 2022-08-17 15:13:06\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:06\n",
            "loss: 0.4891, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:13:06\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:07\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:07\n",
            "loss: 0.5212, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-08-17 15:13:08\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:08\n",
            "loss: 0.4646, acc: 0.8289\n",
            "E2E-ABSA >>> 2022-08-17 15:13:08\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:09\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:09\n",
            "loss: 0.4629, acc: 0.8286\n",
            "E2E-ABSA >>> 2022-08-17 15:13:10\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:10\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:10\n",
            "loss: 0.4905, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:13:11\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:11\n",
            "loss: 0.4372, acc: 0.8342\n",
            "E2E-ABSA >>> 2022-08-17 15:13:12\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:12\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:12\n",
            "loss: 0.4259, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 15:13:13\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:13\n",
            "loss: 0.4364, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-08-17 15:13:14\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:14\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:15\n",
            "loss: 0.4516, acc: 0.8327\n",
            "E2E-ABSA >>> 2022-08-17 15:13:15\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:16\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:16\n",
            "loss: 0.5522, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:13:16\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:17\n",
            "loss: 0.4170, acc: 0.8467\n",
            "E2E-ABSA >>> 2022-08-17 15:13:17\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:18\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:18\n",
            "loss: 0.4658, acc: 0.8203\n",
            "E2E-ABSA >>> 2022-08-17 15:13:18\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:19\n",
            "loss: 0.4028, acc: 0.8538\n",
            "E2E-ABSA >>> 2022-08-17 15:13:19\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            "E2E-ABSA >>> 2022-08-17 15:13:19\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            "you can download the best model from state_dict/td_lstm_SemEval2016_know_val_f1_0.8058\n",
            ">>> test_acc: 0.8058, test_precision: 0.8058, test_recall: 0.8058, test_f1: 0.8058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**TCLSTM**)"
      ],
      "metadata": {
        "id": "1fM7mlSsV81i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset SemEval2016 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XGoxbjXV86z",
        "outputId": "8e46752a-01d3-42e3-b841-6a8e3351c28c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 989.\n",
            "> testing dataset count: 103.\n",
            "cuda memory allocated: 13115392\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 1111800\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f4fe3339b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:57\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_val_f1_0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:58\n",
            "loss: 0.7680, acc: 0.6990\n",
            "E2E-ABSA >>> 2022-08-17 15:13:58\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:58\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:58\n",
            "loss: 0.7253, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-08-17 15:13:59\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:13:59\n",
            "loss: 0.6880, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 15:13:59\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_val_f1_0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:00\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:00\n",
            "loss: 0.6657, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 15:14:00\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_val_f1_0.767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:01\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:01\n",
            "loss: 0.6656, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:14:02\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:02\n",
            "loss: 0.6506, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-08-17 15:14:02\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:03\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:03\n",
            "loss: 0.6020, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-08-17 15:14:03\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:04\n",
            "loss: 0.6424, acc: 0.7310\n",
            "E2E-ABSA >>> 2022-08-17 15:14:04\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_val_f1_0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:04\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:05\n",
            "loss: 0.6018, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-08-17 15:14:05\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:05\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:05\n",
            "loss: 0.5858, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 15:14:06\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:06\n",
            "loss: 0.6160, acc: 0.7446\n",
            "E2E-ABSA >>> 2022-08-17 15:14:06\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:07\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:07\n",
            "loss: 0.5947, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 15:14:07\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:08\n",
            "loss: 0.6018, acc: 0.7479\n",
            "E2E-ABSA >>> 2022-08-17 15:14:08\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:09\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:09\n",
            "loss: 0.6127, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-08-17 15:14:09\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:10\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:10\n",
            "loss: 0.6056, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 15:14:10\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:11\n",
            "loss: 0.5776, acc: 0.7588\n",
            "E2E-ABSA >>> 2022-08-17 15:14:11\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:11\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:11\n",
            "loss: 0.5667, acc: 0.7644\n",
            "E2E-ABSA >>> 2022-08-17 15:14:12\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:12\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:12\n",
            "loss: 0.4702, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:14:13\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:13\n",
            "loss: 0.5517, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:14:13\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:14\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:14\n",
            "loss: 0.5149, acc: 0.8008\n",
            "E2E-ABSA >>> 2022-08-17 15:14:14\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:15\n",
            "loss: 0.5124, acc: 0.7859\n",
            "E2E-ABSA >>> 2022-08-17 15:14:15\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:16\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_val_f1_0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:16\n",
            "loss: 0.5097, acc: 0.7958\n",
            "E2E-ABSA >>> 2022-08-17 15:14:16\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_val_f1_0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:17\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:17\n",
            "loss: 0.5033, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:14:17\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:18\n",
            "loss: 0.4484, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:14:18\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:18\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:18\n",
            "loss: 0.4223, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:14:19\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:19\n",
            "loss: 0.4408, acc: 0.8254\n",
            "E2E-ABSA >>> 2022-08-17 15:14:19\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:20\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:20\n",
            "loss: 0.4081, acc: 0.8272\n",
            "E2E-ABSA >>> 2022-08-17 15:14:20\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:21\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:21\n",
            "loss: 0.3454, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:14:21\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:22\n",
            "loss: 0.4037, acc: 0.8490\n",
            "E2E-ABSA >>> 2022-08-17 15:14:22\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:22\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:23\n",
            "loss: 0.4001, acc: 0.8516\n",
            "E2E-ABSA >>> 2022-08-17 15:14:23\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:24\n",
            "loss: 0.3832, acc: 0.8615\n",
            "E2E-ABSA >>> 2022-08-17 15:14:24\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:24\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:24\n",
            "loss: 0.3611, acc: 0.8734\n",
            "E2E-ABSA >>> 2022-08-17 15:14:25\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:25\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:25\n",
            "loss: 0.4051, acc: 0.8661\n",
            "E2E-ABSA >>> 2022-08-17 15:14:26\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:26\n",
            "loss: 0.3385, acc: 0.8786\n",
            "E2E-ABSA >>> 2022-08-17 15:14:26\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:27\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:27\n",
            "loss: 0.3267, acc: 0.8772\n",
            "E2E-ABSA >>> 2022-08-17 15:14:27\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:28\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:28\n",
            "loss: 0.4284, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 15:14:28\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:29\n",
            "loss: 0.3290, acc: 0.8899\n",
            "E2E-ABSA >>> 2022-08-17 15:14:29\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:29\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:30\n",
            "loss: 0.3008, acc: 0.9028\n",
            "E2E-ABSA >>> 2022-08-17 15:14:30\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:30\n",
            "loss: 0.3192, acc: 0.8873\n",
            "E2E-ABSA >>> 2022-08-17 15:14:30\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:31\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:31\n",
            "loss: 0.3105, acc: 0.8887\n",
            "E2E-ABSA >>> 2022-08-17 15:14:32\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:14:32\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            "E2E-ABSA >>> 2022-08-17 15:14:32\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            "you can download the best model from state_dict/tc_lstm_SemEval2016_val_f1_0.8252\n",
            ">>> test_acc: 0.8252, test_precision: 0.8252, test_recall: 0.8252, test_f1: 0.8252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2016** dataset on model(**TCLSTM**)"
      ],
      "metadata": {
        "id": "6w1G6R_tV8_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset SemEval2016_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sr-PScc_V9FS",
        "outputId": "97866429-2ccd-4a7a-d52c-79ae9df4c60b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1006.\n",
            "> testing dataset count: 103.\n",
            "cuda memory allocated: 16659456\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 1987200\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f889d89fb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:11\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:11\n",
            "loss: 0.7770, acc: 0.6976\n",
            "E2E-ABSA >>> 2022-08-17 15:15:12\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:12\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:13\n",
            "loss: 0.6922, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 15:15:13\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:14\n",
            "loss: 0.7021, acc: 0.7122\n",
            "E2E-ABSA >>> 2022-08-17 15:15:14\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:15\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:15\n",
            "loss: 0.6811, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-08-17 15:15:15\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:16\n",
            "loss: 0.6602, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 15:15:16\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:17\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:17\n",
            "loss: 0.6326, acc: 0.7197\n",
            "E2E-ABSA >>> 2022-08-17 15:15:17\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:18\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:18\n",
            "loss: 0.6229, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:15:19\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:19\n",
            "loss: 0.6247, acc: 0.7386\n",
            "E2E-ABSA >>> 2022-08-17 15:15:20\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:20\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:21\n",
            "loss: 0.6771, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-08-17 15:15:21\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:22\n",
            "loss: 0.6212, acc: 0.7318\n",
            "E2E-ABSA >>> 2022-08-17 15:15:22\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:23\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:23\n",
            "loss: 0.6271, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-08-17 15:15:23\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:24\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:24\n",
            "loss: 0.5756, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:15:25\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:25\n",
            "loss: 0.5802, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:15:25\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:26\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:26\n",
            "loss: 0.6118, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 15:15:27\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:27\n",
            "loss: 0.5451, acc: 0.7757\n",
            "E2E-ABSA >>> 2022-08-17 15:15:28\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:28\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:29\n",
            "loss: 0.5950, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:15:29\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:30\n",
            "loss: 0.5496, acc: 0.7692\n",
            "E2E-ABSA >>> 2022-08-17 15:15:30\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:30\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:31\n",
            "loss: 0.5389, acc: 0.7743\n",
            "E2E-ABSA >>> 2022-08-17 15:15:31\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:32\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:32\n",
            "loss: 0.5648, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 15:15:33\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:33\n",
            "loss: 0.5013, acc: 0.8072\n",
            "E2E-ABSA >>> 2022-08-17 15:15:33\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:34\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:34\n",
            "loss: 0.4833, acc: 0.8274\n",
            "E2E-ABSA >>> 2022-08-17 15:15:35\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:35\n",
            "loss: 0.4858, acc: 0.8082\n",
            "E2E-ABSA >>> 2022-08-17 15:15:36\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:37\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:37\n",
            "loss: 0.4729, acc: 0.8105\n",
            "E2E-ABSA >>> 2022-08-17 15:15:37\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:38\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:38\n",
            "loss: 0.5048, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:15:39\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">> saved: state_dict/tc_lstm_SemEval2016_know_val_f1_0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:39\n",
            "loss: 0.4353, acc: 0.8328\n",
            "E2E-ABSA >>> 2022-08-17 15:15:40\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:40\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:40\n",
            "loss: 0.4569, acc: 0.8051\n",
            "E2E-ABSA >>> 2022-08-17 15:15:41\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:42\n",
            "loss: 0.4409, acc: 0.8299\n",
            "E2E-ABSA >>> 2022-08-17 15:15:42\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:42\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:43\n",
            "loss: 0.3930, acc: 0.8504\n",
            "E2E-ABSA >>> 2022-08-17 15:15:43\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:44\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:44\n",
            "loss: 0.4930, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:15:45\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:45\n",
            "loss: 0.4072, acc: 0.8349\n",
            "E2E-ABSA >>> 2022-08-17 15:15:45\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:46\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:46\n",
            "loss: 0.3393, acc: 0.8798\n",
            "E2E-ABSA >>> 2022-08-17 15:15:47\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:47\n",
            "loss: 0.3636, acc: 0.8538\n",
            "E2E-ABSA >>> 2022-08-17 15:15:48\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:48\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:49\n",
            "loss: 0.3737, acc: 0.8802\n",
            "E2E-ABSA >>> 2022-08-17 15:15:49\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:50\n",
            "loss: 0.3703, acc: 0.8535\n",
            "E2E-ABSA >>> 2022-08-17 15:15:50\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:50\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:51\n",
            "loss: 0.3643, acc: 0.8589\n",
            "E2E-ABSA >>> 2022-08-17 15:15:51\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:52\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:52\n",
            "loss: 0.3459, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:15:53\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:53\n",
            "loss: 0.3796, acc: 0.8465\n",
            "E2E-ABSA >>> 2022-08-17 15:15:53\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:54\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:54\n",
            "loss: 0.3346, acc: 0.8875\n",
            "E2E-ABSA >>> 2022-08-17 15:15:55\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:56\n",
            "loss: 0.3422, acc: 0.8662\n",
            "E2E-ABSA >>> 2022-08-17 15:15:56\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:56\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:57\n",
            "loss: 0.3151, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:15:57\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:58\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:58\n",
            "loss: 0.2868, acc: 0.9125\n",
            "E2E-ABSA >>> 2022-08-17 15:15:59\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:15:59\n",
            "loss: 0.3267, acc: 0.8795\n",
            "E2E-ABSA >>> 2022-08-17 15:15:59\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:00\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:00\n",
            "loss: 0.3011, acc: 0.8672\n",
            "E2E-ABSA >>> 2022-08-17 15:16:01\n",
            ">>> val_acc: 0.7961, val_precision: 0.7961 val_recall: 0.7961, val_f1: 0.7961\n",
            "E2E-ABSA >>> 2022-08-17 15:16:01\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            "you can download the best model from state_dict/tc_lstm_SemEval2016_know_val_f1_0.8252\n",
            ">>> test_acc: 0.8252, test_precision: 0.8252, test_recall: 0.8252, test_f1: 0.8252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2016** dataset on model(**ATAELSTM**)"
      ],
      "metadata": {
        "id": "Pkc5TJTbV9KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset SemEval2016 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSuU7mAGV9Pt",
        "outputId": "936f1be5-fa4e-429c-d6c5-b5d8a8802542"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 14553088\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 1111800\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fa32e11eb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:39\n",
            ">>> val_acc: 0.6610, val_precision: 0.6610 val_recall: 0.6610, val_f1: 0.6610\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.661\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:40\n",
            "loss: 0.8091, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:16:40\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:41\n",
            "loss: 0.7499, acc: 0.7010\n",
            "E2E-ABSA >>> 2022-08-17 15:16:41\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:42\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:42\n",
            "loss: 0.6791, acc: 0.7469\n",
            "E2E-ABSA >>> 2022-08-17 15:16:42\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:43\n",
            "loss: 0.7006, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 15:16:43\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:43\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:44\n",
            "loss: 0.6202, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:16:44\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:45\n",
            "loss: 0.6287, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:16:45\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:45\n",
            "loss: 0.6417, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-08-17 15:16:45\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:46\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:46\n",
            "loss: 0.6387, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:16:47\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:47\n",
            "loss: 0.5997, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:16:47\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:48\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:48\n",
            "loss: 0.5917, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 15:16:49\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:49\n",
            "loss: 0.5754, acc: 0.7612\n",
            "E2E-ABSA >>> 2022-08-17 15:16:49\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:50\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:50\n",
            "loss: 0.5498, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:16:51\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:51\n",
            "loss: 0.5277, acc: 0.7781\n",
            "E2E-ABSA >>> 2022-08-17 15:16:51\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:52\n",
            "loss: 0.5366, acc: 0.7743\n",
            "E2E-ABSA >>> 2022-08-17 15:16:52\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:53\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:53\n",
            "loss: 0.4791, acc: 0.8083\n",
            "E2E-ABSA >>> 2022-08-17 15:16:53\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:54\n",
            "loss: 0.5067, acc: 0.7865\n",
            "E2E-ABSA >>> 2022-08-17 15:16:54\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:55\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:55\n",
            "loss: 0.4974, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:16:55\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:56\n",
            "loss: 0.4978, acc: 0.7837\n",
            "E2E-ABSA >>> 2022-08-17 15:16:56\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:57\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:57\n",
            "loss: 0.5039, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-08-17 15:16:57\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:58\n",
            "loss: 0.4602, acc: 0.8063\n",
            "E2E-ABSA >>> 2022-08-17 15:16:58\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:59\n",
            "loss: 0.4556, acc: 0.8049\n",
            "E2E-ABSA >>> 2022-08-17 15:16:59\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:16:59\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:00\n",
            "loss: 0.4091, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-08-17 15:17:00\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:01\n",
            "loss: 0.4340, acc: 0.8198\n",
            "E2E-ABSA >>> 2022-08-17 15:17:01\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:01\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:02\n",
            "loss: 0.4163, acc: 0.8469\n",
            "E2E-ABSA >>> 2022-08-17 15:17:02\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:02\n",
            "loss: 0.4231, acc: 0.8213\n",
            "E2E-ABSA >>> 2022-08-17 15:17:03\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:03\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:03\n",
            "loss: 0.3723, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:17:04\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:04\n",
            "loss: 0.3997, acc: 0.8297\n",
            "E2E-ABSA >>> 2022-08-17 15:17:05\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.822\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:05\n",
            "loss: 0.4015, acc: 0.8399\n",
            "E2E-ABSA >>> 2022-08-17 15:17:05\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:06\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:06\n",
            "loss: 0.4062, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:17:07\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:07\n",
            "loss: 0.3932, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 15:17:07\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:08\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:08\n",
            "loss: 0.4034, acc: 0.8656\n",
            "E2E-ABSA >>> 2022-08-17 15:17:09\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:09\n",
            "loss: 0.4103, acc: 0.8462\n",
            "E2E-ABSA >>> 2022-08-17 15:17:09\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:10\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:10\n",
            "loss: 0.4380, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:17:11\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_val_f1_0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:11\n",
            "loss: 0.3682, acc: 0.8703\n",
            "E2E-ABSA >>> 2022-08-17 15:17:11\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:12\n",
            "loss: 0.3581, acc: 0.8669\n",
            "E2E-ABSA >>> 2022-08-17 15:17:12\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:12\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:13\n",
            "loss: 0.3491, acc: 0.8771\n",
            "E2E-ABSA >>> 2022-08-17 15:17:13\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:14\n",
            "loss: 0.3471, acc: 0.8729\n",
            "E2E-ABSA >>> 2022-08-17 15:17:14\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:14\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:15\n",
            "loss: 0.3365, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 15:17:15\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:16\n",
            "loss: 0.3316, acc: 0.8838\n",
            "E2E-ABSA >>> 2022-08-17 15:17:16\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:16\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:17\n",
            "loss: 0.3036, acc: 0.8875\n",
            "E2E-ABSA >>> 2022-08-17 15:17:17\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:17\n",
            "loss: 0.3181, acc: 0.8828\n",
            "E2E-ABSA >>> 2022-08-17 15:17:18\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:18\n",
            "loss: 0.3300, acc: 0.8768\n",
            "E2E-ABSA >>> 2022-08-17 15:17:18\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:19\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:19\n",
            "loss: 0.3127, acc: 0.8812\n",
            "E2E-ABSA >>> 2022-08-17 15:17:20\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:20\n",
            "loss: 0.3184, acc: 0.8833\n",
            "E2E-ABSA >>> 2022-08-17 15:17:20\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:21\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:21\n",
            "loss: 0.2719, acc: 0.9031\n",
            "E2E-ABSA >>> 2022-08-17 15:17:22\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:22\n",
            "loss: 0.3102, acc: 0.8938\n",
            "E2E-ABSA >>> 2022-08-17 15:17:22\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:23\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:23\n",
            "loss: 0.2934, acc: 0.8875\n",
            "E2E-ABSA >>> 2022-08-17 15:17:24\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:24\n",
            "loss: 0.3049, acc: 0.8797\n",
            "E2E-ABSA >>> 2022-08-17 15:17:24\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:25\n",
            "loss: 0.2931, acc: 0.8930\n",
            "E2E-ABSA >>> 2022-08-17 15:17:25\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:25\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:26\n",
            "loss: 0.2679, acc: 0.9021\n",
            "E2E-ABSA >>> 2022-08-17 15:17:26\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:27\n",
            "loss: 0.2715, acc: 0.9052\n",
            "E2E-ABSA >>> 2022-08-17 15:17:27\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:27\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:28\n",
            "loss: 0.2985, acc: 0.8906\n",
            "E2E-ABSA >>> 2022-08-17 15:17:28\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:29\n",
            "loss: 0.2992, acc: 0.8825\n",
            "E2E-ABSA >>> 2022-08-17 15:17:29\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:29\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 15:17:29\n",
            "loss: 0.3198, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:17:30\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            "E2E-ABSA >>> 2022-08-17 15:17:30\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            "you can download the best model from state_dict/atae_lstm_SemEval2016_val_f1_0.8305\n",
            ">>> test_acc: 0.8305, test_precision: 0.8305, test_recall: 0.8305, test_f1: 0.8305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2016** dataset on model(**ATAELSTM**)"
      ],
      "metadata": {
        "id": "eaVEvAO5V9Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset SemEval2016_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxg73V-AV9Zo",
        "outputId": "771e4879-8e23-4171-f813-6f265466a16f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 18054656\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 1987200\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fa64bb19b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:10\n",
            ">>> val_acc: 0.6949, val_precision: 0.6949 val_recall: 0.6949, val_f1: 0.6949\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.6949\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:10\n",
            "loss: 0.7719, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 15:18:11\n",
            ">>> val_acc: 0.7203, val_precision: 0.7203 val_recall: 0.7203, val_f1: 0.7203\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7203\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:11\n",
            "loss: 0.7512, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-08-17 15:18:12\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:12\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:13\n",
            "loss: 0.6789, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:18:13\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:14\n",
            "loss: 0.7130, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 15:18:14\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:15\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:15\n",
            "loss: 0.6335, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:18:16\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:16\n",
            "loss: 0.6548, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:18:17\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:17\n",
            "loss: 0.6733, acc: 0.7275\n",
            "E2E-ABSA >>> 2022-08-17 15:18:17\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:18\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:19\n",
            "loss: 0.6757, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 15:18:19\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:20\n",
            "loss: 0.6491, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 15:18:20\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:21\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:21\n",
            "loss: 0.6506, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-08-17 15:18:22\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:22\n",
            "loss: 0.6423, acc: 0.7362\n",
            "E2E-ABSA >>> 2022-08-17 15:18:22\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:23\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:23\n",
            "loss: 0.6329, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 15:18:24\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:24\n",
            "loss: 0.6014, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 15:18:25\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:26\n",
            "loss: 0.6231, acc: 0.7419\n",
            "E2E-ABSA >>> 2022-08-17 15:18:26\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:27\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:27\n",
            "loss: 0.5665, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:18:27\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:28\n",
            "loss: 0.5953, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-08-17 15:18:28\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:29\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:29\n",
            "loss: 0.5930, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:18:30\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:30\n",
            "loss: 0.5865, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-08-17 15:18:31\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:32\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:32\n",
            "loss: 0.5530, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:18:32\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:33\n",
            "loss: 0.5218, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:18:33\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:34\n",
            "loss: 0.5068, acc: 0.7977\n",
            "E2E-ABSA >>> 2022-08-17 15:18:34\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:35\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:35\n",
            "loss: 0.4597, acc: 0.8167\n",
            "E2E-ABSA >>> 2022-08-17 15:18:36\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:37\n",
            "loss: 0.4669, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:18:37\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:37\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:38\n",
            "loss: 0.4433, acc: 0.8156\n",
            "E2E-ABSA >>> 2022-08-17 15:18:38\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:39\n",
            "loss: 0.4481, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:18:39\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:40\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:40\n",
            "loss: 0.3803, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:18:41\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:41\n",
            "loss: 0.4363, acc: 0.8234\n",
            "E2E-ABSA >>> 2022-08-17 15:18:42\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:42\n",
            "loss: 0.4241, acc: 0.8273\n",
            "E2E-ABSA >>> 2022-08-17 15:18:43\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:43\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:44\n",
            "loss: 0.4336, acc: 0.8187\n",
            "E2E-ABSA >>> 2022-08-17 15:18:44\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:45\n",
            "loss: 0.4444, acc: 0.8167\n",
            "E2E-ABSA >>> 2022-08-17 15:18:45\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:46\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:46\n",
            "loss: 0.4466, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 15:18:47\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:47\n",
            "loss: 0.4216, acc: 0.8400\n",
            "E2E-ABSA >>> 2022-08-17 15:18:48\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">> saved: state_dict/atae_lstm_SemEval2016_know_val_f1_0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:48\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:48\n",
            "loss: 0.4462, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:18:49\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:50\n",
            "loss: 0.4114, acc: 0.8422\n",
            "E2E-ABSA >>> 2022-08-17 15:18:50\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:51\n",
            "loss: 0.3776, acc: 0.8561\n",
            "E2E-ABSA >>> 2022-08-17 15:18:51\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:52\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:52\n",
            "loss: 0.3697, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:18:52\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:53\n",
            "loss: 0.3590, acc: 0.8521\n",
            "E2E-ABSA >>> 2022-08-17 15:18:53\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:54\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:54\n",
            "loss: 0.3360, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 15:18:55\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:55\n",
            "loss: 0.3763, acc: 0.8588\n",
            "E2E-ABSA >>> 2022-08-17 15:18:56\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:57\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:57\n",
            "loss: 0.3128, acc: 0.8688\n",
            "E2E-ABSA >>> 2022-08-17 15:18:57\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:58\n",
            "loss: 0.3334, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:18:58\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:18:59\n",
            "loss: 0.3519, acc: 0.8642\n",
            "E2E-ABSA >>> 2022-08-17 15:18:59\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:00\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:00\n",
            "loss: 0.3375, acc: 0.8667\n",
            "E2E-ABSA >>> 2022-08-17 15:19:01\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:01\n",
            "loss: 0.3586, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:19:02\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:02\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:03\n",
            "loss: 0.3074, acc: 0.8812\n",
            "E2E-ABSA >>> 2022-08-17 15:19:03\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:04\n",
            "loss: 0.3412, acc: 0.8762\n",
            "E2E-ABSA >>> 2022-08-17 15:19:04\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:05\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:05\n",
            "loss: 0.3082, acc: 0.8875\n",
            "E2E-ABSA >>> 2022-08-17 15:19:06\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:06\n",
            "loss: 0.3380, acc: 0.8578\n",
            "E2E-ABSA >>> 2022-08-17 15:19:06\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:07\n",
            "loss: 0.3260, acc: 0.8831\n",
            "E2E-ABSA >>> 2022-08-17 15:19:07\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:08\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:08\n",
            "loss: 0.2863, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-08-17 15:19:09\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:10\n",
            "loss: 0.3055, acc: 0.8896\n",
            "E2E-ABSA >>> 2022-08-17 15:19:10\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:11\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:11\n",
            "loss: 0.3510, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:19:11\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:19:12\n",
            "loss: 0.4809, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:19:12\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            "E2E-ABSA >>> 2022-08-17 15:19:12\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            "you can download the best model from state_dict/atae_lstm_SemEval2016_know_val_f1_0.8475\n",
            ">>> test_acc: 0.8475, test_precision: 0.8475, test_recall: 0.8475, test_f1: 0.8475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2016** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "pGcF6MbQV9e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset SemEval2016 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TXVLouvV9kE",
        "outputId": "e1458cf0-947c-477c-8a54-27af27d8116f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 13127680\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 1111800\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f757b679b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:51\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:51\n",
            "loss: 0.7874, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:19:52\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:53\n",
            "loss: 0.7342, acc: 0.7167\n",
            "E2E-ABSA >>> 2022-08-17 15:19:53\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:54\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:54\n",
            "loss: 0.6850, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:19:55\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:55\n",
            "loss: 0.6955, acc: 0.7175\n",
            "E2E-ABSA >>> 2022-08-17 15:19:55\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:56\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:56\n",
            "loss: 0.6626, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:19:57\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:58\n",
            "loss: 0.6459, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 15:19:58\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:19:59\n",
            "loss: 0.6750, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-08-17 15:19:59\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:00\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:00\n",
            "loss: 0.6700, acc: 0.7042\n",
            "E2E-ABSA >>> 2022-08-17 15:20:01\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:02\n",
            "loss: 0.6455, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 15:20:02\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:03\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:03\n",
            "loss: 0.6026, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-08-17 15:20:03\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:04\n",
            "loss: 0.5967, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-08-17 15:20:04\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:05\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:05\n",
            "loss: 0.5503, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 15:20:06\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:07\n",
            "loss: 0.5590, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 15:20:07\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:08\n",
            "loss: 0.5450, acc: 0.7752\n",
            "E2E-ABSA >>> 2022-08-17 15:20:08\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:09\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:09\n",
            "loss: 0.5440, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 15:20:10\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:11\n",
            "loss: 0.5281, acc: 0.7833\n",
            "E2E-ABSA >>> 2022-08-17 15:20:11\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:12\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:12\n",
            "loss: 0.4975, acc: 0.8031\n",
            "E2E-ABSA >>> 2022-08-17 15:20:12\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:13\n",
            "loss: 0.5136, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 15:20:13\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:14\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:14\n",
            "loss: 0.4442, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:20:15\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:16\n",
            "loss: 0.4950, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:20:16\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:17\n",
            "loss: 0.4855, acc: 0.8040\n",
            "E2E-ABSA >>> 2022-08-17 15:20:17\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:18\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:18\n",
            "loss: 0.4967, acc: 0.8063\n",
            "E2E-ABSA >>> 2022-08-17 15:20:19\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:19\n",
            "loss: 0.4847, acc: 0.8052\n",
            "E2E-ABSA >>> 2022-08-17 15:20:20\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:21\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:21\n",
            "loss: 0.4598, acc: 0.8187\n",
            "E2E-ABSA >>> 2022-08-17 15:20:21\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:22\n",
            "loss: 0.4770, acc: 0.8150\n",
            "E2E-ABSA >>> 2022-08-17 15:20:22\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:23\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:23\n",
            "loss: 0.4574, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:20:24\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:25\n",
            "loss: 0.4645, acc: 0.8219\n",
            "E2E-ABSA >>> 2022-08-17 15:20:25\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.822\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:26\n",
            "loss: 0.4569, acc: 0.8228\n",
            "E2E-ABSA >>> 2022-08-17 15:20:26\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:27\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:27\n",
            "loss: 0.4257, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-08-17 15:20:28\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:29\n",
            "loss: 0.4650, acc: 0.8219\n",
            "E2E-ABSA >>> 2022-08-17 15:20:29\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:30\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:30\n",
            "loss: 0.4002, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:20:31\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:31\n",
            "loss: 0.4586, acc: 0.8237\n",
            "E2E-ABSA >>> 2022-08-17 15:20:31\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:32\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:32\n",
            "loss: 0.3795, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:20:33\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:34\n",
            "loss: 0.4538, acc: 0.8234\n",
            "E2E-ABSA >>> 2022-08-17 15:20:34\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:35\n",
            "loss: 0.4424, acc: 0.8354\n",
            "E2E-ABSA >>> 2022-08-17 15:20:35\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:36\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:36\n",
            "loss: 0.4291, acc: 0.8396\n",
            "E2E-ABSA >>> 2022-08-17 15:20:37\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:38\n",
            "loss: 0.4331, acc: 0.8427\n",
            "E2E-ABSA >>> 2022-08-17 15:20:38\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:39\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:39\n",
            "loss: 0.4468, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 15:20:40\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:41\n",
            "loss: 0.4160, acc: 0.8400\n",
            "E2E-ABSA >>> 2022-08-17 15:20:41\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:42\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:42\n",
            "loss: 0.4587, acc: 0.8250\n",
            "E2E-ABSA >>> 2022-08-17 15:20:43\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">> saved: state_dict/ian_SemEval2016_val_f1_0.839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:43\n",
            "loss: 0.3955, acc: 0.8578\n",
            "E2E-ABSA >>> 2022-08-17 15:20:44\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:44\n",
            "loss: 0.4204, acc: 0.8390\n",
            "E2E-ABSA >>> 2022-08-17 15:20:44\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:45\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:46\n",
            "loss: 0.4002, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-08-17 15:20:46\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:47\n",
            "loss: 0.4084, acc: 0.8427\n",
            "E2E-ABSA >>> 2022-08-17 15:20:47\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:48\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:48\n",
            "loss: 0.4441, acc: 0.8219\n",
            "E2E-ABSA >>> 2022-08-17 15:20:49\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:50\n",
            "loss: 0.3881, acc: 0.8525\n",
            "E2E-ABSA >>> 2022-08-17 15:20:50\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:51\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:51\n",
            "loss: 0.5405, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:20:52\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:52\n",
            "loss: 0.4071, acc: 0.8484\n",
            "E2E-ABSA >>> 2022-08-17 15:20:53\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:53\n",
            "loss: 0.4012, acc: 0.8480\n",
            "E2E-ABSA >>> 2022-08-17 15:20:53\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:54\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:55\n",
            "loss: 0.4340, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:20:55\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:56\n",
            "loss: 0.4111, acc: 0.8448\n",
            "E2E-ABSA >>> 2022-08-17 15:20:56\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:57\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:57\n",
            "loss: 0.3551, acc: 0.8656\n",
            "E2E-ABSA >>> 2022-08-17 15:20:58\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:20:59\n",
            "loss: 0.3912, acc: 0.8512\n",
            "E2E-ABSA >>> 2022-08-17 15:20:59\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:00\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:00\n",
            "loss: 0.4676, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 15:21:01\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:01\n",
            "loss: 0.4000, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:21:02\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:03\n",
            "loss: 0.3871, acc: 0.8615\n",
            "E2E-ABSA >>> 2022-08-17 15:21:03\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:04\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:04\n",
            "loss: 0.3637, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:21:04\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:05\n",
            "loss: 0.3819, acc: 0.8573\n",
            "E2E-ABSA >>> 2022-08-17 15:21:05\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:06\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:06\n",
            "loss: 0.3552, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 15:21:07\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:08\n",
            "loss: 0.3601, acc: 0.8662\n",
            "E2E-ABSA >>> 2022-08-17 15:21:08\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:09\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:09\n",
            "loss: 0.3466, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:21:10\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            "E2E-ABSA >>> 2022-08-17 15:21:10\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            "you can download the best model from state_dict/ian_SemEval2016_val_f1_0.839\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.8390, test_precision: 0.8390, test_recall: 0.8390, test_f1: 0.8390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2016** dataset on model(**IAN**)"
      ],
      "metadata": {
        "id": "WH09g2AEV9oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset SemEval2016_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWl0zrVXV9t1",
        "outputId": "adfefa9d-f1ae-44d7-a596-06ec027959d2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 16629248\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 1987200\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f60c64a8b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:49\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/ian_SemEval2016_know_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:49\n",
            "loss: 0.7728, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:21:50\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:51\n",
            "loss: 0.7314, acc: 0.7167\n",
            "E2E-ABSA >>> 2022-08-17 15:21:51\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:52\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:52\n",
            "loss: 0.6994, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:21:53\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:54\n",
            "loss: 0.7070, acc: 0.7175\n",
            "E2E-ABSA >>> 2022-08-17 15:21:54\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:55\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:55\n",
            "loss: 0.6797, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:21:56\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:57\n",
            "loss: 0.6673, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 15:21:57\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:58\n",
            "loss: 0.7084, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-08-17 15:21:58\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:21:59\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:00\n",
            "loss: 0.7199, acc: 0.7042\n",
            "E2E-ABSA >>> 2022-08-17 15:22:00\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:01\n",
            "loss: 0.7079, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-08-17 15:22:01\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:02\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:02\n",
            "loss: 0.6822, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-08-17 15:22:03\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:04\n",
            "loss: 0.7012, acc: 0.7113\n",
            "E2E-ABSA >>> 2022-08-17 15:22:04\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:05\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:05\n",
            "loss: 0.6481, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 15:22:06\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:07\n",
            "loss: 0.6895, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 15:22:07\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:08\n",
            "loss: 0.6923, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-08-17 15:22:08\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:09\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:10\n",
            "loss: 0.7104, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:22:10\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:11\n",
            "loss: 0.7015, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:22:11\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:12\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:13\n",
            "loss: 0.6588, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:22:13\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:14\n",
            "loss: 0.6836, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 15:22:14\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:15\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:16\n",
            "loss: 0.6302, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:22:16\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:17\n",
            "loss: 0.6854, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 15:22:18\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:19\n",
            "loss: 0.6681, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 15:22:19\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 15:22:20\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            "E2E-ABSA >>> 2022-08-17 15:22:20\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            "you can download the best model from state_dict/ian_SemEval2016_know_val_f1_0.7288\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.7288, test_precision: 0.7288, test_recall: 0.7288, test_f1: 0.7288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2016** dataset on model(**MEMNET**)"
      ],
      "metadata": {
        "id": "QOjGznqVV9yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset SemEval2016 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9rLPAPjV93T",
        "outputId": "c857f7d5-eb2f-4116-8673-57cce22f892d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 5902336\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 1111800\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1fa9477b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:22:58\n",
            ">>> val_acc: 0.7203, val_precision: 0.7203 val_recall: 0.7203, val_f1: 0.7203\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7203\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:22:58\n",
            "loss: 0.7923, acc: 0.6854\n",
            "E2E-ABSA >>> 2022-08-17 15:22:58\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:22:59\n",
            "loss: 0.7139, acc: 0.7229\n",
            "E2E-ABSA >>> 2022-08-17 15:22:59\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:00\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:00\n",
            "loss: 0.6955, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-08-17 15:23:00\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:01\n",
            "loss: 0.6573, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-08-17 15:23:01\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:01\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:01\n",
            "loss: 0.6603, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:23:02\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:02\n",
            "loss: 0.5861, acc: 0.7547\n",
            "E2E-ABSA >>> 2022-08-17 15:23:02\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:03\n",
            "loss: 0.5977, acc: 0.7527\n",
            "E2E-ABSA >>> 2022-08-17 15:23:03\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:03\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:04\n",
            "loss: 0.5574, acc: 0.7729\n",
            "E2E-ABSA >>> 2022-08-17 15:23:04\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:04\n",
            "loss: 0.5687, acc: 0.7698\n",
            "E2E-ABSA >>> 2022-08-17 15:23:04\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:05\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:05\n",
            "loss: 0.5198, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:23:06\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:06\n",
            "loss: 0.5222, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 15:23:06\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:07\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:07\n",
            "loss: 0.5229, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 15:23:07\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:07\n",
            "loss: 0.4923, acc: 0.7859\n",
            "E2E-ABSA >>> 2022-08-17 15:23:08\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:08\n",
            "loss: 0.5020, acc: 0.7977\n",
            "E2E-ABSA >>> 2022-08-17 15:23:08\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:09\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:09\n",
            "loss: 0.4951, acc: 0.7854\n",
            "E2E-ABSA >>> 2022-08-17 15:23:10\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:10\n",
            "loss: 0.4882, acc: 0.7927\n",
            "E2E-ABSA >>> 2022-08-17 15:23:10\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:11\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:11\n",
            "loss: 0.4575, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:23:11\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:12\n",
            "loss: 0.4768, acc: 0.8025\n",
            "E2E-ABSA >>> 2022-08-17 15:23:12\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:12\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:12\n",
            "loss: 0.3798, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:23:13\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:13\n",
            "loss: 0.4466, acc: 0.8203\n",
            "E2E-ABSA >>> 2022-08-17 15:23:13\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:14\n",
            "loss: 0.4384, acc: 0.8210\n",
            "E2E-ABSA >>> 2022-08-17 15:23:14\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:14\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:15\n",
            "loss: 0.4343, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-08-17 15:23:15\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:15\n",
            "loss: 0.4385, acc: 0.8354\n",
            "E2E-ABSA >>> 2022-08-17 15:23:16\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:16\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:16\n",
            "loss: 0.4008, acc: 0.8344\n",
            "E2E-ABSA >>> 2022-08-17 15:23:17\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:17\n",
            "loss: 0.3946, acc: 0.8413\n",
            "E2E-ABSA >>> 2022-08-17 15:23:17\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:18\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:18\n",
            "loss: 0.4118, acc: 0.8375\n",
            "E2E-ABSA >>> 2022-08-17 15:23:18\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:18\n",
            "loss: 0.3997, acc: 0.8359\n",
            "E2E-ABSA >>> 2022-08-17 15:23:19\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:19\n",
            "loss: 0.3959, acc: 0.8471\n",
            "E2E-ABSA >>> 2022-08-17 15:23:19\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:20\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:20\n",
            "loss: 0.4112, acc: 0.8479\n",
            "E2E-ABSA >>> 2022-08-17 15:23:20\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:21\n",
            "loss: 0.3887, acc: 0.8448\n",
            "E2E-ABSA >>> 2022-08-17 15:23:21\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:21\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:22\n",
            "loss: 0.3502, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:23:22\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:22\n",
            "loss: 0.3765, acc: 0.8625\n",
            "E2E-ABSA >>> 2022-08-17 15:23:23\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:23\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:23\n",
            "loss: 0.3578, acc: 0.8812\n",
            "E2E-ABSA >>> 2022-08-17 15:23:24\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:24\n",
            "loss: 0.3772, acc: 0.8531\n",
            "E2E-ABSA >>> 2022-08-17 15:23:24\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:25\n",
            "loss: 0.3707, acc: 0.8615\n",
            "E2E-ABSA >>> 2022-08-17 15:23:25\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:25\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:25\n",
            "loss: 0.3943, acc: 0.8500\n",
            "E2E-ABSA >>> 2022-08-17 15:23:26\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:26\n",
            "loss: 0.3621, acc: 0.8656\n",
            "E2E-ABSA >>> 2022-08-17 15:23:26\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:27\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:27\n",
            "loss: 0.3703, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:23:27\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:28\n",
            "loss: 0.3534, acc: 0.8612\n",
            "E2E-ABSA >>> 2022-08-17 15:23:28\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:28\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:29\n",
            "loss: 0.3443, acc: 0.8562\n",
            "E2E-ABSA >>> 2022-08-17 15:23:29\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:29\n",
            "loss: 0.3205, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 15:23:30\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:30\n",
            "loss: 0.3494, acc: 0.8723\n",
            "E2E-ABSA >>> 2022-08-17 15:23:30\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">> saved: state_dict/memnet_SemEval2016_val_f1_0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:31\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:31\n",
            "loss: 0.3427, acc: 0.8792\n",
            "E2E-ABSA >>> 2022-08-17 15:23:31\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:32\n",
            "loss: 0.3405, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 15:23:32\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:32\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:32\n",
            "loss: 0.3254, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:23:33\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:33\n",
            "loss: 0.3314, acc: 0.8862\n",
            "E2E-ABSA >>> 2022-08-17 15:23:33\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:34\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:34\n",
            "loss: 0.3710, acc: 0.8688\n",
            "E2E-ABSA >>> 2022-08-17 15:23:34\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:35\n",
            "loss: 0.3468, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:23:35\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:35\n",
            "loss: 0.3238, acc: 0.8813\n",
            "E2E-ABSA >>> 2022-08-17 15:23:35\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:36\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:36\n",
            "loss: 0.3099, acc: 0.8833\n",
            "E2E-ABSA >>> 2022-08-17 15:23:37\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:37\n",
            "loss: 0.3282, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:23:37\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:38\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:38\n",
            "loss: 0.2844, acc: 0.9000\n",
            "E2E-ABSA >>> 2022-08-17 15:23:38\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:38\n",
            "loss: 0.3166, acc: 0.8888\n",
            "E2E-ABSA >>> 2022-08-17 15:23:39\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:39\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:39\n",
            "loss: 0.2260, acc: 0.9313\n",
            "E2E-ABSA >>> 2022-08-17 15:23:40\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:40\n",
            "loss: 0.2682, acc: 0.9016\n",
            "E2E-ABSA >>> 2022-08-17 15:23:40\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:41\n",
            "loss: 0.3052, acc: 0.8894\n",
            "E2E-ABSA >>> 2022-08-17 15:23:41\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:41\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:42\n",
            "loss: 0.2919, acc: 0.8854\n",
            "E2E-ABSA >>> 2022-08-17 15:23:42\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:42\n",
            "loss: 0.3206, acc: 0.8760\n",
            "E2E-ABSA >>> 2022-08-17 15:23:42\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:43\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:43\n",
            "loss: 0.2828, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-08-17 15:23:43\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:44\n",
            "loss: 0.3048, acc: 0.8862\n",
            "E2E-ABSA >>> 2022-08-17 15:23:44\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:45\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:45\n",
            "loss: 0.3150, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-08-17 15:23:45\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:45\n",
            "loss: 0.2622, acc: 0.9156\n",
            "E2E-ABSA >>> 2022-08-17 15:23:46\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:23:46\n",
            "loss: 0.2916, acc: 0.8849\n",
            "E2E-ABSA >>> 2022-08-17 15:23:46\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            "E2E-ABSA >>> 2022-08-17 15:23:46\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            "you can download the best model from state_dict/memnet_SemEval2016_val_f1_0.8305\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.8305, test_precision: 0.8305, test_recall: 0.8305, test_f1: 0.8305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2016** dataset on model(**MEMNET**)"
      ],
      "metadata": {
        "id": "kkiz9HNjV98D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset SemEval2016_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNy_J5q3V-Cs",
        "outputId": "e8a7b976-35e2-4adc-af60-7a2fa82f7ce6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 9403904\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 1987200\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f68676c2b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:25\n",
            ">>> val_acc: 0.7034, val_precision: 0.7034 val_recall: 0.7034, val_f1: 0.7034\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.7034\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:25\n",
            "loss: 0.7979, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-08-17 15:24:25\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:26\n",
            "loss: 0.7275, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:24:26\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:26\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:26\n",
            "loss: 0.7264, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 15:24:27\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:27\n",
            "loss: 0.6884, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 15:24:27\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:28\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:28\n",
            "loss: 0.7181, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:24:28\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:29\n",
            "loss: 0.6421, acc: 0.7484\n",
            "E2E-ABSA >>> 2022-08-17 15:24:29\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:29\n",
            "loss: 0.6548, acc: 0.7419\n",
            "E2E-ABSA >>> 2022-08-17 15:24:29\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:30\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:30\n",
            "loss: 0.6407, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 15:24:31\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:31\n",
            "loss: 0.6480, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 15:24:31\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:32\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:32\n",
            "loss: 0.5942, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:24:32\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:33\n",
            "loss: 0.6177, acc: 0.7462\n",
            "E2E-ABSA >>> 2022-08-17 15:24:33\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:33\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:33\n",
            "loss: 0.6485, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:24:34\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:34\n",
            "loss: 0.6035, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:24:34\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:35\n",
            "loss: 0.6041, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 15:24:35\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:35\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:36\n",
            "loss: 0.6115, acc: 0.7458\n",
            "E2E-ABSA >>> 2022-08-17 15:24:36\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:36\n",
            "loss: 0.5979, acc: 0.7469\n",
            "E2E-ABSA >>> 2022-08-17 15:24:36\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:37\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:37\n",
            "loss: 0.5637, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:24:37\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:38\n",
            "loss: 0.5921, acc: 0.7488\n",
            "E2E-ABSA >>> 2022-08-17 15:24:38\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:39\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:39\n",
            "loss: 0.5417, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 15:24:39\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:39\n",
            "loss: 0.5724, acc: 0.7672\n",
            "E2E-ABSA >>> 2022-08-17 15:24:40\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:40\n",
            "loss: 0.5559, acc: 0.7635\n",
            "E2E-ABSA >>> 2022-08-17 15:24:40\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:41\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:41\n",
            "loss: 0.5630, acc: 0.7542\n",
            "E2E-ABSA >>> 2022-08-17 15:24:41\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:42\n",
            "loss: 0.5463, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 15:24:42\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:42\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:42\n",
            "loss: 0.5272, acc: 0.7781\n",
            "E2E-ABSA >>> 2022-08-17 15:24:43\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:43\n",
            "loss: 0.5081, acc: 0.7963\n",
            "E2E-ABSA >>> 2022-08-17 15:24:43\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:44\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:44\n",
            "loss: 0.5326, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:24:44\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:45\n",
            "loss: 0.5152, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:24:45\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:46\n",
            "loss: 0.5077, acc: 0.7905\n",
            "E2E-ABSA >>> 2022-08-17 15:24:46\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:46\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:46\n",
            "loss: 0.5101, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 15:24:47\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:47\n",
            "loss: 0.4925, acc: 0.7958\n",
            "E2E-ABSA >>> 2022-08-17 15:24:47\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:48\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:48\n",
            "loss: 0.4431, acc: 0.8063\n",
            "E2E-ABSA >>> 2022-08-17 15:24:48\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:49\n",
            "loss: 0.4805, acc: 0.8237\n",
            "E2E-ABSA >>> 2022-08-17 15:24:49\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:49\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:49\n",
            "loss: 0.4666, acc: 0.8250\n",
            "E2E-ABSA >>> 2022-08-17 15:24:50\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:50\n",
            "loss: 0.4703, acc: 0.8203\n",
            "E2E-ABSA >>> 2022-08-17 15:24:51\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:51\n",
            "loss: 0.4739, acc: 0.8058\n",
            "E2E-ABSA >>> 2022-08-17 15:24:51\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:52\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:52\n",
            "loss: 0.4802, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:24:52\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:53\n",
            "loss: 0.4611, acc: 0.8167\n",
            "E2E-ABSA >>> 2022-08-17 15:24:53\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:53\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.822\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:53\n",
            "loss: 0.4661, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 15:24:54\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:54\n",
            "loss: 0.4548, acc: 0.8263\n",
            "E2E-ABSA >>> 2022-08-17 15:24:54\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:55\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:55\n",
            "loss: 0.4871, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-08-17 15:24:55\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:56\n",
            "loss: 0.4186, acc: 0.8297\n",
            "E2E-ABSA >>> 2022-08-17 15:24:56\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:56\n",
            "loss: 0.4483, acc: 0.8282\n",
            "E2E-ABSA >>> 2022-08-17 15:24:56\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:57\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:57\n",
            "loss: 0.4416, acc: 0.8479\n",
            "E2E-ABSA >>> 2022-08-17 15:24:57\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:58\n",
            "loss: 0.4379, acc: 0.8302\n",
            "E2E-ABSA >>> 2022-08-17 15:24:58\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:59\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:59\n",
            "loss: 0.4379, acc: 0.8313\n",
            "E2E-ABSA >>> 2022-08-17 15:24:59\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:24:59\n",
            "loss: 0.4386, acc: 0.8325\n",
            "E2E-ABSA >>> 2022-08-17 15:25:00\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:00\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:00\n",
            "loss: 0.4911, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-08-17 15:25:01\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:01\n",
            "loss: 0.4533, acc: 0.8297\n",
            "E2E-ABSA >>> 2022-08-17 15:25:01\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:02\n",
            "loss: 0.4185, acc: 0.8390\n",
            "E2E-ABSA >>> 2022-08-17 15:25:02\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:02\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:02\n",
            "loss: 0.4241, acc: 0.8479\n",
            "E2E-ABSA >>> 2022-08-17 15:25:03\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:03\n",
            "loss: 0.4224, acc: 0.8302\n",
            "E2E-ABSA >>> 2022-08-17 15:25:03\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:04\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:04\n",
            "loss: 0.3512, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:25:04\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:05\n",
            "loss: 0.4281, acc: 0.8350\n",
            "E2E-ABSA >>> 2022-08-17 15:25:05\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:06\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:06\n",
            "loss: 0.3545, acc: 0.8938\n",
            "E2E-ABSA >>> 2022-08-17 15:25:06\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:06\n",
            "loss: 0.3689, acc: 0.8547\n",
            "E2E-ABSA >>> 2022-08-17 15:25:07\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:07\n",
            "loss: 0.3952, acc: 0.8543\n",
            "E2E-ABSA >>> 2022-08-17 15:25:07\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:08\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">> saved: state_dict/memnet_SemEval2016_know_val_f1_0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:08\n",
            "loss: 0.3528, acc: 0.8604\n",
            "E2E-ABSA >>> 2022-08-17 15:25:08\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:09\n",
            "loss: 0.3992, acc: 0.8469\n",
            "E2E-ABSA >>> 2022-08-17 15:25:09\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:09\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:09\n",
            "loss: 0.3839, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 15:25:10\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:10\n",
            "loss: 0.3997, acc: 0.8588\n",
            "E2E-ABSA >>> 2022-08-17 15:25:10\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:11\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:11\n",
            "loss: 0.4129, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 15:25:11\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:12\n",
            "loss: 0.3414, acc: 0.8781\n",
            "E2E-ABSA >>> 2022-08-17 15:25:12\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:12\n",
            "loss: 0.3743, acc: 0.8570\n",
            "E2E-ABSA >>> 2022-08-17 15:25:12\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:13\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:13\n",
            "loss: 0.3918, acc: 0.8604\n",
            "E2E-ABSA >>> 2022-08-17 15:25:14\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:14\n",
            "loss: 0.3527, acc: 0.8646\n",
            "E2E-ABSA >>> 2022-08-17 15:25:14\n",
            ">>> val_acc: 0.7712, val_precision: 0.7712 val_recall: 0.7712, val_f1: 0.7712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:15\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:15\n",
            "loss: 0.3220, acc: 0.8781\n",
            "E2E-ABSA >>> 2022-08-17 15:25:15\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:15\n",
            "loss: 0.3459, acc: 0.8700\n",
            "E2E-ABSA >>> 2022-08-17 15:25:16\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:16\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:16\n",
            "loss: 0.4092, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:25:17\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:17\n",
            "loss: 0.3552, acc: 0.8688\n",
            "E2E-ABSA >>> 2022-08-17 15:25:17\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 15:25:18\n",
            "loss: 0.3563, acc: 0.8633\n",
            "E2E-ABSA >>> 2022-08-17 15:25:18\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            "you can download the best model from state_dict/memnet_SemEval2016_know_val_f1_0.8305\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.8305, test_precision: 0.8305, test_recall: 0.8305, test_f1: 0.8305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2016** dataset on model(**CABASC**)"
      ],
      "metadata": {
        "id": "2ISxnkv2V-Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset SemEval2016 --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGJUscGdV-K-",
        "outputId": "25a3208c-fb68-4b59-81eb-393a2ced1190"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 989.\n",
            "> testing dataset count: 103.\n",
            "cuda memory allocated: 10235904\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 1111800\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f3ed212cb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:25:57\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">> saved: state_dict/cabasc_SemEval2016_val_f1_0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:25:58\n",
            "loss: 0.7668, acc: 0.7089\n",
            "E2E-ABSA >>> 2022-08-17 15:25:59\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:00\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:00\n",
            "loss: 0.6466, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 15:26:01\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:03\n",
            "loss: 0.6723, acc: 0.7115\n",
            "E2E-ABSA >>> 2022-08-17 15:26:03\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:04\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:05\n",
            "loss: 0.6293, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-08-17 15:26:06\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:07\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:07\n",
            "loss: 0.6183, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:26:09\n",
            ">>> val_acc: 0.7282, val_precision: 0.7282 val_recall: 0.7282, val_f1: 0.7282\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:09\n",
            "loss: 0.6428, acc: 0.6994\n",
            "E2E-ABSA >>> 2022-08-17 15:26:10\n",
            ">>> val_acc: 0.7282, val_precision: 0.7282 val_recall: 0.7282, val_f1: 0.7282\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:11\n",
            ">>> val_acc: 0.7282, val_precision: 0.7282 val_recall: 0.7282, val_f1: 0.7282\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:12\n",
            "loss: 0.6332, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-08-17 15:26:13\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:14\n",
            "loss: 0.6106, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 15:26:14\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:16\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:16\n",
            "loss: 0.5965, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-08-17 15:26:17\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:18\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:19\n",
            "loss: 0.6392, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 15:26:20\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:21\n",
            "loss: 0.5841, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-08-17 15:26:21\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:23\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:23\n",
            "loss: 0.5684, acc: 0.7642\n",
            "E2E-ABSA >>> 2022-08-17 15:26:24\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">> saved: state_dict/cabasc_SemEval2016_val_f1_0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:25\n",
            "loss: 0.5749, acc: 0.7458\n",
            "E2E-ABSA >>> 2022-08-17 15:26:25\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:27\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:28\n",
            "loss: 0.5780, acc: 0.7517\n",
            "E2E-ABSA >>> 2022-08-17 15:26:28\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:30\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:30\n",
            "loss: 0.5768, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 15:26:31\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:32\n",
            "loss: 0.5499, acc: 0.7588\n",
            "E2E-ABSA >>> 2022-08-17 15:26:32\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:34\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:34\n",
            "loss: 0.5864, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:26:35\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:37\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:37\n",
            "loss: 0.5464, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:26:38\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">> saved: state_dict/cabasc_SemEval2016_val_f1_0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:39\n",
            "loss: 0.5586, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:26:39\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:41\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">> saved: state_dict/cabasc_SemEval2016_val_f1_0.767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:41\n",
            "loss: 0.5341, acc: 0.7852\n",
            "E2E-ABSA >>> 2022-08-17 15:26:42\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:43\n",
            "loss: 0.5319, acc: 0.7766\n",
            "E2E-ABSA >>> 2022-08-17 15:26:44\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:45\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:46\n",
            "loss: 0.5220, acc: 0.7854\n",
            "E2E-ABSA >>> 2022-08-17 15:26:47\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:48\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:48\n",
            "loss: 0.4911, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:26:49\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:50\n",
            "loss: 0.5296, acc: 0.7855\n",
            "E2E-ABSA >>> 2022-08-17 15:26:51\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:52\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:53\n",
            "loss: 0.4927, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:26:54\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:55\n",
            "loss: 0.5157, acc: 0.7899\n",
            "E2E-ABSA >>> 2022-08-17 15:26:55\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:56\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:57\n",
            "loss: 0.5128, acc: 0.7831\n",
            "E2E-ABSA >>> 2022-08-17 15:26:58\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:59\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:26:59\n",
            "loss: 0.5473, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:27:01\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:02\n",
            "loss: 0.4987, acc: 0.7956\n",
            "E2E-ABSA >>> 2022-08-17 15:27:02\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:03\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:04\n",
            "loss: 0.4755, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:27:05\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:06\n",
            "loss: 0.4967, acc: 0.7947\n",
            "E2E-ABSA >>> 2022-08-17 15:27:06\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:08\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:08\n",
            "loss: 0.5092, acc: 0.7928\n",
            "E2E-ABSA >>> 2022-08-17 15:27:09\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:10\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:11\n",
            "loss: 0.5097, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-08-17 15:27:12\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:13\n",
            "loss: 0.4727, acc: 0.8113\n",
            "E2E-ABSA >>> 2022-08-17 15:27:13\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:15\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:15\n",
            "loss: 0.4636, acc: 0.8170\n",
            "E2E-ABSA >>> 2022-08-17 15:27:16\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:17\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:17\n",
            "loss: 0.4153, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 15:27:19\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:20\n",
            "loss: 0.4837, acc: 0.7961\n",
            "E2E-ABSA >>> 2022-08-17 15:27:20\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:21\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:27:22\n",
            "loss: 0.4385, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-08-17 15:27:23\n",
            ">>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            "E2E-ABSA >>> 2022-08-17 15:27:23\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7670, val_precision: 0.7670 val_recall: 0.7670, val_f1: 0.7670\n",
            "you can download the best model from state_dict/cabasc_SemEval2016_val_f1_0.767\n",
            ">>> test_acc: 0.7670, test_precision: 0.7670, test_recall: 0.7670, test_f1: 0.7670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **SemEval2016** dataset on model(**CABASC**)"
      ],
      "metadata": {
        "id": "TdycHmnoV-Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset SemEval2016_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpkEJu1_V-Uk",
        "outputId": "989e75ed-5acc-4c3c-8137-5e7cec1e4568"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 1006.\n",
            "> testing dataset count: 103.\n",
            "cuda memory allocated: 13737472\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 1987200\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fda338d7b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:04\n",
            ">>> val_acc: 0.7282, val_precision: 0.7282 val_recall: 0.7282, val_f1: 0.7282\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.7282\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:06\n",
            "loss: 0.7626, acc: 0.7331\n",
            "E2E-ABSA >>> 2022-08-17 15:28:08\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:12\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:12\n",
            "loss: 0.6724, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-08-17 15:28:15\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:18\n",
            "loss: 0.6928, acc: 0.7122\n",
            "E2E-ABSA >>> 2022-08-17 15:28:19\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:22\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:23\n",
            "loss: 0.6373, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 15:28:26\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:29\n",
            "loss: 0.6748, acc: 0.7140\n",
            "E2E-ABSA >>> 2022-08-17 15:28:29\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:33\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:34\n",
            "loss: 0.6686, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 15:28:36\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:40\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:40\n",
            "loss: 0.6195, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-08-17 15:28:43\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:45\n",
            "loss: 0.6284, acc: 0.7301\n",
            "E2E-ABSA >>> 2022-08-17 15:28:47\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:50\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:51\n",
            "loss: 0.6135, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-08-17 15:28:54\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:28:56\n",
            "loss: 0.6051, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-08-17 15:28:57\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:01\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:02\n",
            "loss: 0.5649, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 15:29:04\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:08\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:08\n",
            "loss: 0.6560, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:29:11\n",
            ">>> val_acc: 0.7379, val_precision: 0.7379 val_recall: 0.7379, val_f1: 0.7379\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:13\n",
            "loss: 0.5970, acc: 0.7297\n",
            "E2E-ABSA >>> 2022-08-17 15:29:15\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:18\n",
            ">>> val_acc: 0.7573, val_precision: 0.7573 val_recall: 0.7573, val_f1: 0.7573\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.7573\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:19\n",
            "loss: 0.5796, acc: 0.7589\n",
            "E2E-ABSA >>> 2022-08-17 15:29:22\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:24\n",
            "loss: 0.5605, acc: 0.7512\n",
            "E2E-ABSA >>> 2022-08-17 15:29:25\n",
            ">>> val_acc: 0.7767, val_precision: 0.7767 val_recall: 0.7767, val_f1: 0.7767\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:29\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:30\n",
            "loss: 0.5830, acc: 0.7225\n",
            "E2E-ABSA >>> 2022-08-17 15:29:32\n",
            ">>> val_acc: 0.7476, val_precision: 0.7476 val_recall: 0.7476, val_f1: 0.7476\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:35\n",
            "loss: 0.5607, acc: 0.7450\n",
            "E2E-ABSA >>> 2022-08-17 15:29:36\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:39\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:41\n",
            "loss: 0.5719, acc: 0.7674\n",
            "E2E-ABSA >>> 2022-08-17 15:29:43\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:46\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:47\n",
            "loss: 0.5658, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:29:50\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:52\n",
            "loss: 0.5450, acc: 0.7553\n",
            "E2E-ABSA >>> 2022-08-17 15:29:53\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:57\n",
            ">>> val_acc: 0.7864, val_precision: 0.7864 val_recall: 0.7864, val_f1: 0.7864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:29:58\n",
            "loss: 0.5529, acc: 0.7798\n",
            "E2E-ABSA >>> 2022-08-17 15:30:00\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:03\n",
            "loss: 0.5447, acc: 0.7748\n",
            "E2E-ABSA >>> 2022-08-17 15:30:04\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:07\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:09\n",
            "loss: 0.5091, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:30:11\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:14\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:15\n",
            "loss: 0.5268, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:30:18\n",
            ">>> val_acc: 0.8350, val_precision: 0.8350 val_recall: 0.8350, val_f1: 0.8350\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.835\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:20\n",
            "loss: 0.5311, acc: 0.7878\n",
            "E2E-ABSA >>> 2022-08-17 15:30:21\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:25\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:26\n",
            "loss: 0.5193, acc: 0.7721\n",
            "E2E-ABSA >>> 2022-08-17 15:30:28\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:31\n",
            "loss: 0.5247, acc: 0.7836\n",
            "E2E-ABSA >>> 2022-08-17 15:30:32\n",
            ">>> val_acc: 0.8350, val_precision: 0.8350 val_recall: 0.8350, val_f1: 0.8350\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:35\n",
            ">>> val_acc: 0.8350, val_precision: 0.8350 val_recall: 0.8350, val_f1: 0.8350\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:37\n",
            "loss: 0.5087, acc: 0.7902\n",
            "E2E-ABSA >>> 2022-08-17 15:30:39\n",
            ">>> val_acc: 0.8350, val_precision: 0.8350 val_recall: 0.8350, val_f1: 0.8350\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:43\n",
            ">>> val_acc: 0.8350, val_precision: 0.8350 val_recall: 0.8350, val_f1: 0.8350\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:43\n",
            "loss: 0.4183, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 15:30:46\n",
            ">>> val_acc: 0.8447, val_precision: 0.8447 val_recall: 0.8447, val_f1: 0.8447\n",
            ">> saved: state_dict/cabasc_SemEval2016_know_val_f1_0.8447\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:48\n",
            "loss: 0.4986, acc: 0.8061\n",
            "E2E-ABSA >>> 2022-08-17 15:30:50\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:53\n",
            ">>> val_acc: 0.8252, val_precision: 0.8252 val_recall: 0.8252, val_f1: 0.8252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:54\n",
            "loss: 0.5438, acc: 0.8173\n",
            "E2E-ABSA >>> 2022-08-17 15:30:57\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:30:59\n",
            "loss: 0.4834, acc: 0.8037\n",
            "E2E-ABSA >>> 2022-08-17 15:31:00\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:04\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:05\n",
            "loss: 0.5435, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 15:31:07\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:10\n",
            "loss: 0.4853, acc: 0.8115\n",
            "E2E-ABSA >>> 2022-08-17 15:31:11\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:14\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:16\n",
            "loss: 0.4834, acc: 0.8268\n",
            "E2E-ABSA >>> 2022-08-17 15:31:18\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:21\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:22\n",
            "loss: 0.5142, acc: 0.7778\n",
            "E2E-ABSA >>> 2022-08-17 15:31:25\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:27\n",
            "loss: 0.4677, acc: 0.8179\n",
            "E2E-ABSA >>> 2022-08-17 15:31:28\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:32\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:33\n",
            "loss: 0.4410, acc: 0.8063\n",
            "E2E-ABSA >>> 2022-08-17 15:31:35\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:38\n",
            "loss: 0.4750, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:31:39\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:42\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:44\n",
            "loss: 0.4771, acc: 0.8185\n",
            "E2E-ABSA >>> 2022-08-17 15:31:46\n",
            ">>> val_acc: 0.8155, val_precision: 0.8155 val_recall: 0.8155, val_f1: 0.8155\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:49\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:50\n",
            "loss: 0.5442, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:31:53\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:31:55\n",
            "loss: 0.4647, acc: 0.8214\n",
            "E2E-ABSA >>> 2022-08-17 15:31:56\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:00\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:01\n",
            "loss: 0.4570, acc: 0.8164\n",
            "E2E-ABSA >>> 2022-08-17 15:32:03\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:06\n",
            "loss: 0.4673, acc: 0.8208\n",
            "E2E-ABSA >>> 2022-08-17 15:32:07\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:10\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:12\n",
            "loss: 0.4627, acc: 0.8079\n",
            "E2E-ABSA >>> 2022-08-17 15:32:14\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:17\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:17\n",
            "loss: 0.6117, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:32:21\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:23\n",
            "loss: 0.4386, acc: 0.8141\n",
            "E2E-ABSA >>> 2022-08-17 15:32:24\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:28\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:32:28\n",
            "loss: 0.4664, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:32:31\n",
            ">>> val_acc: 0.8058, val_precision: 0.8058 val_recall: 0.8058, val_f1: 0.8058\n",
            "E2E-ABSA >>> 2022-08-17 15:32:31\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8447, val_precision: 0.8447 val_recall: 0.8447, val_f1: 0.8447\n",
            "you can download the best model from state_dict/cabasc_SemEval2016_know_val_f1_0.8447\n",
            ">>> test_acc: 0.8447, test_precision: 0.8447, test_recall: 0.8447, test_f1: 0.8447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lstm\n",
        "tdlstm  \n",
        "tclstm  \n",
        "ataelstm  \n",
        "ian \n",
        "memnet  \n",
        "cabasc "
      ],
      "metadata": {
        "id": "88nfASKEV-ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **acl14shortdata** dataset on model(**LSTM**)\n"
      ],
      "metadata": {
        "id": "T_660IqKW2o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset acl14shortdata --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqDLysw-V-eM",
        "outputId": "fb959ae7-d411-4fc8-9650-6a6bda5c6823"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 18208768\n",
            "> n_trainable_params: 723303, n_nontrainable_params: 3828600\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f07d0d2ab00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:11\n",
            "loss: 1.1032, acc: 0.3563\n",
            "E2E-ABSA >>> 2022-08-17 15:33:12\n",
            "loss: 1.0742, acc: 0.4234\n",
            "E2E-ABSA >>> 2022-08-17 15:33:12\n",
            "loss: 1.0578, acc: 0.4544\n",
            "E2E-ABSA >>> 2022-08-17 15:33:13\n",
            ">>> val_acc: 0.4560, val_precision: 0.4560 val_recall: 0.4560, val_f1: 0.4560\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.456\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:13\n",
            "loss: 1.0031, acc: 0.5286\n",
            "E2E-ABSA >>> 2022-08-17 15:33:14\n",
            "loss: 1.0141, acc: 0.5139\n",
            "E2E-ABSA >>> 2022-08-17 15:33:14\n",
            "loss: 1.0222, acc: 0.5015\n",
            "E2E-ABSA >>> 2022-08-17 15:33:15\n",
            "loss: 1.0202, acc: 0.5043\n",
            "E2E-ABSA >>> 2022-08-17 15:33:15\n",
            ">>> val_acc: 0.4640, val_precision: 0.4640 val_recall: 0.4640, val_f1: 0.4640\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.464\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:15\n",
            "loss: 1.0239, acc: 0.4870\n",
            "E2E-ABSA >>> 2022-08-17 15:33:16\n",
            "loss: 1.0111, acc: 0.4987\n",
            "E2E-ABSA >>> 2022-08-17 15:33:16\n",
            "loss: 1.0064, acc: 0.5030\n",
            "E2E-ABSA >>> 2022-08-17 15:33:17\n",
            ">>> val_acc: 0.4736, val_precision: 0.4736 val_recall: 0.4736, val_f1: 0.4736\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.4736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:17\n",
            "loss: 0.9964, acc: 0.5099\n",
            "E2E-ABSA >>> 2022-08-17 15:33:18\n",
            "loss: 0.9980, acc: 0.5148\n",
            "E2E-ABSA >>> 2022-08-17 15:33:18\n",
            "loss: 0.9940, acc: 0.5128\n",
            "E2E-ABSA >>> 2022-08-17 15:33:19\n",
            "loss: 0.9881, acc: 0.5180\n",
            "E2E-ABSA >>> 2022-08-17 15:33:19\n",
            ">>> val_acc: 0.4928, val_precision: 0.4928 val_recall: 0.4928, val_f1: 0.4928\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.4928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:19\n",
            "loss: 0.9774, acc: 0.5285\n",
            "E2E-ABSA >>> 2022-08-17 15:33:20\n",
            "loss: 0.9813, acc: 0.5273\n",
            "E2E-ABSA >>> 2022-08-17 15:33:20\n",
            "loss: 0.9727, acc: 0.5349\n",
            "E2E-ABSA >>> 2022-08-17 15:33:21\n",
            ">>> val_acc: 0.5024, val_precision: 0.5024 val_recall: 0.5024, val_f1: 0.5024\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.5024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:21\n",
            "loss: 0.9941, acc: 0.4969\n",
            "E2E-ABSA >>> 2022-08-17 15:33:22\n",
            "loss: 0.9582, acc: 0.5406\n",
            "E2E-ABSA >>> 2022-08-17 15:33:22\n",
            "loss: 0.9548, acc: 0.5422\n",
            "E2E-ABSA >>> 2022-08-17 15:33:23\n",
            "loss: 0.9445, acc: 0.5544\n",
            "E2E-ABSA >>> 2022-08-17 15:33:23\n",
            ">>> val_acc: 0.5280, val_precision: 0.5280 val_recall: 0.5280, val_f1: 0.5280\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.528\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:23\n",
            "loss: 0.9287, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 15:33:24\n",
            "loss: 0.9149, acc: 0.5678\n",
            "E2E-ABSA >>> 2022-08-17 15:33:24\n",
            "loss: 0.8957, acc: 0.5799\n",
            "E2E-ABSA >>> 2022-08-17 15:33:25\n",
            ">>> val_acc: 0.5984, val_precision: 0.5984 val_recall: 0.5984, val_f1: 0.5984\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.5984\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:25\n",
            "loss: 0.8561, acc: 0.6076\n",
            "E2E-ABSA >>> 2022-08-17 15:33:26\n",
            "loss: 0.8525, acc: 0.5970\n",
            "E2E-ABSA >>> 2022-08-17 15:33:26\n",
            "loss: 0.8407, acc: 0.6104\n",
            "E2E-ABSA >>> 2022-08-17 15:33:27\n",
            "loss: 0.8378, acc: 0.6140\n",
            "E2E-ABSA >>> 2022-08-17 15:33:27\n",
            ">>> val_acc: 0.6016, val_precision: 0.6016 val_recall: 0.6016, val_f1: 0.6016\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6016\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:27\n",
            "loss: 0.7894, acc: 0.6466\n",
            "E2E-ABSA >>> 2022-08-17 15:33:28\n",
            "loss: 0.7937, acc: 0.6444\n",
            "E2E-ABSA >>> 2022-08-17 15:33:28\n",
            "loss: 0.8134, acc: 0.6314\n",
            "E2E-ABSA >>> 2022-08-17 15:33:29\n",
            ">>> val_acc: 0.6080, val_precision: 0.6080 val_recall: 0.6080, val_f1: 0.6080\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:29\n",
            "loss: 0.8012, acc: 0.6289\n",
            "E2E-ABSA >>> 2022-08-17 15:33:30\n",
            "loss: 0.7846, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-08-17 15:33:30\n",
            "loss: 0.7981, acc: 0.6479\n",
            "E2E-ABSA >>> 2022-08-17 15:33:31\n",
            "loss: 0.8026, acc: 0.6412\n",
            "E2E-ABSA >>> 2022-08-17 15:33:31\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:31\n",
            "loss: 0.7619, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 15:33:32\n",
            "loss: 0.7888, acc: 0.6493\n",
            "E2E-ABSA >>> 2022-08-17 15:33:32\n",
            "loss: 0.7896, acc: 0.6440\n",
            "E2E-ABSA >>> 2022-08-17 15:33:33\n",
            ">>> val_acc: 0.6256, val_precision: 0.6256 val_recall: 0.6256, val_f1: 0.6256\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6256\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:33\n",
            "loss: 0.7710, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 15:33:33\n",
            "loss: 0.7757, acc: 0.6431\n",
            "E2E-ABSA >>> 2022-08-17 15:33:34\n",
            "loss: 0.7902, acc: 0.6409\n",
            "E2E-ABSA >>> 2022-08-17 15:33:35\n",
            "loss: 0.7828, acc: 0.6486\n",
            "E2E-ABSA >>> 2022-08-17 15:33:35\n",
            ">>> val_acc: 0.6320, val_precision: 0.6320 val_recall: 0.6320, val_f1: 0.6320\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.632\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:35\n",
            "loss: 0.7861, acc: 0.6505\n",
            "E2E-ABSA >>> 2022-08-17 15:33:36\n",
            "loss: 0.7707, acc: 0.6637\n",
            "E2E-ABSA >>> 2022-08-17 15:33:36\n",
            "loss: 0.7644, acc: 0.6626\n",
            "E2E-ABSA >>> 2022-08-17 15:33:37\n",
            ">>> val_acc: 0.6256, val_precision: 0.6256 val_recall: 0.6256, val_f1: 0.6256\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:37\n",
            "loss: 0.7793, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 15:33:37\n",
            "loss: 0.7547, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-08-17 15:33:38\n",
            "loss: 0.7628, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-08-17 15:33:38\n",
            "loss: 0.7679, acc: 0.6580\n",
            "E2E-ABSA >>> 2022-08-17 15:33:39\n",
            ">>> val_acc: 0.6368, val_precision: 0.6368 val_recall: 0.6368, val_f1: 0.6368\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6368\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:39\n",
            "loss: 0.7712, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-08-17 15:33:40\n",
            "loss: 0.7617, acc: 0.6610\n",
            "E2E-ABSA >>> 2022-08-17 15:33:40\n",
            "loss: 0.7648, acc: 0.6629\n",
            "E2E-ABSA >>> 2022-08-17 15:33:41\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:41\n",
            "loss: 0.7560, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 15:33:41\n",
            "loss: 0.7568, acc: 0.6724\n",
            "E2E-ABSA >>> 2022-08-17 15:33:42\n",
            "loss: 0.7490, acc: 0.6722\n",
            "E2E-ABSA >>> 2022-08-17 15:33:42\n",
            "loss: 0.7632, acc: 0.6633\n",
            "E2E-ABSA >>> 2022-08-17 15:33:43\n",
            ">>> val_acc: 0.6464, val_precision: 0.6464 val_recall: 0.6464, val_f1: 0.6464\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6464\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:43\n",
            "loss: 0.7630, acc: 0.6627\n",
            "E2E-ABSA >>> 2022-08-17 15:33:44\n",
            "loss: 0.7548, acc: 0.6644\n",
            "E2E-ABSA >>> 2022-08-17 15:33:44\n",
            "loss: 0.7537, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-08-17 15:33:45\n",
            ">>> val_acc: 0.6336, val_precision: 0.6336 val_recall: 0.6336, val_f1: 0.6336\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:45\n",
            "loss: 0.7117, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 15:33:45\n",
            "loss: 0.7487, acc: 0.6708\n",
            "E2E-ABSA >>> 2022-08-17 15:33:46\n",
            "loss: 0.7531, acc: 0.6710\n",
            "E2E-ABSA >>> 2022-08-17 15:33:46\n",
            "loss: 0.7545, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-08-17 15:33:47\n",
            ">>> val_acc: 0.6304, val_precision: 0.6304 val_recall: 0.6304, val_f1: 0.6304\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:47\n",
            "loss: 0.7615, acc: 0.6514\n",
            "E2E-ABSA >>> 2022-08-17 15:33:48\n",
            "loss: 0.7429, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-08-17 15:33:48\n",
            "loss: 0.7507, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-08-17 15:33:49\n",
            ">>> val_acc: 0.6368, val_precision: 0.6368 val_recall: 0.6368, val_f1: 0.6368\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:49\n",
            "loss: 0.7289, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-08-17 15:33:49\n",
            "loss: 0.7438, acc: 0.6775\n",
            "E2E-ABSA >>> 2022-08-17 15:33:50\n",
            "loss: 0.7411, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-08-17 15:33:50\n",
            "loss: 0.7419, acc: 0.6751\n",
            "E2E-ABSA >>> 2022-08-17 15:33:51\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:51\n",
            "loss: 0.7374, acc: 0.6802\n",
            "E2E-ABSA >>> 2022-08-17 15:33:51\n",
            "loss: 0.7541, acc: 0.6707\n",
            "E2E-ABSA >>> 2022-08-17 15:33:52\n",
            "loss: 0.7431, acc: 0.6740\n",
            "E2E-ABSA >>> 2022-08-17 15:33:53\n",
            ">>> val_acc: 0.6384, val_precision: 0.6384 val_recall: 0.6384, val_f1: 0.6384\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:53\n",
            "loss: 0.7402, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 15:33:53\n",
            "loss: 0.7263, acc: 0.6846\n",
            "E2E-ABSA >>> 2022-08-17 15:33:54\n",
            "loss: 0.7317, acc: 0.6815\n",
            "E2E-ABSA >>> 2022-08-17 15:33:54\n",
            "loss: 0.7366, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-08-17 15:33:54\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.648\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:55\n",
            "loss: 0.7257, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 15:33:55\n",
            "loss: 0.7356, acc: 0.6755\n",
            "E2E-ABSA >>> 2022-08-17 15:33:56\n",
            "loss: 0.7410, acc: 0.6697\n",
            "E2E-ABSA >>> 2022-08-17 15:33:56\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:56\n",
            "loss: 0.7315, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 15:33:57\n",
            "loss: 0.7500, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-08-17 15:33:58\n",
            "loss: 0.7330, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-08-17 15:33:58\n",
            "loss: 0.7286, acc: 0.6836\n",
            "E2E-ABSA >>> 2022-08-17 15:33:58\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:33:59\n",
            "loss: 0.7351, acc: 0.6767\n",
            "E2E-ABSA >>> 2022-08-17 15:33:59\n",
            "loss: 0.7390, acc: 0.6735\n",
            "E2E-ABSA >>> 2022-08-17 15:34:00\n",
            "loss: 0.7436, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-08-17 15:34:00\n",
            "loss: 0.7361, acc: 0.6765\n",
            "E2E-ABSA >>> 2022-08-17 15:34:00\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:01\n",
            "loss: 0.7372, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-08-17 15:34:01\n",
            "loss: 0.7275, acc: 0.6772\n",
            "E2E-ABSA >>> 2022-08-17 15:34:02\n",
            "loss: 0.7279, acc: 0.6765\n",
            "E2E-ABSA >>> 2022-08-17 15:34:02\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:03\n",
            "loss: 0.7271, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-08-17 15:34:03\n",
            "loss: 0.7261, acc: 0.6921\n",
            "E2E-ABSA >>> 2022-08-17 15:34:04\n",
            "loss: 0.7248, acc: 0.6895\n",
            "E2E-ABSA >>> 2022-08-17 15:34:04\n",
            "loss: 0.7312, acc: 0.6843\n",
            "E2E-ABSA >>> 2022-08-17 15:34:04\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:05\n",
            "loss: 0.7389, acc: 0.6855\n",
            "E2E-ABSA >>> 2022-08-17 15:34:05\n",
            "loss: 0.7379, acc: 0.6792\n",
            "E2E-ABSA >>> 2022-08-17 15:34:06\n",
            "loss: 0.7372, acc: 0.6757\n",
            "E2E-ABSA >>> 2022-08-17 15:34:06\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:06\n",
            "loss: 0.7214, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-08-17 15:34:07\n",
            "loss: 0.7227, acc: 0.6871\n",
            "E2E-ABSA >>> 2022-08-17 15:34:08\n",
            "loss: 0.7289, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-08-17 15:34:08\n",
            "loss: 0.7288, acc: 0.6846\n",
            "E2E-ABSA >>> 2022-08-17 15:34:08\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:09\n",
            "loss: 0.7411, acc: 0.6760\n",
            "E2E-ABSA >>> 2022-08-17 15:34:09\n",
            "loss: 0.7324, acc: 0.6787\n",
            "E2E-ABSA >>> 2022-08-17 15:34:10\n",
            "loss: 0.7270, acc: 0.6809\n",
            "E2E-ABSA >>> 2022-08-17 15:34:10\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:10\n",
            "loss: 0.7324, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 15:34:11\n",
            "loss: 0.7465, acc: 0.6723\n",
            "E2E-ABSA >>> 2022-08-17 15:34:11\n",
            "loss: 0.7332, acc: 0.6766\n",
            "E2E-ABSA >>> 2022-08-17 15:34:12\n",
            "loss: 0.7274, acc: 0.6805\n",
            "E2E-ABSA >>> 2022-08-17 15:34:12\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:13\n",
            "loss: 0.7397, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-08-17 15:34:13\n",
            "loss: 0.7280, acc: 0.6855\n",
            "E2E-ABSA >>> 2022-08-17 15:34:14\n",
            "loss: 0.7276, acc: 0.6825\n",
            "E2E-ABSA >>> 2022-08-17 15:34:14\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:14\n",
            "loss: 0.7519, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 15:34:15\n",
            "loss: 0.7401, acc: 0.6691\n",
            "E2E-ABSA >>> 2022-08-17 15:34:15\n",
            "loss: 0.7307, acc: 0.6737\n",
            "E2E-ABSA >>> 2022-08-17 15:34:16\n",
            "loss: 0.7247, acc: 0.6815\n",
            "E2E-ABSA >>> 2022-08-17 15:34:16\n",
            ">>> val_acc: 0.6320, val_precision: 0.6320 val_recall: 0.6320, val_f1: 0.6320\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:17\n",
            "loss: 0.7102, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-08-17 15:34:17\n",
            "loss: 0.7233, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 15:34:18\n",
            "loss: 0.7194, acc: 0.6800\n",
            "E2E-ABSA >>> 2022-08-17 15:34:18\n",
            ">>> val_acc: 0.6512, val_precision: 0.6512 val_recall: 0.6512, val_f1: 0.6512\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.6512\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:18\n",
            "loss: 0.7536, acc: 0.6465\n",
            "E2E-ABSA >>> 2022-08-17 15:34:19\n",
            "loss: 0.7207, acc: 0.6932\n",
            "E2E-ABSA >>> 2022-08-17 15:34:19\n",
            "loss: 0.7199, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 15:34:20\n",
            "loss: 0.7207, acc: 0.6905\n",
            "E2E-ABSA >>> 2022-08-17 15:34:20\n",
            ">>> val_acc: 0.6272, val_precision: 0.6272 val_recall: 0.6272, val_f1: 0.6272\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:20\n",
            "loss: 0.7446, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-08-17 15:34:21\n",
            "loss: 0.7348, acc: 0.6861\n",
            "E2E-ABSA >>> 2022-08-17 15:34:22\n",
            "loss: 0.7276, acc: 0.6866\n",
            "E2E-ABSA >>> 2022-08-17 15:34:22\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:22\n",
            "loss: 0.6997, acc: 0.6987\n",
            "E2E-ABSA >>> 2022-08-17 15:34:23\n",
            "loss: 0.7162, acc: 0.6909\n",
            "E2E-ABSA >>> 2022-08-17 15:34:23\n",
            "loss: 0.7253, acc: 0.6867\n",
            "E2E-ABSA >>> 2022-08-17 15:34:24\n",
            "loss: 0.7229, acc: 0.6860\n",
            "E2E-ABSA >>> 2022-08-17 15:34:24\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:24\n",
            "loss: 0.7243, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-08-17 15:34:25\n",
            "loss: 0.7176, acc: 0.6864\n",
            "E2E-ABSA >>> 2022-08-17 15:34:25\n",
            "loss: 0.7124, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:34:26\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:26\n",
            "loss: 0.7439, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 15:34:27\n",
            "loss: 0.7096, acc: 0.7082\n",
            "E2E-ABSA >>> 2022-08-17 15:34:27\n",
            "loss: 0.7222, acc: 0.6939\n",
            "E2E-ABSA >>> 2022-08-17 15:34:28\n",
            "loss: 0.7191, acc: 0.6906\n",
            "E2E-ABSA >>> 2022-08-17 15:34:28\n",
            ">>> val_acc: 0.6320, val_precision: 0.6320 val_recall: 0.6320, val_f1: 0.6320\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:28\n",
            "loss: 0.6946, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-08-17 15:34:29\n",
            "loss: 0.7082, acc: 0.6995\n",
            "E2E-ABSA >>> 2022-08-17 15:34:29\n",
            "loss: 0.7142, acc: 0.6926\n",
            "E2E-ABSA >>> 2022-08-17 15:34:30\n",
            ">>> val_acc: 0.6304, val_precision: 0.6304 val_recall: 0.6304, val_f1: 0.6304\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:30\n",
            "loss: 0.7046, acc: 0.6906\n",
            "E2E-ABSA >>> 2022-08-17 15:34:30\n",
            "loss: 0.7168, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-08-17 15:34:31\n",
            "loss: 0.7124, acc: 0.6915\n",
            "E2E-ABSA >>> 2022-08-17 15:34:31\n",
            "loss: 0.7151, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-08-17 15:34:32\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:32\n",
            "loss: 0.7504, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-08-17 15:34:33\n",
            "loss: 0.7200, acc: 0.6894\n",
            "E2E-ABSA >>> 2022-08-17 15:34:33\n",
            "loss: 0.7187, acc: 0.6882\n",
            "E2E-ABSA >>> 2022-08-17 15:34:34\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:34\n",
            "loss: 0.6936, acc: 0.6758\n",
            "E2E-ABSA >>> 2022-08-17 15:34:34\n",
            "loss: 0.6982, acc: 0.6934\n",
            "E2E-ABSA >>> 2022-08-17 15:34:35\n",
            "loss: 0.7122, acc: 0.6872\n",
            "E2E-ABSA >>> 2022-08-17 15:34:35\n",
            "loss: 0.7123, acc: 0.6905\n",
            "E2E-ABSA >>> 2022-08-17 15:34:36\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:36\n",
            "loss: 0.7329, acc: 0.6748\n",
            "E2E-ABSA >>> 2022-08-17 15:34:36\n",
            "loss: 0.7153, acc: 0.6867\n",
            "E2E-ABSA >>> 2022-08-17 15:34:37\n",
            "loss: 0.7123, acc: 0.6925\n",
            "E2E-ABSA >>> 2022-08-17 15:34:38\n",
            ">>> val_acc: 0.6384, val_precision: 0.6384 val_recall: 0.6384, val_f1: 0.6384\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:38\n",
            "loss: 0.6572, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 15:34:38\n",
            "loss: 0.6915, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 15:34:39\n",
            "loss: 0.7137, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-08-17 15:34:39\n",
            "loss: 0.7154, acc: 0.6923\n",
            "E2E-ABSA >>> 2022-08-17 15:34:39\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:40\n",
            "loss: 0.7232, acc: 0.6854\n",
            "E2E-ABSA >>> 2022-08-17 15:34:40\n",
            "loss: 0.7082, acc: 0.6906\n",
            "E2E-ABSA >>> 2022-08-17 15:34:41\n",
            "loss: 0.7184, acc: 0.6865\n",
            "E2E-ABSA >>> 2022-08-17 15:34:41\n",
            ">>> val_acc: 0.6336, val_precision: 0.6336 val_recall: 0.6336, val_f1: 0.6336\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:41\n",
            "loss: 0.7962, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:34:42\n",
            "loss: 0.7124, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-08-17 15:34:43\n",
            "loss: 0.7188, acc: 0.6914\n",
            "E2E-ABSA >>> 2022-08-17 15:34:43\n",
            "loss: 0.7154, acc: 0.6903\n",
            "E2E-ABSA >>> 2022-08-17 15:34:43\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:44\n",
            "loss: 0.7026, acc: 0.7221\n",
            "E2E-ABSA >>> 2022-08-17 15:34:44\n",
            "loss: 0.7100, acc: 0.7019\n",
            "E2E-ABSA >>> 2022-08-17 15:34:45\n",
            "loss: 0.7113, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-08-17 15:34:45\n",
            ">>> val_acc: 0.6336, val_precision: 0.6336 val_recall: 0.6336, val_f1: 0.6336\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:45\n",
            "loss: 0.6797, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:34:46\n",
            "loss: 0.7169, acc: 0.6965\n",
            "E2E-ABSA >>> 2022-08-17 15:34:46\n",
            "loss: 0.7153, acc: 0.6912\n",
            "E2E-ABSA >>> 2022-08-17 15:34:47\n",
            "loss: 0.7090, acc: 0.6926\n",
            "E2E-ABSA >>> 2022-08-17 15:34:47\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:48\n",
            "loss: 0.6806, acc: 0.7079\n",
            "E2E-ABSA >>> 2022-08-17 15:34:48\n",
            "loss: 0.6954, acc: 0.7015\n",
            "E2E-ABSA >>> 2022-08-17 15:34:49\n",
            "loss: 0.7049, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 15:34:49\n",
            "loss: 0.7086, acc: 0.6952\n",
            "E2E-ABSA >>> 2022-08-17 15:34:49\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:50\n",
            "loss: 0.7008, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 15:34:50\n",
            "loss: 0.7113, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-08-17 15:34:51\n",
            "loss: 0.7071, acc: 0.6898\n",
            "E2E-ABSA >>> 2022-08-17 15:34:51\n",
            ">>> val_acc: 0.6352, val_precision: 0.6352 val_recall: 0.6352, val_f1: 0.6352\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:51\n",
            "loss: 0.6967, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-08-17 15:34:52\n",
            "loss: 0.6966, acc: 0.7040\n",
            "E2E-ABSA >>> 2022-08-17 15:34:53\n",
            "loss: 0.7076, acc: 0.6991\n",
            "E2E-ABSA >>> 2022-08-17 15:34:53\n",
            "loss: 0.7080, acc: 0.6981\n",
            "E2E-ABSA >>> 2022-08-17 15:34:53\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:54\n",
            "loss: 0.7022, acc: 0.6868\n",
            "E2E-ABSA >>> 2022-08-17 15:34:54\n",
            "loss: 0.7037, acc: 0.6958\n",
            "E2E-ABSA >>> 2022-08-17 15:34:55\n",
            "loss: 0.7060, acc: 0.6957\n",
            "E2E-ABSA >>> 2022-08-17 15:34:55\n",
            ">>> val_acc: 0.6560, val_precision: 0.6560 val_recall: 0.6560, val_f1: 0.6560\n",
            ">> saved: state_dict/lstm_acl14shortdata_val_f1_0.656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:55\n",
            "loss: 0.7111, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-08-17 15:34:56\n",
            "loss: 0.6902, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 15:34:56\n",
            "loss: 0.7022, acc: 0.6970\n",
            "E2E-ABSA >>> 2022-08-17 15:34:57\n",
            "loss: 0.7086, acc: 0.6895\n",
            "E2E-ABSA >>> 2022-08-17 15:34:57\n",
            ">>> val_acc: 0.6512, val_precision: 0.6512 val_recall: 0.6512, val_f1: 0.6512\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:58\n",
            "loss: 0.7108, acc: 0.6923\n",
            "E2E-ABSA >>> 2022-08-17 15:34:58\n",
            "loss: 0.7023, acc: 0.6930\n",
            "E2E-ABSA >>> 2022-08-17 15:34:59\n",
            "loss: 0.7066, acc: 0.6920\n",
            "E2E-ABSA >>> 2022-08-17 15:34:59\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:34:59\n",
            "loss: 0.6984, acc: 0.6734\n",
            "E2E-ABSA >>> 2022-08-17 15:35:00\n",
            "loss: 0.7044, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-08-17 15:35:00\n",
            "loss: 0.7111, acc: 0.6911\n",
            "E2E-ABSA >>> 2022-08-17 15:35:01\n",
            "loss: 0.7069, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-08-17 15:35:01\n",
            ">>> val_acc: 0.6384, val_precision: 0.6384 val_recall: 0.6384, val_f1: 0.6384\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:01\n",
            "loss: 0.7041, acc: 0.6896\n",
            "E2E-ABSA >>> 2022-08-17 15:35:02\n",
            "loss: 0.7017, acc: 0.6978\n",
            "E2E-ABSA >>> 2022-08-17 15:35:03\n",
            "loss: 0.7112, acc: 0.6951\n",
            "E2E-ABSA >>> 2022-08-17 15:35:03\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:03\n",
            "loss: 0.7119, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-08-17 15:35:04\n",
            "loss: 0.7092, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-08-17 15:35:04\n",
            "loss: 0.7128, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-08-17 15:35:05\n",
            "loss: 0.7045, acc: 0.6970\n",
            "E2E-ABSA >>> 2022-08-17 15:35:05\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:05\n",
            "loss: 0.6919, acc: 0.6935\n",
            "E2E-ABSA >>> 2022-08-17 15:35:06\n",
            "loss: 0.7059, acc: 0.6940\n",
            "E2E-ABSA >>> 2022-08-17 15:35:06\n",
            "loss: 0.7075, acc: 0.6939\n",
            "E2E-ABSA >>> 2022-08-17 15:35:07\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:07\n",
            "loss: 0.7365, acc: 0.6934\n",
            "E2E-ABSA >>> 2022-08-17 15:35:08\n",
            "loss: 0.7004, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-08-17 15:35:08\n",
            "loss: 0.7001, acc: 0.6921\n",
            "E2E-ABSA >>> 2022-08-17 15:35:09\n",
            "loss: 0.7025, acc: 0.6933\n",
            "E2E-ABSA >>> 2022-08-17 15:35:09\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:09\n",
            "loss: 0.6916, acc: 0.7055\n",
            "E2E-ABSA >>> 2022-08-17 15:35:10\n",
            "loss: 0.6969, acc: 0.7035\n",
            "E2E-ABSA >>> 2022-08-17 15:35:10\n",
            "loss: 0.6980, acc: 0.6967\n",
            "E2E-ABSA >>> 2022-08-17 15:35:11\n",
            ">>> val_acc: 0.6272, val_precision: 0.6272 val_recall: 0.6272, val_f1: 0.6272\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:11\n",
            "loss: 0.6571, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 15:35:11\n",
            "loss: 0.6899, acc: 0.7026\n",
            "E2E-ABSA >>> 2022-08-17 15:35:12\n",
            "loss: 0.6986, acc: 0.6954\n",
            "E2E-ABSA >>> 2022-08-17 15:35:12\n",
            "loss: 0.7023, acc: 0.6942\n",
            "E2E-ABSA >>> 2022-08-17 15:35:13\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:13\n",
            "loss: 0.7032, acc: 0.6801\n",
            "E2E-ABSA >>> 2022-08-17 15:35:14\n",
            "loss: 0.6981, acc: 0.6942\n",
            "E2E-ABSA >>> 2022-08-17 15:35:14\n",
            "loss: 0.6983, acc: 0.6975\n",
            "E2E-ABSA >>> 2022-08-17 15:35:15\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:15\n",
            "loss: 0.7475, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-08-17 15:35:15\n",
            "loss: 0.6994, acc: 0.6991\n",
            "E2E-ABSA >>> 2022-08-17 15:35:16\n",
            "loss: 0.7049, acc: 0.6984\n",
            "E2E-ABSA >>> 2022-08-17 15:35:16\n",
            "loss: 0.7088, acc: 0.6946\n",
            "E2E-ABSA >>> 2022-08-17 15:35:17\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:17\n",
            "loss: 0.7131, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-08-17 15:35:17\n",
            "loss: 0.7096, acc: 0.6904\n",
            "E2E-ABSA >>> 2022-08-17 15:35:18\n",
            "loss: 0.7058, acc: 0.6926\n",
            "E2E-ABSA >>> 2022-08-17 15:35:19\n",
            ">>> val_acc: 0.6352, val_precision: 0.6352 val_recall: 0.6352, val_f1: 0.6352\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:19\n",
            "loss: 0.7157, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 15:35:19\n",
            "loss: 0.7066, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-08-17 15:35:20\n",
            "loss: 0.7049, acc: 0.6977\n",
            "E2E-ABSA >>> 2022-08-17 15:35:20\n",
            "loss: 0.7025, acc: 0.6963\n",
            "E2E-ABSA >>> 2022-08-17 15:35:20\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:21\n",
            "loss: 0.6719, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-08-17 15:35:21\n",
            "loss: 0.6899, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-08-17 15:35:22\n",
            "loss: 0.6956, acc: 0.6943\n",
            "E2E-ABSA >>> 2022-08-17 15:35:22\n",
            ">>> val_acc: 0.6384, val_precision: 0.6384 val_recall: 0.6384, val_f1: 0.6384\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:23\n",
            "loss: 0.7158, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:35:23\n",
            "loss: 0.7082, acc: 0.6923\n",
            "E2E-ABSA >>> 2022-08-17 15:35:24\n",
            "loss: 0.6995, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-08-17 15:35:24\n",
            "loss: 0.6984, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-08-17 15:35:24\n",
            ">>> val_acc: 0.6240, val_precision: 0.6240 val_recall: 0.6240, val_f1: 0.6240\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:25\n",
            "loss: 0.6932, acc: 0.6924\n",
            "E2E-ABSA >>> 2022-08-17 15:35:25\n",
            "loss: 0.7026, acc: 0.6947\n",
            "E2E-ABSA >>> 2022-08-17 15:35:26\n",
            "loss: 0.7035, acc: 0.6911\n",
            "E2E-ABSA >>> 2022-08-17 15:35:26\n",
            ">>> val_acc: 0.6304, val_precision: 0.6304 val_recall: 0.6304, val_f1: 0.6304\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:26\n",
            "loss: 0.7210, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-08-17 15:35:27\n",
            "loss: 0.6856, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 15:35:27\n",
            "loss: 0.7052, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-08-17 15:35:28\n",
            "loss: 0.7038, acc: 0.6919\n",
            "E2E-ABSA >>> 2022-08-17 15:35:28\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:29\n",
            "loss: 0.6933, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:35:29\n",
            "loss: 0.6969, acc: 0.7004\n",
            "E2E-ABSA >>> 2022-08-17 15:35:30\n",
            "loss: 0.6923, acc: 0.7036\n",
            "E2E-ABSA >>> 2022-08-17 15:35:30\n",
            ">>> val_acc: 0.6352, val_precision: 0.6352 val_recall: 0.6352, val_f1: 0.6352\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:30\n",
            "loss: 0.6730, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 15:35:31\n",
            "loss: 0.7060, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 15:35:31\n",
            "loss: 0.7006, acc: 0.6938\n",
            "E2E-ABSA >>> 2022-08-17 15:35:32\n",
            "loss: 0.6996, acc: 0.6946\n",
            "E2E-ABSA >>> 2022-08-17 15:35:32\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:33\n",
            "loss: 0.7150, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 15:35:33\n",
            "loss: 0.6972, acc: 0.7047\n",
            "E2E-ABSA >>> 2022-08-17 15:35:34\n",
            "loss: 0.6961, acc: 0.7009\n",
            "E2E-ABSA >>> 2022-08-17 15:35:34\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:34\n",
            "loss: 0.6622, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 15:35:35\n",
            "loss: 0.6572, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-08-17 15:35:35\n",
            "loss: 0.6949, acc: 0.6973\n",
            "E2E-ABSA >>> 2022-08-17 15:35:36\n",
            "loss: 0.6995, acc: 0.6959\n",
            "E2E-ABSA >>> 2022-08-17 15:35:36\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:36\n",
            "loss: 0.7140, acc: 0.7007\n",
            "E2E-ABSA >>> 2022-08-17 15:35:37\n",
            "loss: 0.7080, acc: 0.6949\n",
            "E2E-ABSA >>> 2022-08-17 15:35:38\n",
            "loss: 0.7041, acc: 0.6949\n",
            "E2E-ABSA >>> 2022-08-17 15:35:38\n",
            "loss: 0.7003, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-08-17 15:35:38\n",
            ">>> val_acc: 0.6368, val_precision: 0.6368 val_recall: 0.6368, val_f1: 0.6368\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:39\n",
            "loss: 0.6903, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:35:39\n",
            "loss: 0.6941, acc: 0.6997\n",
            "E2E-ABSA >>> 2022-08-17 15:35:40\n",
            "loss: 0.6941, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 15:35:41\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:41\n",
            "loss: 0.7226, acc: 0.6849\n",
            "E2E-ABSA >>> 2022-08-17 15:35:41\n",
            "loss: 0.6971, acc: 0.7090\n",
            "E2E-ABSA >>> 2022-08-17 15:35:42\n",
            "loss: 0.7045, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 15:35:42\n",
            "loss: 0.6992, acc: 0.6977\n",
            "E2E-ABSA >>> 2022-08-17 15:35:42\n",
            ">>> val_acc: 0.6304, val_precision: 0.6304 val_recall: 0.6304, val_f1: 0.6304\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:43\n",
            "loss: 0.7021, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 15:35:44\n",
            "loss: 0.7085, acc: 0.6945\n",
            "E2E-ABSA >>> 2022-08-17 15:35:44\n",
            "loss: 0.7000, acc: 0.6987\n",
            "E2E-ABSA >>> 2022-08-17 15:35:44\n",
            ">>> val_acc: 0.6384, val_precision: 0.6384 val_recall: 0.6384, val_f1: 0.6384\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:45\n",
            "loss: 0.7124, acc: 0.7145\n",
            "E2E-ABSA >>> 2022-08-17 15:35:45\n",
            "loss: 0.6956, acc: 0.7023\n",
            "E2E-ABSA >>> 2022-08-17 15:35:46\n",
            "loss: 0.6999, acc: 0.6998\n",
            "E2E-ABSA >>> 2022-08-17 15:35:46\n",
            "loss: 0.6973, acc: 0.7004\n",
            "E2E-ABSA >>> 2022-08-17 15:35:46\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:47\n",
            "loss: 0.7000, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-08-17 15:35:47\n",
            "loss: 0.6927, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-08-17 15:35:48\n",
            "loss: 0.6960, acc: 0.6997\n",
            "E2E-ABSA >>> 2022-08-17 15:35:48\n",
            ">>> val_acc: 0.6336, val_precision: 0.6336 val_recall: 0.6336, val_f1: 0.6336\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:49\n",
            "loss: 0.7486, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-08-17 15:35:49\n",
            "loss: 0.7096, acc: 0.6969\n",
            "E2E-ABSA >>> 2022-08-17 15:35:50\n",
            "loss: 0.7039, acc: 0.6995\n",
            "E2E-ABSA >>> 2022-08-17 15:35:50\n",
            "loss: 0.6999, acc: 0.6991\n",
            "E2E-ABSA >>> 2022-08-17 15:35:50\n",
            ">>> val_acc: 0.6544, val_precision: 0.6544 val_recall: 0.6544, val_f1: 0.6544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:51\n",
            "loss: 0.6751, acc: 0.7116\n",
            "E2E-ABSA >>> 2022-08-17 15:35:51\n",
            "loss: 0.6820, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-08-17 15:35:52\n",
            "loss: 0.6940, acc: 0.7014\n",
            "E2E-ABSA >>> 2022-08-17 15:35:52\n",
            ">>> val_acc: 0.6144, val_precision: 0.6144 val_recall: 0.6144, val_f1: 0.6144\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 15:35:52\n",
            "loss: 0.7420, acc: 0.6788\n",
            "E2E-ABSA >>> 2022-08-17 15:35:53\n",
            "loss: 0.7219, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-08-17 15:35:54\n",
            "loss: 0.7098, acc: 0.6909\n",
            "E2E-ABSA >>> 2022-08-17 15:35:54\n",
            "loss: 0.6958, acc: 0.6961\n",
            "E2E-ABSA >>> 2022-08-17 15:35:54\n",
            ">>> val_acc: 0.6368, val_precision: 0.6368 val_recall: 0.6368, val_f1: 0.6368\n",
            "E2E-ABSA >>> 2022-08-17 15:35:54\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6560, val_precision: 0.6560 val_recall: 0.6560, val_f1: 0.6560\n",
            "you can download the best model from state_dict/lstm_acl14shortdata_val_f1_0.656\n",
            ">>> test_acc: 0.6560, test_precision: 0.6560, test_recall: 0.6560, test_f1: 0.6560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **acl14shortdata** dataset on model(**LSTM**)\n"
      ],
      "metadata": {
        "id": "keat3WAkV-iU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name lstm --dataset acl14shortdata_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leo3yMMJV-nP",
        "outputId": "1c1aad61-62b5-4a02-8fa2-e85257455b65"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 19671552\n",
            "> n_trainable_params: 723303, n_nontrainable_params: 4115100\n",
            "> training arguments:\n",
            ">>> model_name: lstm\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1410f88b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lstm.LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:34\n",
            "loss: 1.0988, acc: 0.3531\n",
            "E2E-ABSA >>> 2022-08-17 15:36:35\n",
            "loss: 1.0689, acc: 0.4328\n",
            "E2E-ABSA >>> 2022-08-17 15:36:36\n",
            "loss: 1.0514, acc: 0.4608\n",
            "E2E-ABSA >>> 2022-08-17 15:36:36\n",
            ">>> val_acc: 0.4608, val_precision: 0.4608 val_recall: 0.4608, val_f1: 0.4608\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.4608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:37\n",
            "loss: 1.0004, acc: 0.5312\n",
            "E2E-ABSA >>> 2022-08-17 15:36:37\n",
            "loss: 1.0054, acc: 0.5165\n",
            "E2E-ABSA >>> 2022-08-17 15:36:38\n",
            "loss: 1.0125, acc: 0.5088\n",
            "E2E-ABSA >>> 2022-08-17 15:36:39\n",
            "loss: 1.0106, acc: 0.5126\n",
            "E2E-ABSA >>> 2022-08-17 15:36:39\n",
            ">>> val_acc: 0.4752, val_precision: 0.4752 val_recall: 0.4752, val_f1: 0.4752\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.4752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:40\n",
            "loss: 1.0134, acc: 0.5000\n",
            "E2E-ABSA >>> 2022-08-17 15:36:40\n",
            "loss: 1.0040, acc: 0.5086\n",
            "E2E-ABSA >>> 2022-08-17 15:36:41\n",
            "loss: 0.9989, acc: 0.5139\n",
            "E2E-ABSA >>> 2022-08-17 15:36:42\n",
            ">>> val_acc: 0.4752, val_precision: 0.4752 val_recall: 0.4752, val_f1: 0.4752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:42\n",
            "loss: 0.9903, acc: 0.5199\n",
            "E2E-ABSA >>> 2022-08-17 15:36:43\n",
            "loss: 0.9897, acc: 0.5187\n",
            "E2E-ABSA >>> 2022-08-17 15:36:43\n",
            "loss: 0.9873, acc: 0.5225\n",
            "E2E-ABSA >>> 2022-08-17 15:36:44\n",
            "loss: 0.9852, acc: 0.5240\n",
            "E2E-ABSA >>> 2022-08-17 15:36:44\n",
            ">>> val_acc: 0.4800, val_precision: 0.4800 val_recall: 0.4800, val_f1: 0.4800\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.48\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:45\n",
            "loss: 0.9859, acc: 0.5245\n",
            "E2E-ABSA >>> 2022-08-17 15:36:46\n",
            "loss: 0.9872, acc: 0.5179\n",
            "E2E-ABSA >>> 2022-08-17 15:36:46\n",
            "loss: 0.9818, acc: 0.5233\n",
            "E2E-ABSA >>> 2022-08-17 15:36:47\n",
            ">>> val_acc: 0.4816, val_precision: 0.4816 val_recall: 0.4816, val_f1: 0.4816\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.4816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:47\n",
            "loss: 0.9921, acc: 0.5000\n",
            "E2E-ABSA >>> 2022-08-17 15:36:48\n",
            "loss: 0.9743, acc: 0.5299\n",
            "E2E-ABSA >>> 2022-08-17 15:36:49\n",
            "loss: 0.9748, acc: 0.5224\n",
            "E2E-ABSA >>> 2022-08-17 15:36:49\n",
            "loss: 0.9719, acc: 0.5250\n",
            "E2E-ABSA >>> 2022-08-17 15:36:50\n",
            ">>> val_acc: 0.4816, val_precision: 0.4816 val_recall: 0.4816, val_f1: 0.4816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:50\n",
            "loss: 0.9760, acc: 0.5170\n",
            "E2E-ABSA >>> 2022-08-17 15:36:51\n",
            "loss: 0.9760, acc: 0.5189\n",
            "E2E-ABSA >>> 2022-08-17 15:36:52\n",
            "loss: 0.9687, acc: 0.5252\n",
            "E2E-ABSA >>> 2022-08-17 15:36:52\n",
            ">>> val_acc: 0.4768, val_precision: 0.4768 val_recall: 0.4768, val_f1: 0.4768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:53\n",
            "loss: 0.9613, acc: 0.5295\n",
            "E2E-ABSA >>> 2022-08-17 15:36:53\n",
            "loss: 0.9654, acc: 0.5294\n",
            "E2E-ABSA >>> 2022-08-17 15:36:54\n",
            "loss: 0.9604, acc: 0.5315\n",
            "E2E-ABSA >>> 2022-08-17 15:36:55\n",
            "loss: 0.9626, acc: 0.5298\n",
            "E2E-ABSA >>> 2022-08-17 15:36:55\n",
            ">>> val_acc: 0.4832, val_precision: 0.4832 val_recall: 0.4832, val_f1: 0.4832\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.4832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:56\n",
            "loss: 0.9395, acc: 0.5558\n",
            "E2E-ABSA >>> 2022-08-17 15:36:56\n",
            "loss: 0.9538, acc: 0.5394\n",
            "E2E-ABSA >>> 2022-08-17 15:36:57\n",
            "loss: 0.9594, acc: 0.5339\n",
            "E2E-ABSA >>> 2022-08-17 15:36:58\n",
            ">>> val_acc: 0.4992, val_precision: 0.4992 val_recall: 0.4992, val_f1: 0.4992\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.4992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:36:58\n",
            "loss: 0.9640, acc: 0.5332\n",
            "E2E-ABSA >>> 2022-08-17 15:36:59\n",
            "loss: 0.9550, acc: 0.5374\n",
            "E2E-ABSA >>> 2022-08-17 15:36:59\n",
            "loss: 0.9568, acc: 0.5337\n",
            "E2E-ABSA >>> 2022-08-17 15:37:00\n",
            "loss: 0.9578, acc: 0.5390\n",
            "E2E-ABSA >>> 2022-08-17 15:37:00\n",
            ">>> val_acc: 0.4832, val_precision: 0.4832 val_recall: 0.4832, val_f1: 0.4832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:01\n",
            "loss: 0.9514, acc: 0.5344\n",
            "E2E-ABSA >>> 2022-08-17 15:37:02\n",
            "loss: 0.9587, acc: 0.5410\n",
            "E2E-ABSA >>> 2022-08-17 15:37:02\n",
            "loss: 0.9540, acc: 0.5411\n",
            "E2E-ABSA >>> 2022-08-17 15:37:03\n",
            ">>> val_acc: 0.4832, val_precision: 0.4832 val_recall: 0.4832, val_f1: 0.4832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:03\n",
            "loss: 0.9435, acc: 0.5379\n",
            "E2E-ABSA >>> 2022-08-17 15:37:04\n",
            "loss: 0.9440, acc: 0.5542\n",
            "E2E-ABSA >>> 2022-08-17 15:37:05\n",
            "loss: 0.9546, acc: 0.5422\n",
            "E2E-ABSA >>> 2022-08-17 15:37:05\n",
            "loss: 0.9517, acc: 0.5494\n",
            "E2E-ABSA >>> 2022-08-17 15:37:06\n",
            ">>> val_acc: 0.5056, val_precision: 0.5056 val_recall: 0.5056, val_f1: 0.5056\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:06\n",
            "loss: 0.9550, acc: 0.5436\n",
            "E2E-ABSA >>> 2022-08-17 15:37:07\n",
            "loss: 0.9542, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-08-17 15:37:08\n",
            "loss: 0.9511, acc: 0.5519\n",
            "E2E-ABSA >>> 2022-08-17 15:37:09\n",
            ">>> val_acc: 0.5136, val_precision: 0.5136 val_recall: 0.5136, val_f1: 0.5136\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:09\n",
            "loss: 0.9245, acc: 0.5885\n",
            "E2E-ABSA >>> 2022-08-17 15:37:09\n",
            "loss: 0.9401, acc: 0.5585\n",
            "E2E-ABSA >>> 2022-08-17 15:37:10\n",
            "loss: 0.9456, acc: 0.5558\n",
            "E2E-ABSA >>> 2022-08-17 15:37:11\n",
            "loss: 0.9481, acc: 0.5525\n",
            "E2E-ABSA >>> 2022-08-17 15:37:11\n",
            ">>> val_acc: 0.5184, val_precision: 0.5184 val_recall: 0.5184, val_f1: 0.5184\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:12\n",
            "loss: 0.9534, acc: 0.5495\n",
            "E2E-ABSA >>> 2022-08-17 15:37:13\n",
            "loss: 0.9386, acc: 0.5567\n",
            "E2E-ABSA >>> 2022-08-17 15:37:13\n",
            "loss: 0.9449, acc: 0.5528\n",
            "E2E-ABSA >>> 2022-08-17 15:37:14\n",
            ">>> val_acc: 0.5136, val_precision: 0.5136 val_recall: 0.5136, val_f1: 0.5136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:14\n",
            "loss: 0.9632, acc: 0.5312\n",
            "E2E-ABSA >>> 2022-08-17 15:37:15\n",
            "loss: 0.9384, acc: 0.5641\n",
            "E2E-ABSA >>> 2022-08-17 15:37:16\n",
            "loss: 0.9397, acc: 0.5619\n",
            "E2E-ABSA >>> 2022-08-17 15:37:16\n",
            "loss: 0.9413, acc: 0.5600\n",
            "E2E-ABSA >>> 2022-08-17 15:37:17\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:17\n",
            "loss: 0.9448, acc: 0.5515\n",
            "E2E-ABSA >>> 2022-08-17 15:37:18\n",
            "loss: 0.9520, acc: 0.5525\n",
            "E2E-ABSA >>> 2022-08-17 15:37:19\n",
            "loss: 0.9469, acc: 0.5541\n",
            "E2E-ABSA >>> 2022-08-17 15:37:19\n",
            ">>> val_acc: 0.5168, val_precision: 0.5168 val_recall: 0.5168, val_f1: 0.5168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:19\n",
            "loss: 0.9881, acc: 0.5039\n",
            "E2E-ABSA >>> 2022-08-17 15:37:20\n",
            "loss: 0.9479, acc: 0.5485\n",
            "E2E-ABSA >>> 2022-08-17 15:37:21\n",
            "loss: 0.9406, acc: 0.5602\n",
            "E2E-ABSA >>> 2022-08-17 15:37:22\n",
            "loss: 0.9439, acc: 0.5552\n",
            "E2E-ABSA >>> 2022-08-17 15:37:22\n",
            ">>> val_acc: 0.5168, val_precision: 0.5168 val_recall: 0.5168, val_f1: 0.5168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:22\n",
            "loss: 0.9394, acc: 0.5537\n",
            "E2E-ABSA >>> 2022-08-17 15:37:23\n",
            "loss: 0.9362, acc: 0.5610\n",
            "E2E-ABSA >>> 2022-08-17 15:37:24\n",
            "loss: 0.9373, acc: 0.5608\n",
            "E2E-ABSA >>> 2022-08-17 15:37:25\n",
            ">>> val_acc: 0.5232, val_precision: 0.5232 val_recall: 0.5232, val_f1: 0.5232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:25\n",
            "loss: 0.9368, acc: 0.5521\n",
            "E2E-ABSA >>> 2022-08-17 15:37:25\n",
            "loss: 0.9420, acc: 0.5592\n",
            "E2E-ABSA >>> 2022-08-17 15:37:26\n",
            "loss: 0.9345, acc: 0.5601\n",
            "E2E-ABSA >>> 2022-08-17 15:37:27\n",
            "loss: 0.9375, acc: 0.5603\n",
            "E2E-ABSA >>> 2022-08-17 15:37:27\n",
            ">>> val_acc: 0.5136, val_precision: 0.5136 val_recall: 0.5136, val_f1: 0.5136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:28\n",
            "loss: 0.9501, acc: 0.5531\n",
            "E2E-ABSA >>> 2022-08-17 15:37:28\n",
            "loss: 0.9463, acc: 0.5609\n",
            "E2E-ABSA >>> 2022-08-17 15:37:29\n",
            "loss: 0.9385, acc: 0.5642\n",
            "E2E-ABSA >>> 2022-08-17 15:37:30\n",
            ">>> val_acc: 0.5152, val_precision: 0.5152 val_recall: 0.5152, val_f1: 0.5152\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:30\n",
            "loss: 0.9664, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 15:37:31\n",
            "loss: 0.9413, acc: 0.5602\n",
            "E2E-ABSA >>> 2022-08-17 15:37:31\n",
            "loss: 0.9383, acc: 0.5622\n",
            "E2E-ABSA >>> 2022-08-17 15:37:32\n",
            "loss: 0.9382, acc: 0.5629\n",
            "E2E-ABSA >>> 2022-08-17 15:37:33\n",
            ">>> val_acc: 0.5136, val_precision: 0.5136 val_recall: 0.5136, val_f1: 0.5136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:33\n",
            "loss: 0.9157, acc: 0.5826\n",
            "E2E-ABSA >>> 2022-08-17 15:37:34\n",
            "loss: 0.9420, acc: 0.5533\n",
            "E2E-ABSA >>> 2022-08-17 15:37:34\n",
            "loss: 0.9399, acc: 0.5549\n",
            "E2E-ABSA >>> 2022-08-17 15:37:35\n",
            ">>> val_acc: 0.5200, val_precision: 0.5200 val_recall: 0.5200, val_f1: 0.5200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:35\n",
            "loss: 0.8909, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 15:37:36\n",
            "loss: 0.9444, acc: 0.5505\n",
            "E2E-ABSA >>> 2022-08-17 15:37:37\n",
            "loss: 0.9400, acc: 0.5585\n",
            "E2E-ABSA >>> 2022-08-17 15:37:37\n",
            "loss: 0.9379, acc: 0.5584\n",
            "E2E-ABSA >>> 2022-08-17 15:37:38\n",
            ">>> val_acc: 0.5136, val_precision: 0.5136 val_recall: 0.5136, val_f1: 0.5136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:38\n",
            "loss: 0.9509, acc: 0.5433\n",
            "E2E-ABSA >>> 2022-08-17 15:37:39\n",
            "loss: 0.9492, acc: 0.5440\n",
            "E2E-ABSA >>> 2022-08-17 15:37:40\n",
            "loss: 0.9433, acc: 0.5551\n",
            "E2E-ABSA >>> 2022-08-17 15:37:40\n",
            "loss: 0.9362, acc: 0.5641\n",
            "E2E-ABSA >>> 2022-08-17 15:37:41\n",
            ">>> val_acc: 0.5200, val_precision: 0.5200 val_recall: 0.5200, val_f1: 0.5200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:41\n",
            "loss: 0.9443, acc: 0.5563\n",
            "E2E-ABSA >>> 2022-08-17 15:37:42\n",
            "loss: 0.9316, acc: 0.5687\n",
            "E2E-ABSA >>> 2022-08-17 15:37:43\n",
            "loss: 0.9341, acc: 0.5706\n",
            "E2E-ABSA >>> 2022-08-17 15:37:43\n",
            ">>> val_acc: 0.5200, val_precision: 0.5200 val_recall: 0.5200, val_f1: 0.5200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:44\n",
            "loss: 0.9506, acc: 0.5521\n",
            "E2E-ABSA >>> 2022-08-17 15:37:44\n",
            "loss: 0.9376, acc: 0.5642\n",
            "E2E-ABSA >>> 2022-08-17 15:37:45\n",
            "loss: 0.9356, acc: 0.5648\n",
            "E2E-ABSA >>> 2022-08-17 15:37:46\n",
            "loss: 0.9336, acc: 0.5659\n",
            "E2E-ABSA >>> 2022-08-17 15:37:46\n",
            ">>> val_acc: 0.5296, val_precision: 0.5296 val_recall: 0.5296, val_f1: 0.5296\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5296\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:47\n",
            "loss: 0.9352, acc: 0.5677\n",
            "E2E-ABSA >>> 2022-08-17 15:37:47\n",
            "loss: 0.9291, acc: 0.5705\n",
            "E2E-ABSA >>> 2022-08-17 15:37:48\n",
            "loss: 0.9337, acc: 0.5684\n",
            "E2E-ABSA >>> 2022-08-17 15:37:49\n",
            ">>> val_acc: 0.5216, val_precision: 0.5216 val_recall: 0.5216, val_f1: 0.5216\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:49\n",
            "loss: 0.9342, acc: 0.5668\n",
            "E2E-ABSA >>> 2022-08-17 15:37:50\n",
            "loss: 0.9271, acc: 0.5764\n",
            "E2E-ABSA >>> 2022-08-17 15:37:50\n",
            "loss: 0.9295, acc: 0.5704\n",
            "E2E-ABSA >>> 2022-08-17 15:37:51\n",
            "loss: 0.9333, acc: 0.5660\n",
            "E2E-ABSA >>> 2022-08-17 15:37:51\n",
            ">>> val_acc: 0.5248, val_precision: 0.5248 val_recall: 0.5248, val_f1: 0.5248\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:52\n",
            "loss: 0.9220, acc: 0.5740\n",
            "E2E-ABSA >>> 2022-08-17 15:37:53\n",
            "loss: 0.9305, acc: 0.5651\n",
            "E2E-ABSA >>> 2022-08-17 15:37:53\n",
            "loss: 0.9278, acc: 0.5685\n",
            "E2E-ABSA >>> 2022-08-17 15:37:54\n",
            ">>> val_acc: 0.5248, val_precision: 0.5248 val_recall: 0.5248, val_f1: 0.5248\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:54\n",
            "loss: 0.9484, acc: 0.5594\n",
            "E2E-ABSA >>> 2022-08-17 15:37:55\n",
            "loss: 0.9554, acc: 0.5540\n",
            "E2E-ABSA >>> 2022-08-17 15:37:56\n",
            "loss: 0.9445, acc: 0.5602\n",
            "E2E-ABSA >>> 2022-08-17 15:37:56\n",
            "loss: 0.9329, acc: 0.5669\n",
            "E2E-ABSA >>> 2022-08-17 15:37:56\n",
            ">>> val_acc: 0.5248, val_precision: 0.5248 val_recall: 0.5248, val_f1: 0.5248\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:57\n",
            "loss: 0.9339, acc: 0.5589\n",
            "E2E-ABSA >>> 2022-08-17 15:37:58\n",
            "loss: 0.9333, acc: 0.5638\n",
            "E2E-ABSA >>> 2022-08-17 15:37:58\n",
            "loss: 0.9386, acc: 0.5584\n",
            "E2E-ABSA >>> 2022-08-17 15:37:59\n",
            ">>> val_acc: 0.5184, val_precision: 0.5184 val_recall: 0.5184, val_f1: 0.5184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:37:59\n",
            "loss: 0.9473, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-08-17 15:38:00\n",
            "loss: 0.9301, acc: 0.5685\n",
            "E2E-ABSA >>> 2022-08-17 15:38:01\n",
            "loss: 0.9323, acc: 0.5723\n",
            "E2E-ABSA >>> 2022-08-17 15:38:01\n",
            "loss: 0.9299, acc: 0.5718\n",
            "E2E-ABSA >>> 2022-08-17 15:38:02\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:02\n",
            "loss: 0.9152, acc: 0.5930\n",
            "E2E-ABSA >>> 2022-08-17 15:38:03\n",
            "loss: 0.9350, acc: 0.5666\n",
            "E2E-ABSA >>> 2022-08-17 15:38:04\n",
            "loss: 0.9240, acc: 0.5744\n",
            "E2E-ABSA >>> 2022-08-17 15:38:04\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:04\n",
            "loss: 0.9412, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-08-17 15:38:05\n",
            "loss: 0.9277, acc: 0.5701\n",
            "E2E-ABSA >>> 2022-08-17 15:38:06\n",
            "loss: 0.9305, acc: 0.5679\n",
            "E2E-ABSA >>> 2022-08-17 15:38:07\n",
            "loss: 0.9293, acc: 0.5697\n",
            "E2E-ABSA >>> 2022-08-17 15:38:07\n",
            ">>> val_acc: 0.5232, val_precision: 0.5232 val_recall: 0.5232, val_f1: 0.5232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:07\n",
            "loss: 0.9432, acc: 0.5570\n",
            "E2E-ABSA >>> 2022-08-17 15:38:08\n",
            "loss: 0.9344, acc: 0.5660\n",
            "E2E-ABSA >>> 2022-08-17 15:38:09\n",
            "loss: 0.9298, acc: 0.5683\n",
            "E2E-ABSA >>> 2022-08-17 15:38:10\n",
            ">>> val_acc: 0.5280, val_precision: 0.5280 val_recall: 0.5280, val_f1: 0.5280\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:10\n",
            "loss: 0.9147, acc: 0.5826\n",
            "E2E-ABSA >>> 2022-08-17 15:38:11\n",
            "loss: 0.9251, acc: 0.5659\n",
            "E2E-ABSA >>> 2022-08-17 15:38:11\n",
            "loss: 0.9299, acc: 0.5666\n",
            "E2E-ABSA >>> 2022-08-17 15:38:12\n",
            "loss: 0.9317, acc: 0.5663\n",
            "E2E-ABSA >>> 2022-08-17 15:38:12\n",
            ">>> val_acc: 0.5200, val_precision: 0.5200 val_recall: 0.5200, val_f1: 0.5200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:13\n",
            "loss: 0.9274, acc: 0.5617\n",
            "E2E-ABSA >>> 2022-08-17 15:38:14\n",
            "loss: 0.9194, acc: 0.5785\n",
            "E2E-ABSA >>> 2022-08-17 15:38:14\n",
            "loss: 0.9226, acc: 0.5745\n",
            "E2E-ABSA >>> 2022-08-17 15:38:15\n",
            ">>> val_acc: 0.5280, val_precision: 0.5280 val_recall: 0.5280, val_f1: 0.5280\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:15\n",
            "loss: 0.9320, acc: 0.5677\n",
            "E2E-ABSA >>> 2022-08-17 15:38:16\n",
            "loss: 0.9332, acc: 0.5660\n",
            "E2E-ABSA >>> 2022-08-17 15:38:17\n",
            "loss: 0.9307, acc: 0.5706\n",
            "E2E-ABSA >>> 2022-08-17 15:38:17\n",
            "loss: 0.9253, acc: 0.5733\n",
            "E2E-ABSA >>> 2022-08-17 15:38:18\n",
            ">>> val_acc: 0.5280, val_precision: 0.5280 val_recall: 0.5280, val_f1: 0.5280\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:18\n",
            "loss: 0.9174, acc: 0.5747\n",
            "E2E-ABSA >>> 2022-08-17 15:38:19\n",
            "loss: 0.9295, acc: 0.5654\n",
            "E2E-ABSA >>> 2022-08-17 15:38:20\n",
            "loss: 0.9231, acc: 0.5731\n",
            "E2E-ABSA >>> 2022-08-17 15:38:20\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:20\n",
            "loss: 0.9244, acc: 0.5875\n",
            "E2E-ABSA >>> 2022-08-17 15:38:21\n",
            "loss: 0.9206, acc: 0.5740\n",
            "E2E-ABSA >>> 2022-08-17 15:38:22\n",
            "loss: 0.9202, acc: 0.5705\n",
            "E2E-ABSA >>> 2022-08-17 15:38:23\n",
            "loss: 0.9211, acc: 0.5732\n",
            "E2E-ABSA >>> 2022-08-17 15:38:23\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.536\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:23\n",
            "loss: 0.9402, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-08-17 15:38:24\n",
            "loss: 0.9230, acc: 0.5655\n",
            "E2E-ABSA >>> 2022-08-17 15:38:25\n",
            "loss: 0.9164, acc: 0.5756\n",
            "E2E-ABSA >>> 2022-08-17 15:38:26\n",
            ">>> val_acc: 0.5408, val_precision: 0.5408 val_recall: 0.5408, val_f1: 0.5408\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5408\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:26\n",
            "loss: 0.8769, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 15:38:26\n",
            "loss: 0.8885, acc: 0.5964\n",
            "E2E-ABSA >>> 2022-08-17 15:38:27\n",
            "loss: 0.9063, acc: 0.5799\n",
            "E2E-ABSA >>> 2022-08-17 15:38:28\n",
            "loss: 0.9045, acc: 0.5791\n",
            "E2E-ABSA >>> 2022-08-17 15:38:28\n",
            ">>> val_acc: 0.5312, val_precision: 0.5312 val_recall: 0.5312, val_f1: 0.5312\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:29\n",
            "loss: 0.9040, acc: 0.5918\n",
            "E2E-ABSA >>> 2022-08-17 15:38:29\n",
            "loss: 0.9078, acc: 0.5755\n",
            "E2E-ABSA >>> 2022-08-17 15:38:30\n",
            "loss: 0.9076, acc: 0.5769\n",
            "E2E-ABSA >>> 2022-08-17 15:38:31\n",
            ">>> val_acc: 0.5296, val_precision: 0.5296 val_recall: 0.5296, val_f1: 0.5296\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:31\n",
            "loss: 0.9232, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 15:38:32\n",
            "loss: 0.8863, acc: 0.5971\n",
            "E2E-ABSA >>> 2022-08-17 15:38:32\n",
            "loss: 0.8975, acc: 0.5899\n",
            "E2E-ABSA >>> 2022-08-17 15:38:33\n",
            "loss: 0.9004, acc: 0.5855\n",
            "E2E-ABSA >>> 2022-08-17 15:38:33\n",
            ">>> val_acc: 0.5440, val_precision: 0.5440 val_recall: 0.5440, val_f1: 0.5440\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:34\n",
            "loss: 0.9002, acc: 0.5875\n",
            "E2E-ABSA >>> 2022-08-17 15:38:35\n",
            "loss: 0.8872, acc: 0.6012\n",
            "E2E-ABSA >>> 2022-08-17 15:38:35\n",
            "loss: 0.8955, acc: 0.5925\n",
            "E2E-ABSA >>> 2022-08-17 15:38:36\n",
            ">>> val_acc: 0.5536, val_precision: 0.5536 val_recall: 0.5536, val_f1: 0.5536\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5536\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:36\n",
            "loss: 0.8889, acc: 0.5859\n",
            "E2E-ABSA >>> 2022-08-17 15:38:37\n",
            "loss: 0.8750, acc: 0.6024\n",
            "E2E-ABSA >>> 2022-08-17 15:38:38\n",
            "loss: 0.8918, acc: 0.5913\n",
            "E2E-ABSA >>> 2022-08-17 15:38:38\n",
            "loss: 0.8904, acc: 0.5899\n",
            "E2E-ABSA >>> 2022-08-17 15:38:39\n",
            ">>> val_acc: 0.5504, val_precision: 0.5504 val_recall: 0.5504, val_f1: 0.5504\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:39\n",
            "loss: 0.8647, acc: 0.6038\n",
            "E2E-ABSA >>> 2022-08-17 15:38:40\n",
            "loss: 0.8763, acc: 0.5974\n",
            "E2E-ABSA >>> 2022-08-17 15:38:41\n",
            "loss: 0.8771, acc: 0.6064\n",
            "E2E-ABSA >>> 2022-08-17 15:38:41\n",
            ">>> val_acc: 0.5568, val_precision: 0.5568 val_recall: 0.5568, val_f1: 0.5568\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5568\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:42\n",
            "loss: 0.8801, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 15:38:42\n",
            "loss: 0.8889, acc: 0.5835\n",
            "E2E-ABSA >>> 2022-08-17 15:38:43\n",
            "loss: 0.8800, acc: 0.5925\n",
            "E2E-ABSA >>> 2022-08-17 15:38:44\n",
            "loss: 0.8777, acc: 0.5954\n",
            "E2E-ABSA >>> 2022-08-17 15:38:44\n",
            ">>> val_acc: 0.5568, val_precision: 0.5568 val_recall: 0.5568, val_f1: 0.5568\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:44\n",
            "loss: 0.8675, acc: 0.5817\n",
            "E2E-ABSA >>> 2022-08-17 15:38:45\n",
            "loss: 0.8565, acc: 0.6032\n",
            "E2E-ABSA >>> 2022-08-17 15:38:46\n",
            "loss: 0.8669, acc: 0.6071\n",
            "E2E-ABSA >>> 2022-08-17 15:38:47\n",
            "loss: 0.8698, acc: 0.6041\n",
            "E2E-ABSA >>> 2022-08-17 15:38:47\n",
            ">>> val_acc: 0.5520, val_precision: 0.5520 val_recall: 0.5520, val_f1: 0.5520\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:47\n",
            "loss: 0.8713, acc: 0.6006\n",
            "E2E-ABSA >>> 2022-08-17 15:38:48\n",
            "loss: 0.8724, acc: 0.5975\n",
            "E2E-ABSA >>> 2022-08-17 15:38:49\n",
            "loss: 0.8702, acc: 0.6021\n",
            "E2E-ABSA >>> 2022-08-17 15:38:49\n",
            ">>> val_acc: 0.5632, val_precision: 0.5632 val_recall: 0.5632, val_f1: 0.5632\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5632\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:50\n",
            "loss: 0.8651, acc: 0.6133\n",
            "E2E-ABSA >>> 2022-08-17 15:38:50\n",
            "loss: 0.8647, acc: 0.6052\n",
            "E2E-ABSA >>> 2022-08-17 15:38:51\n",
            "loss: 0.8644, acc: 0.6064\n",
            "E2E-ABSA >>> 2022-08-17 15:38:52\n",
            "loss: 0.8627, acc: 0.6088\n",
            "E2E-ABSA >>> 2022-08-17 15:38:52\n",
            ">>> val_acc: 0.5584, val_precision: 0.5584 val_recall: 0.5584, val_f1: 0.5584\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:53\n",
            "loss: 0.8686, acc: 0.6009\n",
            "E2E-ABSA >>> 2022-08-17 15:38:53\n",
            "loss: 0.8590, acc: 0.6055\n",
            "E2E-ABSA >>> 2022-08-17 15:38:54\n",
            "loss: 0.8574, acc: 0.6115\n",
            "E2E-ABSA >>> 2022-08-17 15:38:55\n",
            ">>> val_acc: 0.5696, val_precision: 0.5696 val_recall: 0.5696, val_f1: 0.5696\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5696\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:55\n",
            "loss: 0.8747, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 15:38:56\n",
            "loss: 0.8445, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 15:38:56\n",
            "loss: 0.8541, acc: 0.6124\n",
            "E2E-ABSA >>> 2022-08-17 15:38:57\n",
            "loss: 0.8601, acc: 0.6066\n",
            "E2E-ABSA >>> 2022-08-17 15:38:57\n",
            ">>> val_acc: 0.5520, val_precision: 0.5520 val_recall: 0.5520, val_f1: 0.5520\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:38:58\n",
            "loss: 0.8579, acc: 0.6196\n",
            "E2E-ABSA >>> 2022-08-17 15:38:59\n",
            "loss: 0.8601, acc: 0.6156\n",
            "E2E-ABSA >>> 2022-08-17 15:38:59\n",
            "loss: 0.8530, acc: 0.6158\n",
            "E2E-ABSA >>> 2022-08-17 15:39:00\n",
            ">>> val_acc: 0.5712, val_precision: 0.5712 val_recall: 0.5712, val_f1: 0.5712\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:00\n",
            "loss: 0.8517, acc: 0.5969\n",
            "E2E-ABSA >>> 2022-08-17 15:39:01\n",
            "loss: 0.8496, acc: 0.6138\n",
            "E2E-ABSA >>> 2022-08-17 15:39:02\n",
            "loss: 0.8516, acc: 0.6135\n",
            "E2E-ABSA >>> 2022-08-17 15:39:02\n",
            "loss: 0.8549, acc: 0.6138\n",
            "E2E-ABSA >>> 2022-08-17 15:39:02\n",
            ">>> val_acc: 0.5472, val_precision: 0.5472 val_recall: 0.5472, val_f1: 0.5472\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:03\n",
            "loss: 0.8364, acc: 0.6193\n",
            "E2E-ABSA >>> 2022-08-17 15:39:04\n",
            "loss: 0.8452, acc: 0.6230\n",
            "E2E-ABSA >>> 2022-08-17 15:39:04\n",
            "loss: 0.8503, acc: 0.6176\n",
            "E2E-ABSA >>> 2022-08-17 15:39:05\n",
            ">>> val_acc: 0.5600, val_precision: 0.5600 val_recall: 0.5600, val_f1: 0.5600\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:05\n",
            "loss: 0.8475, acc: 0.6128\n",
            "E2E-ABSA >>> 2022-08-17 15:39:06\n",
            "loss: 0.8636, acc: 0.6085\n",
            "E2E-ABSA >>> 2022-08-17 15:39:07\n",
            "loss: 0.8573, acc: 0.6120\n",
            "E2E-ABSA >>> 2022-08-17 15:39:07\n",
            "loss: 0.8466, acc: 0.6190\n",
            "E2E-ABSA >>> 2022-08-17 15:39:08\n",
            ">>> val_acc: 0.5792, val_precision: 0.5792 val_recall: 0.5792, val_f1: 0.5792\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5792\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:08\n",
            "loss: 0.8378, acc: 0.6310\n",
            "E2E-ABSA >>> 2022-08-17 15:39:09\n",
            "loss: 0.8319, acc: 0.6304\n",
            "E2E-ABSA >>> 2022-08-17 15:39:10\n",
            "loss: 0.8358, acc: 0.6285\n",
            "E2E-ABSA >>> 2022-08-17 15:39:10\n",
            ">>> val_acc: 0.5728, val_precision: 0.5728 val_recall: 0.5728, val_f1: 0.5728\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:11\n",
            "loss: 0.8412, acc: 0.6230\n",
            "E2E-ABSA >>> 2022-08-17 15:39:11\n",
            "loss: 0.8304, acc: 0.6321\n",
            "E2E-ABSA >>> 2022-08-17 15:39:12\n",
            "loss: 0.8414, acc: 0.6218\n",
            "E2E-ABSA >>> 2022-08-17 15:39:13\n",
            "loss: 0.8438, acc: 0.6197\n",
            "E2E-ABSA >>> 2022-08-17 15:39:13\n",
            ">>> val_acc: 0.5776, val_precision: 0.5776 val_recall: 0.5776, val_f1: 0.5776\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:14\n",
            "loss: 0.8150, acc: 0.6453\n",
            "E2E-ABSA >>> 2022-08-17 15:39:14\n",
            "loss: 0.8313, acc: 0.6312\n",
            "E2E-ABSA >>> 2022-08-17 15:39:15\n",
            "loss: 0.8343, acc: 0.6275\n",
            "E2E-ABSA >>> 2022-08-17 15:39:16\n",
            ">>> val_acc: 0.5728, val_precision: 0.5728 val_recall: 0.5728, val_f1: 0.5728\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:16\n",
            "loss: 0.7937, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 15:39:16\n",
            "loss: 0.8267, acc: 0.6201\n",
            "E2E-ABSA >>> 2022-08-17 15:39:17\n",
            "loss: 0.8402, acc: 0.6212\n",
            "E2E-ABSA >>> 2022-08-17 15:39:18\n",
            "loss: 0.8368, acc: 0.6239\n",
            "E2E-ABSA >>> 2022-08-17 15:39:18\n",
            ">>> val_acc: 0.5728, val_precision: 0.5728 val_recall: 0.5728, val_f1: 0.5728\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:19\n",
            "loss: 0.8172, acc: 0.6382\n",
            "E2E-ABSA >>> 2022-08-17 15:39:19\n",
            "loss: 0.8264, acc: 0.6328\n",
            "E2E-ABSA >>> 2022-08-17 15:39:20\n",
            "loss: 0.8295, acc: 0.6282\n",
            "E2E-ABSA >>> 2022-08-17 15:39:21\n",
            ">>> val_acc: 0.5744, val_precision: 0.5744 val_recall: 0.5744, val_f1: 0.5744\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:21\n",
            "loss: 0.8580, acc: 0.6016\n",
            "E2E-ABSA >>> 2022-08-17 15:39:22\n",
            "loss: 0.8417, acc: 0.6215\n",
            "E2E-ABSA >>> 2022-08-17 15:39:22\n",
            "loss: 0.8400, acc: 0.6211\n",
            "E2E-ABSA >>> 2022-08-17 15:39:23\n",
            "loss: 0.8344, acc: 0.6267\n",
            "E2E-ABSA >>> 2022-08-17 15:39:23\n",
            ">>> val_acc: 0.5872, val_precision: 0.5872 val_recall: 0.5872, val_f1: 0.5872\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:24\n",
            "loss: 0.8515, acc: 0.6189\n",
            "E2E-ABSA >>> 2022-08-17 15:39:25\n",
            "loss: 0.8399, acc: 0.6210\n",
            "E2E-ABSA >>> 2022-08-17 15:39:25\n",
            "loss: 0.8356, acc: 0.6259\n",
            "E2E-ABSA >>> 2022-08-17 15:39:26\n",
            ">>> val_acc: 0.5840, val_precision: 0.5840 val_recall: 0.5840, val_f1: 0.5840\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:26\n",
            "loss: 0.8400, acc: 0.6188\n",
            "E2E-ABSA >>> 2022-08-17 15:39:27\n",
            "loss: 0.8264, acc: 0.6427\n",
            "E2E-ABSA >>> 2022-08-17 15:39:28\n",
            "loss: 0.8200, acc: 0.6409\n",
            "E2E-ABSA >>> 2022-08-17 15:39:28\n",
            "loss: 0.8257, acc: 0.6361\n",
            "E2E-ABSA >>> 2022-08-17 15:39:29\n",
            ">>> val_acc: 0.5824, val_precision: 0.5824 val_recall: 0.5824, val_f1: 0.5824\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:29\n",
            "loss: 0.7983, acc: 0.6434\n",
            "E2E-ABSA >>> 2022-08-17 15:39:30\n",
            "loss: 0.8175, acc: 0.6339\n",
            "E2E-ABSA >>> 2022-08-17 15:39:30\n",
            "loss: 0.8192, acc: 0.6306\n",
            "E2E-ABSA >>> 2022-08-17 15:39:31\n",
            ">>> val_acc: 0.5888, val_precision: 0.5888 val_recall: 0.5888, val_f1: 0.5888\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.5888\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:31\n",
            "loss: 0.7853, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-08-17 15:39:32\n",
            "loss: 0.8323, acc: 0.6196\n",
            "E2E-ABSA >>> 2022-08-17 15:39:33\n",
            "loss: 0.8233, acc: 0.6276\n",
            "E2E-ABSA >>> 2022-08-17 15:39:34\n",
            "loss: 0.8246, acc: 0.6286\n",
            "E2E-ABSA >>> 2022-08-17 15:39:34\n",
            ">>> val_acc: 0.5872, val_precision: 0.5872 val_recall: 0.5872, val_f1: 0.5872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:34\n",
            "loss: 0.7967, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-08-17 15:39:35\n",
            "loss: 0.8222, acc: 0.6319\n",
            "E2E-ABSA >>> 2022-08-17 15:39:36\n",
            "loss: 0.8267, acc: 0.6267\n",
            "E2E-ABSA >>> 2022-08-17 15:39:37\n",
            ">>> val_acc: 0.5808, val_precision: 0.5808 val_recall: 0.5808, val_f1: 0.5808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:37\n",
            "loss: 0.8498, acc: 0.6146\n",
            "E2E-ABSA >>> 2022-08-17 15:39:37\n",
            "loss: 0.8120, acc: 0.6367\n",
            "E2E-ABSA >>> 2022-08-17 15:39:38\n",
            "loss: 0.8263, acc: 0.6297\n",
            "E2E-ABSA >>> 2022-08-17 15:39:39\n",
            "loss: 0.8237, acc: 0.6314\n",
            "E2E-ABSA >>> 2022-08-17 15:39:39\n",
            ">>> val_acc: 0.5872, val_precision: 0.5872 val_recall: 0.5872, val_f1: 0.5872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:40\n",
            "loss: 0.8265, acc: 0.6135\n",
            "E2E-ABSA >>> 2022-08-17 15:39:40\n",
            "loss: 0.8149, acc: 0.6285\n",
            "E2E-ABSA >>> 2022-08-17 15:39:41\n",
            "loss: 0.8117, acc: 0.6334\n",
            "E2E-ABSA >>> 2022-08-17 15:39:42\n",
            ">>> val_acc: 0.5856, val_precision: 0.5856 val_recall: 0.5856, val_f1: 0.5856\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:42\n",
            "loss: 0.8959, acc: 0.5703\n",
            "E2E-ABSA >>> 2022-08-17 15:39:43\n",
            "loss: 0.8161, acc: 0.6360\n",
            "E2E-ABSA >>> 2022-08-17 15:39:43\n",
            "loss: 0.8180, acc: 0.6370\n",
            "E2E-ABSA >>> 2022-08-17 15:39:44\n",
            "loss: 0.8176, acc: 0.6351\n",
            "E2E-ABSA >>> 2022-08-17 15:39:44\n",
            ">>> val_acc: 0.6016, val_precision: 0.6016 val_recall: 0.6016, val_f1: 0.6016\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.6016\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:45\n",
            "loss: 0.8357, acc: 0.6239\n",
            "E2E-ABSA >>> 2022-08-17 15:39:45\n",
            "loss: 0.8336, acc: 0.6246\n",
            "E2E-ABSA >>> 2022-08-17 15:39:46\n",
            "loss: 0.8185, acc: 0.6365\n",
            "E2E-ABSA >>> 2022-08-17 15:39:47\n",
            ">>> val_acc: 0.5888, val_precision: 0.5888 val_recall: 0.5888, val_f1: 0.5888\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:47\n",
            "loss: 0.7545, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 15:39:48\n",
            "loss: 0.8046, acc: 0.6478\n",
            "E2E-ABSA >>> 2022-08-17 15:39:48\n",
            "loss: 0.8193, acc: 0.6348\n",
            "E2E-ABSA >>> 2022-08-17 15:39:49\n",
            "loss: 0.8123, acc: 0.6396\n",
            "E2E-ABSA >>> 2022-08-17 15:39:50\n",
            ">>> val_acc: 0.5888, val_precision: 0.5888 val_recall: 0.5888, val_f1: 0.5888\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:50\n",
            "loss: 0.8195, acc: 0.6226\n",
            "E2E-ABSA >>> 2022-08-17 15:39:51\n",
            "loss: 0.8083, acc: 0.6349\n",
            "E2E-ABSA >>> 2022-08-17 15:39:51\n",
            "loss: 0.8123, acc: 0.6322\n",
            "E2E-ABSA >>> 2022-08-17 15:39:52\n",
            "loss: 0.8103, acc: 0.6356\n",
            "E2E-ABSA >>> 2022-08-17 15:39:52\n",
            ">>> val_acc: 0.5920, val_precision: 0.5920 val_recall: 0.5920, val_f1: 0.5920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:53\n",
            "loss: 0.7990, acc: 0.6506\n",
            "E2E-ABSA >>> 2022-08-17 15:39:54\n",
            "loss: 0.8121, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-08-17 15:39:54\n",
            "loss: 0.8088, acc: 0.6429\n",
            "E2E-ABSA >>> 2022-08-17 15:39:55\n",
            ">>> val_acc: 0.6064, val_precision: 0.6064 val_recall: 0.6064, val_f1: 0.6064\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.6064\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:55\n",
            "loss: 0.8294, acc: 0.6276\n",
            "E2E-ABSA >>> 2022-08-17 15:39:56\n",
            "loss: 0.8072, acc: 0.6486\n",
            "E2E-ABSA >>> 2022-08-17 15:39:57\n",
            "loss: 0.8150, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 15:39:57\n",
            "loss: 0.8107, acc: 0.6399\n",
            "E2E-ABSA >>> 2022-08-17 15:39:57\n",
            ">>> val_acc: 0.6016, val_precision: 0.6016 val_recall: 0.6016, val_f1: 0.6016\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 15:39:58\n",
            "loss: 0.8109, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 15:39:59\n",
            "loss: 0.8126, acc: 0.6448\n",
            "E2E-ABSA >>> 2022-08-17 15:40:00\n",
            "loss: 0.8149, acc: 0.6375\n",
            "E2E-ABSA >>> 2022-08-17 15:40:00\n",
            ">>> val_acc: 0.6000, val_precision: 0.6000 val_recall: 0.6000, val_f1: 0.6000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:00\n",
            "loss: 0.8049, acc: 0.6491\n",
            "E2E-ABSA >>> 2022-08-17 15:40:01\n",
            "loss: 0.7941, acc: 0.6519\n",
            "E2E-ABSA >>> 2022-08-17 15:40:02\n",
            "loss: 0.8116, acc: 0.6383\n",
            "E2E-ABSA >>> 2022-08-17 15:40:02\n",
            "loss: 0.8099, acc: 0.6386\n",
            "E2E-ABSA >>> 2022-08-17 15:40:03\n",
            ">>> val_acc: 0.5936, val_precision: 0.5936 val_recall: 0.5936, val_f1: 0.5936\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:03\n",
            "loss: 0.7998, acc: 0.6352\n",
            "E2E-ABSA >>> 2022-08-17 15:40:04\n",
            "loss: 0.8026, acc: 0.6410\n",
            "E2E-ABSA >>> 2022-08-17 15:40:05\n",
            "loss: 0.8028, acc: 0.6440\n",
            "E2E-ABSA >>> 2022-08-17 15:40:05\n",
            ">>> val_acc: 0.5952, val_precision: 0.5952 val_recall: 0.5952, val_f1: 0.5952\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:06\n",
            "loss: 0.8419, acc: 0.6109\n",
            "E2E-ABSA >>> 2022-08-17 15:40:06\n",
            "loss: 0.8133, acc: 0.6375\n",
            "E2E-ABSA >>> 2022-08-17 15:40:07\n",
            "loss: 0.8137, acc: 0.6370\n",
            "E2E-ABSA >>> 2022-08-17 15:40:08\n",
            "loss: 0.8058, acc: 0.6426\n",
            "E2E-ABSA >>> 2022-08-17 15:40:08\n",
            ">>> val_acc: 0.6032, val_precision: 0.6032 val_recall: 0.6032, val_f1: 0.6032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:09\n",
            "loss: 0.8069, acc: 0.6506\n",
            "E2E-ABSA >>> 2022-08-17 15:40:09\n",
            "loss: 0.8023, acc: 0.6513\n",
            "E2E-ABSA >>> 2022-08-17 15:40:10\n",
            "loss: 0.7971, acc: 0.6495\n",
            "E2E-ABSA >>> 2022-08-17 15:40:10\n",
            ">>> val_acc: 0.5888, val_precision: 0.5888 val_recall: 0.5888, val_f1: 0.5888\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:11\n",
            "loss: 0.8324, acc: 0.6319\n",
            "E2E-ABSA >>> 2022-08-17 15:40:11\n",
            "loss: 0.8308, acc: 0.6241\n",
            "E2E-ABSA >>> 2022-08-17 15:40:12\n",
            "loss: 0.8156, acc: 0.6298\n",
            "E2E-ABSA >>> 2022-08-17 15:40:13\n",
            "loss: 0.8043, acc: 0.6349\n",
            "E2E-ABSA >>> 2022-08-17 15:40:13\n",
            ">>> val_acc: 0.6080, val_precision: 0.6080 val_recall: 0.6080, val_f1: 0.6080\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:14\n",
            "loss: 0.7783, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-08-17 15:40:14\n",
            "loss: 0.7952, acc: 0.6549\n",
            "E2E-ABSA >>> 2022-08-17 15:40:15\n",
            "loss: 0.8003, acc: 0.6481\n",
            "E2E-ABSA >>> 2022-08-17 15:40:16\n",
            ">>> val_acc: 0.6096, val_precision: 0.6096 val_recall: 0.6096, val_f1: 0.6096\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.6096\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:16\n",
            "loss: 0.8329, acc: 0.6191\n",
            "E2E-ABSA >>> 2022-08-17 15:40:17\n",
            "loss: 0.7866, acc: 0.6600\n",
            "E2E-ABSA >>> 2022-08-17 15:40:17\n",
            "loss: 0.7906, acc: 0.6495\n",
            "E2E-ABSA >>> 2022-08-17 15:40:18\n",
            "loss: 0.7919, acc: 0.6497\n",
            "E2E-ABSA >>> 2022-08-17 15:40:18\n",
            ">>> val_acc: 0.6128, val_precision: 0.6128 val_recall: 0.6128, val_f1: 0.6128\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.6128\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:19\n",
            "loss: 0.7851, acc: 0.6555\n",
            "E2E-ABSA >>> 2022-08-17 15:40:20\n",
            "loss: 0.7814, acc: 0.6535\n",
            "E2E-ABSA >>> 2022-08-17 15:40:20\n",
            "loss: 0.7954, acc: 0.6482\n",
            "E2E-ABSA >>> 2022-08-17 15:40:21\n",
            ">>> val_acc: 0.5984, val_precision: 0.5984 val_recall: 0.5984, val_f1: 0.5984\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:21\n",
            "loss: 0.8119, acc: 0.6272\n",
            "E2E-ABSA >>> 2022-08-17 15:40:22\n",
            "loss: 0.7984, acc: 0.6445\n",
            "E2E-ABSA >>> 2022-08-17 15:40:23\n",
            "loss: 0.7922, acc: 0.6472\n",
            "E2E-ABSA >>> 2022-08-17 15:40:23\n",
            "loss: 0.7941, acc: 0.6475\n",
            "E2E-ABSA >>> 2022-08-17 15:40:24\n",
            ">>> val_acc: 0.6144, val_precision: 0.6144 val_recall: 0.6144, val_f1: 0.6144\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.6144\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:24\n",
            "loss: 0.8073, acc: 0.6513\n",
            "E2E-ABSA >>> 2022-08-17 15:40:25\n",
            "loss: 0.8035, acc: 0.6442\n",
            "E2E-ABSA >>> 2022-08-17 15:40:26\n",
            "loss: 0.7933, acc: 0.6495\n",
            "E2E-ABSA >>> 2022-08-17 15:40:26\n",
            ">>> val_acc: 0.6128, val_precision: 0.6128 val_recall: 0.6128, val_f1: 0.6128\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:27\n",
            "loss: 0.7818, acc: 0.6432\n",
            "E2E-ABSA >>> 2022-08-17 15:40:27\n",
            "loss: 0.7836, acc: 0.6502\n",
            "E2E-ABSA >>> 2022-08-17 15:40:28\n",
            "loss: 0.7915, acc: 0.6470\n",
            "E2E-ABSA >>> 2022-08-17 15:40:29\n",
            "loss: 0.7882, acc: 0.6495\n",
            "E2E-ABSA >>> 2022-08-17 15:40:29\n",
            ">>> val_acc: 0.6160, val_precision: 0.6160 val_recall: 0.6160, val_f1: 0.6160\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.616\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:30\n",
            "loss: 0.7864, acc: 0.6424\n",
            "E2E-ABSA >>> 2022-08-17 15:40:30\n",
            "loss: 0.7800, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-08-17 15:40:31\n",
            "loss: 0.7903, acc: 0.6441\n",
            "E2E-ABSA >>> 2022-08-17 15:40:32\n",
            ">>> val_acc: 0.6096, val_precision: 0.6096 val_recall: 0.6096, val_f1: 0.6096\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:32\n",
            "loss: 0.7553, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 15:40:32\n",
            "loss: 0.7779, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-08-17 15:40:33\n",
            "loss: 0.7840, acc: 0.6585\n",
            "E2E-ABSA >>> 2022-08-17 15:40:34\n",
            "loss: 0.7836, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 15:40:34\n",
            ">>> val_acc: 0.6208, val_precision: 0.6208 val_recall: 0.6208, val_f1: 0.6208\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.6208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:35\n",
            "loss: 0.7958, acc: 0.6425\n",
            "E2E-ABSA >>> 2022-08-17 15:40:35\n",
            "loss: 0.7928, acc: 0.6488\n",
            "E2E-ABSA >>> 2022-08-17 15:40:36\n",
            "loss: 0.7871, acc: 0.6502\n",
            "E2E-ABSA >>> 2022-08-17 15:40:37\n",
            ">>> val_acc: 0.6256, val_precision: 0.6256 val_recall: 0.6256, val_f1: 0.6256\n",
            ">> saved: state_dict/lstm_acl14shortdata_know_val_f1_0.6256\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:37\n",
            "loss: 0.7782, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-08-17 15:40:38\n",
            "loss: 0.7911, acc: 0.6509\n",
            "E2E-ABSA >>> 2022-08-17 15:40:38\n",
            "loss: 0.7897, acc: 0.6453\n",
            "E2E-ABSA >>> 2022-08-17 15:40:39\n",
            "loss: 0.7856, acc: 0.6487\n",
            "E2E-ABSA >>> 2022-08-17 15:40:40\n",
            ">>> val_acc: 0.6160, val_precision: 0.6160 val_recall: 0.6160, val_f1: 0.6160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:40\n",
            "loss: 0.8265, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 15:40:41\n",
            "loss: 0.8029, acc: 0.6391\n",
            "E2E-ABSA >>> 2022-08-17 15:40:42\n",
            "loss: 0.7872, acc: 0.6489\n",
            "E2E-ABSA >>> 2022-08-17 15:40:43\n",
            ">>> val_acc: 0.6160, val_precision: 0.6160 val_recall: 0.6160, val_f1: 0.6160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:43\n",
            "loss: 0.7133, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 15:40:43\n",
            "loss: 0.7763, acc: 0.6674\n",
            "E2E-ABSA >>> 2022-08-17 15:40:44\n",
            "loss: 0.7882, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-08-17 15:40:45\n",
            "loss: 0.7803, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-08-17 15:40:45\n",
            ">>> val_acc: 0.6160, val_precision: 0.6160 val_recall: 0.6160, val_f1: 0.6160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:46\n",
            "loss: 0.7949, acc: 0.6396\n",
            "E2E-ABSA >>> 2022-08-17 15:40:46\n",
            "loss: 0.7922, acc: 0.6469\n",
            "E2E-ABSA >>> 2022-08-17 15:40:47\n",
            "loss: 0.7830, acc: 0.6495\n",
            "E2E-ABSA >>> 2022-08-17 15:40:48\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:48\n",
            "loss: 0.7344, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 15:40:49\n",
            "loss: 0.7936, acc: 0.6377\n",
            "E2E-ABSA >>> 2022-08-17 15:40:49\n",
            "loss: 0.7908, acc: 0.6436\n",
            "E2E-ABSA >>> 2022-08-17 15:40:50\n",
            "loss: 0.7863, acc: 0.6453\n",
            "E2E-ABSA >>> 2022-08-17 15:40:50\n",
            ">>> val_acc: 0.6112, val_precision: 0.6112 val_recall: 0.6112, val_f1: 0.6112\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:51\n",
            "loss: 0.7931, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 15:40:51\n",
            "loss: 0.7873, acc: 0.6514\n",
            "E2E-ABSA >>> 2022-08-17 15:40:52\n",
            "loss: 0.7880, acc: 0.6487\n",
            "E2E-ABSA >>> 2022-08-17 15:40:53\n",
            ">>> val_acc: 0.6208, val_precision: 0.6208 val_recall: 0.6208, val_f1: 0.6208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:53\n",
            "loss: 0.7624, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 15:40:54\n",
            "loss: 0.7695, acc: 0.6653\n",
            "E2E-ABSA >>> 2022-08-17 15:40:54\n",
            "loss: 0.7712, acc: 0.6605\n",
            "E2E-ABSA >>> 2022-08-17 15:40:55\n",
            "loss: 0.7791, acc: 0.6519\n",
            "E2E-ABSA >>> 2022-08-17 15:40:56\n",
            ">>> val_acc: 0.6144, val_precision: 0.6144 val_recall: 0.6144, val_f1: 0.6144\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-08-17 15:40:56\n",
            "loss: 0.7745, acc: 0.6635\n",
            "E2E-ABSA >>> 2022-08-17 15:40:57\n",
            "loss: 0.7712, acc: 0.6550\n",
            "E2E-ABSA >>> 2022-08-17 15:40:57\n",
            "loss: 0.7774, acc: 0.6550\n",
            "E2E-ABSA >>> 2022-08-17 15:40:58\n",
            "loss: 0.7796, acc: 0.6530\n",
            "E2E-ABSA >>> 2022-08-17 15:40:58\n",
            ">>> val_acc: 0.6256, val_precision: 0.6256 val_recall: 0.6256, val_f1: 0.6256\n",
            "you can download the best model from state_dict/lstm_acl14shortdata_know_val_f1_0.6256\n",
            ">>> test_acc: 0.6256, test_precision: 0.6256, test_recall: 0.6256, test_f1: 0.6256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **acl14shortdata** dataset on model(**TDLSTM**)\n",
        "\n"
      ],
      "metadata": {
        "id": "C2NhNJIBV-ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset acl14shortdata --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmRwrlLtV-w8",
        "outputId": "38d7713a-ae38-4bbb-ac75-c0b90f773ae5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 4720.\n",
            "> testing dataset count: 520.\n",
            "cuda memory allocated: 21102080\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 3828600\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f5714a71b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:38\n",
            "loss: 1.0478, acc: 0.4681\n",
            "E2E-ABSA >>> 2022-08-17 15:41:39\n",
            "loss: 1.0318, acc: 0.4875\n",
            "E2E-ABSA >>> 2022-08-17 15:41:39\n",
            ">>> val_acc: 0.4731, val_precision: 0.4731 val_recall: 0.4731, val_f1: 0.4731\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.4731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:40\n",
            "loss: 1.0501, acc: 0.4375\n",
            "E2E-ABSA >>> 2022-08-17 15:41:40\n",
            "loss: 0.9893, acc: 0.5143\n",
            "E2E-ABSA >>> 2022-08-17 15:41:41\n",
            "loss: 0.9815, acc: 0.5247\n",
            "E2E-ABSA >>> 2022-08-17 15:41:42\n",
            ">>> val_acc: 0.5115, val_precision: 0.5115 val_recall: 0.5115, val_f1: 0.5115\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.5115\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:42\n",
            "loss: 0.9493, acc: 0.5250\n",
            "E2E-ABSA >>> 2022-08-17 15:41:43\n",
            "loss: 0.9458, acc: 0.5545\n",
            "E2E-ABSA >>> 2022-08-17 15:41:44\n",
            "loss: 0.9582, acc: 0.5402\n",
            "E2E-ABSA >>> 2022-08-17 15:41:45\n",
            ">>> val_acc: 0.5250, val_precision: 0.5250 val_recall: 0.5250, val_f1: 0.5250\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.525\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:45\n",
            "loss: 0.9453, acc: 0.5125\n",
            "E2E-ABSA >>> 2022-08-17 15:41:46\n",
            "loss: 0.9394, acc: 0.5560\n",
            "E2E-ABSA >>> 2022-08-17 15:41:46\n",
            "loss: 0.9439, acc: 0.5520\n",
            "E2E-ABSA >>> 2022-08-17 15:41:47\n",
            ">>> val_acc: 0.5250, val_precision: 0.5250 val_recall: 0.5250, val_f1: 0.5250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:47\n",
            "loss: 0.9149, acc: 0.5719\n",
            "E2E-ABSA >>> 2022-08-17 15:41:48\n",
            "loss: 0.9315, acc: 0.5583\n",
            "E2E-ABSA >>> 2022-08-17 15:41:49\n",
            "loss: 0.9339, acc: 0.5548\n",
            "E2E-ABSA >>> 2022-08-17 15:41:50\n",
            ">>> val_acc: 0.5250, val_precision: 0.5250 val_recall: 0.5250, val_f1: 0.5250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:50\n",
            "loss: 0.9247, acc: 0.5675\n",
            "E2E-ABSA >>> 2022-08-17 15:41:51\n",
            "loss: 0.9103, acc: 0.5725\n",
            "E2E-ABSA >>> 2022-08-17 15:41:52\n",
            "loss: 0.9150, acc: 0.5667\n",
            "E2E-ABSA >>> 2022-08-17 15:41:52\n",
            ">>> val_acc: 0.5385, val_precision: 0.5385 val_recall: 0.5385, val_f1: 0.5385\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.5385\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:53\n",
            "loss: 0.9645, acc: 0.5333\n",
            "E2E-ABSA >>> 2022-08-17 15:41:53\n",
            "loss: 0.9155, acc: 0.5639\n",
            "E2E-ABSA >>> 2022-08-17 15:41:54\n",
            "loss: 0.9002, acc: 0.5813\n",
            "E2E-ABSA >>> 2022-08-17 15:41:55\n",
            ">>> val_acc: 0.5500, val_precision: 0.5500 val_recall: 0.5500, val_f1: 0.5500\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.55\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:55\n",
            "loss: 0.8755, acc: 0.5804\n",
            "E2E-ABSA >>> 2022-08-17 15:41:56\n",
            "loss: 0.8900, acc: 0.5889\n",
            "E2E-ABSA >>> 2022-08-17 15:41:57\n",
            "loss: 0.8867, acc: 0.5960\n",
            "E2E-ABSA >>> 2022-08-17 15:41:57\n",
            ">>> val_acc: 0.5462, val_precision: 0.5462 val_recall: 0.5462, val_f1: 0.5462\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:41:58\n",
            "loss: 0.8905, acc: 0.5859\n",
            "E2E-ABSA >>> 2022-08-17 15:41:59\n",
            "loss: 0.8858, acc: 0.5906\n",
            "E2E-ABSA >>> 2022-08-17 15:41:59\n",
            "loss: 0.8760, acc: 0.6023\n",
            "E2E-ABSA >>> 2022-08-17 15:42:00\n",
            ">>> val_acc: 0.5635, val_precision: 0.5635 val_recall: 0.5635, val_f1: 0.5635\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.5635\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:00\n",
            "loss: 0.8572, acc: 0.5958\n",
            "E2E-ABSA >>> 2022-08-17 15:42:01\n",
            "loss: 0.8511, acc: 0.6159\n",
            "E2E-ABSA >>> 2022-08-17 15:42:02\n",
            "loss: 0.8516, acc: 0.6153\n",
            "E2E-ABSA >>> 2022-08-17 15:42:03\n",
            ">>> val_acc: 0.5846, val_precision: 0.5846 val_recall: 0.5846, val_f1: 0.5846\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.5846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:03\n",
            "loss: 0.8157, acc: 0.6288\n",
            "E2E-ABSA >>> 2022-08-17 15:42:04\n",
            "loss: 0.8361, acc: 0.6238\n",
            "E2E-ABSA >>> 2022-08-17 15:42:05\n",
            "loss: 0.8313, acc: 0.6358\n",
            "E2E-ABSA >>> 2022-08-17 15:42:05\n",
            ">>> val_acc: 0.6077, val_precision: 0.6077 val_recall: 0.6077, val_f1: 0.6077\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6077\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:06\n",
            "loss: 0.8294, acc: 0.6341\n",
            "E2E-ABSA >>> 2022-08-17 15:42:06\n",
            "loss: 0.8181, acc: 0.6391\n",
            "E2E-ABSA >>> 2022-08-17 15:42:07\n",
            "loss: 0.8167, acc: 0.6409\n",
            "E2E-ABSA >>> 2022-08-17 15:42:08\n",
            ">>> val_acc: 0.6269, val_precision: 0.6269 val_recall: 0.6269, val_f1: 0.6269\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6269\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:08\n",
            "loss: 0.7903, acc: 0.6479\n",
            "E2E-ABSA >>> 2022-08-17 15:42:09\n",
            "loss: 0.7956, acc: 0.6477\n",
            "E2E-ABSA >>> 2022-08-17 15:42:10\n",
            "loss: 0.8004, acc: 0.6466\n",
            "E2E-ABSA >>> 2022-08-17 15:42:10\n",
            ">>> val_acc: 0.6385, val_precision: 0.6385 val_recall: 0.6385, val_f1: 0.6385\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6385\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:11\n",
            "loss: 0.7713, acc: 0.6740\n",
            "E2E-ABSA >>> 2022-08-17 15:42:12\n",
            "loss: 0.7845, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-08-17 15:42:12\n",
            "loss: 0.7781, acc: 0.6616\n",
            "E2E-ABSA >>> 2022-08-17 15:42:13\n",
            ">>> val_acc: 0.6442, val_precision: 0.6442 val_recall: 0.6442, val_f1: 0.6442\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6442\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:13\n",
            "loss: 0.7598, acc: 0.6804\n",
            "E2E-ABSA >>> 2022-08-17 15:42:14\n",
            "loss: 0.7618, acc: 0.6794\n",
            "E2E-ABSA >>> 2022-08-17 15:42:15\n",
            "loss: 0.7676, acc: 0.6699\n",
            "E2E-ABSA >>> 2022-08-17 15:42:15\n",
            ">>> val_acc: 0.6577, val_precision: 0.6577 val_recall: 0.6577, val_f1: 0.6577\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6577\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:16\n",
            "loss: 0.7448, acc: 0.6717\n",
            "E2E-ABSA >>> 2022-08-17 15:42:17\n",
            "loss: 0.7444, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-08-17 15:42:18\n",
            "loss: 0.7437, acc: 0.6848\n",
            "E2E-ABSA >>> 2022-08-17 15:42:18\n",
            ">>> val_acc: 0.6615, val_precision: 0.6615 val_recall: 0.6615, val_f1: 0.6615\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6615\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:19\n",
            "loss: 0.7399, acc: 0.6859\n",
            "E2E-ABSA >>> 2022-08-17 15:42:20\n",
            "loss: 0.7439, acc: 0.6806\n",
            "E2E-ABSA >>> 2022-08-17 15:42:20\n",
            "loss: 0.7354, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:42:21\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:21\n",
            "loss: 0.7260, acc: 0.6897\n",
            "E2E-ABSA >>> 2022-08-17 15:42:22\n",
            "loss: 0.7213, acc: 0.6916\n",
            "E2E-ABSA >>> 2022-08-17 15:42:23\n",
            "loss: 0.7273, acc: 0.6868\n",
            "E2E-ABSA >>> 2022-08-17 15:42:23\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:24\n",
            "loss: 0.7365, acc: 0.6861\n",
            "E2E-ABSA >>> 2022-08-17 15:42:25\n",
            "loss: 0.7293, acc: 0.6868\n",
            "E2E-ABSA >>> 2022-08-17 15:42:26\n",
            "loss: 0.7241, acc: 0.6877\n",
            "E2E-ABSA >>> 2022-08-17 15:42:26\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:27\n",
            "loss: 0.7240, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 15:42:27\n",
            "loss: 0.7228, acc: 0.6965\n",
            "E2E-ABSA >>> 2022-08-17 15:42:28\n",
            "loss: 0.7181, acc: 0.6972\n",
            "E2E-ABSA >>> 2022-08-17 15:42:28\n",
            ">>> val_acc: 0.6654, val_precision: 0.6654 val_recall: 0.6654, val_f1: 0.6654\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:29\n",
            "loss: 0.7000, acc: 0.6919\n",
            "E2E-ABSA >>> 2022-08-17 15:42:30\n",
            "loss: 0.7096, acc: 0.6925\n",
            "E2E-ABSA >>> 2022-08-17 15:42:31\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:31\n",
            "loss: 0.6876, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:42:32\n",
            "loss: 0.7117, acc: 0.6935\n",
            "E2E-ABSA >>> 2022-08-17 15:42:32\n",
            "loss: 0.7112, acc: 0.6921\n",
            "E2E-ABSA >>> 2022-08-17 15:42:33\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:33\n",
            "loss: 0.7091, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 15:42:34\n",
            "loss: 0.6819, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 15:42:35\n",
            "loss: 0.7009, acc: 0.7074\n",
            "E2E-ABSA >>> 2022-08-17 15:42:36\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:36\n",
            "loss: 0.7496, acc: 0.6708\n",
            "E2E-ABSA >>> 2022-08-17 15:42:37\n",
            "loss: 0.7053, acc: 0.6864\n",
            "E2E-ABSA >>> 2022-08-17 15:42:38\n",
            "loss: 0.6989, acc: 0.6997\n",
            "E2E-ABSA >>> 2022-08-17 15:42:38\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:38\n",
            "loss: 0.6684, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 15:42:39\n",
            "loss: 0.7090, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-08-17 15:42:40\n",
            "loss: 0.7036, acc: 0.7037\n",
            "E2E-ABSA >>> 2022-08-17 15:42:41\n",
            ">>> val_acc: 0.6635, val_precision: 0.6635 val_recall: 0.6635, val_f1: 0.6635\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:41\n",
            "loss: 0.6798, acc: 0.7100\n",
            "E2E-ABSA >>> 2022-08-17 15:42:42\n",
            "loss: 0.7045, acc: 0.6930\n",
            "E2E-ABSA >>> 2022-08-17 15:42:43\n",
            "loss: 0.7017, acc: 0.6983\n",
            "E2E-ABSA >>> 2022-08-17 15:42:43\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:44\n",
            "loss: 0.7127, acc: 0.6896\n",
            "E2E-ABSA >>> 2022-08-17 15:42:44\n",
            "loss: 0.7008, acc: 0.6880\n",
            "E2E-ABSA >>> 2022-08-17 15:42:45\n",
            "loss: 0.6997, acc: 0.6978\n",
            "E2E-ABSA >>> 2022-08-17 15:42:46\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:46\n",
            "loss: 0.6452, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-08-17 15:42:47\n",
            "loss: 0.6777, acc: 0.7171\n",
            "E2E-ABSA >>> 2022-08-17 15:42:48\n",
            "loss: 0.6853, acc: 0.7101\n",
            "E2E-ABSA >>> 2022-08-17 15:42:48\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:49\n",
            "loss: 0.7012, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:42:49\n",
            "loss: 0.6998, acc: 0.7067\n",
            "E2E-ABSA >>> 2022-08-17 15:42:50\n",
            "loss: 0.6904, acc: 0.7042\n",
            "E2E-ABSA >>> 2022-08-17 15:42:51\n",
            ">>> val_acc: 0.6808, val_precision: 0.6808 val_recall: 0.6808, val_f1: 0.6808\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:51\n",
            "loss: 0.6923, acc: 0.7069\n",
            "E2E-ABSA >>> 2022-08-17 15:42:52\n",
            "loss: 0.6853, acc: 0.7078\n",
            "E2E-ABSA >>> 2022-08-17 15:42:53\n",
            "loss: 0.6882, acc: 0.7051\n",
            "E2E-ABSA >>> 2022-08-17 15:42:53\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:54\n",
            "loss: 0.7016, acc: 0.7037\n",
            "E2E-ABSA >>> 2022-08-17 15:42:55\n",
            "loss: 0.6888, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 15:42:55\n",
            "loss: 0.6837, acc: 0.7047\n",
            "E2E-ABSA >>> 2022-08-17 15:42:56\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:56\n",
            "loss: 0.6791, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 15:42:57\n",
            "loss: 0.6757, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-08-17 15:42:58\n",
            "loss: 0.6805, acc: 0.7120\n",
            "E2E-ABSA >>> 2022-08-17 15:42:58\n",
            ">>> val_acc: 0.6577, val_precision: 0.6577 val_recall: 0.6577, val_f1: 0.6577\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:42:59\n",
            "loss: 0.6680, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 15:43:00\n",
            "loss: 0.6816, acc: 0.7129\n",
            "E2E-ABSA >>> 2022-08-17 15:43:01\n",
            "loss: 0.6783, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-08-17 15:43:01\n",
            ">>> val_acc: 0.6615, val_precision: 0.6615 val_recall: 0.6615, val_f1: 0.6615\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:02\n",
            "loss: 0.6792, acc: 0.7058\n",
            "E2E-ABSA >>> 2022-08-17 15:43:02\n",
            "loss: 0.6934, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-08-17 15:43:03\n",
            "loss: 0.6801, acc: 0.7158\n",
            "E2E-ABSA >>> 2022-08-17 15:43:04\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:04\n",
            "loss: 0.6718, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-08-17 15:43:05\n",
            "loss: 0.6786, acc: 0.7081\n",
            "E2E-ABSA >>> 2022-08-17 15:43:06\n",
            "loss: 0.6757, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 15:43:06\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:07\n",
            "loss: 0.6990, acc: 0.7008\n",
            "E2E-ABSA >>> 2022-08-17 15:43:07\n",
            "loss: 0.6857, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-08-17 15:43:08\n",
            "loss: 0.6793, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 15:43:09\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:10\n",
            "loss: 0.6903, acc: 0.7008\n",
            "E2E-ABSA >>> 2022-08-17 15:43:10\n",
            "loss: 0.6712, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-08-17 15:43:11\n",
            "loss: 0.6700, acc: 0.7136\n",
            "E2E-ABSA >>> 2022-08-17 15:43:11\n",
            ">>> val_acc: 0.6712, val_precision: 0.6712 val_recall: 0.6712, val_f1: 0.6712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:12\n",
            "loss: 0.6816, acc: 0.7022\n",
            "E2E-ABSA >>> 2022-08-17 15:43:13\n",
            "loss: 0.6750, acc: 0.7064\n",
            "E2E-ABSA >>> 2022-08-17 15:43:14\n",
            "loss: 0.6735, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-08-17 15:43:14\n",
            ">>> val_acc: 0.6712, val_precision: 0.6712 val_recall: 0.6712, val_f1: 0.6712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:15\n",
            "loss: 0.6771, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-08-17 15:43:15\n",
            "loss: 0.6753, acc: 0.7112\n",
            "E2E-ABSA >>> 2022-08-17 15:43:16\n",
            "loss: 0.6724, acc: 0.7134\n",
            "E2E-ABSA >>> 2022-08-17 15:43:16\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:17\n",
            "loss: 0.6626, acc: 0.7289\n",
            "E2E-ABSA >>> 2022-08-17 15:43:18\n",
            "loss: 0.6672, acc: 0.7247\n",
            "E2E-ABSA >>> 2022-08-17 15:43:19\n",
            "loss: 0.6694, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 15:43:19\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:20\n",
            "loss: 0.6510, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:43:20\n",
            "loss: 0.6689, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 15:43:21\n",
            ">>> val_acc: 0.6712, val_precision: 0.6712 val_recall: 0.6712, val_f1: 0.6712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:21\n",
            "loss: 0.6417, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:43:22\n",
            "loss: 0.6674, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 15:43:23\n",
            "loss: 0.6671, acc: 0.7192\n",
            "E2E-ABSA >>> 2022-08-17 15:43:24\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:24\n",
            "loss: 0.6507, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:43:25\n",
            "loss: 0.6580, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-08-17 15:43:26\n",
            "loss: 0.6548, acc: 0.7229\n",
            "E2E-ABSA >>> 2022-08-17 15:43:26\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_val_f1_0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:27\n",
            "loss: 0.6620, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 15:43:27\n",
            "loss: 0.6460, acc: 0.7348\n",
            "E2E-ABSA >>> 2022-08-17 15:43:28\n",
            "loss: 0.6558, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 15:43:29\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:29\n",
            "loss: 0.6622, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 15:43:30\n",
            "loss: 0.6621, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-08-17 15:43:31\n",
            "loss: 0.6644, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-08-17 15:43:31\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:32\n",
            "loss: 0.6452, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-08-17 15:43:32\n",
            "loss: 0.6598, acc: 0.7270\n",
            "E2E-ABSA >>> 2022-08-17 15:43:33\n",
            "loss: 0.6626, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-08-17 15:43:34\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:34\n",
            "loss: 0.6690, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 15:43:35\n",
            "loss: 0.6538, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-08-17 15:43:36\n",
            "loss: 0.6522, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-08-17 15:43:36\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:37\n",
            "loss: 0.6506, acc: 0.7429\n",
            "E2E-ABSA >>> 2022-08-17 15:43:37\n",
            "loss: 0.6675, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-08-17 15:43:38\n",
            "loss: 0.6608, acc: 0.7239\n",
            "E2E-ABSA >>> 2022-08-17 15:43:39\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:39\n",
            "loss: 0.6807, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-08-17 15:43:40\n",
            "loss: 0.6856, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:43:41\n",
            "loss: 0.6629, acc: 0.7221\n",
            "E2E-ABSA >>> 2022-08-17 15:43:41\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:42\n",
            "loss: 0.6704, acc: 0.7111\n",
            "E2E-ABSA >>> 2022-08-17 15:43:43\n",
            "loss: 0.6571, acc: 0.7185\n",
            "E2E-ABSA >>> 2022-08-17 15:43:43\n",
            "loss: 0.6585, acc: 0.7235\n",
            "E2E-ABSA >>> 2022-08-17 15:43:44\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:44\n",
            "loss: 0.6270, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-08-17 15:43:45\n",
            "loss: 0.6516, acc: 0.7179\n",
            "E2E-ABSA >>> 2022-08-17 15:43:46\n",
            "loss: 0.6510, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-08-17 15:43:46\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:47\n",
            "loss: 0.6329, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-08-17 15:43:48\n",
            "loss: 0.6577, acc: 0.7194\n",
            "E2E-ABSA >>> 2022-08-17 15:43:48\n",
            "loss: 0.6530, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 15:43:49\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:49\n",
            "loss: 0.6114, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 15:43:50\n",
            "loss: 0.6318, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 15:43:51\n",
            "loss: 0.6457, acc: 0.7322\n",
            "E2E-ABSA >>> 2022-08-17 15:43:51\n",
            ">>> val_acc: 0.6596, val_precision: 0.6596 val_recall: 0.6596, val_f1: 0.6596\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:52\n",
            "loss: 0.6522, acc: 0.7288\n",
            "E2E-ABSA >>> 2022-08-17 15:43:53\n",
            "loss: 0.6478, acc: 0.7307\n",
            "E2E-ABSA >>> 2022-08-17 15:43:54\n",
            "loss: 0.6549, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-08-17 15:43:54\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:54\n",
            "loss: 0.6302, acc: 0.7286\n",
            "E2E-ABSA >>> 2022-08-17 15:43:55\n",
            "loss: 0.6379, acc: 0.7349\n",
            "E2E-ABSA >>> 2022-08-17 15:43:56\n",
            "loss: 0.6493, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-08-17 15:43:56\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:43:57\n",
            "loss: 0.6516, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 15:43:58\n",
            "loss: 0.6460, acc: 0.7329\n",
            "E2E-ABSA >>> 2022-08-17 15:43:59\n",
            "loss: 0.6475, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 15:43:59\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:00\n",
            "loss: 0.6306, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:44:00\n",
            "loss: 0.6435, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 15:44:01\n",
            "loss: 0.6497, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-08-17 15:44:02\n",
            ">>> val_acc: 0.6635, val_precision: 0.6635 val_recall: 0.6635, val_f1: 0.6635\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:02\n",
            "loss: 0.6207, acc: 0.7485\n",
            "E2E-ABSA >>> 2022-08-17 15:44:03\n",
            "loss: 0.6490, acc: 0.7270\n",
            "E2E-ABSA >>> 2022-08-17 15:44:04\n",
            "loss: 0.6424, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-08-17 15:44:04\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:05\n",
            "loss: 0.6308, acc: 0.7479\n",
            "E2E-ABSA >>> 2022-08-17 15:44:06\n",
            "loss: 0.6458, acc: 0.7372\n",
            "E2E-ABSA >>> 2022-08-17 15:44:06\n",
            "loss: 0.6460, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-08-17 15:44:06\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:07\n",
            "loss: 0.6379, acc: 0.7342\n",
            "E2E-ABSA >>> 2022-08-17 15:44:08\n",
            "loss: 0.6387, acc: 0.7340\n",
            "E2E-ABSA >>> 2022-08-17 15:44:09\n",
            "loss: 0.6432, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 15:44:09\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:10\n",
            "loss: 0.6300, acc: 0.7400\n",
            "E2E-ABSA >>> 2022-08-17 15:44:11\n",
            "loss: 0.6360, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 15:44:11\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:11\n",
            "loss: 0.5564, acc: 0.8250\n",
            "E2E-ABSA >>> 2022-08-17 15:44:12\n",
            "loss: 0.6226, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 15:44:13\n",
            "loss: 0.6367, acc: 0.7387\n",
            "E2E-ABSA >>> 2022-08-17 15:44:14\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:14\n",
            "loss: 0.6838, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:44:15\n",
            "loss: 0.6562, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 15:44:16\n",
            "loss: 0.6409, acc: 0.7310\n",
            "E2E-ABSA >>> 2022-08-17 15:44:16\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:17\n",
            "loss: 0.5247, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:44:17\n",
            "loss: 0.6360, acc: 0.7446\n",
            "E2E-ABSA >>> 2022-08-17 15:44:18\n",
            "loss: 0.6396, acc: 0.7346\n",
            "E2E-ABSA >>> 2022-08-17 15:44:19\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:19\n",
            "loss: 0.6276, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 15:44:20\n",
            "loss: 0.6391, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-08-17 15:44:21\n",
            "loss: 0.6304, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 15:44:21\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:22\n",
            "loss: 0.6397, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:44:22\n",
            "loss: 0.6393, acc: 0.7355\n",
            "E2E-ABSA >>> 2022-08-17 15:44:23\n",
            "loss: 0.6423, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 15:44:24\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:24\n",
            "loss: 0.6466, acc: 0.7333\n",
            "E2E-ABSA >>> 2022-08-17 15:44:25\n",
            "loss: 0.6460, acc: 0.7341\n",
            "E2E-ABSA >>> 2022-08-17 15:44:26\n",
            "loss: 0.6438, acc: 0.7337\n",
            "E2E-ABSA >>> 2022-08-17 15:44:26\n",
            ">>> val_acc: 0.6712, val_precision: 0.6712 val_recall: 0.6712, val_f1: 0.6712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:27\n",
            "loss: 0.6317, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 15:44:27\n",
            "loss: 0.6384, acc: 0.7347\n",
            "E2E-ABSA >>> 2022-08-17 15:44:28\n",
            "loss: 0.6273, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:44:29\n",
            ">>> val_acc: 0.6712, val_precision: 0.6712 val_recall: 0.6712, val_f1: 0.6712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:29\n",
            "loss: 0.6313, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 15:44:30\n",
            "loss: 0.6319, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:44:31\n",
            "loss: 0.6379, acc: 0.7372\n",
            "E2E-ABSA >>> 2022-08-17 15:44:31\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:32\n",
            "loss: 0.6165, acc: 0.7444\n",
            "E2E-ABSA >>> 2022-08-17 15:44:33\n",
            "loss: 0.6414, acc: 0.7276\n",
            "E2E-ABSA >>> 2022-08-17 15:44:33\n",
            "loss: 0.6350, acc: 0.7329\n",
            "E2E-ABSA >>> 2022-08-17 15:44:34\n",
            ">>> val_acc: 0.6712, val_precision: 0.6712 val_recall: 0.6712, val_f1: 0.6712\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:34\n",
            "loss: 0.6163, acc: 0.7538\n",
            "E2E-ABSA >>> 2022-08-17 15:44:35\n",
            "loss: 0.6379, acc: 0.7296\n",
            "E2E-ABSA >>> 2022-08-17 15:44:36\n",
            "loss: 0.6324, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-08-17 15:44:36\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:37\n",
            "loss: 0.6466, acc: 0.7364\n",
            "E2E-ABSA >>> 2022-08-17 15:44:38\n",
            "loss: 0.6311, acc: 0.7351\n",
            "E2E-ABSA >>> 2022-08-17 15:44:38\n",
            "loss: 0.6276, acc: 0.7395\n",
            "E2E-ABSA >>> 2022-08-17 15:44:39\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:44:39\n",
            "loss: 0.6669, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 15:44:40\n",
            "loss: 0.6410, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 15:44:41\n",
            "loss: 0.6309, acc: 0.7368\n",
            "E2E-ABSA >>> 2022-08-17 15:44:41\n",
            ">>> val_acc: 0.6712, val_precision: 0.6712 val_recall: 0.6712, val_f1: 0.6712\n",
            "E2E-ABSA >>> 2022-08-17 15:44:41\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            "you can download the best model from state_dict/td_lstm_acl14shortdata_val_f1_0.6846\n",
            ">>> test_acc: 0.6846, test_precision: 0.6846, test_recall: 0.6846, test_f1: 0.6846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **acl14shortdata** dataset on model(**TDLSTM**)\n",
        "\n"
      ],
      "metadata": {
        "id": "MPeI8ZAIV-1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name td_lstm --dataset acl14shortdata_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZvxqSTqV-57",
        "outputId": "d062dc93-8a60-4768-ad39-37e9d9e2af35"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 4917.\n",
            "> testing dataset count: 539.\n",
            "cuda memory allocated: 22564864\n",
            "> n_trainable_params: 1446603, n_nontrainable_params: 4115100\n",
            "> training arguments:\n",
            ">>> model_name: td_lstm\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fcaa7f07b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.td_lstm.TD_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:22\n",
            "loss: 1.0457, acc: 0.4794\n",
            "E2E-ABSA >>> 2022-08-17 15:45:23\n",
            "loss: 1.0287, acc: 0.4928\n",
            "E2E-ABSA >>> 2022-08-17 15:45:24\n",
            "loss: 1.0183, acc: 0.4981\n",
            "E2E-ABSA >>> 2022-08-17 15:45:24\n",
            ">>> val_acc: 0.4768, val_precision: 0.4768 val_recall: 0.4768, val_f1: 0.4768\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.4768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:25\n",
            "loss: 0.9817, acc: 0.5319\n",
            "E2E-ABSA >>> 2022-08-17 15:45:26\n",
            "loss: 0.9814, acc: 0.5306\n",
            "E2E-ABSA >>> 2022-08-17 15:45:27\n",
            "loss: 0.9740, acc: 0.5319\n",
            "E2E-ABSA >>> 2022-08-17 15:45:28\n",
            ">>> val_acc: 0.5121, val_precision: 0.5121 val_recall: 0.5121, val_f1: 0.5121\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.5121\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:28\n",
            "loss: 0.9507, acc: 0.5573\n",
            "E2E-ABSA >>> 2022-08-17 15:45:30\n",
            "loss: 0.9521, acc: 0.5472\n",
            "E2E-ABSA >>> 2022-08-17 15:45:31\n",
            "loss: 0.9510, acc: 0.5473\n",
            "E2E-ABSA >>> 2022-08-17 15:45:31\n",
            ">>> val_acc: 0.5306, val_precision: 0.5306 val_recall: 0.5306, val_f1: 0.5306\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.5306\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:32\n",
            "loss: 0.9506, acc: 0.5329\n",
            "E2E-ABSA >>> 2022-08-17 15:45:33\n",
            "loss: 0.9371, acc: 0.5518\n",
            "E2E-ABSA >>> 2022-08-17 15:45:34\n",
            "loss: 0.9331, acc: 0.5559\n",
            "E2E-ABSA >>> 2022-08-17 15:45:34\n",
            ">>> val_acc: 0.5325, val_precision: 0.5325 val_recall: 0.5325, val_f1: 0.5325\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.5325\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:35\n",
            "loss: 0.9228, acc: 0.5726\n",
            "E2E-ABSA >>> 2022-08-17 15:45:36\n",
            "loss: 0.9230, acc: 0.5688\n",
            "E2E-ABSA >>> 2022-08-17 15:45:37\n",
            "loss: 0.9188, acc: 0.5672\n",
            "E2E-ABSA >>> 2022-08-17 15:45:38\n",
            ">>> val_acc: 0.5343, val_precision: 0.5343 val_recall: 0.5343, val_f1: 0.5343\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.5343\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:39\n",
            "loss: 0.9276, acc: 0.5646\n",
            "E2E-ABSA >>> 2022-08-17 15:45:40\n",
            "loss: 0.9102, acc: 0.5750\n",
            "E2E-ABSA >>> 2022-08-17 15:45:41\n",
            "loss: 0.9033, acc: 0.5803\n",
            "E2E-ABSA >>> 2022-08-17 15:45:41\n",
            ">>> val_acc: 0.5455, val_precision: 0.5455 val_recall: 0.5455, val_f1: 0.5455\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.5455\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:42\n",
            "loss: 0.8954, acc: 0.5805\n",
            "E2E-ABSA >>> 2022-08-17 15:45:43\n",
            "loss: 0.8893, acc: 0.5905\n",
            "E2E-ABSA >>> 2022-08-17 15:45:44\n",
            "loss: 0.8893, acc: 0.5895\n",
            "E2E-ABSA >>> 2022-08-17 15:45:45\n",
            ">>> val_acc: 0.5622, val_precision: 0.5622 val_recall: 0.5622, val_f1: 0.5622\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.5622\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:45\n",
            "loss: 0.8854, acc: 0.5909\n",
            "E2E-ABSA >>> 2022-08-17 15:45:46\n",
            "loss: 0.8800, acc: 0.5994\n",
            "E2E-ABSA >>> 2022-08-17 15:45:47\n",
            "loss: 0.8758, acc: 0.6004\n",
            "E2E-ABSA >>> 2022-08-17 15:45:48\n",
            ">>> val_acc: 0.5547, val_precision: 0.5547 val_recall: 0.5547, val_f1: 0.5547\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:48\n",
            "loss: 0.8627, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 15:45:49\n",
            "loss: 0.8663, acc: 0.6117\n",
            "E2E-ABSA >>> 2022-08-17 15:45:50\n",
            "loss: 0.8605, acc: 0.6176\n",
            "E2E-ABSA >>> 2022-08-17 15:45:51\n",
            ">>> val_acc: 0.5900, val_precision: 0.5900 val_recall: 0.5900, val_f1: 0.5900\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.59\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:52\n",
            "loss: 0.8160, acc: 0.6451\n",
            "E2E-ABSA >>> 2022-08-17 15:45:53\n",
            "loss: 0.8393, acc: 0.6313\n",
            "E2E-ABSA >>> 2022-08-17 15:45:54\n",
            "loss: 0.8382, acc: 0.6291\n",
            "E2E-ABSA >>> 2022-08-17 15:45:55\n",
            ">>> val_acc: 0.5993, val_precision: 0.5993 val_recall: 0.5993, val_f1: 0.5993\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.5993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:55\n",
            "loss: 0.8331, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 15:45:56\n",
            "loss: 0.8175, acc: 0.6557\n",
            "E2E-ABSA >>> 2022-08-17 15:45:57\n",
            "loss: 0.8208, acc: 0.6418\n",
            "E2E-ABSA >>> 2022-08-17 15:45:58\n",
            ">>> val_acc: 0.6011, val_precision: 0.6011 val_recall: 0.6011, val_f1: 0.6011\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6011\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:45:58\n",
            "loss: 0.7938, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 15:45:59\n",
            "loss: 0.8215, acc: 0.6412\n",
            "E2E-ABSA >>> 2022-08-17 15:46:00\n",
            "loss: 0.8109, acc: 0.6501\n",
            "E2E-ABSA >>> 2022-08-17 15:46:01\n",
            ">>> val_acc: 0.6234, val_precision: 0.6234 val_recall: 0.6234, val_f1: 0.6234\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6234\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:01\n",
            "loss: 0.7850, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:46:02\n",
            "loss: 0.7904, acc: 0.6520\n",
            "E2E-ABSA >>> 2022-08-17 15:46:03\n",
            "loss: 0.7902, acc: 0.6513\n",
            "E2E-ABSA >>> 2022-08-17 15:46:04\n",
            "loss: 0.7934, acc: 0.6468\n",
            "E2E-ABSA >>> 2022-08-17 15:46:05\n",
            ">>> val_acc: 0.6401, val_precision: 0.6401 val_recall: 0.6401, val_f1: 0.6401\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6401\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:06\n",
            "loss: 0.7865, acc: 0.6569\n",
            "E2E-ABSA >>> 2022-08-17 15:46:07\n",
            "loss: 0.7779, acc: 0.6620\n",
            "E2E-ABSA >>> 2022-08-17 15:46:08\n",
            "loss: 0.7810, acc: 0.6573\n",
            "E2E-ABSA >>> 2022-08-17 15:46:08\n",
            ">>> val_acc: 0.6475, val_precision: 0.6475 val_recall: 0.6475, val_f1: 0.6475\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:09\n",
            "loss: 0.7727, acc: 0.6712\n",
            "E2E-ABSA >>> 2022-08-17 15:46:10\n",
            "loss: 0.7716, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-08-17 15:46:11\n",
            "loss: 0.7676, acc: 0.6649\n",
            "E2E-ABSA >>> 2022-08-17 15:46:11\n",
            ">>> val_acc: 0.6549, val_precision: 0.6549 val_recall: 0.6549, val_f1: 0.6549\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6549\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:12\n",
            "loss: 0.7580, acc: 0.6617\n",
            "E2E-ABSA >>> 2022-08-17 15:46:13\n",
            "loss: 0.7398, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-08-17 15:46:14\n",
            "loss: 0.7418, acc: 0.6824\n",
            "E2E-ABSA >>> 2022-08-17 15:46:15\n",
            ">>> val_acc: 0.6568, val_precision: 0.6568 val_recall: 0.6568, val_f1: 0.6568\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6568\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:15\n",
            "loss: 0.7443, acc: 0.6892\n",
            "E2E-ABSA >>> 2022-08-17 15:46:16\n",
            "loss: 0.7318, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-08-17 15:46:17\n",
            "loss: 0.7312, acc: 0.6900\n",
            "E2E-ABSA >>> 2022-08-17 15:46:18\n",
            ">>> val_acc: 0.6586, val_precision: 0.6586 val_recall: 0.6586, val_f1: 0.6586\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6586\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:19\n",
            "loss: 0.7288, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-08-17 15:46:20\n",
            "loss: 0.7246, acc: 0.6921\n",
            "E2E-ABSA >>> 2022-08-17 15:46:21\n",
            "loss: 0.7248, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-08-17 15:46:21\n",
            ">>> val_acc: 0.6623, val_precision: 0.6623 val_recall: 0.6623, val_f1: 0.6623\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6623\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:22\n",
            "loss: 0.7243, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 15:46:23\n",
            "loss: 0.7202, acc: 0.6919\n",
            "E2E-ABSA >>> 2022-08-17 15:46:24\n",
            "loss: 0.7214, acc: 0.6909\n",
            "E2E-ABSA >>> 2022-08-17 15:46:25\n",
            ">>> val_acc: 0.6660, val_precision: 0.6660 val_recall: 0.6660, val_f1: 0.6660\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.666\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:25\n",
            "loss: 0.7224, acc: 0.6680\n",
            "E2E-ABSA >>> 2022-08-17 15:46:26\n",
            "loss: 0.7115, acc: 0.6934\n",
            "E2E-ABSA >>> 2022-08-17 15:46:27\n",
            "loss: 0.7091, acc: 0.6978\n",
            "E2E-ABSA >>> 2022-08-17 15:46:28\n",
            ">>> val_acc: 0.6679, val_precision: 0.6679 val_recall: 0.6679, val_f1: 0.6679\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6679\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:28\n",
            "loss: 0.7176, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 15:46:30\n",
            "loss: 0.7005, acc: 0.7071\n",
            "E2E-ABSA >>> 2022-08-17 15:46:31\n",
            "loss: 0.7129, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-08-17 15:46:31\n",
            ">>> val_acc: 0.6679, val_precision: 0.6679 val_recall: 0.6679, val_f1: 0.6679\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:32\n",
            "loss: 0.7535, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-08-17 15:46:33\n",
            "loss: 0.7175, acc: 0.6899\n",
            "E2E-ABSA >>> 2022-08-17 15:46:34\n",
            "loss: 0.7101, acc: 0.6977\n",
            "E2E-ABSA >>> 2022-08-17 15:46:35\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:35\n",
            "loss: 0.6388, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 15:46:36\n",
            "loss: 0.6984, acc: 0.7107\n",
            "E2E-ABSA >>> 2022-08-17 15:46:37\n",
            "loss: 0.6986, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 15:46:38\n",
            ">>> val_acc: 0.6568, val_precision: 0.6568 val_recall: 0.6568, val_f1: 0.6568\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:38\n",
            "loss: 0.6051, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:46:39\n",
            "loss: 0.6798, acc: 0.7166\n",
            "E2E-ABSA >>> 2022-08-17 15:46:40\n",
            "loss: 0.6925, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 15:46:41\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:41\n",
            "loss: 0.7032, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 15:46:42\n",
            "loss: 0.6894, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-08-17 15:46:43\n",
            "loss: 0.6989, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-08-17 15:46:44\n",
            "loss: 0.6941, acc: 0.7116\n",
            "E2E-ABSA >>> 2022-08-17 15:46:45\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:46\n",
            "loss: 0.7078, acc: 0.7019\n",
            "E2E-ABSA >>> 2022-08-17 15:46:47\n",
            "loss: 0.7018, acc: 0.7006\n",
            "E2E-ABSA >>> 2022-08-17 15:46:48\n",
            "loss: 0.6920, acc: 0.7092\n",
            "E2E-ABSA >>> 2022-08-17 15:46:48\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:49\n",
            "loss: 0.6797, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 15:46:50\n",
            "loss: 0.6920, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-08-17 15:46:51\n",
            "loss: 0.6861, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-08-17 15:46:51\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:52\n",
            "loss: 0.6666, acc: 0.7180\n",
            "E2E-ABSA >>> 2022-08-17 15:46:53\n",
            "loss: 0.6871, acc: 0.7140\n",
            "E2E-ABSA >>> 2022-08-17 15:46:54\n",
            "loss: 0.6847, acc: 0.7154\n",
            "E2E-ABSA >>> 2022-08-17 15:46:55\n",
            ">>> val_acc: 0.6735, val_precision: 0.6735 val_recall: 0.6735, val_f1: 0.6735\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:55\n",
            "loss: 0.6723, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 15:46:56\n",
            "loss: 0.6797, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-08-17 15:46:57\n",
            "loss: 0.6866, acc: 0.7095\n",
            "E2E-ABSA >>> 2022-08-17 15:46:58\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:46:59\n",
            "loss: 0.6956, acc: 0.7096\n",
            "E2E-ABSA >>> 2022-08-17 15:47:00\n",
            "loss: 0.6913, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-08-17 15:47:01\n",
            "loss: 0.6793, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-08-17 15:47:01\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:02\n",
            "loss: 0.6382, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:47:03\n",
            "loss: 0.6788, acc: 0.7133\n",
            "E2E-ABSA >>> 2022-08-17 15:47:04\n",
            "loss: 0.6742, acc: 0.7154\n",
            "E2E-ABSA >>> 2022-08-17 15:47:04\n",
            ">>> val_acc: 0.6772, val_precision: 0.6772 val_recall: 0.6772, val_f1: 0.6772\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:05\n",
            "loss: 0.6792, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 15:47:06\n",
            "loss: 0.6868, acc: 0.7142\n",
            "E2E-ABSA >>> 2022-08-17 15:47:07\n",
            "loss: 0.6768, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-08-17 15:47:08\n",
            ">>> val_acc: 0.6735, val_precision: 0.6735 val_recall: 0.6735, val_f1: 0.6735\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:08\n",
            "loss: 0.6647, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 15:47:09\n",
            "loss: 0.6709, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 15:47:10\n",
            "loss: 0.6773, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 15:47:11\n",
            ">>> val_acc: 0.6735, val_precision: 0.6735 val_recall: 0.6735, val_f1: 0.6735\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:11\n",
            "loss: 0.6585, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-08-17 15:47:12\n",
            "loss: 0.6691, acc: 0.7279\n",
            "E2E-ABSA >>> 2022-08-17 15:47:13\n",
            "loss: 0.6759, acc: 0.7190\n",
            "E2E-ABSA >>> 2022-08-17 15:47:14\n",
            ">>> val_acc: 0.6790, val_precision: 0.6790 val_recall: 0.6790, val_f1: 0.6790\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:15\n",
            "loss: 0.7454, acc: 0.6942\n",
            "E2E-ABSA >>> 2022-08-17 15:47:16\n",
            "loss: 0.6905, acc: 0.7119\n",
            "E2E-ABSA >>> 2022-08-17 15:47:17\n",
            "loss: 0.6671, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-08-17 15:47:18\n",
            ">>> val_acc: 0.6735, val_precision: 0.6735 val_recall: 0.6735, val_f1: 0.6735\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:18\n",
            "loss: 0.6401, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:47:19\n",
            "loss: 0.6770, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 15:47:20\n",
            "loss: 0.6681, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 15:47:21\n",
            ">>> val_acc: 0.6772, val_precision: 0.6772 val_recall: 0.6772, val_f1: 0.6772\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:21\n",
            "loss: 0.6618, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:47:22\n",
            "loss: 0.6530, acc: 0.7333\n",
            "E2E-ABSA >>> 2022-08-17 15:47:23\n",
            "loss: 0.6665, acc: 0.7217\n",
            "E2E-ABSA >>> 2022-08-17 15:47:24\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">> saved: state_dict/td_lstm_acl14shortdata_know_val_f1_0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:24\n",
            "loss: 0.6478, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:47:25\n",
            "loss: 0.6589, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-08-17 15:47:26\n",
            "loss: 0.6553, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 15:47:27\n",
            "loss: 0.6617, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-08-17 15:47:27\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:28\n",
            "loss: 0.6637, acc: 0.7181\n",
            "E2E-ABSA >>> 2022-08-17 15:47:29\n",
            "loss: 0.6685, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-08-17 15:47:30\n",
            "loss: 0.6612, acc: 0.7238\n",
            "E2E-ABSA >>> 2022-08-17 15:47:31\n",
            ">>> val_acc: 0.6790, val_precision: 0.6790 val_recall: 0.6790, val_f1: 0.6790\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:32\n",
            "loss: 0.6646, acc: 0.7152\n",
            "E2E-ABSA >>> 2022-08-17 15:47:33\n",
            "loss: 0.6601, acc: 0.7211\n",
            "E2E-ABSA >>> 2022-08-17 15:47:34\n",
            "loss: 0.6591, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-08-17 15:47:34\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:35\n",
            "loss: 0.6470, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 15:47:36\n",
            "loss: 0.6616, acc: 0.7253\n",
            "E2E-ABSA >>> 2022-08-17 15:47:37\n",
            "loss: 0.6536, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-08-17 15:47:37\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:38\n",
            "loss: 0.6536, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 15:47:39\n",
            "loss: 0.6523, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-08-17 15:47:40\n",
            "loss: 0.6496, acc: 0.7289\n",
            "E2E-ABSA >>> 2022-08-17 15:47:41\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:41\n",
            "loss: 0.6435, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:47:42\n",
            "loss: 0.6586, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 15:47:43\n",
            "loss: 0.6534, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-08-17 15:47:44\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:44\n",
            "loss: 0.6427, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 15:47:45\n",
            "loss: 0.6538, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 15:47:47\n",
            "loss: 0.6508, acc: 0.7349\n",
            "E2E-ABSA >>> 2022-08-17 15:47:47\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:48\n",
            "loss: 0.6395, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 15:47:49\n",
            "loss: 0.6600, acc: 0.7209\n",
            "E2E-ABSA >>> 2022-08-17 15:47:50\n",
            "loss: 0.6521, acc: 0.7258\n",
            "E2E-ABSA >>> 2022-08-17 15:47:51\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:51\n",
            "loss: 0.6516, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 15:47:52\n",
            "loss: 0.6503, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-08-17 15:47:53\n",
            "loss: 0.6482, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 15:47:54\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:54\n",
            "loss: 0.6497, acc: 0.7207\n",
            "E2E-ABSA >>> 2022-08-17 15:47:55\n",
            "loss: 0.6571, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 15:47:56\n",
            "loss: 0.6533, acc: 0.7247\n",
            "E2E-ABSA >>> 2022-08-17 15:47:57\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:47:57\n",
            "loss: 0.6536, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 15:47:58\n",
            "loss: 0.6384, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 15:47:59\n",
            "loss: 0.6362, acc: 0.7369\n",
            "E2E-ABSA >>> 2022-08-17 15:48:00\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:01\n",
            "loss: 0.6936, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-08-17 15:48:02\n",
            "loss: 0.6607, acc: 0.7247\n",
            "E2E-ABSA >>> 2022-08-17 15:48:03\n",
            "loss: 0.6516, acc: 0.7315\n",
            "E2E-ABSA >>> 2022-08-17 15:48:04\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:04\n",
            "loss: 0.6493, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 15:48:05\n",
            "loss: 0.6428, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-08-17 15:48:06\n",
            "loss: 0.6387, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 15:48:07\n",
            "loss: 0.6457, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 15:48:07\n",
            ">>> val_acc: 0.6939, val_precision: 0.6939 val_recall: 0.6939, val_f1: 0.6939\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:08\n",
            "loss: 0.6627, acc: 0.7269\n",
            "E2E-ABSA >>> 2022-08-17 15:48:09\n",
            "loss: 0.6563, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-08-17 15:48:10\n",
            "loss: 0.6439, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 15:48:10\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:11\n",
            "loss: 0.6381, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 15:48:12\n",
            "loss: 0.6343, acc: 0.7419\n",
            "E2E-ABSA >>> 2022-08-17 15:48:13\n",
            "loss: 0.6390, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 15:48:14\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:15\n",
            "loss: 0.6245, acc: 0.7463\n",
            "E2E-ABSA >>> 2022-08-17 15:48:16\n",
            "loss: 0.6391, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 15:48:17\n",
            "loss: 0.6472, acc: 0.7324\n",
            "E2E-ABSA >>> 2022-08-17 15:48:17\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:18\n",
            "loss: 0.6462, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 15:48:19\n",
            "loss: 0.6491, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 15:48:20\n",
            "loss: 0.6441, acc: 0.7262\n",
            "E2E-ABSA >>> 2022-08-17 15:48:20\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:21\n",
            "loss: 0.6360, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 15:48:22\n",
            "loss: 0.6395, acc: 0.7362\n",
            "E2E-ABSA >>> 2022-08-17 15:48:23\n",
            "loss: 0.6324, acc: 0.7425\n",
            "E2E-ABSA >>> 2022-08-17 15:48:24\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:24\n",
            "loss: 0.6306, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 15:48:25\n",
            "loss: 0.6328, acc: 0.7410\n",
            "E2E-ABSA >>> 2022-08-17 15:48:26\n",
            "loss: 0.6385, acc: 0.7368\n",
            "E2E-ABSA >>> 2022-08-17 15:48:27\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:27\n",
            "loss: 0.6134, acc: 0.7512\n",
            "E2E-ABSA >>> 2022-08-17 15:48:28\n",
            "loss: 0.6345, acc: 0.7360\n",
            "E2E-ABSA >>> 2022-08-17 15:48:29\n",
            "loss: 0.6341, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-08-17 15:48:30\n",
            ">>> val_acc: 0.6809, val_precision: 0.6809 val_recall: 0.6809, val_f1: 0.6809\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:31\n",
            "loss: 0.6183, acc: 0.7543\n",
            "E2E-ABSA >>> 2022-08-17 15:48:32\n",
            "loss: 0.6476, acc: 0.7313\n",
            "E2E-ABSA >>> 2022-08-17 15:48:33\n",
            "loss: 0.6394, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 15:48:33\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:34\n",
            "loss: 0.6431, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 15:48:35\n",
            "loss: 0.6471, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 15:48:36\n",
            "loss: 0.6412, acc: 0.7317\n",
            "E2E-ABSA >>> 2022-08-17 15:48:37\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:37\n",
            "loss: 0.6682, acc: 0.7210\n",
            "E2E-ABSA >>> 2022-08-17 15:48:38\n",
            "loss: 0.6342, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 15:48:39\n",
            "loss: 0.6234, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-08-17 15:48:40\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:40\n",
            "loss: 0.6063, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-08-17 15:48:41\n",
            "loss: 0.6123, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-08-17 15:48:42\n",
            "loss: 0.6283, acc: 0.7423\n",
            "E2E-ABSA >>> 2022-08-17 15:48:43\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:43\n",
            "loss: 0.6326, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 15:48:44\n",
            "loss: 0.6405, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 15:48:45\n",
            "loss: 0.6340, acc: 0.7347\n",
            "E2E-ABSA >>> 2022-08-17 15:48:46\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:46\n",
            "loss: 0.5005, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:48:48\n",
            "loss: 0.6393, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-08-17 15:48:49\n",
            "loss: 0.6337, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-08-17 15:48:50\n",
            "loss: 0.6272, acc: 0.7403\n",
            "E2E-ABSA >>> 2022-08-17 15:48:50\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:51\n",
            "loss: 0.6305, acc: 0.7363\n",
            "E2E-ABSA >>> 2022-08-17 15:48:52\n",
            "loss: 0.6353, acc: 0.7388\n",
            "E2E-ABSA >>> 2022-08-17 15:48:53\n",
            "loss: 0.6262, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-08-17 15:48:53\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:54\n",
            "loss: 0.6489, acc: 0.7152\n",
            "E2E-ABSA >>> 2022-08-17 15:48:55\n",
            "loss: 0.6409, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 15:48:56\n",
            "loss: 0.6302, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 15:48:56\n",
            ">>> val_acc: 0.6790, val_precision: 0.6790 val_recall: 0.6790, val_f1: 0.6790\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:48:57\n",
            "loss: 0.6139, acc: 0.7508\n",
            "E2E-ABSA >>> 2022-08-17 15:48:58\n",
            "loss: 0.6298, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 15:48:59\n",
            "loss: 0.6314, acc: 0.7337\n",
            "E2E-ABSA >>> 2022-08-17 15:49:00\n",
            ">>> val_acc: 0.6772, val_precision: 0.6772 val_recall: 0.6772, val_f1: 0.6772\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:00\n",
            "loss: 0.5987, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:49:01\n",
            "loss: 0.6057, acc: 0.7587\n",
            "E2E-ABSA >>> 2022-08-17 15:49:02\n",
            "loss: 0.6240, acc: 0.7447\n",
            "E2E-ABSA >>> 2022-08-17 15:49:03\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            "E2E-ABSA >>> 2022-08-17 15:49:03\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            "you can download the best model from state_dict/td_lstm_acl14shortdata_know_val_f1_0.6976\n",
            ">>> test_acc: 0.6976, test_precision: 0.6976, test_recall: 0.6976, test_f1: 0.6976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **SemEval2014** dataset on model(**TCLSTM**)\n",
        "\n"
      ],
      "metadata": {
        "id": "CrHesOyWV--I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset acl14shortdata --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi1Nt7OdV_CX",
        "outputId": "732de6c9-7c38-40f0-d804-8c9e140ed63a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 4720.\n",
            "> testing dataset count: 520.\n",
            "cuda memory allocated: 23982592\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 3828600\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fc7ffd4ab00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:43\n",
            "loss: 1.0669, acc: 0.4156\n",
            "E2E-ABSA >>> 2022-08-17 15:49:43\n",
            "loss: 1.0319, acc: 0.4656\n",
            "E2E-ABSA >>> 2022-08-17 15:49:44\n",
            ">>> val_acc: 0.5019, val_precision: 0.5019 val_recall: 0.5019, val_f1: 0.5019\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5019\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:44\n",
            "loss: 1.0156, acc: 0.4750\n",
            "E2E-ABSA >>> 2022-08-17 15:49:45\n",
            "loss: 0.9392, acc: 0.5696\n",
            "E2E-ABSA >>> 2022-08-17 15:49:46\n",
            "loss: 0.9496, acc: 0.5546\n",
            "E2E-ABSA >>> 2022-08-17 15:49:47\n",
            ">>> val_acc: 0.5365, val_precision: 0.5365 val_recall: 0.5365, val_f1: 0.5365\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5365\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:47\n",
            "loss: 0.9375, acc: 0.5687\n",
            "E2E-ABSA >>> 2022-08-17 15:49:48\n",
            "loss: 0.9436, acc: 0.5670\n",
            "E2E-ABSA >>> 2022-08-17 15:49:49\n",
            "loss: 0.9329, acc: 0.5717\n",
            "E2E-ABSA >>> 2022-08-17 15:49:50\n",
            ">>> val_acc: 0.5423, val_precision: 0.5423 val_recall: 0.5423, val_f1: 0.5423\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5423\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:50\n",
            "loss: 0.9086, acc: 0.5542\n",
            "E2E-ABSA >>> 2022-08-17 15:49:51\n",
            "loss: 0.9145, acc: 0.5875\n",
            "E2E-ABSA >>> 2022-08-17 15:49:52\n",
            "loss: 0.9204, acc: 0.5770\n",
            "E2E-ABSA >>> 2022-08-17 15:49:52\n",
            ">>> val_acc: 0.5519, val_precision: 0.5519 val_recall: 0.5519, val_f1: 0.5519\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5519\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:53\n",
            "loss: 0.9074, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 15:49:54\n",
            "loss: 0.8913, acc: 0.5948\n",
            "E2E-ABSA >>> 2022-08-17 15:49:54\n",
            "loss: 0.8970, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 15:49:55\n",
            ">>> val_acc: 0.5519, val_precision: 0.5519 val_recall: 0.5519, val_f1: 0.5519\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:55\n",
            "loss: 0.8955, acc: 0.6175\n",
            "E2E-ABSA >>> 2022-08-17 15:49:56\n",
            "loss: 0.8994, acc: 0.5900\n",
            "E2E-ABSA >>> 2022-08-17 15:49:57\n",
            "loss: 0.8869, acc: 0.6000\n",
            "E2E-ABSA >>> 2022-08-17 15:49:58\n",
            ">>> val_acc: 0.5481, val_precision: 0.5481 val_recall: 0.5481, val_f1: 0.5481\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:49:58\n",
            "loss: 0.8516, acc: 0.6167\n",
            "E2E-ABSA >>> 2022-08-17 15:49:59\n",
            "loss: 0.8655, acc: 0.6077\n",
            "E2E-ABSA >>> 2022-08-17 15:50:00\n",
            "loss: 0.8679, acc: 0.6060\n",
            "E2E-ABSA >>> 2022-08-17 15:50:00\n",
            ">>> val_acc: 0.5577, val_precision: 0.5577 val_recall: 0.5577, val_f1: 0.5577\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5577\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:01\n",
            "loss: 0.8446, acc: 0.6107\n",
            "E2E-ABSA >>> 2022-08-17 15:50:02\n",
            "loss: 0.8592, acc: 0.6106\n",
            "E2E-ABSA >>> 2022-08-17 15:50:02\n",
            "loss: 0.8587, acc: 0.6109\n",
            "E2E-ABSA >>> 2022-08-17 15:50:03\n",
            ">>> val_acc: 0.5808, val_precision: 0.5808 val_recall: 0.5808, val_f1: 0.5808\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:04\n",
            "loss: 0.8464, acc: 0.6281\n",
            "E2E-ABSA >>> 2022-08-17 15:50:04\n",
            "loss: 0.8494, acc: 0.6170\n",
            "E2E-ABSA >>> 2022-08-17 15:50:05\n",
            "loss: 0.8386, acc: 0.6268\n",
            "E2E-ABSA >>> 2022-08-17 15:50:06\n",
            ">>> val_acc: 0.5865, val_precision: 0.5865 val_recall: 0.5865, val_f1: 0.5865\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:06\n",
            "loss: 0.8244, acc: 0.6194\n",
            "E2E-ABSA >>> 2022-08-17 15:50:07\n",
            "loss: 0.8353, acc: 0.6198\n",
            "E2E-ABSA >>> 2022-08-17 15:50:08\n",
            "loss: 0.8210, acc: 0.6347\n",
            "E2E-ABSA >>> 2022-08-17 15:50:08\n",
            ">>> val_acc: 0.5885, val_precision: 0.5885 val_recall: 0.5885, val_f1: 0.5885\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:09\n",
            "loss: 0.8342, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 15:50:10\n",
            "loss: 0.8204, acc: 0.6329\n",
            "E2E-ABSA >>> 2022-08-17 15:50:11\n",
            "loss: 0.8126, acc: 0.6418\n",
            "E2E-ABSA >>> 2022-08-17 15:50:11\n",
            ">>> val_acc: 0.5942, val_precision: 0.5942 val_recall: 0.5942, val_f1: 0.5942\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.5942\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:12\n",
            "loss: 0.8099, acc: 0.6568\n",
            "E2E-ABSA >>> 2022-08-17 15:50:13\n",
            "loss: 0.7907, acc: 0.6601\n",
            "E2E-ABSA >>> 2022-08-17 15:50:13\n",
            "loss: 0.7943, acc: 0.6547\n",
            "E2E-ABSA >>> 2022-08-17 15:50:14\n",
            ">>> val_acc: 0.6173, val_precision: 0.6173 val_recall: 0.6173, val_f1: 0.6173\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6173\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:14\n",
            "loss: 0.7722, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 15:50:15\n",
            "loss: 0.7756, acc: 0.6617\n",
            "E2E-ABSA >>> 2022-08-17 15:50:16\n",
            "loss: 0.7873, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-08-17 15:50:17\n",
            ">>> val_acc: 0.6154, val_precision: 0.6154 val_recall: 0.6154, val_f1: 0.6154\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:17\n",
            "loss: 0.7597, acc: 0.6635\n",
            "E2E-ABSA >>> 2022-08-17 15:50:18\n",
            "loss: 0.7648, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-08-17 15:50:19\n",
            "loss: 0.7701, acc: 0.6634\n",
            "E2E-ABSA >>> 2022-08-17 15:50:19\n",
            ">>> val_acc: 0.6173, val_precision: 0.6173 val_recall: 0.6173, val_f1: 0.6173\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:20\n",
            "loss: 0.7741, acc: 0.6705\n",
            "E2E-ABSA >>> 2022-08-17 15:50:21\n",
            "loss: 0.7719, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-08-17 15:50:21\n",
            "loss: 0.7620, acc: 0.6708\n",
            "E2E-ABSA >>> 2022-08-17 15:50:22\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:23\n",
            "loss: 0.7759, acc: 0.6608\n",
            "E2E-ABSA >>> 2022-08-17 15:50:23\n",
            "loss: 0.7549, acc: 0.6754\n",
            "E2E-ABSA >>> 2022-08-17 15:50:24\n",
            "loss: 0.7454, acc: 0.6795\n",
            "E2E-ABSA >>> 2022-08-17 15:50:25\n",
            ">>> val_acc: 0.6385, val_precision: 0.6385 val_recall: 0.6385, val_f1: 0.6385\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6385\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:25\n",
            "loss: 0.7196, acc: 0.6984\n",
            "E2E-ABSA >>> 2022-08-17 15:50:26\n",
            "loss: 0.7395, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-08-17 15:50:27\n",
            "loss: 0.7403, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-08-17 15:50:27\n",
            ">>> val_acc: 0.6404, val_precision: 0.6404 val_recall: 0.6404, val_f1: 0.6404\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6404\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:28\n",
            "loss: 0.7335, acc: 0.6779\n",
            "E2E-ABSA >>> 2022-08-17 15:50:29\n",
            "loss: 0.7191, acc: 0.6861\n",
            "E2E-ABSA >>> 2022-08-17 15:50:30\n",
            "loss: 0.7245, acc: 0.6879\n",
            "E2E-ABSA >>> 2022-08-17 15:50:30\n",
            ">>> val_acc: 0.6577, val_precision: 0.6577 val_recall: 0.6577, val_f1: 0.6577\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6577\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:31\n",
            "loss: 0.7187, acc: 0.6819\n",
            "E2E-ABSA >>> 2022-08-17 15:50:32\n",
            "loss: 0.7141, acc: 0.6928\n",
            "E2E-ABSA >>> 2022-08-17 15:50:32\n",
            "loss: 0.7138, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 15:50:33\n",
            ">>> val_acc: 0.6500, val_precision: 0.6500 val_recall: 0.6500, val_f1: 0.6500\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:33\n",
            "loss: 0.7149, acc: 0.6954\n",
            "E2E-ABSA >>> 2022-08-17 15:50:35\n",
            "loss: 0.6939, acc: 0.7016\n",
            "E2E-ABSA >>> 2022-08-17 15:50:36\n",
            "loss: 0.7038, acc: 0.6985\n",
            "E2E-ABSA >>> 2022-08-17 15:50:36\n",
            ">>> val_acc: 0.6596, val_precision: 0.6596 val_recall: 0.6596, val_f1: 0.6596\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6596\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:37\n",
            "loss: 0.6892, acc: 0.7075\n",
            "E2E-ABSA >>> 2022-08-17 15:50:37\n",
            "loss: 0.6871, acc: 0.7050\n",
            "E2E-ABSA >>> 2022-08-17 15:50:38\n",
            ">>> val_acc: 0.6654, val_precision: 0.6654 val_recall: 0.6654, val_f1: 0.6654\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6654\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:38\n",
            "loss: 0.6054, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:50:39\n",
            "loss: 0.6881, acc: 0.6976\n",
            "E2E-ABSA >>> 2022-08-17 15:50:40\n",
            "loss: 0.6842, acc: 0.7091\n",
            "E2E-ABSA >>> 2022-08-17 15:50:41\n",
            ">>> val_acc: 0.6519, val_precision: 0.6519 val_recall: 0.6519, val_f1: 0.6519\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:41\n",
            "loss: 0.6928, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:50:42\n",
            "loss: 0.6871, acc: 0.6994\n",
            "E2E-ABSA >>> 2022-08-17 15:50:43\n",
            "loss: 0.6730, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 15:50:44\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:44\n",
            "loss: 0.6252, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:50:45\n",
            "loss: 0.6709, acc: 0.7082\n",
            "E2E-ABSA >>> 2022-08-17 15:50:46\n",
            "loss: 0.6711, acc: 0.7145\n",
            "E2E-ABSA >>> 2022-08-17 15:50:46\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:46\n",
            "loss: 0.6660, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 15:50:47\n",
            "loss: 0.6652, acc: 0.7198\n",
            "E2E-ABSA >>> 2022-08-17 15:50:48\n",
            "loss: 0.6694, acc: 0.7134\n",
            "E2E-ABSA >>> 2022-08-17 15:50:49\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:49\n",
            "loss: 0.6426, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 15:50:50\n",
            "loss: 0.6648, acc: 0.7145\n",
            "E2E-ABSA >>> 2022-08-17 15:50:51\n",
            "loss: 0.6639, acc: 0.7181\n",
            "E2E-ABSA >>> 2022-08-17 15:50:52\n",
            ">>> val_acc: 0.6808, val_precision: 0.6808 val_recall: 0.6808, val_f1: 0.6808\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:52\n",
            "loss: 0.6356, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:50:53\n",
            "loss: 0.6718, acc: 0.7144\n",
            "E2E-ABSA >>> 2022-08-17 15:50:54\n",
            "loss: 0.6708, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:50:54\n",
            ">>> val_acc: 0.6808, val_precision: 0.6808 val_recall: 0.6808, val_f1: 0.6808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:55\n",
            "loss: 0.6694, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-08-17 15:50:55\n",
            "loss: 0.6655, acc: 0.7130\n",
            "E2E-ABSA >>> 2022-08-17 15:50:56\n",
            "loss: 0.6631, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-08-17 15:50:57\n",
            ">>> val_acc: 0.6885, val_precision: 0.6885 val_recall: 0.6885, val_f1: 0.6885\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.6885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:50:57\n",
            "loss: 0.6634, acc: 0.7141\n",
            "E2E-ABSA >>> 2022-08-17 15:50:58\n",
            "loss: 0.6608, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 15:50:59\n",
            "loss: 0.6633, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-08-17 15:51:00\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:00\n",
            "loss: 0.6878, acc: 0.6958\n",
            "E2E-ABSA >>> 2022-08-17 15:51:01\n",
            "loss: 0.6532, acc: 0.7272\n",
            "E2E-ABSA >>> 2022-08-17 15:51:02\n",
            "loss: 0.6536, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 15:51:02\n",
            ">>> val_acc: 0.7000, val_precision: 0.7000 val_recall: 0.7000, val_f1: 0.7000\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.7\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:03\n",
            "loss: 0.6687, acc: 0.7175\n",
            "E2E-ABSA >>> 2022-08-17 15:51:03\n",
            "loss: 0.6653, acc: 0.7196\n",
            "E2E-ABSA >>> 2022-08-17 15:51:04\n",
            "loss: 0.6569, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-08-17 15:51:05\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:05\n",
            "loss: 0.6447, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-08-17 15:51:06\n",
            "loss: 0.6469, acc: 0.7185\n",
            "E2E-ABSA >>> 2022-08-17 15:51:07\n",
            "loss: 0.6501, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-08-17 15:51:07\n",
            ">>> val_acc: 0.6904, val_precision: 0.6904 val_recall: 0.6904, val_f1: 0.6904\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:08\n",
            "loss: 0.6459, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:51:09\n",
            "loss: 0.6505, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 15:51:10\n",
            "loss: 0.6470, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 15:51:10\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:11\n",
            "loss: 0.6104, acc: 0.7317\n",
            "E2E-ABSA >>> 2022-08-17 15:51:11\n",
            "loss: 0.6353, acc: 0.7269\n",
            "E2E-ABSA >>> 2022-08-17 15:51:12\n",
            "loss: 0.6454, acc: 0.7257\n",
            "E2E-ABSA >>> 2022-08-17 15:51:13\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:13\n",
            "loss: 0.6451, acc: 0.7339\n",
            "E2E-ABSA >>> 2022-08-17 15:51:14\n",
            "loss: 0.6329, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:51:15\n",
            "loss: 0.6434, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 15:51:15\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:16\n",
            "loss: 0.6382, acc: 0.7408\n",
            "E2E-ABSA >>> 2022-08-17 15:51:17\n",
            "loss: 0.6346, acc: 0.7386\n",
            "E2E-ABSA >>> 2022-08-17 15:51:18\n",
            "loss: 0.6397, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-08-17 15:51:18\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:19\n",
            "loss: 0.6270, acc: 0.7445\n",
            "E2E-ABSA >>> 2022-08-17 15:51:19\n",
            "loss: 0.6194, acc: 0.7476\n",
            "E2E-ABSA >>> 2022-08-17 15:51:20\n",
            "loss: 0.6394, acc: 0.7324\n",
            "E2E-ABSA >>> 2022-08-17 15:51:21\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:21\n",
            "loss: 0.6248, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-08-17 15:51:22\n",
            "loss: 0.6231, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-08-17 15:51:23\n",
            "loss: 0.6300, acc: 0.7346\n",
            "E2E-ABSA >>> 2022-08-17 15:51:23\n",
            ">>> val_acc: 0.6885, val_precision: 0.6885 val_recall: 0.6885, val_f1: 0.6885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:24\n",
            "loss: 0.6208, acc: 0.7431\n",
            "E2E-ABSA >>> 2022-08-17 15:51:25\n",
            "loss: 0.6271, acc: 0.7382\n",
            "E2E-ABSA >>> 2022-08-17 15:51:26\n",
            "loss: 0.6341, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 15:51:26\n",
            ">>> val_acc: 0.6692, val_precision: 0.6692 val_recall: 0.6692, val_f1: 0.6692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:27\n",
            "loss: 0.6135, acc: 0.7487\n",
            "E2E-ABSA >>> 2022-08-17 15:51:27\n",
            "loss: 0.6277, acc: 0.7369\n",
            "E2E-ABSA >>> 2022-08-17 15:51:28\n",
            "loss: 0.6315, acc: 0.7324\n",
            "E2E-ABSA >>> 2022-08-17 15:51:28\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:29\n",
            "loss: 0.6400, acc: 0.7331\n",
            "E2E-ABSA >>> 2022-08-17 15:51:30\n",
            "loss: 0.6297, acc: 0.7425\n",
            "E2E-ABSA >>> 2022-08-17 15:51:31\n",
            ">>> val_acc: 0.6808, val_precision: 0.6808 val_recall: 0.6808, val_f1: 0.6808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:31\n",
            "loss: 0.7737, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 15:51:32\n",
            "loss: 0.6128, acc: 0.7494\n",
            "E2E-ABSA >>> 2022-08-17 15:51:33\n",
            "loss: 0.6171, acc: 0.7427\n",
            "E2E-ABSA >>> 2022-08-17 15:51:34\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:34\n",
            "loss: 0.5931, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 15:51:35\n",
            "loss: 0.6132, acc: 0.7420\n",
            "E2E-ABSA >>> 2022-08-17 15:51:35\n",
            "loss: 0.6143, acc: 0.7390\n",
            "E2E-ABSA >>> 2022-08-17 15:51:36\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:36\n",
            "loss: 0.5797, acc: 0.7667\n",
            "E2E-ABSA >>> 2022-08-17 15:51:37\n",
            "loss: 0.6360, acc: 0.7359\n",
            "E2E-ABSA >>> 2022-08-17 15:51:38\n",
            "loss: 0.6186, acc: 0.7424\n",
            "E2E-ABSA >>> 2022-08-17 15:51:39\n",
            ">>> val_acc: 0.6942, val_precision: 0.6942 val_recall: 0.6942, val_f1: 0.6942\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:39\n",
            "loss: 0.5986, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 15:51:40\n",
            "loss: 0.6161, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:51:41\n",
            "loss: 0.6263, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 15:51:41\n",
            ">>> val_acc: 0.6923, val_precision: 0.6923 val_recall: 0.6923, val_f1: 0.6923\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:42\n",
            "loss: 0.6151, acc: 0.7425\n",
            "E2E-ABSA >>> 2022-08-17 15:51:43\n",
            "loss: 0.6269, acc: 0.7370\n",
            "E2E-ABSA >>> 2022-08-17 15:51:43\n",
            "loss: 0.6252, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 15:51:44\n",
            ">>> val_acc: 0.6904, val_precision: 0.6904 val_recall: 0.6904, val_f1: 0.6904\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:44\n",
            "loss: 0.5780, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:51:45\n",
            "loss: 0.6150, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-08-17 15:51:46\n",
            "loss: 0.6144, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 15:51:47\n",
            ">>> val_acc: 0.6904, val_precision: 0.6904 val_recall: 0.6904, val_f1: 0.6904\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:47\n",
            "loss: 0.6398, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:51:48\n",
            "loss: 0.6213, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 15:51:49\n",
            "loss: 0.6189, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-08-17 15:51:49\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:50\n",
            "loss: 0.5872, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 15:51:51\n",
            "loss: 0.6094, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-08-17 15:51:51\n",
            "loss: 0.6173, acc: 0.7409\n",
            "E2E-ABSA >>> 2022-08-17 15:51:52\n",
            ">>> val_acc: 0.6904, val_precision: 0.6904 val_recall: 0.6904, val_f1: 0.6904\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:52\n",
            "loss: 0.5816, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-08-17 15:51:53\n",
            "loss: 0.6096, acc: 0.7401\n",
            "E2E-ABSA >>> 2022-08-17 15:51:54\n",
            "loss: 0.6191, acc: 0.7423\n",
            "E2E-ABSA >>> 2022-08-17 15:51:55\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:55\n",
            "loss: 0.6137, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-08-17 15:51:56\n",
            "loss: 0.6056, acc: 0.7488\n",
            "E2E-ABSA >>> 2022-08-17 15:51:57\n",
            "loss: 0.6093, acc: 0.7458\n",
            "E2E-ABSA >>> 2022-08-17 15:51:57\n",
            ">>> val_acc: 0.6904, val_precision: 0.6904 val_recall: 0.6904, val_f1: 0.6904\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:51:58\n",
            "loss: 0.6028, acc: 0.7489\n",
            "E2E-ABSA >>> 2022-08-17 15:51:59\n",
            "loss: 0.6167, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-08-17 15:51:59\n",
            "loss: 0.6077, acc: 0.7512\n",
            "E2E-ABSA >>> 2022-08-17 15:52:00\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:00\n",
            "loss: 0.6168, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:52:01\n",
            "loss: 0.6222, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 15:52:02\n",
            "loss: 0.6135, acc: 0.7495\n",
            "E2E-ABSA >>> 2022-08-17 15:52:03\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:03\n",
            "loss: 0.6158, acc: 0.7413\n",
            "E2E-ABSA >>> 2022-08-17 15:52:04\n",
            "loss: 0.6075, acc: 0.7489\n",
            "E2E-ABSA >>> 2022-08-17 15:52:05\n",
            "loss: 0.6049, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 15:52:05\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:06\n",
            "loss: 0.5878, acc: 0.7598\n",
            "E2E-ABSA >>> 2022-08-17 15:52:07\n",
            "loss: 0.5866, acc: 0.7592\n",
            "E2E-ABSA >>> 2022-08-17 15:52:08\n",
            "loss: 0.5974, acc: 0.7523\n",
            "E2E-ABSA >>> 2022-08-17 15:52:08\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:09\n",
            "loss: 0.6173, acc: 0.7417\n",
            "E2E-ABSA >>> 2022-08-17 15:52:09\n",
            "loss: 0.6000, acc: 0.7514\n",
            "E2E-ABSA >>> 2022-08-17 15:52:10\n",
            "loss: 0.5983, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-08-17 15:52:11\n",
            ">>> val_acc: 0.6942, val_precision: 0.6942 val_recall: 0.6942, val_f1: 0.6942\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:11\n",
            "loss: 0.6116, acc: 0.7414\n",
            "E2E-ABSA >>> 2022-08-17 15:52:12\n",
            "loss: 0.6058, acc: 0.7503\n",
            "E2E-ABSA >>> 2022-08-17 15:52:13\n",
            "loss: 0.6013, acc: 0.7522\n",
            "E2E-ABSA >>> 2022-08-17 15:52:13\n",
            ">>> val_acc: 0.6923, val_precision: 0.6923 val_recall: 0.6923, val_f1: 0.6923\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:14\n",
            "loss: 0.6057, acc: 0.7456\n",
            "E2E-ABSA >>> 2022-08-17 15:52:15\n",
            "loss: 0.6118, acc: 0.7463\n",
            "E2E-ABSA >>> 2022-08-17 15:52:16\n",
            "loss: 0.5980, acc: 0.7507\n",
            "E2E-ABSA >>> 2022-08-17 15:52:16\n",
            ">>> val_acc: 0.6923, val_precision: 0.6923 val_recall: 0.6923, val_f1: 0.6923\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:16\n",
            "loss: 0.5523, acc: 0.7826\n",
            "E2E-ABSA >>> 2022-08-17 15:52:17\n",
            "loss: 0.5893, acc: 0.7576\n",
            "E2E-ABSA >>> 2022-08-17 15:52:18\n",
            "loss: 0.5992, acc: 0.7532\n",
            "E2E-ABSA >>> 2022-08-17 15:52:18\n",
            ">>> val_acc: 0.6942, val_precision: 0.6942 val_recall: 0.6942, val_f1: 0.6942\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:19\n",
            "loss: 0.5899, acc: 0.7546\n",
            "E2E-ABSA >>> 2022-08-17 15:52:20\n",
            "loss: 0.5878, acc: 0.7561\n",
            "E2E-ABSA >>> 2022-08-17 15:52:21\n",
            "loss: 0.5955, acc: 0.7547\n",
            "E2E-ABSA >>> 2022-08-17 15:52:21\n",
            ">>> val_acc: 0.7019, val_precision: 0.7019 val_recall: 0.7019, val_f1: 0.7019\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_val_f1_0.7019\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:22\n",
            "loss: 0.5984, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-08-17 15:52:23\n",
            "loss: 0.5863, acc: 0.7606\n",
            "E2E-ABSA >>> 2022-08-17 15:52:24\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:24\n",
            "loss: 0.6170, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:52:25\n",
            "loss: 0.5877, acc: 0.7482\n",
            "E2E-ABSA >>> 2022-08-17 15:52:25\n",
            "loss: 0.5867, acc: 0.7561\n",
            "E2E-ABSA >>> 2022-08-17 15:52:26\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:26\n",
            "loss: 0.4966, acc: 0.7937\n",
            "E2E-ABSA >>> 2022-08-17 15:52:27\n",
            "loss: 0.5618, acc: 0.7733\n",
            "E2E-ABSA >>> 2022-08-17 15:52:28\n",
            "loss: 0.5841, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 15:52:29\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:29\n",
            "loss: 0.5565, acc: 0.7542\n",
            "E2E-ABSA >>> 2022-08-17 15:52:30\n",
            "loss: 0.5613, acc: 0.7663\n",
            "E2E-ABSA >>> 2022-08-17 15:52:31\n",
            "loss: 0.5790, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-08-17 15:52:31\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:32\n",
            "loss: 0.5262, acc: 0.8187\n",
            "E2E-ABSA >>> 2022-08-17 15:52:32\n",
            "loss: 0.5702, acc: 0.7729\n",
            "E2E-ABSA >>> 2022-08-17 15:52:33\n",
            "loss: 0.5838, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 15:52:34\n",
            ">>> val_acc: 0.6731, val_precision: 0.6731 val_recall: 0.6731, val_f1: 0.6731\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:34\n",
            "loss: 0.5167, acc: 0.7950\n",
            "E2E-ABSA >>> 2022-08-17 15:52:35\n",
            "loss: 0.5741, acc: 0.7755\n",
            "E2E-ABSA >>> 2022-08-17 15:52:36\n",
            "loss: 0.5908, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-08-17 15:52:37\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:37\n",
            "loss: 0.6171, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 15:52:38\n",
            "loss: 0.6024, acc: 0.7558\n",
            "E2E-ABSA >>> 2022-08-17 15:52:39\n",
            "loss: 0.5876, acc: 0.7603\n",
            "E2E-ABSA >>> 2022-08-17 15:52:39\n",
            ">>> val_acc: 0.6635, val_precision: 0.6635 val_recall: 0.6635, val_f1: 0.6635\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:40\n",
            "loss: 0.5975, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:52:40\n",
            "loss: 0.5776, acc: 0.7597\n",
            "E2E-ABSA >>> 2022-08-17 15:52:41\n",
            "loss: 0.5800, acc: 0.7620\n",
            "E2E-ABSA >>> 2022-08-17 15:52:42\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:42\n",
            "loss: 0.5394, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 15:52:43\n",
            "loss: 0.5650, acc: 0.7737\n",
            "E2E-ABSA >>> 2022-08-17 15:52:44\n",
            "loss: 0.5810, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 15:52:45\n",
            ">>> val_acc: 0.6654, val_precision: 0.6654 val_recall: 0.6654, val_f1: 0.6654\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:45\n",
            "loss: 0.5328, acc: 0.7889\n",
            "E2E-ABSA >>> 2022-08-17 15:52:46\n",
            "loss: 0.5651, acc: 0.7647\n",
            "E2E-ABSA >>> 2022-08-17 15:52:47\n",
            "loss: 0.5786, acc: 0.7612\n",
            "E2E-ABSA >>> 2022-08-17 15:52:47\n",
            ">>> val_acc: 0.6885, val_precision: 0.6885 val_recall: 0.6885, val_f1: 0.6885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:48\n",
            "loss: 0.5816, acc: 0.7588\n",
            "E2E-ABSA >>> 2022-08-17 15:52:48\n",
            "loss: 0.5839, acc: 0.7638\n",
            "E2E-ABSA >>> 2022-08-17 15:52:49\n",
            "loss: 0.5747, acc: 0.7642\n",
            "E2E-ABSA >>> 2022-08-17 15:52:50\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:50\n",
            "loss: 0.5453, acc: 0.7807\n",
            "E2E-ABSA >>> 2022-08-17 15:52:51\n",
            "loss: 0.5553, acc: 0.7754\n",
            "E2E-ABSA >>> 2022-08-17 15:52:52\n",
            "loss: 0.5732, acc: 0.7645\n",
            "E2E-ABSA >>> 2022-08-17 15:52:52\n",
            ">>> val_acc: 0.6885, val_precision: 0.6885 val_recall: 0.6885, val_f1: 0.6885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:53\n",
            "loss: 0.5830, acc: 0.7573\n",
            "E2E-ABSA >>> 2022-08-17 15:52:54\n",
            "loss: 0.5657, acc: 0.7645\n",
            "E2E-ABSA >>> 2022-08-17 15:52:55\n",
            "loss: 0.5780, acc: 0.7623\n",
            "E2E-ABSA >>> 2022-08-17 15:52:55\n",
            ">>> val_acc: 0.6923, val_precision: 0.6923 val_recall: 0.6923, val_f1: 0.6923\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:56\n",
            "loss: 0.5643, acc: 0.7798\n",
            "E2E-ABSA >>> 2022-08-17 15:52:57\n",
            "loss: 0.5708, acc: 0.7720\n",
            "E2E-ABSA >>> 2022-08-17 15:52:57\n",
            "loss: 0.5709, acc: 0.7658\n",
            "E2E-ABSA >>> 2022-08-17 15:52:58\n",
            ">>> val_acc: 0.6769, val_precision: 0.6769 val_recall: 0.6769, val_f1: 0.6769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:52:58\n",
            "loss: 0.5397, acc: 0.7786\n",
            "E2E-ABSA >>> 2022-08-17 15:52:59\n",
            "loss: 0.5642, acc: 0.7647\n",
            "E2E-ABSA >>> 2022-08-17 15:53:00\n",
            "loss: 0.5742, acc: 0.7600\n",
            "E2E-ABSA >>> 2022-08-17 15:53:00\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:01\n",
            "loss: 0.5706, acc: 0.7658\n",
            "E2E-ABSA >>> 2022-08-17 15:53:02\n",
            "loss: 0.5722, acc: 0.7693\n",
            "E2E-ABSA >>> 2022-08-17 15:53:03\n",
            "loss: 0.5742, acc: 0.7677\n",
            "E2E-ABSA >>> 2022-08-17 15:53:03\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:04\n",
            "loss: 0.5847, acc: 0.7664\n",
            "E2E-ABSA >>> 2022-08-17 15:53:05\n",
            "loss: 0.5848, acc: 0.7573\n",
            "E2E-ABSA >>> 2022-08-17 15:53:06\n",
            "loss: 0.5712, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 15:53:06\n",
            ">>> val_acc: 0.6808, val_precision: 0.6808 val_recall: 0.6808, val_f1: 0.6808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:07\n",
            "loss: 0.5751, acc: 0.7706\n",
            "E2E-ABSA >>> 2022-08-17 15:53:08\n",
            "loss: 0.5666, acc: 0.7716\n",
            "E2E-ABSA >>> 2022-08-17 15:53:08\n",
            "loss: 0.5658, acc: 0.7711\n",
            "E2E-ABSA >>> 2022-08-17 15:53:09\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:09\n",
            "loss: 0.5577, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 15:53:10\n",
            "loss: 0.5606, acc: 0.7711\n",
            "E2E-ABSA >>> 2022-08-17 15:53:11\n",
            "loss: 0.5679, acc: 0.7653\n",
            "E2E-ABSA >>> 2022-08-17 15:53:11\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:12\n",
            "loss: 0.5589, acc: 0.7803\n",
            "E2E-ABSA >>> 2022-08-17 15:53:13\n",
            "loss: 0.5693, acc: 0.7654\n",
            "E2E-ABSA >>> 2022-08-17 15:53:14\n",
            "loss: 0.5685, acc: 0.7667\n",
            "E2E-ABSA >>> 2022-08-17 15:53:14\n",
            ">>> val_acc: 0.6750, val_precision: 0.6750 val_recall: 0.6750, val_f1: 0.6750\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:15\n",
            "loss: 0.5845, acc: 0.7506\n",
            "E2E-ABSA >>> 2022-08-17 15:53:16\n",
            "loss: 0.5708, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 15:53:17\n",
            ">>> val_acc: 0.6885, val_precision: 0.6885 val_recall: 0.6885, val_f1: 0.6885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:17\n",
            "loss: 0.6950, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 15:53:17\n",
            "loss: 0.5760, acc: 0.7720\n",
            "E2E-ABSA >>> 2022-08-17 15:53:18\n",
            "loss: 0.5604, acc: 0.7771\n",
            "E2E-ABSA >>> 2022-08-17 15:53:19\n",
            ">>> val_acc: 0.6981, val_precision: 0.6981 val_recall: 0.6981, val_f1: 0.6981\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:19\n",
            "loss: 0.4976, acc: 0.8187\n",
            "E2E-ABSA >>> 2022-08-17 15:53:20\n",
            "loss: 0.5537, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:53:21\n",
            "loss: 0.5518, acc: 0.7771\n",
            "E2E-ABSA >>> 2022-08-17 15:53:22\n",
            ">>> val_acc: 0.6923, val_precision: 0.6923 val_recall: 0.6923, val_f1: 0.6923\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:22\n",
            "loss: 0.4841, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-08-17 15:53:23\n",
            "loss: 0.5358, acc: 0.7826\n",
            "E2E-ABSA >>> 2022-08-17 15:53:24\n",
            "loss: 0.5458, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 15:53:24\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:25\n",
            "loss: 0.5989, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 15:53:25\n",
            "loss: 0.5450, acc: 0.7724\n",
            "E2E-ABSA >>> 2022-08-17 15:53:26\n",
            "loss: 0.5525, acc: 0.7736\n",
            "E2E-ABSA >>> 2022-08-17 15:53:27\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:27\n",
            "loss: 0.5840, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 15:53:28\n",
            "loss: 0.5512, acc: 0.7715\n",
            "E2E-ABSA >>> 2022-08-17 15:53:29\n",
            "loss: 0.5626, acc: 0.7694\n",
            "E2E-ABSA >>> 2022-08-17 15:53:30\n",
            ">>> val_acc: 0.6885, val_precision: 0.6885 val_recall: 0.6885, val_f1: 0.6885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:30\n",
            "loss: 0.5814, acc: 0.7667\n",
            "E2E-ABSA >>> 2022-08-17 15:53:31\n",
            "loss: 0.5542, acc: 0.7716\n",
            "E2E-ABSA >>> 2022-08-17 15:53:32\n",
            "loss: 0.5554, acc: 0.7693\n",
            "E2E-ABSA >>> 2022-08-17 15:53:32\n",
            ">>> val_acc: 0.6788, val_precision: 0.6788 val_recall: 0.6788, val_f1: 0.6788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:33\n",
            "loss: 0.5383, acc: 0.7857\n",
            "E2E-ABSA >>> 2022-08-17 15:53:33\n",
            "loss: 0.5601, acc: 0.7759\n",
            "E2E-ABSA >>> 2022-08-17 15:53:34\n",
            "loss: 0.5594, acc: 0.7739\n",
            "E2E-ABSA >>> 2022-08-17 15:53:35\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:35\n",
            "loss: 0.5699, acc: 0.7547\n",
            "E2E-ABSA >>> 2022-08-17 15:53:36\n",
            "loss: 0.5594, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 15:53:37\n",
            "loss: 0.5564, acc: 0.7703\n",
            "E2E-ABSA >>> 2022-08-17 15:53:38\n",
            ">>> val_acc: 0.6808, val_precision: 0.6808 val_recall: 0.6808, val_f1: 0.6808\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 15:53:38\n",
            "loss: 0.5746, acc: 0.7542\n",
            "E2E-ABSA >>> 2022-08-17 15:53:39\n",
            "loss: 0.5549, acc: 0.7668\n",
            "E2E-ABSA >>> 2022-08-17 15:53:40\n",
            "loss: 0.5496, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 15:53:40\n",
            ">>> val_acc: 0.6673, val_precision: 0.6673 val_recall: 0.6673, val_f1: 0.6673\n",
            "E2E-ABSA >>> 2022-08-17 15:53:40\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7019, val_precision: 0.7019 val_recall: 0.7019, val_f1: 0.7019\n",
            "you can download the best model from state_dict/tc_lstm_acl14shortdata_val_f1_0.7019\n",
            ">>> test_acc: 0.7019, test_precision: 0.7019, test_recall: 0.7019, test_f1: 0.7019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **acl14shortdata** dataset on model(**TCLSTM**)\n"
      ],
      "metadata": {
        "id": "ifC0IDucV_GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name tc_lstm --dataset acl14shortdata_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ElgpXGOV_KZ",
        "outputId": "5b9d611b-ed59-4d7a-f5cd-d8d7dded4132"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 4917.\n",
            "> testing dataset count: 539.\n",
            "cuda memory allocated: 25445376\n",
            "> n_trainable_params: 2166603, n_nontrainable_params: 4115100\n",
            "> training arguments:\n",
            ">>> model_name: tc_lstm\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f7e4c07ab00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.tc_lstm.TC_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:20\n",
            "loss: 1.0567, acc: 0.4331\n",
            "E2E-ABSA >>> 2022-08-17 15:54:22\n",
            "loss: 1.0251, acc: 0.4803\n",
            "E2E-ABSA >>> 2022-08-17 15:54:23\n",
            "loss: 1.0085, acc: 0.4969\n",
            "E2E-ABSA >>> 2022-08-17 15:54:23\n",
            ">>> val_acc: 0.5232, val_precision: 0.5232 val_recall: 0.5232, val_f1: 0.5232\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:24\n",
            "loss: 0.9616, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-08-17 15:54:25\n",
            "loss: 0.9509, acc: 0.5537\n",
            "E2E-ABSA >>> 2022-08-17 15:54:26\n",
            "loss: 0.9454, acc: 0.5574\n",
            "E2E-ABSA >>> 2022-08-17 15:54:27\n",
            ">>> val_acc: 0.5417, val_precision: 0.5417 val_recall: 0.5417, val_f1: 0.5417\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5417\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:28\n",
            "loss: 0.9068, acc: 0.5789\n",
            "E2E-ABSA >>> 2022-08-17 15:54:29\n",
            "loss: 0.9218, acc: 0.5713\n",
            "E2E-ABSA >>> 2022-08-17 15:54:30\n",
            "loss: 0.9234, acc: 0.5744\n",
            "E2E-ABSA >>> 2022-08-17 15:54:30\n",
            ">>> val_acc: 0.5473, val_precision: 0.5473 val_recall: 0.5473, val_f1: 0.5473\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5473\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:31\n",
            "loss: 0.8969, acc: 0.5987\n",
            "E2E-ABSA >>> 2022-08-17 15:54:32\n",
            "loss: 0.9096, acc: 0.5849\n",
            "E2E-ABSA >>> 2022-08-17 15:54:33\n",
            "loss: 0.9070, acc: 0.5851\n",
            "E2E-ABSA >>> 2022-08-17 15:54:34\n",
            ">>> val_acc: 0.5492, val_precision: 0.5492 val_recall: 0.5492, val_f1: 0.5492\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5492\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:35\n",
            "loss: 0.8944, acc: 0.5956\n",
            "E2E-ABSA >>> 2022-08-17 15:54:36\n",
            "loss: 0.8937, acc: 0.5945\n",
            "E2E-ABSA >>> 2022-08-17 15:54:37\n",
            "loss: 0.8935, acc: 0.5924\n",
            "E2E-ABSA >>> 2022-08-17 15:54:37\n",
            ">>> val_acc: 0.5547, val_precision: 0.5547 val_recall: 0.5547, val_f1: 0.5547\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5547\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:38\n",
            "loss: 0.8751, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 15:54:39\n",
            "loss: 0.8807, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 15:54:40\n",
            "loss: 0.8723, acc: 0.6072\n",
            "E2E-ABSA >>> 2022-08-17 15:54:41\n",
            ">>> val_acc: 0.5659, val_precision: 0.5659 val_recall: 0.5659, val_f1: 0.5659\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:42\n",
            "loss: 0.8762, acc: 0.6034\n",
            "E2E-ABSA >>> 2022-08-17 15:54:43\n",
            "loss: 0.8708, acc: 0.6081\n",
            "E2E-ABSA >>> 2022-08-17 15:54:44\n",
            "loss: 0.8665, acc: 0.6089\n",
            "E2E-ABSA >>> 2022-08-17 15:54:45\n",
            ">>> val_acc: 0.5714, val_precision: 0.5714 val_recall: 0.5714, val_f1: 0.5714\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5714\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:45\n",
            "loss: 0.8242, acc: 0.6321\n",
            "E2E-ABSA >>> 2022-08-17 15:54:46\n",
            "loss: 0.8479, acc: 0.6111\n",
            "E2E-ABSA >>> 2022-08-17 15:54:47\n",
            "loss: 0.8444, acc: 0.6178\n",
            "E2E-ABSA >>> 2022-08-17 15:54:48\n",
            ">>> val_acc: 0.5937, val_precision: 0.5937 val_recall: 0.5937, val_f1: 0.5937\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5937\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:49\n",
            "loss: 0.8505, acc: 0.6302\n",
            "E2E-ABSA >>> 2022-08-17 15:54:50\n",
            "loss: 0.8330, acc: 0.6291\n",
            "E2E-ABSA >>> 2022-08-17 15:54:51\n",
            "loss: 0.8292, acc: 0.6316\n",
            "E2E-ABSA >>> 2022-08-17 15:54:52\n",
            ">>> val_acc: 0.5974, val_precision: 0.5974 val_recall: 0.5974, val_f1: 0.5974\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5974\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:52\n",
            "loss: 0.8016, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-08-17 15:54:53\n",
            "loss: 0.8190, acc: 0.6401\n",
            "E2E-ABSA >>> 2022-08-17 15:54:54\n",
            "loss: 0.8133, acc: 0.6472\n",
            "E2E-ABSA >>> 2022-08-17 15:54:56\n",
            ">>> val_acc: 0.5993, val_precision: 0.5993 val_recall: 0.5993, val_f1: 0.5993\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.5993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:56\n",
            "loss: 0.7610, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 15:54:57\n",
            "loss: 0.8100, acc: 0.6385\n",
            "E2E-ABSA >>> 2022-08-17 15:54:58\n",
            "loss: 0.8040, acc: 0.6452\n",
            "E2E-ABSA >>> 2022-08-17 15:54:59\n",
            ">>> val_acc: 0.6141, val_precision: 0.6141 val_recall: 0.6141, val_f1: 0.6141\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6141\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 15:54:59\n",
            "loss: 0.8439, acc: 0.5885\n",
            "E2E-ABSA >>> 2022-08-17 15:55:00\n",
            "loss: 0.7946, acc: 0.6574\n",
            "E2E-ABSA >>> 2022-08-17 15:55:02\n",
            "loss: 0.7899, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-08-17 15:55:03\n",
            ">>> val_acc: 0.6215, val_precision: 0.6215 val_recall: 0.6215, val_f1: 0.6215\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6215\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:03\n",
            "loss: 0.8303, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 15:55:04\n",
            "loss: 0.7658, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-08-17 15:55:05\n",
            "loss: 0.7652, acc: 0.6694\n",
            "E2E-ABSA >>> 2022-08-17 15:55:06\n",
            "loss: 0.7779, acc: 0.6643\n",
            "E2E-ABSA >>> 2022-08-17 15:55:06\n",
            ">>> val_acc: 0.6197, val_precision: 0.6197 val_recall: 0.6197, val_f1: 0.6197\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:07\n",
            "loss: 0.7798, acc: 0.6589\n",
            "E2E-ABSA >>> 2022-08-17 15:55:09\n",
            "loss: 0.7712, acc: 0.6614\n",
            "E2E-ABSA >>> 2022-08-17 15:55:10\n",
            "loss: 0.7656, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-08-17 15:55:10\n",
            ">>> val_acc: 0.6271, val_precision: 0.6271 val_recall: 0.6271, val_f1: 0.6271\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6271\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:11\n",
            "loss: 0.7455, acc: 0.6839\n",
            "E2E-ABSA >>> 2022-08-17 15:55:12\n",
            "loss: 0.7558, acc: 0.6695\n",
            "E2E-ABSA >>> 2022-08-17 15:55:13\n",
            "loss: 0.7588, acc: 0.6710\n",
            "E2E-ABSA >>> 2022-08-17 15:55:14\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:15\n",
            "loss: 0.7552, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-08-17 15:55:16\n",
            "loss: 0.7548, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 15:55:17\n",
            "loss: 0.7482, acc: 0.6777\n",
            "E2E-ABSA >>> 2022-08-17 15:55:17\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:18\n",
            "loss: 0.7001, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 15:55:19\n",
            "loss: 0.7367, acc: 0.6860\n",
            "E2E-ABSA >>> 2022-08-17 15:55:20\n",
            "loss: 0.7352, acc: 0.6815\n",
            "E2E-ABSA >>> 2022-08-17 15:55:21\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:22\n",
            "loss: 0.7269, acc: 0.6836\n",
            "E2E-ABSA >>> 2022-08-17 15:55:23\n",
            "loss: 0.7337, acc: 0.6791\n",
            "E2E-ABSA >>> 2022-08-17 15:55:24\n",
            "loss: 0.7336, acc: 0.6811\n",
            "E2E-ABSA >>> 2022-08-17 15:55:24\n",
            ">>> val_acc: 0.6456, val_precision: 0.6456 val_recall: 0.6456, val_f1: 0.6456\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6456\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:25\n",
            "loss: 0.7119, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 15:55:26\n",
            "loss: 0.7337, acc: 0.6823\n",
            "E2E-ABSA >>> 2022-08-17 15:55:27\n",
            "loss: 0.7190, acc: 0.6885\n",
            "E2E-ABSA >>> 2022-08-17 15:55:28\n",
            ">>> val_acc: 0.6716, val_precision: 0.6716 val_recall: 0.6716, val_f1: 0.6716\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6716\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:29\n",
            "loss: 0.7161, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-08-17 15:55:30\n",
            "loss: 0.7168, acc: 0.6905\n",
            "E2E-ABSA >>> 2022-08-17 15:55:31\n",
            "loss: 0.7162, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-08-17 15:55:32\n",
            ">>> val_acc: 0.6605, val_precision: 0.6605 val_recall: 0.6605, val_f1: 0.6605\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:32\n",
            "loss: 0.7061, acc: 0.6922\n",
            "E2E-ABSA >>> 2022-08-17 15:55:33\n",
            "loss: 0.7170, acc: 0.6946\n",
            "E2E-ABSA >>> 2022-08-17 15:55:35\n",
            "loss: 0.7135, acc: 0.6969\n",
            "E2E-ABSA >>> 2022-08-17 15:55:35\n",
            ">>> val_acc: 0.6642, val_precision: 0.6642 val_recall: 0.6642, val_f1: 0.6642\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:36\n",
            "loss: 0.6765, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-08-17 15:55:37\n",
            "loss: 0.6711, acc: 0.7178\n",
            "E2E-ABSA >>> 2022-08-17 15:55:38\n",
            "loss: 0.6885, acc: 0.7042\n",
            "E2E-ABSA >>> 2022-08-17 15:55:39\n",
            ">>> val_acc: 0.6642, val_precision: 0.6642 val_recall: 0.6642, val_f1: 0.6642\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:39\n",
            "loss: 0.6664, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-08-17 15:55:40\n",
            "loss: 0.6899, acc: 0.7082\n",
            "E2E-ABSA >>> 2022-08-17 15:55:41\n",
            "loss: 0.6815, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-08-17 15:55:43\n",
            ">>> val_acc: 0.6642, val_precision: 0.6642 val_recall: 0.6642, val_f1: 0.6642\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:43\n",
            "loss: 0.7025, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:55:44\n",
            "loss: 0.6685, acc: 0.7112\n",
            "E2E-ABSA >>> 2022-08-17 15:55:45\n",
            "loss: 0.6822, acc: 0.7034\n",
            "E2E-ABSA >>> 2022-08-17 15:55:46\n",
            ">>> val_acc: 0.6698, val_precision: 0.6698 val_recall: 0.6698, val_f1: 0.6698\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:46\n",
            "loss: 0.6144, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 15:55:47\n",
            "loss: 0.6821, acc: 0.7211\n",
            "E2E-ABSA >>> 2022-08-17 15:55:48\n",
            "loss: 0.6856, acc: 0.7142\n",
            "E2E-ABSA >>> 2022-08-17 15:55:49\n",
            "loss: 0.6804, acc: 0.7136\n",
            "E2E-ABSA >>> 2022-08-17 15:55:50\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:51\n",
            "loss: 0.6608, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-08-17 15:55:52\n",
            "loss: 0.6681, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-08-17 15:55:53\n",
            "loss: 0.6746, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 15:55:53\n",
            ">>> val_acc: 0.6642, val_precision: 0.6642 val_recall: 0.6642, val_f1: 0.6642\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:54\n",
            "loss: 0.6660, acc: 0.7160\n",
            "E2E-ABSA >>> 2022-08-17 15:55:55\n",
            "loss: 0.6694, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 15:55:56\n",
            "loss: 0.6710, acc: 0.7162\n",
            "E2E-ABSA >>> 2022-08-17 15:55:57\n",
            ">>> val_acc: 0.6679, val_precision: 0.6679 val_recall: 0.6679, val_f1: 0.6679\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 15:55:58\n",
            "loss: 0.6512, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 15:55:59\n",
            "loss: 0.6573, acc: 0.7269\n",
            "E2E-ABSA >>> 2022-08-17 15:56:00\n",
            "loss: 0.6659, acc: 0.7221\n",
            "E2E-ABSA >>> 2022-08-17 15:56:00\n",
            ">>> val_acc: 0.6772, val_precision: 0.6772 val_recall: 0.6772, val_f1: 0.6772\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:01\n",
            "loss: 0.6635, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 15:56:02\n",
            "loss: 0.6671, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 15:56:03\n",
            "loss: 0.6638, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 15:56:04\n",
            ">>> val_acc: 0.6549, val_precision: 0.6549 val_recall: 0.6549, val_f1: 0.6549\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:05\n",
            "loss: 0.6591, acc: 0.7169\n",
            "E2E-ABSA >>> 2022-08-17 15:56:06\n",
            "loss: 0.6583, acc: 0.7169\n",
            "E2E-ABSA >>> 2022-08-17 15:56:07\n",
            "loss: 0.6609, acc: 0.7183\n",
            "E2E-ABSA >>> 2022-08-17 15:56:08\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:08\n",
            "loss: 0.6436, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 15:56:09\n",
            "loss: 0.6438, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 15:56:10\n",
            "loss: 0.6556, acc: 0.7180\n",
            "E2E-ABSA >>> 2022-08-17 15:56:11\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.692\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:12\n",
            "loss: 0.6561, acc: 0.7356\n",
            "E2E-ABSA >>> 2022-08-17 15:56:13\n",
            "loss: 0.6535, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-08-17 15:56:14\n",
            "loss: 0.6491, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 15:56:15\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:15\n",
            "loss: 0.6810, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 15:56:16\n",
            "loss: 0.6569, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 15:56:17\n",
            "loss: 0.6530, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 15:56:18\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:19\n",
            "loss: 0.6446, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-08-17 15:56:20\n",
            "loss: 0.6590, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-08-17 15:56:21\n",
            "loss: 0.6577, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 15:56:22\n",
            ">>> val_acc: 0.6716, val_precision: 0.6716 val_recall: 0.6716, val_f1: 0.6716\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:22\n",
            "loss: 0.6746, acc: 0.7210\n",
            "E2E-ABSA >>> 2022-08-17 15:56:23\n",
            "loss: 0.6498, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 15:56:24\n",
            "loss: 0.6445, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 15:56:25\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:26\n",
            "loss: 0.6066, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 15:56:27\n",
            "loss: 0.6442, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-08-17 15:56:28\n",
            "loss: 0.6500, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-08-17 15:56:29\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:29\n",
            "loss: 0.5445, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-08-17 15:56:30\n",
            "loss: 0.6265, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:56:31\n",
            "loss: 0.6318, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 15:56:32\n",
            ">>> val_acc: 0.6753, val_precision: 0.6753 val_recall: 0.6753, val_f1: 0.6753\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:33\n",
            "loss: 0.5806, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 15:56:34\n",
            "loss: 0.6296, acc: 0.7410\n",
            "E2E-ABSA >>> 2022-08-17 15:56:35\n",
            "loss: 0.6212, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-08-17 15:56:36\n",
            "loss: 0.6405, acc: 0.7329\n",
            "E2E-ABSA >>> 2022-08-17 15:56:36\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:37\n",
            "loss: 0.6329, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 15:56:38\n",
            "loss: 0.6374, acc: 0.7315\n",
            "E2E-ABSA >>> 2022-08-17 15:56:39\n",
            "loss: 0.6384, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 15:56:40\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:41\n",
            "loss: 0.6061, acc: 0.7507\n",
            "E2E-ABSA >>> 2022-08-17 15:56:42\n",
            "loss: 0.6319, acc: 0.7364\n",
            "E2E-ABSA >>> 2022-08-17 15:56:43\n",
            "loss: 0.6385, acc: 0.7339\n",
            "E2E-ABSA >>> 2022-08-17 15:56:43\n",
            ">>> val_acc: 0.6939, val_precision: 0.6939 val_recall: 0.6939, val_f1: 0.6939\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.6939\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:44\n",
            "loss: 0.6254, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 15:56:45\n",
            "loss: 0.6337, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 15:56:46\n",
            "loss: 0.6325, acc: 0.7366\n",
            "E2E-ABSA >>> 2022-08-17 15:56:47\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:48\n",
            "loss: 0.5983, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-08-17 15:56:49\n",
            "loss: 0.6162, acc: 0.7413\n",
            "E2E-ABSA >>> 2022-08-17 15:56:50\n",
            "loss: 0.6273, acc: 0.7397\n",
            "E2E-ABSA >>> 2022-08-17 15:56:50\n",
            ">>> val_acc: 0.6957, val_precision: 0.6957 val_recall: 0.6957, val_f1: 0.6957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:51\n",
            "loss: 0.6620, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 15:56:52\n",
            "loss: 0.6430, acc: 0.7363\n",
            "E2E-ABSA >>> 2022-08-17 15:56:53\n",
            "loss: 0.6301, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-08-17 15:56:54\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:55\n",
            "loss: 0.6258, acc: 0.7522\n",
            "E2E-ABSA >>> 2022-08-17 15:56:56\n",
            "loss: 0.6468, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-08-17 15:56:57\n",
            "loss: 0.6333, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 15:56:58\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 15:56:58\n",
            "loss: 0.6524, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:56:59\n",
            "loss: 0.6271, acc: 0.7302\n",
            "E2E-ABSA >>> 2022-08-17 15:57:00\n",
            "loss: 0.6267, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-08-17 15:57:01\n",
            ">>> val_acc: 0.6957, val_precision: 0.6957 val_recall: 0.6957, val_f1: 0.6957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:02\n",
            "loss: 0.6545, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 15:57:03\n",
            "loss: 0.6235, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 15:57:04\n",
            "loss: 0.6279, acc: 0.7404\n",
            "E2E-ABSA >>> 2022-08-17 15:57:05\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:05\n",
            "loss: 0.5748, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 15:57:06\n",
            "loss: 0.6100, acc: 0.7415\n",
            "E2E-ABSA >>> 2022-08-17 15:57:07\n",
            "loss: 0.6221, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-08-17 15:57:08\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:09\n",
            "loss: 0.5847, acc: 0.7318\n",
            "E2E-ABSA >>> 2022-08-17 15:57:10\n",
            "loss: 0.6031, acc: 0.7404\n",
            "E2E-ABSA >>> 2022-08-17 15:57:11\n",
            "loss: 0.6100, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-08-17 15:57:12\n",
            ">>> val_acc: 0.6957, val_precision: 0.6957 val_recall: 0.6957, val_f1: 0.6957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:12\n",
            "loss: 0.5876, acc: 0.7617\n",
            "E2E-ABSA >>> 2022-08-17 15:57:13\n",
            "loss: 0.6122, acc: 0.7570\n",
            "E2E-ABSA >>> 2022-08-17 15:57:14\n",
            "loss: 0.6147, acc: 0.7477\n",
            "E2E-ABSA >>> 2022-08-17 15:57:15\n",
            ">>> val_acc: 0.6660, val_precision: 0.6660 val_recall: 0.6660, val_f1: 0.6660\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:16\n",
            "loss: 0.5953, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:57:17\n",
            "loss: 0.6284, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 15:57:18\n",
            "loss: 0.6220, acc: 0.7386\n",
            "E2E-ABSA >>> 2022-08-17 15:57:19\n",
            "loss: 0.6195, acc: 0.7417\n",
            "E2E-ABSA >>> 2022-08-17 15:57:19\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:20\n",
            "loss: 0.6335, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 15:57:21\n",
            "loss: 0.6151, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 15:57:22\n",
            "loss: 0.6192, acc: 0.7450\n",
            "E2E-ABSA >>> 2022-08-17 15:57:23\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:24\n",
            "loss: 0.6044, acc: 0.7493\n",
            "E2E-ABSA >>> 2022-08-17 15:57:25\n",
            "loss: 0.6065, acc: 0.7526\n",
            "E2E-ABSA >>> 2022-08-17 15:57:26\n",
            "loss: 0.6122, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 15:57:26\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:27\n",
            "loss: 0.6159, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-08-17 15:57:28\n",
            "loss: 0.6111, acc: 0.7429\n",
            "E2E-ABSA >>> 2022-08-17 15:57:29\n",
            "loss: 0.6125, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-08-17 15:57:30\n",
            ">>> val_acc: 0.6957, val_precision: 0.6957 val_recall: 0.6957, val_f1: 0.6957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:31\n",
            "loss: 0.5623, acc: 0.7623\n",
            "E2E-ABSA >>> 2022-08-17 15:57:32\n",
            "loss: 0.5924, acc: 0.7557\n",
            "E2E-ABSA >>> 2022-08-17 15:57:33\n",
            "loss: 0.6072, acc: 0.7489\n",
            "E2E-ABSA >>> 2022-08-17 15:57:33\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:34\n",
            "loss: 0.6036, acc: 0.7546\n",
            "E2E-ABSA >>> 2022-08-17 15:57:35\n",
            "loss: 0.5996, acc: 0.7489\n",
            "E2E-ABSA >>> 2022-08-17 15:57:36\n",
            "loss: 0.6050, acc: 0.7463\n",
            "E2E-ABSA >>> 2022-08-17 15:57:37\n",
            ">>> val_acc: 0.6957, val_precision: 0.6957 val_recall: 0.6957, val_f1: 0.6957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:37\n",
            "loss: 0.6048, acc: 0.7479\n",
            "E2E-ABSA >>> 2022-08-17 15:57:39\n",
            "loss: 0.6132, acc: 0.7398\n",
            "E2E-ABSA >>> 2022-08-17 15:57:40\n",
            "loss: 0.6058, acc: 0.7462\n",
            "E2E-ABSA >>> 2022-08-17 15:57:40\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:41\n",
            "loss: 0.5832, acc: 0.7548\n",
            "E2E-ABSA >>> 2022-08-17 15:57:42\n",
            "loss: 0.5947, acc: 0.7574\n",
            "E2E-ABSA >>> 2022-08-17 15:57:43\n",
            "loss: 0.6043, acc: 0.7488\n",
            "E2E-ABSA >>> 2022-08-17 15:57:44\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:44\n",
            "loss: 0.6028, acc: 0.7599\n",
            "E2E-ABSA >>> 2022-08-17 15:57:45\n",
            "loss: 0.6014, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-08-17 15:57:47\n",
            "loss: 0.6066, acc: 0.7428\n",
            "E2E-ABSA >>> 2022-08-17 15:57:47\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:48\n",
            "loss: 0.6079, acc: 0.7517\n",
            "E2E-ABSA >>> 2022-08-17 15:57:49\n",
            "loss: 0.6186, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 15:57:50\n",
            "loss: 0.5977, acc: 0.7503\n",
            "E2E-ABSA >>> 2022-08-17 15:57:51\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:51\n",
            "loss: 0.5706, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 15:57:52\n",
            "loss: 0.6068, acc: 0.7407\n",
            "E2E-ABSA >>> 2022-08-17 15:57:53\n",
            "loss: 0.6055, acc: 0.7445\n",
            "E2E-ABSA >>> 2022-08-17 15:57:55\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:55\n",
            "loss: 0.6108, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 15:57:56\n",
            "loss: 0.5914, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:57:57\n",
            "loss: 0.5944, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-08-17 15:57:58\n",
            ">>> val_acc: 0.7069, val_precision: 0.7069 val_recall: 0.7069, val_f1: 0.7069\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.7069\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 15:57:58\n",
            "loss: 0.4991, acc: 0.8177\n",
            "E2E-ABSA >>> 2022-08-17 15:57:59\n",
            "loss: 0.5958, acc: 0.7511\n",
            "E2E-ABSA >>> 2022-08-17 15:58:01\n",
            "loss: 0.5986, acc: 0.7456\n",
            "E2E-ABSA >>> 2022-08-17 15:58:02\n",
            ">>> val_acc: 0.7069, val_precision: 0.7069 val_recall: 0.7069, val_f1: 0.7069\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:02\n",
            "loss: 0.5643, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 15:58:03\n",
            "loss: 0.5989, acc: 0.7536\n",
            "E2E-ABSA >>> 2022-08-17 15:58:04\n",
            "loss: 0.5998, acc: 0.7574\n",
            "E2E-ABSA >>> 2022-08-17 15:58:05\n",
            "loss: 0.6000, acc: 0.7516\n",
            "E2E-ABSA >>> 2022-08-17 15:58:06\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:07\n",
            "loss: 0.6007, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 15:58:08\n",
            "loss: 0.5879, acc: 0.7538\n",
            "E2E-ABSA >>> 2022-08-17 15:58:09\n",
            "loss: 0.5984, acc: 0.7504\n",
            "E2E-ABSA >>> 2022-08-17 15:58:09\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:10\n",
            "loss: 0.5828, acc: 0.7706\n",
            "E2E-ABSA >>> 2022-08-17 15:58:11\n",
            "loss: 0.5993, acc: 0.7557\n",
            "E2E-ABSA >>> 2022-08-17 15:58:12\n",
            "loss: 0.5982, acc: 0.7569\n",
            "E2E-ABSA >>> 2022-08-17 15:58:13\n",
            ">>> val_acc: 0.6790, val_precision: 0.6790 val_recall: 0.6790, val_f1: 0.6790\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:13\n",
            "loss: 0.6060, acc: 0.7453\n",
            "E2E-ABSA >>> 2022-08-17 15:58:15\n",
            "loss: 0.5942, acc: 0.7503\n",
            "E2E-ABSA >>> 2022-08-17 15:58:16\n",
            "loss: 0.5914, acc: 0.7502\n",
            "E2E-ABSA >>> 2022-08-17 15:58:16\n",
            ">>> val_acc: 0.7069, val_precision: 0.7069 val_recall: 0.7069, val_f1: 0.7069\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:17\n",
            "loss: 0.6190, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 15:58:18\n",
            "loss: 0.5970, acc: 0.7605\n",
            "E2E-ABSA >>> 2022-08-17 15:58:19\n",
            "loss: 0.5940, acc: 0.7576\n",
            "E2E-ABSA >>> 2022-08-17 15:58:20\n",
            ">>> val_acc: 0.7013, val_precision: 0.7013 val_recall: 0.7013, val_f1: 0.7013\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:20\n",
            "loss: 0.5723, acc: 0.7588\n",
            "E2E-ABSA >>> 2022-08-17 15:58:22\n",
            "loss: 0.5866, acc: 0.7557\n",
            "E2E-ABSA >>> 2022-08-17 15:58:23\n",
            "loss: 0.5920, acc: 0.7550\n",
            "E2E-ABSA >>> 2022-08-17 15:58:23\n",
            ">>> val_acc: 0.6865, val_precision: 0.6865 val_recall: 0.6865, val_f1: 0.6865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:24\n",
            "loss: 0.5901, acc: 0.7511\n",
            "E2E-ABSA >>> 2022-08-17 15:58:25\n",
            "loss: 0.5980, acc: 0.7460\n",
            "E2E-ABSA >>> 2022-08-17 15:58:26\n",
            "loss: 0.5952, acc: 0.7498\n",
            "E2E-ABSA >>> 2022-08-17 15:58:27\n",
            ">>> val_acc: 0.7069, val_precision: 0.7069 val_recall: 0.7069, val_f1: 0.7069\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:27\n",
            "loss: 0.5680, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-08-17 15:58:29\n",
            "loss: 0.5736, acc: 0.7665\n",
            "E2E-ABSA >>> 2022-08-17 15:58:30\n",
            "loss: 0.5900, acc: 0.7563\n",
            "E2E-ABSA >>> 2022-08-17 15:58:30\n",
            ">>> val_acc: 0.6827, val_precision: 0.6827 val_recall: 0.6827, val_f1: 0.6827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:31\n",
            "loss: 0.6109, acc: 0.7453\n",
            "E2E-ABSA >>> 2022-08-17 15:58:32\n",
            "loss: 0.5964, acc: 0.7536\n",
            "E2E-ABSA >>> 2022-08-17 15:58:33\n",
            "loss: 0.5838, acc: 0.7586\n",
            "E2E-ABSA >>> 2022-08-17 15:58:34\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:34\n",
            "loss: 0.5828, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-08-17 15:58:35\n",
            "loss: 0.5778, acc: 0.7609\n",
            "E2E-ABSA >>> 2022-08-17 15:58:37\n",
            "loss: 0.5865, acc: 0.7511\n",
            "E2E-ABSA >>> 2022-08-17 15:58:38\n",
            ">>> val_acc: 0.7069, val_precision: 0.7069 val_recall: 0.7069, val_f1: 0.7069\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:38\n",
            "loss: 0.5679, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 15:58:39\n",
            "loss: 0.5892, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 15:58:40\n",
            "loss: 0.5771, acc: 0.7659\n",
            "E2E-ABSA >>> 2022-08-17 15:58:41\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:41\n",
            "loss: 0.6058, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 15:58:42\n",
            "loss: 0.5744, acc: 0.7548\n",
            "E2E-ABSA >>> 2022-08-17 15:58:44\n",
            "loss: 0.5779, acc: 0.7590\n",
            "E2E-ABSA >>> 2022-08-17 15:58:45\n",
            ">>> val_acc: 0.6957, val_precision: 0.6957 val_recall: 0.6957, val_f1: 0.6957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:45\n",
            "loss: 0.6836, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 15:58:46\n",
            "loss: 0.5739, acc: 0.7575\n",
            "E2E-ABSA >>> 2022-08-17 15:58:47\n",
            "loss: 0.5855, acc: 0.7536\n",
            "E2E-ABSA >>> 2022-08-17 15:58:48\n",
            "loss: 0.5821, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 15:58:48\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:49\n",
            "loss: 0.5626, acc: 0.7638\n",
            "E2E-ABSA >>> 2022-08-17 15:58:50\n",
            "loss: 0.5757, acc: 0.7569\n",
            "E2E-ABSA >>> 2022-08-17 15:58:52\n",
            "loss: 0.5789, acc: 0.7565\n",
            "E2E-ABSA >>> 2022-08-17 15:58:52\n",
            ">>> val_acc: 0.6957, val_precision: 0.6957 val_recall: 0.6957, val_f1: 0.6957\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:53\n",
            "loss: 0.5553, acc: 0.7758\n",
            "E2E-ABSA >>> 2022-08-17 15:58:54\n",
            "loss: 0.5748, acc: 0.7633\n",
            "E2E-ABSA >>> 2022-08-17 15:58:55\n",
            "loss: 0.5775, acc: 0.7607\n",
            "E2E-ABSA >>> 2022-08-17 15:58:55\n",
            ">>> val_acc: 0.6939, val_precision: 0.6939 val_recall: 0.6939, val_f1: 0.6939\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 15:58:56\n",
            "loss: 0.5583, acc: 0.7746\n",
            "E2E-ABSA >>> 2022-08-17 15:58:57\n",
            "loss: 0.5649, acc: 0.7745\n",
            "E2E-ABSA >>> 2022-08-17 15:58:59\n",
            "loss: 0.5722, acc: 0.7663\n",
            "E2E-ABSA >>> 2022-08-17 15:58:59\n",
            ">>> val_acc: 0.6994, val_precision: 0.6994 val_recall: 0.6994, val_f1: 0.6994\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:00\n",
            "loss: 0.5645, acc: 0.7607\n",
            "E2E-ABSA >>> 2022-08-17 15:59:01\n",
            "loss: 0.5777, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 15:59:02\n",
            "loss: 0.5698, acc: 0.7661\n",
            "E2E-ABSA >>> 2022-08-17 15:59:03\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:03\n",
            "loss: 0.5485, acc: 0.7757\n",
            "E2E-ABSA >>> 2022-08-17 15:59:04\n",
            "loss: 0.5617, acc: 0.7727\n",
            "E2E-ABSA >>> 2022-08-17 15:59:06\n",
            "loss: 0.5709, acc: 0.7612\n",
            "E2E-ABSA >>> 2022-08-17 15:59:06\n",
            ">>> val_acc: 0.7013, val_precision: 0.7013 val_recall: 0.7013, val_f1: 0.7013\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:07\n",
            "loss: 0.5990, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 15:59:08\n",
            "loss: 0.5884, acc: 0.7559\n",
            "E2E-ABSA >>> 2022-08-17 15:59:09\n",
            "loss: 0.5777, acc: 0.7637\n",
            "E2E-ABSA >>> 2022-08-17 15:59:10\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:10\n",
            "loss: 0.5877, acc: 0.7716\n",
            "E2E-ABSA >>> 2022-08-17 15:59:11\n",
            "loss: 0.5744, acc: 0.7677\n",
            "E2E-ABSA >>> 2022-08-17 15:59:12\n",
            "loss: 0.5787, acc: 0.7622\n",
            "E2E-ABSA >>> 2022-08-17 15:59:13\n",
            ">>> val_acc: 0.7124, val_precision: 0.7124 val_recall: 0.7124, val_f1: 0.7124\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.7124\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:14\n",
            "loss: 0.5750, acc: 0.7599\n",
            "E2E-ABSA >>> 2022-08-17 15:59:15\n",
            "loss: 0.5895, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 15:59:16\n",
            "loss: 0.5742, acc: 0.7613\n",
            "E2E-ABSA >>> 2022-08-17 15:59:17\n",
            ">>> val_acc: 0.7013, val_precision: 0.7013 val_recall: 0.7013, val_f1: 0.7013\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:17\n",
            "loss: 0.5831, acc: 0.7517\n",
            "E2E-ABSA >>> 2022-08-17 15:59:18\n",
            "loss: 0.5771, acc: 0.7652\n",
            "E2E-ABSA >>> 2022-08-17 15:59:19\n",
            "loss: 0.5743, acc: 0.7662\n",
            "E2E-ABSA >>> 2022-08-17 15:59:20\n",
            ">>> val_acc: 0.7069, val_precision: 0.7069 val_recall: 0.7069, val_f1: 0.7069\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:21\n",
            "loss: 0.5841, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 15:59:22\n",
            "loss: 0.5505, acc: 0.7759\n",
            "E2E-ABSA >>> 2022-08-17 15:59:23\n",
            "loss: 0.5616, acc: 0.7717\n",
            "E2E-ABSA >>> 2022-08-17 15:59:24\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:24\n",
            "loss: 0.5993, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 15:59:25\n",
            "loss: 0.5656, acc: 0.7651\n",
            "E2E-ABSA >>> 2022-08-17 15:59:26\n",
            "loss: 0.5632, acc: 0.7699\n",
            "E2E-ABSA >>> 2022-08-17 15:59:28\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:28\n",
            "loss: 0.5890, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 15:59:29\n",
            "loss: 0.5599, acc: 0.7751\n",
            "E2E-ABSA >>> 2022-08-17 15:59:30\n",
            "loss: 0.5648, acc: 0.7671\n",
            "E2E-ABSA >>> 2022-08-17 15:59:31\n",
            ">>> val_acc: 0.7106, val_precision: 0.7106 val_recall: 0.7106, val_f1: 0.7106\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:31\n",
            "loss: 0.4873, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 15:59:32\n",
            "loss: 0.5699, acc: 0.7668\n",
            "E2E-ABSA >>> 2022-08-17 15:59:33\n",
            "loss: 0.5597, acc: 0.7705\n",
            "E2E-ABSA >>> 2022-08-17 15:59:34\n",
            "loss: 0.5694, acc: 0.7660\n",
            "E2E-ABSA >>> 2022-08-17 15:59:35\n",
            ">>> val_acc: 0.6772, val_precision: 0.6772 val_recall: 0.6772, val_f1: 0.6772\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:36\n",
            "loss: 0.5655, acc: 0.7611\n",
            "E2E-ABSA >>> 2022-08-17 15:59:37\n",
            "loss: 0.5572, acc: 0.7691\n",
            "E2E-ABSA >>> 2022-08-17 15:59:38\n",
            "loss: 0.5692, acc: 0.7650\n",
            "E2E-ABSA >>> 2022-08-17 15:59:38\n",
            ">>> val_acc: 0.7032, val_precision: 0.7032 val_recall: 0.7032, val_f1: 0.7032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:39\n",
            "loss: 0.5805, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 15:59:40\n",
            "loss: 0.5729, acc: 0.7606\n",
            "E2E-ABSA >>> 2022-08-17 15:59:41\n",
            "loss: 0.5635, acc: 0.7669\n",
            "E2E-ABSA >>> 2022-08-17 15:59:42\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:43\n",
            "loss: 0.5680, acc: 0.7711\n",
            "E2E-ABSA >>> 2022-08-17 15:59:44\n",
            "loss: 0.5544, acc: 0.7757\n",
            "E2E-ABSA >>> 2022-08-17 15:59:45\n",
            "loss: 0.5678, acc: 0.7690\n",
            "E2E-ABSA >>> 2022-08-17 15:59:45\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:46\n",
            "loss: 0.5699, acc: 0.7691\n",
            "E2E-ABSA >>> 2022-08-17 15:59:47\n",
            "loss: 0.5690, acc: 0.7689\n",
            "E2E-ABSA >>> 2022-08-17 15:59:48\n",
            "loss: 0.5643, acc: 0.7698\n",
            "E2E-ABSA >>> 2022-08-17 15:59:49\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:50\n",
            "loss: 0.5542, acc: 0.7568\n",
            "E2E-ABSA >>> 2022-08-17 15:59:51\n",
            "loss: 0.5529, acc: 0.7683\n",
            "E2E-ABSA >>> 2022-08-17 15:59:52\n",
            "loss: 0.5548, acc: 0.7699\n",
            "E2E-ABSA >>> 2022-08-17 15:59:52\n",
            ">>> val_acc: 0.7143, val_precision: 0.7143 val_recall: 0.7143, val_f1: 0.7143\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.7143\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:53\n",
            "loss: 0.5263, acc: 0.7879\n",
            "E2E-ABSA >>> 2022-08-17 15:59:54\n",
            "loss: 0.5524, acc: 0.7756\n",
            "E2E-ABSA >>> 2022-08-17 15:59:55\n",
            "loss: 0.5580, acc: 0.7722\n",
            "E2E-ABSA >>> 2022-08-17 15:59:56\n",
            ">>> val_acc: 0.6883, val_precision: 0.6883 val_recall: 0.6883, val_f1: 0.6883\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-08-17 15:59:57\n",
            "loss: 0.5404, acc: 0.7839\n",
            "E2E-ABSA >>> 2022-08-17 15:59:58\n",
            "loss: 0.5556, acc: 0.7665\n",
            "E2E-ABSA >>> 2022-08-17 15:59:59\n",
            "loss: 0.5602, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:00:00\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-08-17 16:00:00\n",
            "loss: 0.5093, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-08-17 16:00:01\n",
            "loss: 0.5340, acc: 0.7888\n",
            "E2E-ABSA >>> 2022-08-17 16:00:02\n",
            "loss: 0.5528, acc: 0.7768\n",
            "E2E-ABSA >>> 2022-08-17 16:00:03\n",
            ">>> val_acc: 0.6920, val_precision: 0.6920 val_recall: 0.6920, val_f1: 0.6920\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-08-17 16:00:03\n",
            "loss: 0.5483, acc: 0.7773\n",
            "E2E-ABSA >>> 2022-08-17 16:00:05\n",
            "loss: 0.5605, acc: 0.7704\n",
            "E2E-ABSA >>> 2022-08-17 16:00:06\n",
            "loss: 0.5519, acc: 0.7702\n",
            "E2E-ABSA >>> 2022-08-17 16:00:07\n",
            ">>> val_acc: 0.6846, val_precision: 0.6846 val_recall: 0.6846, val_f1: 0.6846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-08-17 16:00:07\n",
            "loss: 0.5886, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 16:00:08\n",
            "loss: 0.5565, acc: 0.7747\n",
            "E2E-ABSA >>> 2022-08-17 16:00:09\n",
            "loss: 0.5470, acc: 0.7804\n",
            "E2E-ABSA >>> 2022-08-17 16:00:10\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-08-17 16:00:10\n",
            "loss: 0.4177, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 16:00:11\n",
            "loss: 0.5402, acc: 0.7829\n",
            "E2E-ABSA >>> 2022-08-17 16:00:13\n",
            "loss: 0.5505, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 16:00:14\n",
            ">>> val_acc: 0.7180, val_precision: 0.7180 val_recall: 0.7180, val_f1: 0.7180\n",
            ">> saved: state_dict/tc_lstm_acl14shortdata_know_val_f1_0.718\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-08-17 16:00:14\n",
            "loss: 0.5160, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:00:15\n",
            "loss: 0.5564, acc: 0.7697\n",
            "E2E-ABSA >>> 2022-08-17 16:00:16\n",
            "loss: 0.5505, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 16:00:17\n",
            "loss: 0.5533, acc: 0.7732\n",
            "E2E-ABSA >>> 2022-08-17 16:00:17\n",
            ">>> val_acc: 0.6902, val_precision: 0.6902 val_recall: 0.6902, val_f1: 0.6902\n",
            "you can download the best model from state_dict/tc_lstm_acl14shortdata_know_val_f1_0.718\n",
            ">>> test_acc: 0.7180, test_precision: 0.7180, test_recall: 0.7180, test_f1: 0.7180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **acl14shortdata** dataset on model(**ATAELSTM**)\n"
      ],
      "metadata": {
        "id": "M7YrWWyAV_Op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset acl14shortdata --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLFRZ11uV_S_",
        "outputId": "0de3a6c4-2e4a-4d14-e3b6-9548b054af1f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 25442816\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 3828600\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f01d7b8eb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 16:00:58\n",
            "loss: 1.0820, acc: 0.3875\n",
            "E2E-ABSA >>> 2022-08-17 16:00:58\n",
            "loss: 1.0470, acc: 0.4519\n",
            "E2E-ABSA >>> 2022-08-17 16:00:59\n",
            "loss: 1.0296, acc: 0.4719\n",
            "E2E-ABSA >>> 2022-08-17 16:01:00\n",
            ">>> val_acc: 0.5024, val_precision: 0.5024 val_recall: 0.5024, val_f1: 0.5024\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.5024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:00\n",
            "loss: 0.9591, acc: 0.5417\n",
            "E2E-ABSA >>> 2022-08-17 16:01:01\n",
            "loss: 0.9517, acc: 0.5511\n",
            "E2E-ABSA >>> 2022-08-17 16:01:02\n",
            "loss: 0.9569, acc: 0.5421\n",
            "E2E-ABSA >>> 2022-08-17 16:01:03\n",
            "loss: 0.9529, acc: 0.5483\n",
            "E2E-ABSA >>> 2022-08-17 16:01:03\n",
            ">>> val_acc: 0.5440, val_precision: 0.5440 val_recall: 0.5440, val_f1: 0.5440\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:04\n",
            "loss: 0.9324, acc: 0.5742\n",
            "E2E-ABSA >>> 2022-08-17 16:01:05\n",
            "loss: 0.9218, acc: 0.5733\n",
            "E2E-ABSA >>> 2022-08-17 16:01:06\n",
            "loss: 0.9211, acc: 0.5769\n",
            "E2E-ABSA >>> 2022-08-17 16:01:07\n",
            ">>> val_acc: 0.5520, val_precision: 0.5520 val_recall: 0.5520, val_f1: 0.5520\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.552\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:07\n",
            "loss: 0.8895, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 16:01:08\n",
            "loss: 0.8887, acc: 0.5994\n",
            "E2E-ABSA >>> 2022-08-17 16:01:09\n",
            "loss: 0.8904, acc: 0.5976\n",
            "E2E-ABSA >>> 2022-08-17 16:01:10\n",
            "loss: 0.8856, acc: 0.6057\n",
            "E2E-ABSA >>> 2022-08-17 16:01:10\n",
            ">>> val_acc: 0.5664, val_precision: 0.5664 val_recall: 0.5664, val_f1: 0.5664\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.5664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:11\n",
            "loss: 0.8730, acc: 0.6148\n",
            "E2E-ABSA >>> 2022-08-17 16:01:12\n",
            "loss: 0.8661, acc: 0.6120\n",
            "E2E-ABSA >>> 2022-08-17 16:01:13\n",
            "loss: 0.8674, acc: 0.6109\n",
            "E2E-ABSA >>> 2022-08-17 16:01:13\n",
            ">>> val_acc: 0.5840, val_precision: 0.5840 val_recall: 0.5840, val_f1: 0.5840\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.584\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:14\n",
            "loss: 0.8740, acc: 0.6016\n",
            "E2E-ABSA >>> 2022-08-17 16:01:15\n",
            "loss: 0.8459, acc: 0.6214\n",
            "E2E-ABSA >>> 2022-08-17 16:01:16\n",
            "loss: 0.8364, acc: 0.6294\n",
            "E2E-ABSA >>> 2022-08-17 16:01:17\n",
            "loss: 0.8348, acc: 0.6320\n",
            "E2E-ABSA >>> 2022-08-17 16:01:17\n",
            ">>> val_acc: 0.5984, val_precision: 0.5984 val_recall: 0.5984, val_f1: 0.5984\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.5984\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:18\n",
            "loss: 0.8041, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 16:01:19\n",
            "loss: 0.8024, acc: 0.6556\n",
            "E2E-ABSA >>> 2022-08-17 16:01:20\n",
            "loss: 0.8019, acc: 0.6530\n",
            "E2E-ABSA >>> 2022-08-17 16:01:20\n",
            ">>> val_acc: 0.6144, val_precision: 0.6144 val_recall: 0.6144, val_f1: 0.6144\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6144\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:21\n",
            "loss: 0.7733, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 16:01:22\n",
            "loss: 0.7502, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-08-17 16:01:22\n",
            "loss: 0.7705, acc: 0.6584\n",
            "E2E-ABSA >>> 2022-08-17 16:01:23\n",
            "loss: 0.7731, acc: 0.6572\n",
            "E2E-ABSA >>> 2022-08-17 16:01:24\n",
            ">>> val_acc: 0.6304, val_precision: 0.6304 val_recall: 0.6304, val_f1: 0.6304\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6304\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:24\n",
            "loss: 0.7458, acc: 0.6838\n",
            "E2E-ABSA >>> 2022-08-17 16:01:25\n",
            "loss: 0.7582, acc: 0.6681\n",
            "E2E-ABSA >>> 2022-08-17 16:01:26\n",
            "loss: 0.7466, acc: 0.6747\n",
            "E2E-ABSA >>> 2022-08-17 16:01:27\n",
            ">>> val_acc: 0.6256, val_precision: 0.6256 val_recall: 0.6256, val_f1: 0.6256\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:27\n",
            "loss: 0.7822, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-08-17 16:01:28\n",
            "loss: 0.7308, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-08-17 16:01:29\n",
            "loss: 0.7308, acc: 0.6789\n",
            "E2E-ABSA >>> 2022-08-17 16:01:30\n",
            "loss: 0.7359, acc: 0.6800\n",
            "E2E-ABSA >>> 2022-08-17 16:01:30\n",
            ">>> val_acc: 0.6464, val_precision: 0.6464 val_recall: 0.6464, val_f1: 0.6464\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6464\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:31\n",
            "loss: 0.7172, acc: 0.6945\n",
            "E2E-ABSA >>> 2022-08-17 16:01:32\n",
            "loss: 0.7216, acc: 0.6854\n",
            "E2E-ABSA >>> 2022-08-17 16:01:33\n",
            "loss: 0.7253, acc: 0.6873\n",
            "E2E-ABSA >>> 2022-08-17 16:01:34\n",
            ">>> val_acc: 0.6512, val_precision: 0.6512 val_recall: 0.6512, val_f1: 0.6512\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6512\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:34\n",
            "loss: 0.6988, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 16:01:35\n",
            "loss: 0.7076, acc: 0.6929\n",
            "E2E-ABSA >>> 2022-08-17 16:01:36\n",
            "loss: 0.7127, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-08-17 16:01:37\n",
            "loss: 0.7134, acc: 0.6955\n",
            "E2E-ABSA >>> 2022-08-17 16:01:37\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:38\n",
            "loss: 0.7009, acc: 0.6941\n",
            "E2E-ABSA >>> 2022-08-17 16:01:39\n",
            "loss: 0.7089, acc: 0.6935\n",
            "E2E-ABSA >>> 2022-08-17 16:01:39\n",
            "loss: 0.7126, acc: 0.6907\n",
            "E2E-ABSA >>> 2022-08-17 16:01:40\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:41\n",
            "loss: 0.6908, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:01:41\n",
            "loss: 0.6927, acc: 0.7107\n",
            "E2E-ABSA >>> 2022-08-17 16:01:42\n",
            "loss: 0.6967, acc: 0.7048\n",
            "E2E-ABSA >>> 2022-08-17 16:01:43\n",
            "loss: 0.6931, acc: 0.7033\n",
            "E2E-ABSA >>> 2022-08-17 16:01:44\n",
            ">>> val_acc: 0.6640, val_precision: 0.6640 val_recall: 0.6640, val_f1: 0.6640\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:44\n",
            "loss: 0.6823, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-08-17 16:01:45\n",
            "loss: 0.6947, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 16:01:46\n",
            "loss: 0.6922, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 16:01:47\n",
            ">>> val_acc: 0.6672, val_precision: 0.6672 val_recall: 0.6672, val_f1: 0.6672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:47\n",
            "loss: 0.6840, acc: 0.6906\n",
            "E2E-ABSA >>> 2022-08-17 16:01:48\n",
            "loss: 0.6794, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 16:01:49\n",
            "loss: 0.6892, acc: 0.7037\n",
            "E2E-ABSA >>> 2022-08-17 16:01:50\n",
            "loss: 0.6907, acc: 0.7055\n",
            "E2E-ABSA >>> 2022-08-17 16:01:50\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:51\n",
            "loss: 0.6731, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:01:52\n",
            "loss: 0.6841, acc: 0.7035\n",
            "E2E-ABSA >>> 2022-08-17 16:01:53\n",
            "loss: 0.6812, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-08-17 16:01:54\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:54\n",
            "loss: 0.6530, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 16:01:55\n",
            "loss: 0.6841, acc: 0.7171\n",
            "E2E-ABSA >>> 2022-08-17 16:01:55\n",
            "loss: 0.6773, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 16:01:56\n",
            "loss: 0.6810, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-08-17 16:01:57\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 16:01:57\n",
            "loss: 0.7020, acc: 0.6982\n",
            "E2E-ABSA >>> 2022-08-17 16:01:58\n",
            "loss: 0.6854, acc: 0.7066\n",
            "E2E-ABSA >>> 2022-08-17 16:01:59\n",
            "loss: 0.6796, acc: 0.7098\n",
            "E2E-ABSA >>> 2022-08-17 16:02:00\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:00\n",
            "loss: 0.7259, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:02:01\n",
            "loss: 0.6784, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 16:02:02\n",
            "loss: 0.6627, acc: 0.7143\n",
            "E2E-ABSA >>> 2022-08-17 16:02:03\n",
            "loss: 0.6674, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 16:02:04\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:04\n",
            "loss: 0.6847, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 16:02:05\n",
            "loss: 0.6818, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-08-17 16:02:06\n",
            "loss: 0.6773, acc: 0.7108\n",
            "E2E-ABSA >>> 2022-08-17 16:02:07\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:07\n",
            "loss: 0.6423, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:02:08\n",
            "loss: 0.6575, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-08-17 16:02:09\n",
            "loss: 0.6694, acc: 0.7169\n",
            "E2E-ABSA >>> 2022-08-17 16:02:10\n",
            "loss: 0.6616, acc: 0.7218\n",
            "E2E-ABSA >>> 2022-08-17 16:02:10\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:11\n",
            "loss: 0.6476, acc: 0.7400\n",
            "E2E-ABSA >>> 2022-08-17 16:02:12\n",
            "loss: 0.6493, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 16:02:12\n",
            "loss: 0.6587, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 16:02:13\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:14\n",
            "loss: 0.6058, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:02:14\n",
            "loss: 0.6462, acc: 0.7157\n",
            "E2E-ABSA >>> 2022-08-17 16:02:15\n",
            "loss: 0.6466, acc: 0.7197\n",
            "E2E-ABSA >>> 2022-08-17 16:02:16\n",
            "loss: 0.6549, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-08-17 16:02:17\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:17\n",
            "loss: 0.6588, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-08-17 16:02:18\n",
            "loss: 0.6706, acc: 0.7225\n",
            "E2E-ABSA >>> 2022-08-17 16:02:19\n",
            "loss: 0.6638, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 16:02:20\n",
            "loss: 0.6610, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 16:02:20\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:21\n",
            "loss: 0.6341, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-08-17 16:02:22\n",
            "loss: 0.6484, acc: 0.7269\n",
            "E2E-ABSA >>> 2022-08-17 16:02:23\n",
            "loss: 0.6502, acc: 0.7269\n",
            "E2E-ABSA >>> 2022-08-17 16:02:24\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:24\n",
            "loss: 0.6766, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-08-17 16:02:25\n",
            "loss: 0.6577, acc: 0.7289\n",
            "E2E-ABSA >>> 2022-08-17 16:02:26\n",
            "loss: 0.6519, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 16:02:27\n",
            "loss: 0.6547, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-08-17 16:02:27\n",
            ">>> val_acc: 0.6864, val_precision: 0.6864 val_recall: 0.6864, val_f1: 0.6864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:28\n",
            "loss: 0.6577, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 16:02:29\n",
            "loss: 0.6541, acc: 0.7235\n",
            "E2E-ABSA >>> 2022-08-17 16:02:30\n",
            "loss: 0.6532, acc: 0.7213\n",
            "E2E-ABSA >>> 2022-08-17 16:02:30\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.696\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:31\n",
            "loss: 0.6473, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 16:02:32\n",
            "loss: 0.6346, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 16:02:32\n",
            "loss: 0.6462, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-08-17 16:02:33\n",
            "loss: 0.6500, acc: 0.7253\n",
            "E2E-ABSA >>> 2022-08-17 16:02:34\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:34\n",
            "loss: 0.6369, acc: 0.7398\n",
            "E2E-ABSA >>> 2022-08-17 16:02:35\n",
            "loss: 0.6524, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-08-17 16:02:36\n",
            "loss: 0.6507, acc: 0.7290\n",
            "E2E-ABSA >>> 2022-08-17 16:02:37\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:37\n",
            "loss: 0.6280, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:02:38\n",
            "loss: 0.6457, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-08-17 16:02:39\n",
            "loss: 0.6467, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 16:02:40\n",
            "loss: 0.6458, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-08-17 16:02:40\n",
            ">>> val_acc: 0.6864, val_precision: 0.6864 val_recall: 0.6864, val_f1: 0.6864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:41\n",
            "loss: 0.6306, acc: 0.7337\n",
            "E2E-ABSA >>> 2022-08-17 16:02:42\n",
            "loss: 0.6315, acc: 0.7337\n",
            "E2E-ABSA >>> 2022-08-17 16:02:43\n",
            "loss: 0.6448, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-08-17 16:02:44\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:44\n",
            "loss: 0.6005, acc: 0.7517\n",
            "E2E-ABSA >>> 2022-08-17 16:02:45\n",
            "loss: 0.6264, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 16:02:46\n",
            "loss: 0.6309, acc: 0.7397\n",
            "E2E-ABSA >>> 2022-08-17 16:02:47\n",
            "loss: 0.6402, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-08-17 16:02:47\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:48\n",
            "loss: 0.6612, acc: 0.7202\n",
            "E2E-ABSA >>> 2022-08-17 16:02:49\n",
            "loss: 0.6305, acc: 0.7364\n",
            "E2E-ABSA >>> 2022-08-17 16:02:49\n",
            "loss: 0.6371, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:02:50\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:50\n",
            "loss: 0.6569, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 16:02:51\n",
            "loss: 0.6285, acc: 0.7401\n",
            "E2E-ABSA >>> 2022-08-17 16:02:52\n",
            "loss: 0.6389, acc: 0.7322\n",
            "E2E-ABSA >>> 2022-08-17 16:02:53\n",
            "loss: 0.6395, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-08-17 16:02:53\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:54\n",
            "loss: 0.6297, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:02:56\n",
            "loss: 0.6300, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 16:02:56\n",
            "loss: 0.6331, acc: 0.7350\n",
            "E2E-ABSA >>> 2022-08-17 16:02:57\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 16:02:57\n",
            "loss: 0.6065, acc: 0.7366\n",
            "E2E-ABSA >>> 2022-08-17 16:02:58\n",
            "loss: 0.6291, acc: 0.7275\n",
            "E2E-ABSA >>> 2022-08-17 16:02:59\n",
            "loss: 0.6298, acc: 0.7297\n",
            "E2E-ABSA >>> 2022-08-17 16:03:00\n",
            "loss: 0.6356, acc: 0.7287\n",
            "E2E-ABSA >>> 2022-08-17 16:03:01\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:01\n",
            "loss: 0.6037, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 16:03:02\n",
            "loss: 0.6110, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-08-17 16:03:03\n",
            "loss: 0.6219, acc: 0.7405\n",
            "E2E-ABSA >>> 2022-08-17 16:03:04\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:04\n",
            "loss: 0.6591, acc: 0.7370\n",
            "E2E-ABSA >>> 2022-08-17 16:03:05\n",
            "loss: 0.6350, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 16:03:06\n",
            "loss: 0.6237, acc: 0.7372\n",
            "E2E-ABSA >>> 2022-08-17 16:03:07\n",
            "loss: 0.6341, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:03:07\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:08\n",
            "loss: 0.6678, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-08-17 16:03:09\n",
            "loss: 0.6503, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 16:03:10\n",
            "loss: 0.6342, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:03:11\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:11\n",
            "loss: 0.6896, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 16:03:12\n",
            "loss: 0.6424, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-08-17 16:03:12\n",
            "loss: 0.6364, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-08-17 16:03:13\n",
            "loss: 0.6344, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-08-17 16:03:14\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:14\n",
            "loss: 0.6391, acc: 0.7270\n",
            "E2E-ABSA >>> 2022-08-17 16:03:15\n",
            "loss: 0.6202, acc: 0.7340\n",
            "E2E-ABSA >>> 2022-08-17 16:03:16\n",
            "loss: 0.6228, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:03:17\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:17\n",
            "loss: 0.5382, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 16:03:18\n",
            "loss: 0.6017, acc: 0.7419\n",
            "E2E-ABSA >>> 2022-08-17 16:03:19\n",
            "loss: 0.6199, acc: 0.7364\n",
            "E2E-ABSA >>> 2022-08-17 16:03:20\n",
            "loss: 0.6235, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 16:03:20\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:21\n",
            "loss: 0.5936, acc: 0.7471\n",
            "E2E-ABSA >>> 2022-08-17 16:03:22\n",
            "loss: 0.6105, acc: 0.7420\n",
            "E2E-ABSA >>> 2022-08-17 16:03:23\n",
            "loss: 0.6148, acc: 0.7424\n",
            "E2E-ABSA >>> 2022-08-17 16:03:24\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:24\n",
            "loss: 0.5608, acc: 0.7760\n",
            "E2E-ABSA >>> 2022-08-17 16:03:25\n",
            "loss: 0.6124, acc: 0.7394\n",
            "E2E-ABSA >>> 2022-08-17 16:03:26\n",
            "loss: 0.6239, acc: 0.7353\n",
            "E2E-ABSA >>> 2022-08-17 16:03:26\n",
            "loss: 0.6248, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 16:03:27\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:28\n",
            "loss: 0.6256, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 16:03:28\n",
            "loss: 0.6145, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 16:03:29\n",
            "loss: 0.6125, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 16:03:30\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:30\n",
            "loss: 0.5318, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 16:03:31\n",
            "loss: 0.6199, acc: 0.7390\n",
            "E2E-ABSA >>> 2022-08-17 16:03:32\n",
            "loss: 0.6135, acc: 0.7407\n",
            "E2E-ABSA >>> 2022-08-17 16:03:33\n",
            "loss: 0.6146, acc: 0.7409\n",
            "E2E-ABSA >>> 2022-08-17 16:03:34\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_val_f1_0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:34\n",
            "loss: 0.6074, acc: 0.7533\n",
            "E2E-ABSA >>> 2022-08-17 16:03:35\n",
            "loss: 0.6108, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-08-17 16:03:36\n",
            "loss: 0.6217, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 16:03:37\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:37\n",
            "loss: 0.6947, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:03:38\n",
            "loss: 0.6078, acc: 0.7458\n",
            "E2E-ABSA >>> 2022-08-17 16:03:39\n",
            "loss: 0.6226, acc: 0.7365\n",
            "E2E-ABSA >>> 2022-08-17 16:03:40\n",
            "loss: 0.6238, acc: 0.7364\n",
            "E2E-ABSA >>> 2022-08-17 16:03:40\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:41\n",
            "loss: 0.5954, acc: 0.7716\n",
            "E2E-ABSA >>> 2022-08-17 16:03:42\n",
            "loss: 0.6149, acc: 0.7467\n",
            "E2E-ABSA >>> 2022-08-17 16:03:42\n",
            "loss: 0.6140, acc: 0.7421\n",
            "E2E-ABSA >>> 2022-08-17 16:03:43\n",
            "loss: 0.6158, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 16:03:43\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:44\n",
            "loss: 0.5730, acc: 0.7638\n",
            "E2E-ABSA >>> 2022-08-17 16:03:45\n",
            "loss: 0.5938, acc: 0.7550\n",
            "E2E-ABSA >>> 2022-08-17 16:03:46\n",
            "loss: 0.6120, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-08-17 16:03:47\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:47\n",
            "loss: 0.6027, acc: 0.7539\n",
            "E2E-ABSA >>> 2022-08-17 16:03:48\n",
            "loss: 0.6175, acc: 0.7466\n",
            "E2E-ABSA >>> 2022-08-17 16:03:49\n",
            "loss: 0.6165, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-08-17 16:03:50\n",
            "loss: 0.6137, acc: 0.7434\n",
            "E2E-ABSA >>> 2022-08-17 16:03:50\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:51\n",
            "loss: 0.5782, acc: 0.7624\n",
            "E2E-ABSA >>> 2022-08-17 16:03:52\n",
            "loss: 0.5976, acc: 0.7548\n",
            "E2E-ABSA >>> 2022-08-17 16:03:53\n",
            "loss: 0.6060, acc: 0.7540\n",
            "E2E-ABSA >>> 2022-08-17 16:03:53\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:54\n",
            "loss: 0.6266, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:03:55\n",
            "loss: 0.6201, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-08-17 16:03:56\n",
            "loss: 0.6226, acc: 0.7423\n",
            "E2E-ABSA >>> 2022-08-17 16:03:56\n",
            "loss: 0.6129, acc: 0.7469\n",
            "E2E-ABSA >>> 2022-08-17 16:03:57\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 16:03:58\n",
            "loss: 0.5955, acc: 0.7548\n",
            "E2E-ABSA >>> 2022-08-17 16:03:58\n",
            "loss: 0.5989, acc: 0.7536\n",
            "E2E-ABSA >>> 2022-08-17 16:03:59\n",
            "loss: 0.6020, acc: 0.7506\n",
            "E2E-ABSA >>> 2022-08-17 16:04:00\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:00\n",
            "loss: 0.5695, acc: 0.7547\n",
            "E2E-ABSA >>> 2022-08-17 16:04:01\n",
            "loss: 0.6010, acc: 0.7491\n",
            "E2E-ABSA >>> 2022-08-17 16:04:02\n",
            "loss: 0.6020, acc: 0.7487\n",
            "E2E-ABSA >>> 2022-08-17 16:04:03\n",
            "loss: 0.6070, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 16:04:03\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:04\n",
            "loss: 0.6062, acc: 0.7464\n",
            "E2E-ABSA >>> 2022-08-17 16:04:05\n",
            "loss: 0.6118, acc: 0.7397\n",
            "E2E-ABSA >>> 2022-08-17 16:04:06\n",
            "loss: 0.6082, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 16:04:07\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:07\n",
            "loss: 0.6100, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-08-17 16:04:08\n",
            "loss: 0.6149, acc: 0.7413\n",
            "E2E-ABSA >>> 2022-08-17 16:04:09\n",
            "loss: 0.6093, acc: 0.7428\n",
            "E2E-ABSA >>> 2022-08-17 16:04:10\n",
            "loss: 0.6067, acc: 0.7433\n",
            "E2E-ABSA >>> 2022-08-17 16:04:10\n",
            ">>> val_acc: 0.7072, val_precision: 0.7072 val_recall: 0.7072, val_f1: 0.7072\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:11\n",
            "loss: 0.5930, acc: 0.7507\n",
            "E2E-ABSA >>> 2022-08-17 16:04:11\n",
            "loss: 0.6047, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-08-17 16:04:12\n",
            "loss: 0.6034, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-08-17 16:04:13\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:13\n",
            "loss: 0.6013, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-08-17 16:04:14\n",
            "loss: 0.5923, acc: 0.7543\n",
            "E2E-ABSA >>> 2022-08-17 16:04:15\n",
            "loss: 0.6095, acc: 0.7470\n",
            "E2E-ABSA >>> 2022-08-17 16:04:16\n",
            "loss: 0.6033, acc: 0.7523\n",
            "E2E-ABSA >>> 2022-08-17 16:04:16\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:17\n",
            "loss: 0.6092, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-08-17 16:04:18\n",
            "loss: 0.6126, acc: 0.7434\n",
            "E2E-ABSA >>> 2022-08-17 16:04:19\n",
            "loss: 0.6040, acc: 0.7498\n",
            "E2E-ABSA >>> 2022-08-17 16:04:20\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:20\n",
            "loss: 0.6015, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 16:04:21\n",
            "loss: 0.5902, acc: 0.7515\n",
            "E2E-ABSA >>> 2022-08-17 16:04:22\n",
            "loss: 0.5901, acc: 0.7533\n",
            "E2E-ABSA >>> 2022-08-17 16:04:23\n",
            "loss: 0.5928, acc: 0.7515\n",
            "E2E-ABSA >>> 2022-08-17 16:04:23\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:24\n",
            "loss: 0.5905, acc: 0.7467\n",
            "E2E-ABSA >>> 2022-08-17 16:04:24\n",
            "loss: 0.5907, acc: 0.7518\n",
            "E2E-ABSA >>> 2022-08-17 16:04:25\n",
            "loss: 0.5964, acc: 0.7502\n",
            "E2E-ABSA >>> 2022-08-17 16:04:26\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:26\n",
            "loss: 0.5727, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 16:04:27\n",
            "loss: 0.6047, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 16:04:28\n",
            "loss: 0.6020, acc: 0.7433\n",
            "E2E-ABSA >>> 2022-08-17 16:04:29\n",
            "loss: 0.5971, acc: 0.7490\n",
            "E2E-ABSA >>> 2022-08-17 16:04:29\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:30\n",
            "loss: 0.5773, acc: 0.7587\n",
            "E2E-ABSA >>> 2022-08-17 16:04:31\n",
            "loss: 0.5924, acc: 0.7515\n",
            "E2E-ABSA >>> 2022-08-17 16:04:32\n",
            "loss: 0.5966, acc: 0.7486\n",
            "E2E-ABSA >>> 2022-08-17 16:04:33\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:33\n",
            "loss: 0.6192, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 16:04:34\n",
            "loss: 0.5768, acc: 0.7672\n",
            "E2E-ABSA >>> 2022-08-17 16:04:35\n",
            "loss: 0.5924, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-08-17 16:04:36\n",
            "loss: 0.5898, acc: 0.7537\n",
            "E2E-ABSA >>> 2022-08-17 16:04:36\n",
            ">>> val_acc: 0.6992, val_precision: 0.6992 val_recall: 0.6992, val_f1: 0.6992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:37\n",
            "loss: 0.5773, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-08-17 16:04:37\n",
            "loss: 0.5918, acc: 0.7467\n",
            "E2E-ABSA >>> 2022-08-17 16:04:38\n",
            "loss: 0.5914, acc: 0.7505\n",
            "E2E-ABSA >>> 2022-08-17 16:04:39\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:39\n",
            "loss: 0.5848, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:04:40\n",
            "loss: 0.5798, acc: 0.7619\n",
            "E2E-ABSA >>> 2022-08-17 16:04:41\n",
            "loss: 0.5929, acc: 0.7561\n",
            "E2E-ABSA >>> 2022-08-17 16:04:42\n",
            "loss: 0.5936, acc: 0.7540\n",
            "E2E-ABSA >>> 2022-08-17 16:04:42\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:43\n",
            "loss: 0.5765, acc: 0.7627\n",
            "E2E-ABSA >>> 2022-08-17 16:04:44\n",
            "loss: 0.5830, acc: 0.7557\n",
            "E2E-ABSA >>> 2022-08-17 16:04:45\n",
            "loss: 0.5851, acc: 0.7588\n",
            "E2E-ABSA >>> 2022-08-17 16:04:46\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:46\n",
            "loss: 0.5941, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-08-17 16:04:47\n",
            "loss: 0.5656, acc: 0.7640\n",
            "E2E-ABSA >>> 2022-08-17 16:04:48\n",
            "loss: 0.5862, acc: 0.7550\n",
            "E2E-ABSA >>> 2022-08-17 16:04:48\n",
            "loss: 0.5867, acc: 0.7530\n",
            "E2E-ABSA >>> 2022-08-17 16:04:49\n",
            ">>> val_acc: 0.7072, val_precision: 0.7072 val_recall: 0.7072, val_f1: 0.7072\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:50\n",
            "loss: 0.5763, acc: 0.7542\n",
            "E2E-ABSA >>> 2022-08-17 16:04:50\n",
            "loss: 0.5937, acc: 0.7508\n",
            "E2E-ABSA >>> 2022-08-17 16:04:51\n",
            "loss: 0.5870, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-08-17 16:04:52\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:52\n",
            "loss: 0.5528, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 16:04:53\n",
            "loss: 0.5759, acc: 0.7645\n",
            "E2E-ABSA >>> 2022-08-17 16:04:54\n",
            "loss: 0.5851, acc: 0.7584\n",
            "E2E-ABSA >>> 2022-08-17 16:04:55\n",
            "loss: 0.5853, acc: 0.7553\n",
            "E2E-ABSA >>> 2022-08-17 16:04:56\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:56\n",
            "loss: 0.5615, acc: 0.7701\n",
            "E2E-ABSA >>> 2022-08-17 16:04:57\n",
            "loss: 0.5832, acc: 0.7616\n",
            "E2E-ABSA >>> 2022-08-17 16:04:58\n",
            "loss: 0.5848, acc: 0.7603\n",
            "E2E-ABSA >>> 2022-08-17 16:04:59\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 16:04:59\n",
            "loss: 0.4918, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 16:05:00\n",
            "loss: 0.5724, acc: 0.7632\n",
            "E2E-ABSA >>> 2022-08-17 16:05:01\n",
            "loss: 0.5761, acc: 0.7623\n",
            "E2E-ABSA >>> 2022-08-17 16:05:02\n",
            "loss: 0.5754, acc: 0.7595\n",
            "E2E-ABSA >>> 2022-08-17 16:05:02\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 16:05:03\n",
            "loss: 0.5697, acc: 0.7680\n",
            "E2E-ABSA >>> 2022-08-17 16:05:04\n",
            "loss: 0.5730, acc: 0.7627\n",
            "E2E-ABSA >>> 2022-08-17 16:05:04\n",
            "loss: 0.5818, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 16:05:05\n",
            "loss: 0.5820, acc: 0.7587\n",
            "E2E-ABSA >>> 2022-08-17 16:05:05\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 16:05:06\n",
            "loss: 0.6164, acc: 0.7450\n",
            "E2E-ABSA >>> 2022-08-17 16:05:07\n",
            "loss: 0.5850, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-08-17 16:05:08\n",
            "loss: 0.5835, acc: 0.7590\n",
            "E2E-ABSA >>> 2022-08-17 16:05:09\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 16:05:09\n",
            "loss: 0.5981, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-08-17 16:05:10\n",
            "loss: 0.5668, acc: 0.7635\n",
            "E2E-ABSA >>> 2022-08-17 16:05:11\n",
            "loss: 0.5703, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 16:05:12\n",
            "loss: 0.5812, acc: 0.7615\n",
            "E2E-ABSA >>> 2022-08-17 16:05:12\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            "E2E-ABSA >>> 2022-08-17 16:05:12\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            "you can download the best model from state_dict/atae_lstm_acl14shortdata_val_f1_0.7104\n",
            ">>> test_acc: 0.7104, test_precision: 0.7104, test_recall: 0.7104, test_f1: 0.7104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **acl14shortdata** dataset on model(**ATAELSTM**)\n",
        "\n"
      ],
      "metadata": {
        "id": "xODWdvSfXio0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name atae_lstm --dataset acl14shortdata_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXbe6avvXiwl",
        "outputId": "bd480869-890a-42e0-dbfd-22c5cb4a1510"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 26883072\n",
            "> n_trainable_params: 2525703, n_nontrainable_params: 4115100\n",
            "> training arguments:\n",
            ">>> model_name: atae_lstm\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fb142afab00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.atae_lstm.ATAE_LSTM'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 16:05:53\n",
            "loss: 1.0911, acc: 0.3625\n",
            "E2E-ABSA >>> 2022-08-17 16:05:54\n",
            "loss: 1.0517, acc: 0.4394\n",
            "E2E-ABSA >>> 2022-08-17 16:05:55\n",
            "loss: 1.0328, acc: 0.4633\n",
            "E2E-ABSA >>> 2022-08-17 16:05:56\n",
            ">>> val_acc: 0.4992, val_precision: 0.4992 val_recall: 0.4992, val_f1: 0.4992\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.4992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 16:05:57\n",
            "loss: 0.9684, acc: 0.5352\n",
            "E2E-ABSA >>> 2022-08-17 16:05:58\n",
            "loss: 0.9604, acc: 0.5486\n",
            "E2E-ABSA >>> 2022-08-17 16:05:59\n",
            "loss: 0.9677, acc: 0.5386\n",
            "E2E-ABSA >>> 2022-08-17 16:06:00\n",
            "loss: 0.9652, acc: 0.5429\n",
            "E2E-ABSA >>> 2022-08-17 16:06:01\n",
            ">>> val_acc: 0.5328, val_precision: 0.5328 val_recall: 0.5328, val_f1: 0.5328\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.5328\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:02\n",
            "loss: 0.9506, acc: 0.5573\n",
            "E2E-ABSA >>> 2022-08-17 16:06:03\n",
            "loss: 0.9431, acc: 0.5577\n",
            "E2E-ABSA >>> 2022-08-17 16:06:04\n",
            "loss: 0.9473, acc: 0.5519\n",
            "E2E-ABSA >>> 2022-08-17 16:06:05\n",
            ">>> val_acc: 0.5312, val_precision: 0.5312 val_recall: 0.5312, val_f1: 0.5312\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:05\n",
            "loss: 0.9315, acc: 0.5540\n",
            "E2E-ABSA >>> 2022-08-17 16:06:06\n",
            "loss: 0.9260, acc: 0.5595\n",
            "E2E-ABSA >>> 2022-08-17 16:06:08\n",
            "loss: 0.9293, acc: 0.5607\n",
            "E2E-ABSA >>> 2022-08-17 16:06:09\n",
            "loss: 0.9255, acc: 0.5652\n",
            "E2E-ABSA >>> 2022-08-17 16:06:09\n",
            ">>> val_acc: 0.5344, val_precision: 0.5344 val_recall: 0.5344, val_f1: 0.5344\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.5344\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:10\n",
            "loss: 0.9216, acc: 0.5842\n",
            "E2E-ABSA >>> 2022-08-17 16:06:11\n",
            "loss: 0.9186, acc: 0.5811\n",
            "E2E-ABSA >>> 2022-08-17 16:06:12\n",
            "loss: 0.9186, acc: 0.5760\n",
            "E2E-ABSA >>> 2022-08-17 16:06:13\n",
            ">>> val_acc: 0.5488, val_precision: 0.5488 val_recall: 0.5488, val_f1: 0.5488\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.5488\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:14\n",
            "loss: 0.9303, acc: 0.5656\n",
            "E2E-ABSA >>> 2022-08-17 16:06:15\n",
            "loss: 0.9099, acc: 0.5920\n",
            "E2E-ABSA >>> 2022-08-17 16:06:16\n",
            "loss: 0.9012, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 16:06:17\n",
            "loss: 0.9037, acc: 0.5917\n",
            "E2E-ABSA >>> 2022-08-17 16:06:18\n",
            ">>> val_acc: 0.5552, val_precision: 0.5552 val_recall: 0.5552, val_f1: 0.5552\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.5552\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:19\n",
            "loss: 0.8841, acc: 0.6072\n",
            "E2E-ABSA >>> 2022-08-17 16:06:20\n",
            "loss: 0.8830, acc: 0.6070\n",
            "E2E-ABSA >>> 2022-08-17 16:06:21\n",
            "loss: 0.8867, acc: 0.6013\n",
            "E2E-ABSA >>> 2022-08-17 16:06:22\n",
            ">>> val_acc: 0.5520, val_precision: 0.5520 val_recall: 0.5520, val_f1: 0.5520\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:22\n",
            "loss: 0.8817, acc: 0.5799\n",
            "E2E-ABSA >>> 2022-08-17 16:06:24\n",
            "loss: 0.8585, acc: 0.6062\n",
            "E2E-ABSA >>> 2022-08-17 16:06:25\n",
            "loss: 0.8758, acc: 0.5969\n",
            "E2E-ABSA >>> 2022-08-17 16:06:26\n",
            "loss: 0.8781, acc: 0.5978\n",
            "E2E-ABSA >>> 2022-08-17 16:06:26\n",
            ">>> val_acc: 0.5616, val_precision: 0.5616 val_recall: 0.5616, val_f1: 0.5616\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.5616\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:27\n",
            "loss: 0.8521, acc: 0.6183\n",
            "E2E-ABSA >>> 2022-08-17 16:06:28\n",
            "loss: 0.8639, acc: 0.6104\n",
            "E2E-ABSA >>> 2022-08-17 16:06:30\n",
            "loss: 0.8544, acc: 0.6153\n",
            "E2E-ABSA >>> 2022-08-17 16:06:31\n",
            ">>> val_acc: 0.5840, val_precision: 0.5840 val_recall: 0.5840, val_f1: 0.5840\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.584\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:31\n",
            "loss: 0.8740, acc: 0.5898\n",
            "E2E-ABSA >>> 2022-08-17 16:06:32\n",
            "loss: 0.8285, acc: 0.6335\n",
            "E2E-ABSA >>> 2022-08-17 16:06:33\n",
            "loss: 0.8283, acc: 0.6320\n",
            "E2E-ABSA >>> 2022-08-17 16:06:34\n",
            "loss: 0.8301, acc: 0.6340\n",
            "E2E-ABSA >>> 2022-08-17 16:06:35\n",
            ">>> val_acc: 0.5888, val_precision: 0.5888 val_recall: 0.5888, val_f1: 0.5888\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.5888\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:36\n",
            "loss: 0.7987, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-08-17 16:06:37\n",
            "loss: 0.8066, acc: 0.6424\n",
            "E2E-ABSA >>> 2022-08-17 16:06:38\n",
            "loss: 0.8038, acc: 0.6435\n",
            "E2E-ABSA >>> 2022-08-17 16:06:39\n",
            ">>> val_acc: 0.6176, val_precision: 0.6176 val_recall: 0.6176, val_f1: 0.6176\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6176\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:39\n",
            "loss: 0.7813, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-08-17 16:06:41\n",
            "loss: 0.7754, acc: 0.6650\n",
            "E2E-ABSA >>> 2022-08-17 16:06:42\n",
            "loss: 0.7767, acc: 0.6647\n",
            "E2E-ABSA >>> 2022-08-17 16:06:43\n",
            "loss: 0.7742, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-08-17 16:06:43\n",
            ">>> val_acc: 0.6160, val_precision: 0.6160 val_recall: 0.6160, val_f1: 0.6160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:44\n",
            "loss: 0.7527, acc: 0.6678\n",
            "E2E-ABSA >>> 2022-08-17 16:06:45\n",
            "loss: 0.7562, acc: 0.6694\n",
            "E2E-ABSA >>> 2022-08-17 16:06:46\n",
            "loss: 0.7547, acc: 0.6757\n",
            "E2E-ABSA >>> 2022-08-17 16:06:47\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.648\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:48\n",
            "loss: 0.7339, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 16:06:49\n",
            "loss: 0.7272, acc: 0.7056\n",
            "E2E-ABSA >>> 2022-08-17 16:06:50\n",
            "loss: 0.7355, acc: 0.6917\n",
            "E2E-ABSA >>> 2022-08-17 16:06:51\n",
            "loss: 0.7279, acc: 0.6941\n",
            "E2E-ABSA >>> 2022-08-17 16:06:52\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:53\n",
            "loss: 0.6999, acc: 0.7075\n",
            "E2E-ABSA >>> 2022-08-17 16:06:54\n",
            "loss: 0.7182, acc: 0.6933\n",
            "E2E-ABSA >>> 2022-08-17 16:06:55\n",
            "loss: 0.7174, acc: 0.6935\n",
            "E2E-ABSA >>> 2022-08-17 16:06:56\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 16:06:56\n",
            "loss: 0.7140, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 16:06:57\n",
            "loss: 0.7057, acc: 0.6990\n",
            "E2E-ABSA >>> 2022-08-17 16:06:58\n",
            "loss: 0.7144, acc: 0.6920\n",
            "E2E-ABSA >>> 2022-08-17 16:07:00\n",
            "loss: 0.7164, acc: 0.6936\n",
            "E2E-ABSA >>> 2022-08-17 16:07:00\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:01\n",
            "loss: 0.6964, acc: 0.6994\n",
            "E2E-ABSA >>> 2022-08-17 16:07:02\n",
            "loss: 0.7045, acc: 0.6990\n",
            "E2E-ABSA >>> 2022-08-17 16:07:03\n",
            "loss: 0.7015, acc: 0.7013\n",
            "E2E-ABSA >>> 2022-08-17 16:07:04\n",
            ">>> val_acc: 0.6576, val_precision: 0.6576 val_recall: 0.6576, val_f1: 0.6576\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6576\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:05\n",
            "loss: 0.6814, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 16:07:06\n",
            "loss: 0.6979, acc: 0.6999\n",
            "E2E-ABSA >>> 2022-08-17 16:07:07\n",
            "loss: 0.6904, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-08-17 16:07:08\n",
            "loss: 0.6942, acc: 0.7077\n",
            "E2E-ABSA >>> 2022-08-17 16:07:09\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:09\n",
            "loss: 0.7141, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-08-17 16:07:10\n",
            "loss: 0.7009, acc: 0.7058\n",
            "E2E-ABSA >>> 2022-08-17 16:07:12\n",
            "loss: 0.6924, acc: 0.7086\n",
            "E2E-ABSA >>> 2022-08-17 16:07:13\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:13\n",
            "loss: 0.7047, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 16:07:14\n",
            "loss: 0.6828, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-08-17 16:07:15\n",
            "loss: 0.6678, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:07:16\n",
            "loss: 0.6725, acc: 0.7155\n",
            "E2E-ABSA >>> 2022-08-17 16:07:17\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:18\n",
            "loss: 0.6852, acc: 0.7073\n",
            "E2E-ABSA >>> 2022-08-17 16:07:19\n",
            "loss: 0.6833, acc: 0.7164\n",
            "E2E-ABSA >>> 2022-08-17 16:07:20\n",
            "loss: 0.6820, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-08-17 16:07:21\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:21\n",
            "loss: 0.6474, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:07:22\n",
            "loss: 0.6579, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-08-17 16:07:23\n",
            "loss: 0.6715, acc: 0.7218\n",
            "E2E-ABSA >>> 2022-08-17 16:07:25\n",
            "loss: 0.6630, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 16:07:25\n",
            ">>> val_acc: 0.6672, val_precision: 0.6672 val_recall: 0.6672, val_f1: 0.6672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:26\n",
            "loss: 0.6574, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-08-17 16:07:27\n",
            "loss: 0.6572, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-08-17 16:07:28\n",
            "loss: 0.6600, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-08-17 16:07:29\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:30\n",
            "loss: 0.5889, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:07:31\n",
            "loss: 0.6464, acc: 0.7314\n",
            "E2E-ABSA >>> 2022-08-17 16:07:32\n",
            "loss: 0.6438, acc: 0.7286\n",
            "E2E-ABSA >>> 2022-08-17 16:07:33\n",
            "loss: 0.6539, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-08-17 16:07:34\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:34\n",
            "loss: 0.6561, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-08-17 16:07:35\n",
            "loss: 0.6606, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 16:07:37\n",
            "loss: 0.6547, acc: 0.7237\n",
            "E2E-ABSA >>> 2022-08-17 16:07:38\n",
            "loss: 0.6554, acc: 0.7247\n",
            "E2E-ABSA >>> 2022-08-17 16:07:38\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:39\n",
            "loss: 0.6320, acc: 0.7531\n",
            "E2E-ABSA >>> 2022-08-17 16:07:40\n",
            "loss: 0.6421, acc: 0.7366\n",
            "E2E-ABSA >>> 2022-08-17 16:07:41\n",
            "loss: 0.6449, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 16:07:42\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:43\n",
            "loss: 0.6565, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:07:44\n",
            "loss: 0.6479, acc: 0.7382\n",
            "E2E-ABSA >>> 2022-08-17 16:07:45\n",
            "loss: 0.6460, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 16:07:46\n",
            "loss: 0.6488, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-08-17 16:07:46\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:48\n",
            "loss: 0.6447, acc: 0.7318\n",
            "E2E-ABSA >>> 2022-08-17 16:07:49\n",
            "loss: 0.6478, acc: 0.7286\n",
            "E2E-ABSA >>> 2022-08-17 16:07:50\n",
            "loss: 0.6462, acc: 0.7287\n",
            "E2E-ABSA >>> 2022-08-17 16:07:51\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:51\n",
            "loss: 0.6494, acc: 0.7372\n",
            "E2E-ABSA >>> 2022-08-17 16:07:53\n",
            "loss: 0.6282, acc: 0.7387\n",
            "E2E-ABSA >>> 2022-08-17 16:07:54\n",
            "loss: 0.6387, acc: 0.7408\n",
            "E2E-ABSA >>> 2022-08-17 16:07:55\n",
            "loss: 0.6418, acc: 0.7329\n",
            "E2E-ABSA >>> 2022-08-17 16:07:55\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 16:07:56\n",
            "loss: 0.6336, acc: 0.7439\n",
            "E2E-ABSA >>> 2022-08-17 16:07:57\n",
            "loss: 0.6416, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 16:07:59\n",
            "loss: 0.6422, acc: 0.7322\n",
            "E2E-ABSA >>> 2022-08-17 16:07:59\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:00\n",
            "loss: 0.6031, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 16:08:01\n",
            "loss: 0.6293, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 16:08:02\n",
            "loss: 0.6355, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:08:03\n",
            "loss: 0.6369, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 16:08:04\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:05\n",
            "loss: 0.6213, acc: 0.7486\n",
            "E2E-ABSA >>> 2022-08-17 16:08:06\n",
            "loss: 0.6222, acc: 0.7417\n",
            "E2E-ABSA >>> 2022-08-17 16:08:07\n",
            "loss: 0.6358, acc: 0.7333\n",
            "E2E-ABSA >>> 2022-08-17 16:08:08\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:08\n",
            "loss: 0.5970, acc: 0.7708\n",
            "E2E-ABSA >>> 2022-08-17 16:08:09\n",
            "loss: 0.6132, acc: 0.7532\n",
            "E2E-ABSA >>> 2022-08-17 16:08:10\n",
            "loss: 0.6212, acc: 0.7481\n",
            "E2E-ABSA >>> 2022-08-17 16:08:12\n",
            "loss: 0.6283, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-08-17 16:08:12\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:13\n",
            "loss: 0.6408, acc: 0.7426\n",
            "E2E-ABSA >>> 2022-08-17 16:08:14\n",
            "loss: 0.6197, acc: 0.7490\n",
            "E2E-ABSA >>> 2022-08-17 16:08:15\n",
            "loss: 0.6261, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-08-17 16:08:16\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:17\n",
            "loss: 0.6464, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 16:08:18\n",
            "loss: 0.6143, acc: 0.7543\n",
            "E2E-ABSA >>> 2022-08-17 16:08:19\n",
            "loss: 0.6263, acc: 0.7408\n",
            "E2E-ABSA >>> 2022-08-17 16:08:20\n",
            "loss: 0.6263, acc: 0.7398\n",
            "E2E-ABSA >>> 2022-08-17 16:08:20\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:21\n",
            "loss: 0.6072, acc: 0.7516\n",
            "E2E-ABSA >>> 2022-08-17 16:08:22\n",
            "loss: 0.6160, acc: 0.7476\n",
            "E2E-ABSA >>> 2022-08-17 16:08:24\n",
            "loss: 0.6179, acc: 0.7471\n",
            "E2E-ABSA >>> 2022-08-17 16:08:25\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:25\n",
            "loss: 0.5850, acc: 0.7612\n",
            "E2E-ABSA >>> 2022-08-17 16:08:26\n",
            "loss: 0.6148, acc: 0.7407\n",
            "E2E-ABSA >>> 2022-08-17 16:08:27\n",
            "loss: 0.6165, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:08:28\n",
            "loss: 0.6234, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 16:08:29\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:30\n",
            "loss: 0.5976, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 16:08:31\n",
            "loss: 0.5916, acc: 0.7525\n",
            "E2E-ABSA >>> 2022-08-17 16:08:32\n",
            "loss: 0.6073, acc: 0.7459\n",
            "E2E-ABSA >>> 2022-08-17 16:08:33\n",
            ">>> val_acc: 0.7120, val_precision: 0.7120 val_recall: 0.7120, val_f1: 0.7120\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:33\n",
            "loss: 0.6425, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:08:34\n",
            "loss: 0.6235, acc: 0.7460\n",
            "E2E-ABSA >>> 2022-08-17 16:08:36\n",
            "loss: 0.6122, acc: 0.7492\n",
            "E2E-ABSA >>> 2022-08-17 16:08:37\n",
            "loss: 0.6199, acc: 0.7459\n",
            "E2E-ABSA >>> 2022-08-17 16:08:37\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:38\n",
            "loss: 0.6562, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-08-17 16:08:39\n",
            "loss: 0.6391, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 16:08:40\n",
            "loss: 0.6186, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 16:08:41\n",
            ">>> val_acc: 0.6992, val_precision: 0.6992 val_recall: 0.6992, val_f1: 0.6992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:42\n",
            "loss: 0.6486, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 16:08:43\n",
            "loss: 0.6164, acc: 0.7359\n",
            "E2E-ABSA >>> 2022-08-17 16:08:44\n",
            "loss: 0.6151, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-08-17 16:08:45\n",
            "loss: 0.6160, acc: 0.7436\n",
            "E2E-ABSA >>> 2022-08-17 16:08:46\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:46\n",
            "loss: 0.6168, acc: 0.7472\n",
            "E2E-ABSA >>> 2022-08-17 16:08:47\n",
            "loss: 0.6002, acc: 0.7560\n",
            "E2E-ABSA >>> 2022-08-17 16:08:49\n",
            "loss: 0.6051, acc: 0.7519\n",
            "E2E-ABSA >>> 2022-08-17 16:08:50\n",
            ">>> val_acc: 0.7120, val_precision: 0.7120 val_recall: 0.7120, val_f1: 0.7120\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:50\n",
            "loss: 0.5274, acc: 0.8008\n",
            "E2E-ABSA >>> 2022-08-17 16:08:51\n",
            "loss: 0.5907, acc: 0.7538\n",
            "E2E-ABSA >>> 2022-08-17 16:08:52\n",
            "loss: 0.6035, acc: 0.7474\n",
            "E2E-ABSA >>> 2022-08-17 16:08:53\n",
            "loss: 0.6074, acc: 0.7466\n",
            "E2E-ABSA >>> 2022-08-17 16:08:54\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:55\n",
            "loss: 0.5907, acc: 0.7549\n",
            "E2E-ABSA >>> 2022-08-17 16:08:56\n",
            "loss: 0.5995, acc: 0.7553\n",
            "E2E-ABSA >>> 2022-08-17 16:08:57\n",
            "loss: 0.5986, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 16:08:58\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 16:08:58\n",
            "loss: 0.5424, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-08-17 16:08:59\n",
            "loss: 0.6050, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 16:09:01\n",
            "loss: 0.6118, acc: 0.7518\n",
            "E2E-ABSA >>> 2022-08-17 16:09:02\n",
            "loss: 0.6076, acc: 0.7522\n",
            "E2E-ABSA >>> 2022-08-17 16:09:02\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:03\n",
            "loss: 0.6035, acc: 0.7490\n",
            "E2E-ABSA >>> 2022-08-17 16:09:04\n",
            "loss: 0.5962, acc: 0.7520\n",
            "E2E-ABSA >>> 2022-08-17 16:09:05\n",
            "loss: 0.5978, acc: 0.7517\n",
            "E2E-ABSA >>> 2022-08-17 16:09:07\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:07\n",
            "loss: 0.5190, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 16:09:08\n",
            "loss: 0.5963, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:09:09\n",
            "loss: 0.5913, acc: 0.7608\n",
            "E2E-ABSA >>> 2022-08-17 16:09:10\n",
            "loss: 0.5969, acc: 0.7541\n",
            "E2E-ABSA >>> 2022-08-17 16:09:11\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:12\n",
            "loss: 0.5937, acc: 0.7667\n",
            "E2E-ABSA >>> 2022-08-17 16:09:13\n",
            "loss: 0.5921, acc: 0.7660\n",
            "E2E-ABSA >>> 2022-08-17 16:09:14\n",
            "loss: 0.6021, acc: 0.7576\n",
            "E2E-ABSA >>> 2022-08-17 16:09:15\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:15\n",
            "loss: 0.6687, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:09:16\n",
            "loss: 0.5926, acc: 0.7620\n",
            "E2E-ABSA >>> 2022-08-17 16:09:17\n",
            "loss: 0.6030, acc: 0.7509\n",
            "E2E-ABSA >>> 2022-08-17 16:09:18\n",
            "loss: 0.6054, acc: 0.7486\n",
            "E2E-ABSA >>> 2022-08-17 16:09:19\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:20\n",
            "loss: 0.5636, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-08-17 16:09:21\n",
            "loss: 0.5917, acc: 0.7603\n",
            "E2E-ABSA >>> 2022-08-17 16:09:22\n",
            "loss: 0.5955, acc: 0.7577\n",
            "E2E-ABSA >>> 2022-08-17 16:09:23\n",
            "loss: 0.5958, acc: 0.7564\n",
            "E2E-ABSA >>> 2022-08-17 16:09:23\n",
            ">>> val_acc: 0.7216, val_precision: 0.7216 val_recall: 0.7216, val_f1: 0.7216\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:25\n",
            "loss: 0.5613, acc: 0.7700\n",
            "E2E-ABSA >>> 2022-08-17 16:09:26\n",
            "loss: 0.5792, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 16:09:27\n",
            "loss: 0.5948, acc: 0.7552\n",
            "E2E-ABSA >>> 2022-08-17 16:09:28\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:28\n",
            "loss: 0.5929, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:09:29\n",
            "loss: 0.6046, acc: 0.7525\n",
            "E2E-ABSA >>> 2022-08-17 16:09:30\n",
            "loss: 0.5960, acc: 0.7513\n",
            "E2E-ABSA >>> 2022-08-17 16:09:31\n",
            "loss: 0.5918, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-08-17 16:09:32\n",
            ">>> val_acc: 0.7200, val_precision: 0.7200 val_recall: 0.7200, val_f1: 0.7200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:33\n",
            "loss: 0.5522, acc: 0.7767\n",
            "E2E-ABSA >>> 2022-08-17 16:09:34\n",
            "loss: 0.5781, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:09:35\n",
            "loss: 0.5848, acc: 0.7618\n",
            "E2E-ABSA >>> 2022-08-17 16:09:36\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:36\n",
            "loss: 0.6111, acc: 0.7557\n",
            "E2E-ABSA >>> 2022-08-17 16:09:37\n",
            "loss: 0.6043, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-08-17 16:09:39\n",
            "loss: 0.6043, acc: 0.7505\n",
            "E2E-ABSA >>> 2022-08-17 16:09:40\n",
            "loss: 0.5919, acc: 0.7591\n",
            "E2E-ABSA >>> 2022-08-17 16:09:40\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:41\n",
            "loss: 0.5736, acc: 0.7690\n",
            "E2E-ABSA >>> 2022-08-17 16:09:42\n",
            "loss: 0.5783, acc: 0.7686\n",
            "E2E-ABSA >>> 2022-08-17 16:09:43\n",
            "loss: 0.5807, acc: 0.7648\n",
            "E2E-ABSA >>> 2022-08-17 16:09:44\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:45\n",
            "loss: 0.5417, acc: 0.7781\n",
            "E2E-ABSA >>> 2022-08-17 16:09:46\n",
            "loss: 0.5828, acc: 0.7625\n",
            "E2E-ABSA >>> 2022-08-17 16:09:47\n",
            "loss: 0.5789, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-08-17 16:09:48\n",
            "loss: 0.5835, acc: 0.7585\n",
            "E2E-ABSA >>> 2022-08-17 16:09:48\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:49\n",
            "loss: 0.5808, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:09:51\n",
            "loss: 0.5834, acc: 0.7646\n",
            "E2E-ABSA >>> 2022-08-17 16:09:52\n",
            "loss: 0.5833, acc: 0.7632\n",
            "E2E-ABSA >>> 2022-08-17 16:09:53\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:53\n",
            "loss: 0.5664, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:09:54\n",
            "loss: 0.5888, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-08-17 16:09:55\n",
            "loss: 0.5875, acc: 0.7582\n",
            "E2E-ABSA >>> 2022-08-17 16:09:56\n",
            "loss: 0.5846, acc: 0.7571\n",
            "E2E-ABSA >>> 2022-08-17 16:09:57\n",
            ">>> val_acc: 0.7264, val_precision: 0.7264 val_recall: 0.7264, val_f1: 0.7264\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.7264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 16:09:58\n",
            "loss: 0.5645, acc: 0.7723\n",
            "E2E-ABSA >>> 2022-08-17 16:09:59\n",
            "loss: 0.5749, acc: 0.7670\n",
            "E2E-ABSA >>> 2022-08-17 16:10:00\n",
            "loss: 0.5795, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-08-17 16:10:01\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:01\n",
            "loss: 0.5659, acc: 0.7598\n",
            "E2E-ABSA >>> 2022-08-17 16:10:02\n",
            "loss: 0.5694, acc: 0.7699\n",
            "E2E-ABSA >>> 2022-08-17 16:10:04\n",
            "loss: 0.5836, acc: 0.7632\n",
            "E2E-ABSA >>> 2022-08-17 16:10:05\n",
            "loss: 0.5798, acc: 0.7668\n",
            "E2E-ABSA >>> 2022-08-17 16:10:05\n",
            ">>> val_acc: 0.7264, val_precision: 0.7264 val_recall: 0.7264, val_f1: 0.7264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:06\n",
            "loss: 0.5878, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 16:10:07\n",
            "loss: 0.5868, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 16:10:08\n",
            "loss: 0.5810, acc: 0.7632\n",
            "E2E-ABSA >>> 2022-08-17 16:10:09\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:10\n",
            "loss: 0.5753, acc: 0.7634\n",
            "E2E-ABSA >>> 2022-08-17 16:10:11\n",
            "loss: 0.5765, acc: 0.7651\n",
            "E2E-ABSA >>> 2022-08-17 16:10:12\n",
            "loss: 0.5706, acc: 0.7675\n",
            "E2E-ABSA >>> 2022-08-17 16:10:13\n",
            "loss: 0.5714, acc: 0.7675\n",
            "E2E-ABSA >>> 2022-08-17 16:10:14\n",
            ">>> val_acc: 0.7152, val_precision: 0.7152 val_recall: 0.7152, val_f1: 0.7152\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:15\n",
            "loss: 0.5654, acc: 0.7673\n",
            "E2E-ABSA >>> 2022-08-17 16:10:16\n",
            "loss: 0.5646, acc: 0.7660\n",
            "E2E-ABSA >>> 2022-08-17 16:10:17\n",
            "loss: 0.5721, acc: 0.7638\n",
            "E2E-ABSA >>> 2022-08-17 16:10:18\n",
            ">>> val_acc: 0.7072, val_precision: 0.7072 val_recall: 0.7072, val_f1: 0.7072\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:18\n",
            "loss: 0.5449, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 16:10:19\n",
            "loss: 0.5773, acc: 0.7616\n",
            "E2E-ABSA >>> 2022-08-17 16:10:21\n",
            "loss: 0.5742, acc: 0.7651\n",
            "E2E-ABSA >>> 2022-08-17 16:10:22\n",
            "loss: 0.5742, acc: 0.7660\n",
            "E2E-ABSA >>> 2022-08-17 16:10:22\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:23\n",
            "loss: 0.5468, acc: 0.7717\n",
            "E2E-ABSA >>> 2022-08-17 16:10:24\n",
            "loss: 0.5672, acc: 0.7645\n",
            "E2E-ABSA >>> 2022-08-17 16:10:25\n",
            "loss: 0.5721, acc: 0.7574\n",
            "E2E-ABSA >>> 2022-08-17 16:10:26\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:27\n",
            "loss: 0.5900, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 16:10:28\n",
            "loss: 0.5512, acc: 0.7792\n",
            "E2E-ABSA >>> 2022-08-17 16:10:29\n",
            "loss: 0.5688, acc: 0.7690\n",
            "E2E-ABSA >>> 2022-08-17 16:10:30\n",
            "loss: 0.5645, acc: 0.7713\n",
            "E2E-ABSA >>> 2022-08-17 16:10:31\n",
            ">>> val_acc: 0.7216, val_precision: 0.7216 val_recall: 0.7216, val_f1: 0.7216\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:31\n",
            "loss: 0.5540, acc: 0.7629\n",
            "E2E-ABSA >>> 2022-08-17 16:10:33\n",
            "loss: 0.5710, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 16:10:34\n",
            "loss: 0.5700, acc: 0.7628\n",
            "E2E-ABSA >>> 2022-08-17 16:10:35\n",
            ">>> val_acc: 0.7152, val_precision: 0.7152 val_recall: 0.7152, val_f1: 0.7152\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:35\n",
            "loss: 0.5438, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-08-17 16:10:36\n",
            "loss: 0.5420, acc: 0.7775\n",
            "E2E-ABSA >>> 2022-08-17 16:10:37\n",
            "loss: 0.5648, acc: 0.7700\n",
            "E2E-ABSA >>> 2022-08-17 16:10:38\n",
            "loss: 0.5669, acc: 0.7650\n",
            "E2E-ABSA >>> 2022-08-17 16:10:39\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:40\n",
            "loss: 0.5637, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-08-17 16:10:41\n",
            "loss: 0.5605, acc: 0.7652\n",
            "E2E-ABSA >>> 2022-08-17 16:10:42\n",
            "loss: 0.5631, acc: 0.7652\n",
            "E2E-ABSA >>> 2022-08-17 16:10:43\n",
            ">>> val_acc: 0.7200, val_precision: 0.7200 val_recall: 0.7200, val_f1: 0.7200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:43\n",
            "loss: 0.5534, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 16:10:44\n",
            "loss: 0.5366, acc: 0.7885\n",
            "E2E-ABSA >>> 2022-08-17 16:10:46\n",
            "loss: 0.5580, acc: 0.7754\n",
            "E2E-ABSA >>> 2022-08-17 16:10:47\n",
            "loss: 0.5589, acc: 0.7706\n",
            "E2E-ABSA >>> 2022-08-17 16:10:47\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:48\n",
            "loss: 0.5523, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 16:10:49\n",
            "loss: 0.5648, acc: 0.7637\n",
            "E2E-ABSA >>> 2022-08-17 16:10:50\n",
            "loss: 0.5569, acc: 0.7675\n",
            "E2E-ABSA >>> 2022-08-17 16:10:51\n",
            ">>> val_acc: 0.7248, val_precision: 0.7248 val_recall: 0.7248, val_f1: 0.7248\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:52\n",
            "loss: 0.4862, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 16:10:53\n",
            "loss: 0.5412, acc: 0.7870\n",
            "E2E-ABSA >>> 2022-08-17 16:10:54\n",
            "loss: 0.5564, acc: 0.7728\n",
            "E2E-ABSA >>> 2022-08-17 16:10:55\n",
            "loss: 0.5594, acc: 0.7729\n",
            "E2E-ABSA >>> 2022-08-17 16:10:56\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 16:10:56\n",
            "loss: 0.5281, acc: 0.7868\n",
            "E2E-ABSA >>> 2022-08-17 16:10:57\n",
            "loss: 0.5610, acc: 0.7720\n",
            "E2E-ABSA >>> 2022-08-17 16:10:59\n",
            "loss: 0.5619, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 16:11:00\n",
            ">>> val_acc: 0.7280, val_precision: 0.7280 val_recall: 0.7280, val_f1: 0.7280\n",
            ">> saved: state_dict/atae_lstm_acl14shortdata_know_val_f1_0.728\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:00\n",
            "loss: 0.4624, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 16:11:01\n",
            "loss: 0.5374, acc: 0.7921\n",
            "E2E-ABSA >>> 2022-08-17 16:11:02\n",
            "loss: 0.5458, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 16:11:03\n",
            "loss: 0.5503, acc: 0.7753\n",
            "E2E-ABSA >>> 2022-08-17 16:11:04\n",
            ">>> val_acc: 0.7264, val_precision: 0.7264 val_recall: 0.7264, val_f1: 0.7264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:05\n",
            "loss: 0.5290, acc: 0.7849\n",
            "E2E-ABSA >>> 2022-08-17 16:11:06\n",
            "loss: 0.5448, acc: 0.7763\n",
            "E2E-ABSA >>> 2022-08-17 16:11:07\n",
            "loss: 0.5555, acc: 0.7741\n",
            "E2E-ABSA >>> 2022-08-17 16:11:08\n",
            "loss: 0.5553, acc: 0.7731\n",
            "E2E-ABSA >>> 2022-08-17 16:11:08\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:09\n",
            "loss: 0.5817, acc: 0.7562\n",
            "E2E-ABSA >>> 2022-08-17 16:11:10\n",
            "loss: 0.5552, acc: 0.7744\n",
            "E2E-ABSA >>> 2022-08-17 16:11:12\n",
            "loss: 0.5540, acc: 0.7725\n",
            "E2E-ABSA >>> 2022-08-17 16:11:12\n",
            ">>> val_acc: 0.7152, val_precision: 0.7152 val_recall: 0.7152, val_f1: 0.7152\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:13\n",
            "loss: 0.5510, acc: 0.7799\n",
            "E2E-ABSA >>> 2022-08-17 16:11:14\n",
            "loss: 0.5377, acc: 0.7800\n",
            "E2E-ABSA >>> 2022-08-17 16:11:15\n",
            "loss: 0.5435, acc: 0.7797\n",
            "E2E-ABSA >>> 2022-08-17 16:11:16\n",
            "loss: 0.5534, acc: 0.7741\n",
            "E2E-ABSA >>> 2022-08-17 16:11:17\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:18\n",
            "loss: 0.5560, acc: 0.7695\n",
            "E2E-ABSA >>> 2022-08-17 16:11:19\n",
            "loss: 0.5614, acc: 0.7701\n",
            "E2E-ABSA >>> 2022-08-17 16:11:20\n",
            "loss: 0.5551, acc: 0.7743\n",
            "E2E-ABSA >>> 2022-08-17 16:11:21\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:21\n",
            "loss: 0.5435, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-08-17 16:11:22\n",
            "loss: 0.5471, acc: 0.7639\n",
            "E2E-ABSA >>> 2022-08-17 16:11:23\n",
            "loss: 0.5555, acc: 0.7679\n",
            "E2E-ABSA >>> 2022-08-17 16:11:25\n",
            "loss: 0.5516, acc: 0.7707\n",
            "E2E-ABSA >>> 2022-08-17 16:11:25\n",
            ">>> val_acc: 0.7152, val_precision: 0.7152 val_recall: 0.7152, val_f1: 0.7152\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:26\n",
            "loss: 0.5591, acc: 0.7717\n",
            "E2E-ABSA >>> 2022-08-17 16:11:27\n",
            "loss: 0.5536, acc: 0.7718\n",
            "E2E-ABSA >>> 2022-08-17 16:11:28\n",
            "loss: 0.5478, acc: 0.7765\n",
            "E2E-ABSA >>> 2022-08-17 16:11:29\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:30\n",
            "loss: 0.5646, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 16:11:31\n",
            "loss: 0.5417, acc: 0.7728\n",
            "E2E-ABSA >>> 2022-08-17 16:11:32\n",
            "loss: 0.5491, acc: 0.7719\n",
            "E2E-ABSA >>> 2022-08-17 16:11:33\n",
            "loss: 0.5446, acc: 0.7763\n",
            "E2E-ABSA >>> 2022-08-17 16:11:33\n",
            ">>> val_acc: 0.7200, val_precision: 0.7200 val_recall: 0.7200, val_f1: 0.7200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:34\n",
            "loss: 0.5399, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 16:11:35\n",
            "loss: 0.5386, acc: 0.7733\n",
            "E2E-ABSA >>> 2022-08-17 16:11:37\n",
            "loss: 0.5419, acc: 0.7769\n",
            "E2E-ABSA >>> 2022-08-17 16:11:37\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:38\n",
            "loss: 0.5015, acc: 0.8108\n",
            "E2E-ABSA >>> 2022-08-17 16:11:39\n",
            "loss: 0.5356, acc: 0.7877\n",
            "E2E-ABSA >>> 2022-08-17 16:11:40\n",
            "loss: 0.5468, acc: 0.7794\n",
            "E2E-ABSA >>> 2022-08-17 16:11:41\n",
            "loss: 0.5463, acc: 0.7759\n",
            "E2E-ABSA >>> 2022-08-17 16:11:42\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:43\n",
            "loss: 0.5281, acc: 0.7820\n",
            "E2E-ABSA >>> 2022-08-17 16:11:44\n",
            "loss: 0.5320, acc: 0.7806\n",
            "E2E-ABSA >>> 2022-08-17 16:11:45\n",
            "loss: 0.5406, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 16:11:46\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:46\n",
            "loss: 0.4860, acc: 0.8008\n",
            "E2E-ABSA >>> 2022-08-17 16:11:47\n",
            "loss: 0.5131, acc: 0.7945\n",
            "E2E-ABSA >>> 2022-08-17 16:11:48\n",
            "loss: 0.5297, acc: 0.7872\n",
            "E2E-ABSA >>> 2022-08-17 16:11:50\n",
            "loss: 0.5393, acc: 0.7820\n",
            "E2E-ABSA >>> 2022-08-17 16:11:50\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:51\n",
            "loss: 0.5420, acc: 0.7828\n",
            "E2E-ABSA >>> 2022-08-17 16:11:52\n",
            "loss: 0.5409, acc: 0.7767\n",
            "E2E-ABSA >>> 2022-08-17 16:11:53\n",
            "loss: 0.5388, acc: 0.7766\n",
            "E2E-ABSA >>> 2022-08-17 16:11:54\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:55\n",
            "loss: 0.5281, acc: 0.7746\n",
            "E2E-ABSA >>> 2022-08-17 16:11:56\n",
            "loss: 0.5394, acc: 0.7715\n",
            "E2E-ABSA >>> 2022-08-17 16:11:57\n",
            "loss: 0.5347, acc: 0.7741\n",
            "E2E-ABSA >>> 2022-08-17 16:11:58\n",
            "loss: 0.5356, acc: 0.7776\n",
            "E2E-ABSA >>> 2022-08-17 16:11:58\n",
            ">>> val_acc: 0.7200, val_precision: 0.7200 val_recall: 0.7200, val_f1: 0.7200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 16:11:59\n",
            "loss: 0.5247, acc: 0.7911\n",
            "E2E-ABSA >>> 2022-08-17 16:12:00\n",
            "loss: 0.5261, acc: 0.7901\n",
            "E2E-ABSA >>> 2022-08-17 16:12:02\n",
            "loss: 0.5277, acc: 0.7894\n",
            "E2E-ABSA >>> 2022-08-17 16:12:03\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:03\n",
            "loss: 0.4943, acc: 0.7995\n",
            "E2E-ABSA >>> 2022-08-17 16:12:04\n",
            "loss: 0.5191, acc: 0.8034\n",
            "E2E-ABSA >>> 2022-08-17 16:12:05\n",
            "loss: 0.5183, acc: 0.7963\n",
            "E2E-ABSA >>> 2022-08-17 16:12:06\n",
            "loss: 0.5347, acc: 0.7826\n",
            "E2E-ABSA >>> 2022-08-17 16:12:07\n",
            ">>> val_acc: 0.7200, val_precision: 0.7200 val_recall: 0.7200, val_f1: 0.7200\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:08\n",
            "loss: 0.5336, acc: 0.7743\n",
            "E2E-ABSA >>> 2022-08-17 16:12:09\n",
            "loss: 0.5380, acc: 0.7780\n",
            "E2E-ABSA >>> 2022-08-17 16:12:10\n",
            "loss: 0.5321, acc: 0.7829\n",
            "E2E-ABSA >>> 2022-08-17 16:12:11\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:11\n",
            "loss: 0.5203, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-08-17 16:12:12\n",
            "loss: 0.5383, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 16:12:13\n",
            "loss: 0.5334, acc: 0.7884\n",
            "E2E-ABSA >>> 2022-08-17 16:12:15\n",
            "loss: 0.5349, acc: 0.7850\n",
            "E2E-ABSA >>> 2022-08-17 16:12:15\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:16\n",
            "loss: 0.5187, acc: 0.7960\n",
            "E2E-ABSA >>> 2022-08-17 16:12:17\n",
            "loss: 0.5144, acc: 0.7939\n",
            "E2E-ABSA >>> 2022-08-17 16:12:18\n",
            "loss: 0.5323, acc: 0.7850\n",
            "E2E-ABSA >>> 2022-08-17 16:12:19\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:19\n",
            "loss: 0.5324, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 16:12:21\n",
            "loss: 0.5013, acc: 0.8050\n",
            "E2E-ABSA >>> 2022-08-17 16:12:22\n",
            "loss: 0.5263, acc: 0.7894\n",
            "E2E-ABSA >>> 2022-08-17 16:12:23\n",
            "loss: 0.5304, acc: 0.7852\n",
            "E2E-ABSA >>> 2022-08-17 16:12:23\n",
            ">>> val_acc: 0.7216, val_precision: 0.7216 val_recall: 0.7216, val_f1: 0.7216\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:24\n",
            "loss: 0.5295, acc: 0.7920\n",
            "E2E-ABSA >>> 2022-08-17 16:12:25\n",
            "loss: 0.5181, acc: 0.7938\n",
            "E2E-ABSA >>> 2022-08-17 16:12:26\n",
            "loss: 0.5308, acc: 0.7829\n",
            "E2E-ABSA >>> 2022-08-17 16:12:28\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:28\n",
            "loss: 0.5562, acc: 0.7865\n",
            "E2E-ABSA >>> 2022-08-17 16:12:29\n",
            "loss: 0.5291, acc: 0.7879\n",
            "E2E-ABSA >>> 2022-08-17 16:12:30\n",
            "loss: 0.5311, acc: 0.7833\n",
            "E2E-ABSA >>> 2022-08-17 16:12:31\n",
            "loss: 0.5242, acc: 0.7853\n",
            "E2E-ABSA >>> 2022-08-17 16:12:32\n",
            ">>> val_acc: 0.7120, val_precision: 0.7120 val_recall: 0.7120, val_f1: 0.7120\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:32\n",
            "loss: 0.4943, acc: 0.8021\n",
            "E2E-ABSA >>> 2022-08-17 16:12:34\n",
            "loss: 0.5170, acc: 0.7953\n",
            "E2E-ABSA >>> 2022-08-17 16:12:35\n",
            "loss: 0.5204, acc: 0.7906\n",
            "E2E-ABSA >>> 2022-08-17 16:12:36\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:36\n",
            "loss: 0.4731, acc: 0.8359\n",
            "E2E-ABSA >>> 2022-08-17 16:12:38\n",
            "loss: 0.5186, acc: 0.7847\n",
            "E2E-ABSA >>> 2022-08-17 16:12:39\n",
            "loss: 0.5161, acc: 0.7885\n",
            "E2E-ABSA >>> 2022-08-17 16:12:40\n",
            "loss: 0.5190, acc: 0.7881\n",
            "E2E-ABSA >>> 2022-08-17 16:12:40\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:41\n",
            "loss: 0.4879, acc: 0.8069\n",
            "E2E-ABSA >>> 2022-08-17 16:12:42\n",
            "loss: 0.5067, acc: 0.7945\n",
            "E2E-ABSA >>> 2022-08-17 16:12:43\n",
            "loss: 0.5129, acc: 0.7886\n",
            "E2E-ABSA >>> 2022-08-17 16:12:45\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:45\n",
            "loss: 0.5124, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 16:12:46\n",
            "loss: 0.5266, acc: 0.7975\n",
            "E2E-ABSA >>> 2022-08-17 16:12:47\n",
            "loss: 0.5165, acc: 0.7923\n",
            "E2E-ABSA >>> 2022-08-17 16:12:48\n",
            "loss: 0.5245, acc: 0.7839\n",
            "E2E-ABSA >>> 2022-08-17 16:12:49\n",
            ">>> val_acc: 0.7120, val_precision: 0.7120 val_recall: 0.7120, val_f1: 0.7120\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-08-17 16:12:49\n",
            "loss: 0.5049, acc: 0.7909\n",
            "E2E-ABSA >>> 2022-08-17 16:12:51\n",
            "loss: 0.5107, acc: 0.7899\n",
            "E2E-ABSA >>> 2022-08-17 16:12:52\n",
            "loss: 0.5177, acc: 0.7840\n",
            "E2E-ABSA >>> 2022-08-17 16:12:53\n",
            "loss: 0.5205, acc: 0.7830\n",
            "E2E-ABSA >>> 2022-08-17 16:12:53\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            "you can download the best model from state_dict/atae_lstm_acl14shortdata_know_val_f1_0.728\n",
            ">>> test_acc: 0.7280, test_precision: 0.7280, test_recall: 0.7280, test_f1: 0.7280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **acl14shortdata** dataset on model(**IAN**)\n",
        "\n"
      ],
      "metadata": {
        "id": "UvhZ6yVKXi3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset acl14shortdata --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuUvRu5RXi9p",
        "outputId": "708bc566-0110-4b9b-c37c-ab037baed4c9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 23994880\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 3828600\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f67ea64fb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:13:33\n",
            "loss: 1.0595, acc: 0.4431\n",
            "E2E-ABSA >>> 2022-08-17 16:13:34\n",
            "loss: 1.0417, acc: 0.4725\n",
            "E2E-ABSA >>> 2022-08-17 16:13:36\n",
            "loss: 1.0271, acc: 0.4877\n",
            "E2E-ABSA >>> 2022-08-17 16:13:36\n",
            ">>> val_acc: 0.4672, val_precision: 0.4672 val_recall: 0.4672, val_f1: 0.4672\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.4672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:13:37\n",
            "loss: 0.9959, acc: 0.5208\n",
            "E2E-ABSA >>> 2022-08-17 16:13:38\n",
            "loss: 0.9884, acc: 0.5215\n",
            "E2E-ABSA >>> 2022-08-17 16:13:39\n",
            "loss: 0.9890, acc: 0.5159\n",
            "E2E-ABSA >>> 2022-08-17 16:13:41\n",
            "loss: 0.9854, acc: 0.5192\n",
            "E2E-ABSA >>> 2022-08-17 16:13:41\n",
            ">>> val_acc: 0.4832, val_precision: 0.4832 val_recall: 0.4832, val_f1: 0.4832\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.4832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:13:42\n",
            "loss: 0.9814, acc: 0.5169\n",
            "E2E-ABSA >>> 2022-08-17 16:13:43\n",
            "loss: 0.9719, acc: 0.5246\n",
            "E2E-ABSA >>> 2022-08-17 16:13:45\n",
            "loss: 0.9657, acc: 0.5279\n",
            "E2E-ABSA >>> 2022-08-17 16:13:45\n",
            ">>> val_acc: 0.5024, val_precision: 0.5024 val_recall: 0.5024, val_f1: 0.5024\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.5024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:13:46\n",
            "loss: 0.9445, acc: 0.5440\n",
            "E2E-ABSA >>> 2022-08-17 16:13:47\n",
            "loss: 0.9452, acc: 0.5482\n",
            "E2E-ABSA >>> 2022-08-17 16:13:48\n",
            "loss: 0.9402, acc: 0.5484\n",
            "E2E-ABSA >>> 2022-08-17 16:13:50\n",
            "loss: 0.9448, acc: 0.5485\n",
            "E2E-ABSA >>> 2022-08-17 16:13:50\n",
            ">>> val_acc: 0.5456, val_precision: 0.5456 val_recall: 0.5456, val_f1: 0.5456\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.5456\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:13:51\n",
            "loss: 0.9486, acc: 0.5550\n",
            "E2E-ABSA >>> 2022-08-17 16:13:52\n",
            "loss: 0.9349, acc: 0.5661\n",
            "E2E-ABSA >>> 2022-08-17 16:13:53\n",
            "loss: 0.9332, acc: 0.5659\n",
            "E2E-ABSA >>> 2022-08-17 16:13:54\n",
            ">>> val_acc: 0.5392, val_precision: 0.5392 val_recall: 0.5392, val_f1: 0.5392\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:13:55\n",
            "loss: 0.9386, acc: 0.5547\n",
            "E2E-ABSA >>> 2022-08-17 16:13:56\n",
            "loss: 0.9181, acc: 0.5705\n",
            "E2E-ABSA >>> 2022-08-17 16:13:57\n",
            "loss: 0.9171, acc: 0.5768\n",
            "E2E-ABSA >>> 2022-08-17 16:13:58\n",
            "loss: 0.9106, acc: 0.5790\n",
            "E2E-ABSA >>> 2022-08-17 16:13:59\n",
            ">>> val_acc: 0.5584, val_precision: 0.5584 val_recall: 0.5584, val_f1: 0.5584\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.5584\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:00\n",
            "loss: 0.8768, acc: 0.6051\n",
            "E2E-ABSA >>> 2022-08-17 16:14:01\n",
            "loss: 0.8811, acc: 0.5991\n",
            "E2E-ABSA >>> 2022-08-17 16:14:02\n",
            "loss: 0.8745, acc: 0.6055\n",
            "E2E-ABSA >>> 2022-08-17 16:14:03\n",
            ">>> val_acc: 0.6080, val_precision: 0.6080 val_recall: 0.6080, val_f1: 0.6080\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:04\n",
            "loss: 0.8098, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 16:14:05\n",
            "loss: 0.8287, acc: 0.6218\n",
            "E2E-ABSA >>> 2022-08-17 16:14:06\n",
            "loss: 0.8285, acc: 0.6255\n",
            "E2E-ABSA >>> 2022-08-17 16:14:08\n",
            "loss: 0.8187, acc: 0.6300\n",
            "E2E-ABSA >>> 2022-08-17 16:14:08\n",
            ">>> val_acc: 0.6176, val_precision: 0.6176 val_recall: 0.6176, val_f1: 0.6176\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6176\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:09\n",
            "loss: 0.7946, acc: 0.6421\n",
            "E2E-ABSA >>> 2022-08-17 16:14:10\n",
            "loss: 0.7923, acc: 0.6457\n",
            "E2E-ABSA >>> 2022-08-17 16:14:11\n",
            "loss: 0.7819, acc: 0.6477\n",
            "E2E-ABSA >>> 2022-08-17 16:14:12\n",
            ">>> val_acc: 0.6368, val_precision: 0.6368 val_recall: 0.6368, val_f1: 0.6368\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6368\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:13\n",
            "loss: 0.6999, acc: 0.7129\n",
            "E2E-ABSA >>> 2022-08-17 16:14:14\n",
            "loss: 0.7575, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-08-17 16:14:15\n",
            "loss: 0.7588, acc: 0.6654\n",
            "E2E-ABSA >>> 2022-08-17 16:14:17\n",
            "loss: 0.7707, acc: 0.6595\n",
            "E2E-ABSA >>> 2022-08-17 16:14:17\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:18\n",
            "loss: 0.7571, acc: 0.6656\n",
            "E2E-ABSA >>> 2022-08-17 16:14:19\n",
            "loss: 0.7628, acc: 0.6594\n",
            "E2E-ABSA >>> 2022-08-17 16:14:20\n",
            "loss: 0.7548, acc: 0.6670\n",
            "E2E-ABSA >>> 2022-08-17 16:14:21\n",
            ">>> val_acc: 0.6384, val_precision: 0.6384 val_recall: 0.6384, val_f1: 0.6384\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:22\n",
            "loss: 0.7739, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 16:14:23\n",
            "loss: 0.7529, acc: 0.6772\n",
            "E2E-ABSA >>> 2022-08-17 16:14:24\n",
            "loss: 0.7456, acc: 0.6749\n",
            "E2E-ABSA >>> 2022-08-17 16:14:25\n",
            "loss: 0.7498, acc: 0.6732\n",
            "E2E-ABSA >>> 2022-08-17 16:14:26\n",
            ">>> val_acc: 0.6368, val_precision: 0.6368 val_recall: 0.6368, val_f1: 0.6368\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:27\n",
            "loss: 0.7334, acc: 0.6727\n",
            "E2E-ABSA >>> 2022-08-17 16:14:28\n",
            "loss: 0.7321, acc: 0.6829\n",
            "E2E-ABSA >>> 2022-08-17 16:14:29\n",
            "loss: 0.7432, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-08-17 16:14:30\n",
            ">>> val_acc: 0.6544, val_precision: 0.6544 val_recall: 0.6544, val_f1: 0.6544\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:31\n",
            "loss: 0.6870, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-08-17 16:14:32\n",
            "loss: 0.7402, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-08-17 16:14:33\n",
            "loss: 0.7426, acc: 0.6811\n",
            "E2E-ABSA >>> 2022-08-17 16:14:35\n",
            "loss: 0.7447, acc: 0.6802\n",
            "E2E-ABSA >>> 2022-08-17 16:14:35\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:36\n",
            "loss: 0.7200, acc: 0.7101\n",
            "E2E-ABSA >>> 2022-08-17 16:14:37\n",
            "loss: 0.7292, acc: 0.6948\n",
            "E2E-ABSA >>> 2022-08-17 16:14:38\n",
            "loss: 0.7414, acc: 0.6850\n",
            "E2E-ABSA >>> 2022-08-17 16:14:39\n",
            ">>> val_acc: 0.6576, val_precision: 0.6576 val_recall: 0.6576, val_f1: 0.6576\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6576\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:40\n",
            "loss: 0.7214, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 16:14:41\n",
            "loss: 0.7150, acc: 0.6969\n",
            "E2E-ABSA >>> 2022-08-17 16:14:42\n",
            "loss: 0.7227, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 16:14:43\n",
            "loss: 0.7242, acc: 0.6902\n",
            "E2E-ABSA >>> 2022-08-17 16:14:44\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:45\n",
            "loss: 0.7332, acc: 0.6838\n",
            "E2E-ABSA >>> 2022-08-17 16:14:46\n",
            "loss: 0.7254, acc: 0.6886\n",
            "E2E-ABSA >>> 2022-08-17 16:14:47\n",
            "loss: 0.7288, acc: 0.6861\n",
            "E2E-ABSA >>> 2022-08-17 16:14:48\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:49\n",
            "loss: 0.7318, acc: 0.6602\n",
            "E2E-ABSA >>> 2022-08-17 16:14:50\n",
            "loss: 0.7194, acc: 0.6864\n",
            "E2E-ABSA >>> 2022-08-17 16:14:51\n",
            "loss: 0.7290, acc: 0.6777\n",
            "E2E-ABSA >>> 2022-08-17 16:14:52\n",
            "loss: 0.7262, acc: 0.6841\n",
            "E2E-ABSA >>> 2022-08-17 16:14:53\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:54\n",
            "loss: 0.7652, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 16:14:55\n",
            "loss: 0.7421, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-08-17 16:14:56\n",
            "loss: 0.7293, acc: 0.6901\n",
            "E2E-ABSA >>> 2022-08-17 16:14:57\n",
            ">>> val_acc: 0.6576, val_precision: 0.6576 val_recall: 0.6576, val_f1: 0.6576\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:14:58\n",
            "loss: 0.7237, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 16:14:59\n",
            "loss: 0.7325, acc: 0.6836\n",
            "E2E-ABSA >>> 2022-08-17 16:15:00\n",
            "loss: 0.7266, acc: 0.6893\n",
            "E2E-ABSA >>> 2022-08-17 16:15:01\n",
            "loss: 0.7202, acc: 0.6949\n",
            "E2E-ABSA >>> 2022-08-17 16:15:02\n",
            ">>> val_acc: 0.6640, val_precision: 0.6640 val_recall: 0.6640, val_f1: 0.6640\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:03\n",
            "loss: 0.7011, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 16:15:04\n",
            "loss: 0.6948, acc: 0.7094\n",
            "E2E-ABSA >>> 2022-08-17 16:15:06\n",
            "loss: 0.7087, acc: 0.6966\n",
            "E2E-ABSA >>> 2022-08-17 16:15:07\n",
            ">>> val_acc: 0.6544, val_precision: 0.6544 val_recall: 0.6544, val_f1: 0.6544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:07\n",
            "loss: 0.6823, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:15:08\n",
            "loss: 0.7213, acc: 0.6869\n",
            "E2E-ABSA >>> 2022-08-17 16:15:09\n",
            "loss: 0.7205, acc: 0.6893\n",
            "E2E-ABSA >>> 2022-08-17 16:15:11\n",
            "loss: 0.7182, acc: 0.6905\n",
            "E2E-ABSA >>> 2022-08-17 16:15:11\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:12\n",
            "loss: 0.7294, acc: 0.6920\n",
            "E2E-ABSA >>> 2022-08-17 16:15:13\n",
            "loss: 0.7277, acc: 0.6903\n",
            "E2E-ABSA >>> 2022-08-17 16:15:14\n",
            "loss: 0.7239, acc: 0.6904\n",
            "E2E-ABSA >>> 2022-08-17 16:15:16\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:16\n",
            "loss: 0.6450, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:15:17\n",
            "loss: 0.7133, acc: 0.6935\n",
            "E2E-ABSA >>> 2022-08-17 16:15:18\n",
            "loss: 0.7085, acc: 0.7001\n",
            "E2E-ABSA >>> 2022-08-17 16:15:20\n",
            "loss: 0.7074, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-08-17 16:15:20\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:21\n",
            "loss: 0.7027, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:15:22\n",
            "loss: 0.7055, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-08-17 16:15:23\n",
            "loss: 0.7027, acc: 0.7019\n",
            "E2E-ABSA >>> 2022-08-17 16:15:25\n",
            "loss: 0.7083, acc: 0.6934\n",
            "E2E-ABSA >>> 2022-08-17 16:15:25\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:26\n",
            "loss: 0.7174, acc: 0.6900\n",
            "E2E-ABSA >>> 2022-08-17 16:15:27\n",
            "loss: 0.7133, acc: 0.6906\n",
            "E2E-ABSA >>> 2022-08-17 16:15:28\n",
            "loss: 0.7060, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-08-17 16:15:29\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:30\n",
            "loss: 0.7304, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-08-17 16:15:31\n",
            "loss: 0.7114, acc: 0.6985\n",
            "E2E-ABSA >>> 2022-08-17 16:15:32\n",
            "loss: 0.7050, acc: 0.7021\n",
            "E2E-ABSA >>> 2022-08-17 16:15:33\n",
            "loss: 0.7048, acc: 0.7017\n",
            "E2E-ABSA >>> 2022-08-17 16:15:34\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:35\n",
            "loss: 0.7001, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-08-17 16:15:36\n",
            "loss: 0.6928, acc: 0.7098\n",
            "E2E-ABSA >>> 2022-08-17 16:15:37\n",
            "loss: 0.7030, acc: 0.7033\n",
            "E2E-ABSA >>> 2022-08-17 16:15:38\n",
            ">>> val_acc: 0.6640, val_precision: 0.6640 val_recall: 0.6640, val_f1: 0.6640\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:39\n",
            "loss: 0.7021, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-08-17 16:15:40\n",
            "loss: 0.6889, acc: 0.7118\n",
            "E2E-ABSA >>> 2022-08-17 16:15:41\n",
            "loss: 0.7062, acc: 0.7016\n",
            "E2E-ABSA >>> 2022-08-17 16:15:42\n",
            "loss: 0.7003, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 16:15:43\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:44\n",
            "loss: 0.6996, acc: 0.6970\n",
            "E2E-ABSA >>> 2022-08-17 16:15:45\n",
            "loss: 0.6890, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-08-17 16:15:46\n",
            "loss: 0.6982, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-08-17 16:15:47\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:48\n",
            "loss: 0.7353, acc: 0.6859\n",
            "E2E-ABSA >>> 2022-08-17 16:15:49\n",
            "loss: 0.6977, acc: 0.7022\n",
            "E2E-ABSA >>> 2022-08-17 16:15:50\n",
            "loss: 0.7013, acc: 0.7003\n",
            "E2E-ABSA >>> 2022-08-17 16:15:51\n",
            "loss: 0.6966, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 16:15:52\n",
            ">>> val_acc: 0.6672, val_precision: 0.6672 val_recall: 0.6672, val_f1: 0.6672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:53\n",
            "loss: 0.6891, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-08-17 16:15:54\n",
            "loss: 0.6994, acc: 0.6951\n",
            "E2E-ABSA >>> 2022-08-17 16:15:55\n",
            "loss: 0.7025, acc: 0.6955\n",
            "E2E-ABSA >>> 2022-08-17 16:15:56\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:15:57\n",
            "loss: 0.6872, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-08-17 16:15:58\n",
            "loss: 0.6963, acc: 0.6962\n",
            "E2E-ABSA >>> 2022-08-17 16:15:59\n",
            "loss: 0.7035, acc: 0.7002\n",
            "E2E-ABSA >>> 2022-08-17 16:16:00\n",
            "loss: 0.6936, acc: 0.7052\n",
            "E2E-ABSA >>> 2022-08-17 16:16:01\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:02\n",
            "loss: 0.7109, acc: 0.6882\n",
            "E2E-ABSA >>> 2022-08-17 16:16:03\n",
            "loss: 0.7009, acc: 0.7024\n",
            "E2E-ABSA >>> 2022-08-17 16:16:04\n",
            "loss: 0.6950, acc: 0.7069\n",
            "E2E-ABSA >>> 2022-08-17 16:16:05\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:05\n",
            "loss: 0.6981, acc: 0.6836\n",
            "E2E-ABSA >>> 2022-08-17 16:16:07\n",
            "loss: 0.6987, acc: 0.7003\n",
            "E2E-ABSA >>> 2022-08-17 16:16:08\n",
            "loss: 0.6985, acc: 0.7029\n",
            "E2E-ABSA >>> 2022-08-17 16:16:09\n",
            "loss: 0.6925, acc: 0.7059\n",
            "E2E-ABSA >>> 2022-08-17 16:16:09\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:10\n",
            "loss: 0.6553, acc: 0.7211\n",
            "E2E-ABSA >>> 2022-08-17 16:16:12\n",
            "loss: 0.6862, acc: 0.7045\n",
            "E2E-ABSA >>> 2022-08-17 16:16:13\n",
            "loss: 0.6882, acc: 0.7065\n",
            "E2E-ABSA >>> 2022-08-17 16:16:14\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:14\n",
            "loss: 0.6581, acc: 0.6942\n",
            "E2E-ABSA >>> 2022-08-17 16:16:15\n",
            "loss: 0.6915, acc: 0.7090\n",
            "E2E-ABSA >>> 2022-08-17 16:16:17\n",
            "loss: 0.6901, acc: 0.7081\n",
            "E2E-ABSA >>> 2022-08-17 16:16:18\n",
            "loss: 0.6888, acc: 0.7077\n",
            "E2E-ABSA >>> 2022-08-17 16:16:18\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:19\n",
            "loss: 0.6834, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-08-17 16:16:20\n",
            "loss: 0.6818, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-08-17 16:16:22\n",
            "loss: 0.6919, acc: 0.7036\n",
            "E2E-ABSA >>> 2022-08-17 16:16:23\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:23\n",
            "loss: 0.7078, acc: 0.6901\n",
            "E2E-ABSA >>> 2022-08-17 16:16:24\n",
            "loss: 0.6820, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-08-17 16:16:25\n",
            "loss: 0.6933, acc: 0.7040\n",
            "E2E-ABSA >>> 2022-08-17 16:16:27\n",
            "loss: 0.6882, acc: 0.7058\n",
            "E2E-ABSA >>> 2022-08-17 16:16:27\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:28\n",
            "loss: 0.6759, acc: 0.7179\n",
            "E2E-ABSA >>> 2022-08-17 16:16:29\n",
            "loss: 0.6843, acc: 0.7064\n",
            "E2E-ABSA >>> 2022-08-17 16:16:30\n",
            "loss: 0.6848, acc: 0.7034\n",
            "E2E-ABSA >>> 2022-08-17 16:16:32\n",
            ">>> val_acc: 0.6640, val_precision: 0.6640 val_recall: 0.6640, val_f1: 0.6640\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:32\n",
            "loss: 0.7093, acc: 0.6844\n",
            "E2E-ABSA >>> 2022-08-17 16:16:33\n",
            "loss: 0.6837, acc: 0.7073\n",
            "E2E-ABSA >>> 2022-08-17 16:16:34\n",
            "loss: 0.6831, acc: 0.7088\n",
            "E2E-ABSA >>> 2022-08-17 16:16:35\n",
            "loss: 0.6832, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 16:16:36\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:37\n",
            "loss: 0.6655, acc: 0.7215\n",
            "E2E-ABSA >>> 2022-08-17 16:16:38\n",
            "loss: 0.6769, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 16:16:39\n",
            "loss: 0.6805, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 16:16:40\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:41\n",
            "loss: 0.6745, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-08-17 16:16:42\n",
            "loss: 0.6760, acc: 0.7134\n",
            "E2E-ABSA >>> 2022-08-17 16:16:43\n",
            "loss: 0.6702, acc: 0.7159\n",
            "E2E-ABSA >>> 2022-08-17 16:16:44\n",
            "loss: 0.6811, acc: 0.7130\n",
            "E2E-ABSA >>> 2022-08-17 16:16:45\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:46\n",
            "loss: 0.6790, acc: 0.7080\n",
            "E2E-ABSA >>> 2022-08-17 16:16:47\n",
            "loss: 0.6740, acc: 0.7142\n",
            "E2E-ABSA >>> 2022-08-17 16:16:48\n",
            "loss: 0.6810, acc: 0.7102\n",
            "E2E-ABSA >>> 2022-08-17 16:16:49\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:49\n",
            "loss: 0.7292, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 16:16:51\n",
            "loss: 0.6801, acc: 0.7104\n",
            "E2E-ABSA >>> 2022-08-17 16:16:52\n",
            "loss: 0.6744, acc: 0.7185\n",
            "E2E-ABSA >>> 2022-08-17 16:16:53\n",
            "loss: 0.6820, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 16:16:54\n",
            ">>> val_acc: 0.6672, val_precision: 0.6672 val_recall: 0.6672, val_f1: 0.6672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:55\n",
            "loss: 0.6943, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 16:16:56\n",
            "loss: 0.6891, acc: 0.7074\n",
            "E2E-ABSA >>> 2022-08-17 16:16:57\n",
            "loss: 0.6851, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-08-17 16:16:58\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:16:58\n",
            "loss: 0.6809, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 16:17:00\n",
            "loss: 0.6953, acc: 0.7112\n",
            "E2E-ABSA >>> 2022-08-17 16:17:01\n",
            "loss: 0.6782, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-08-17 16:17:02\n",
            "loss: 0.6762, acc: 0.7153\n",
            "E2E-ABSA >>> 2022-08-17 16:17:03\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:04\n",
            "loss: 0.6822, acc: 0.6998\n",
            "E2E-ABSA >>> 2022-08-17 16:17:05\n",
            "loss: 0.6745, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 16:17:06\n",
            "loss: 0.6780, acc: 0.7158\n",
            "E2E-ABSA >>> 2022-08-17 16:17:07\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:07\n",
            "loss: 0.6750, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:17:09\n",
            "loss: 0.6774, acc: 0.7025\n",
            "E2E-ABSA >>> 2022-08-17 16:17:10\n",
            "loss: 0.6833, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-08-17 16:17:11\n",
            "loss: 0.6822, acc: 0.7093\n",
            "E2E-ABSA >>> 2022-08-17 16:17:12\n",
            ">>> val_acc: 0.6640, val_precision: 0.6640 val_recall: 0.6640, val_f1: 0.6640\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:12\n",
            "loss: 0.6534, acc: 0.7236\n",
            "E2E-ABSA >>> 2022-08-17 16:17:14\n",
            "loss: 0.6751, acc: 0.7126\n",
            "E2E-ABSA >>> 2022-08-17 16:17:15\n",
            "loss: 0.6884, acc: 0.7051\n",
            "E2E-ABSA >>> 2022-08-17 16:17:16\n",
            "loss: 0.6803, acc: 0.7107\n",
            "E2E-ABSA >>> 2022-08-17 16:17:16\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:18\n",
            "loss: 0.6834, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:17:19\n",
            "loss: 0.6788, acc: 0.7075\n",
            "E2E-ABSA >>> 2022-08-17 16:17:20\n",
            "loss: 0.6782, acc: 0.7090\n",
            "E2E-ABSA >>> 2022-08-17 16:17:21\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:21\n",
            "loss: 0.6579, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 16:17:23\n",
            "loss: 0.6853, acc: 0.7107\n",
            "E2E-ABSA >>> 2022-08-17 16:17:24\n",
            "loss: 0.6795, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 16:17:25\n",
            "loss: 0.6782, acc: 0.7116\n",
            "E2E-ABSA >>> 2022-08-17 16:17:25\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:27\n",
            "loss: 0.6615, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-08-17 16:17:28\n",
            "loss: 0.6724, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 16:17:29\n",
            "loss: 0.6736, acc: 0.7166\n",
            "E2E-ABSA >>> 2022-08-17 16:17:30\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:30\n",
            "loss: 0.7082, acc: 0.6918\n",
            "E2E-ABSA >>> 2022-08-17 16:17:32\n",
            "loss: 0.6856, acc: 0.7036\n",
            "E2E-ABSA >>> 2022-08-17 16:17:33\n",
            "loss: 0.6843, acc: 0.7098\n",
            "E2E-ABSA >>> 2022-08-17 16:17:34\n",
            "loss: 0.6777, acc: 0.7120\n",
            "E2E-ABSA >>> 2022-08-17 16:17:34\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:36\n",
            "loss: 0.7008, acc: 0.7038\n",
            "E2E-ABSA >>> 2022-08-17 16:17:37\n",
            "loss: 0.6829, acc: 0.7152\n",
            "E2E-ABSA >>> 2022-08-17 16:17:38\n",
            "loss: 0.6752, acc: 0.7140\n",
            "E2E-ABSA >>> 2022-08-17 16:17:39\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:39\n",
            "loss: 0.6332, acc: 0.7297\n",
            "E2E-ABSA >>> 2022-08-17 16:17:41\n",
            "loss: 0.6734, acc: 0.7138\n",
            "E2E-ABSA >>> 2022-08-17 16:17:42\n",
            "loss: 0.6707, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 16:17:43\n",
            "loss: 0.6724, acc: 0.7131\n",
            "E2E-ABSA >>> 2022-08-17 16:17:43\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:44\n",
            "loss: 0.6706, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 16:17:46\n",
            "loss: 0.6634, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-08-17 16:17:47\n",
            "loss: 0.6726, acc: 0.7114\n",
            "E2E-ABSA >>> 2022-08-17 16:17:48\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:48\n",
            "loss: 0.6848, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-08-17 16:17:49\n",
            "loss: 0.6867, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-08-17 16:17:51\n",
            "loss: 0.6709, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 16:17:52\n",
            "loss: 0.6725, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-08-17 16:17:52\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:53\n",
            "loss: 0.6448, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 16:17:55\n",
            "loss: 0.6590, acc: 0.7143\n",
            "E2E-ABSA >>> 2022-08-17 16:17:56\n",
            "loss: 0.6693, acc: 0.7132\n",
            "E2E-ABSA >>> 2022-08-17 16:17:57\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:17:57\n",
            "loss: 0.6881, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 16:17:58\n",
            "loss: 0.6700, acc: 0.7211\n",
            "E2E-ABSA >>> 2022-08-17 16:18:00\n",
            "loss: 0.6760, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 16:18:01\n",
            "loss: 0.6750, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-08-17 16:18:01\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:02\n",
            "loss: 0.6641, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 16:18:03\n",
            "loss: 0.6541, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-08-17 16:18:05\n",
            "loss: 0.6652, acc: 0.7192\n",
            "E2E-ABSA >>> 2022-08-17 16:18:06\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:06\n",
            "loss: 0.6319, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:18:07\n",
            "loss: 0.6547, acc: 0.7329\n",
            "E2E-ABSA >>> 2022-08-17 16:18:09\n",
            "loss: 0.6665, acc: 0.7209\n",
            "E2E-ABSA >>> 2022-08-17 16:18:10\n",
            "loss: 0.6715, acc: 0.7180\n",
            "E2E-ABSA >>> 2022-08-17 16:18:10\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:11\n",
            "loss: 0.6768, acc: 0.7130\n",
            "E2E-ABSA >>> 2022-08-17 16:18:12\n",
            "loss: 0.6622, acc: 0.7195\n",
            "E2E-ABSA >>> 2022-08-17 16:18:14\n",
            "loss: 0.6745, acc: 0.7124\n",
            "E2E-ABSA >>> 2022-08-17 16:18:15\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:15\n",
            "loss: 0.6817, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 16:18:16\n",
            "loss: 0.6534, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 16:18:17\n",
            "loss: 0.6672, acc: 0.7176\n",
            "E2E-ABSA >>> 2022-08-17 16:18:19\n",
            "loss: 0.6685, acc: 0.7166\n",
            "E2E-ABSA >>> 2022-08-17 16:18:19\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:20\n",
            "loss: 0.6471, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 16:18:21\n",
            "loss: 0.6557, acc: 0.7293\n",
            "E2E-ABSA >>> 2022-08-17 16:18:22\n",
            "loss: 0.6549, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-08-17 16:18:24\n",
            ">>> val_acc: 0.6864, val_precision: 0.6864 val_recall: 0.6864, val_f1: 0.6864\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:24\n",
            "loss: 0.6985, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-08-17 16:18:25\n",
            "loss: 0.6685, acc: 0.7161\n",
            "E2E-ABSA >>> 2022-08-17 16:18:26\n",
            "loss: 0.6627, acc: 0.7193\n",
            "E2E-ABSA >>> 2022-08-17 16:18:27\n",
            "loss: 0.6630, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-08-17 16:18:28\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:29\n",
            "loss: 0.6631, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-08-17 16:18:30\n",
            "loss: 0.6755, acc: 0.7180\n",
            "E2E-ABSA >>> 2022-08-17 16:18:31\n",
            "loss: 0.6623, acc: 0.7234\n",
            "E2E-ABSA >>> 2022-08-17 16:18:33\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:33\n",
            "loss: 0.6367, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 16:18:34\n",
            "loss: 0.6695, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 16:18:35\n",
            "loss: 0.6630, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:18:36\n",
            "loss: 0.6677, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-08-17 16:18:37\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:38\n",
            "loss: 0.6401, acc: 0.7236\n",
            "E2E-ABSA >>> 2022-08-17 16:18:39\n",
            "loss: 0.6688, acc: 0.7199\n",
            "E2E-ABSA >>> 2022-08-17 16:18:40\n",
            "loss: 0.6725, acc: 0.7178\n",
            "E2E-ABSA >>> 2022-08-17 16:18:42\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:42\n",
            "loss: 0.6124, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:18:43\n",
            "loss: 0.6396, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 16:18:44\n",
            "loss: 0.6619, acc: 0.7238\n",
            "E2E-ABSA >>> 2022-08-17 16:18:45\n",
            "loss: 0.6645, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 16:18:46\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:47\n",
            "loss: 0.6619, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 16:18:48\n",
            "loss: 0.6630, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-08-17 16:18:49\n",
            "loss: 0.6613, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 16:18:50\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">> saved: state_dict/ian_acl14shortdata_val_f1_0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:51\n",
            "loss: 0.6697, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:18:52\n",
            "loss: 0.6532, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-08-17 16:18:53\n",
            "loss: 0.6553, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 16:18:54\n",
            "loss: 0.6536, acc: 0.7252\n",
            "E2E-ABSA >>> 2022-08-17 16:18:55\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:18:56\n",
            "loss: 0.6720, acc: 0.7132\n",
            "E2E-ABSA >>> 2022-08-17 16:18:57\n",
            "loss: 0.6584, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 16:18:58\n",
            "loss: 0.6596, acc: 0.7251\n",
            "E2E-ABSA >>> 2022-08-17 16:18:59\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:00\n",
            "loss: 0.7462, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 16:19:01\n",
            "loss: 0.6594, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 16:19:02\n",
            "loss: 0.6650, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 16:19:03\n",
            "loss: 0.6637, acc: 0.7218\n",
            "E2E-ABSA >>> 2022-08-17 16:19:04\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:05\n",
            "loss: 0.6079, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:19:06\n",
            "loss: 0.6521, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 16:19:07\n",
            "loss: 0.6539, acc: 0.7195\n",
            "E2E-ABSA >>> 2022-08-17 16:19:08\n",
            "loss: 0.6597, acc: 0.7192\n",
            "E2E-ABSA >>> 2022-08-17 16:19:08\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:10\n",
            "loss: 0.6810, acc: 0.7163\n",
            "E2E-ABSA >>> 2022-08-17 16:19:11\n",
            "loss: 0.6795, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-08-17 16:19:12\n",
            "loss: 0.6675, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:19:13\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:13\n",
            "loss: 0.6856, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 16:19:15\n",
            "loss: 0.6623, acc: 0.7192\n",
            "E2E-ABSA >>> 2022-08-17 16:19:16\n",
            "loss: 0.6592, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 16:19:17\n",
            "loss: 0.6611, acc: 0.7218\n",
            "E2E-ABSA >>> 2022-08-17 16:19:17\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:18\n",
            "loss: 0.6622, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-08-17 16:19:20\n",
            "loss: 0.6656, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 16:19:21\n",
            "loss: 0.6627, acc: 0.7196\n",
            "E2E-ABSA >>> 2022-08-17 16:19:22\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:22\n",
            "loss: 0.6324, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 16:19:23\n",
            "loss: 0.6573, acc: 0.7209\n",
            "E2E-ABSA >>> 2022-08-17 16:19:25\n",
            "loss: 0.6654, acc: 0.7182\n",
            "E2E-ABSA >>> 2022-08-17 16:19:26\n",
            "loss: 0.6557, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 16:19:26\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:27\n",
            "loss: 0.6524, acc: 0.7289\n",
            "E2E-ABSA >>> 2022-08-17 16:19:29\n",
            "loss: 0.6466, acc: 0.7357\n",
            "E2E-ABSA >>> 2022-08-17 16:19:30\n",
            "loss: 0.6503, acc: 0.7322\n",
            "E2E-ABSA >>> 2022-08-17 16:19:31\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:31\n",
            "loss: 0.6836, acc: 0.7047\n",
            "E2E-ABSA >>> 2022-08-17 16:19:32\n",
            "loss: 0.6538, acc: 0.7317\n",
            "E2E-ABSA >>> 2022-08-17 16:19:34\n",
            "loss: 0.6557, acc: 0.7255\n",
            "E2E-ABSA >>> 2022-08-17 16:19:35\n",
            "loss: 0.6573, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-08-17 16:19:35\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:36\n",
            "loss: 0.6497, acc: 0.7195\n",
            "E2E-ABSA >>> 2022-08-17 16:19:37\n",
            "loss: 0.6507, acc: 0.7221\n",
            "E2E-ABSA >>> 2022-08-17 16:19:39\n",
            "loss: 0.6563, acc: 0.7201\n",
            "E2E-ABSA >>> 2022-08-17 16:19:40\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:40\n",
            "loss: 0.6441, acc: 0.7413\n",
            "E2E-ABSA >>> 2022-08-17 16:19:41\n",
            "loss: 0.6507, acc: 0.7215\n",
            "E2E-ABSA >>> 2022-08-17 16:19:42\n",
            "loss: 0.6545, acc: 0.7209\n",
            "E2E-ABSA >>> 2022-08-17 16:19:44\n",
            "loss: 0.6559, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:19:44\n",
            ">>> val_acc: 0.6672, val_precision: 0.6672 val_recall: 0.6672, val_f1: 0.6672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:45\n",
            "loss: 0.6755, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:19:46\n",
            "loss: 0.6590, acc: 0.7215\n",
            "E2E-ABSA >>> 2022-08-17 16:19:47\n",
            "loss: 0.6506, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-08-17 16:19:48\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:49\n",
            "loss: 0.6913, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 16:19:50\n",
            "loss: 0.6559, acc: 0.7315\n",
            "E2E-ABSA >>> 2022-08-17 16:19:52\n",
            "loss: 0.6527, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-08-17 16:19:53\n",
            "loss: 0.6524, acc: 0.7287\n",
            "E2E-ABSA >>> 2022-08-17 16:19:53\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:54\n",
            "loss: 0.6371, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:19:56\n",
            "loss: 0.6570, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-08-17 16:19:57\n",
            "loss: 0.6558, acc: 0.7270\n",
            "E2E-ABSA >>> 2022-08-17 16:19:58\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:19:58\n",
            "loss: 0.6820, acc: 0.7121\n",
            "E2E-ABSA >>> 2022-08-17 16:19:59\n",
            "loss: 0.6466, acc: 0.7280\n",
            "E2E-ABSA >>> 2022-08-17 16:20:01\n",
            "loss: 0.6572, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 16:20:02\n",
            "loss: 0.6514, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-08-17 16:20:02\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:03\n",
            "loss: 0.6367, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-08-17 16:20:05\n",
            "loss: 0.6501, acc: 0.7294\n",
            "E2E-ABSA >>> 2022-08-17 16:20:06\n",
            "loss: 0.6560, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 16:20:07\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:07\n",
            "loss: 0.6988, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 16:20:08\n",
            "loss: 0.6491, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 16:20:10\n",
            "loss: 0.6519, acc: 0.7215\n",
            "E2E-ABSA >>> 2022-08-17 16:20:11\n",
            "loss: 0.6494, acc: 0.7265\n",
            "E2E-ABSA >>> 2022-08-17 16:20:11\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:12\n",
            "loss: 0.6731, acc: 0.7092\n",
            "E2E-ABSA >>> 2022-08-17 16:20:13\n",
            "loss: 0.6521, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 16:20:15\n",
            "loss: 0.6439, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:20:16\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:16\n",
            "loss: 0.6107, acc: 0.7469\n",
            "E2E-ABSA >>> 2022-08-17 16:20:17\n",
            "loss: 0.6467, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 16:20:19\n",
            "loss: 0.6490, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 16:20:20\n",
            "loss: 0.6492, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 16:20:20\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:21\n",
            "loss: 0.6380, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 16:20:22\n",
            "loss: 0.6458, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 16:20:24\n",
            "loss: 0.6491, acc: 0.7276\n",
            "E2E-ABSA >>> 2022-08-17 16:20:25\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:25\n",
            "loss: 0.5744, acc: 0.7852\n",
            "E2E-ABSA >>> 2022-08-17 16:20:26\n",
            "loss: 0.6394, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 16:20:27\n",
            "loss: 0.6452, acc: 0.7286\n",
            "E2E-ABSA >>> 2022-08-17 16:20:29\n",
            "loss: 0.6512, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-08-17 16:20:29\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:30\n",
            "loss: 0.6564, acc: 0.7236\n",
            "E2E-ABSA >>> 2022-08-17 16:20:31\n",
            "loss: 0.6521, acc: 0.7294\n",
            "E2E-ABSA >>> 2022-08-17 16:20:32\n",
            "loss: 0.6544, acc: 0.7273\n",
            "E2E-ABSA >>> 2022-08-17 16:20:34\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:34\n",
            "loss: 0.6298, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:20:35\n",
            "loss: 0.6616, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 16:20:36\n",
            "loss: 0.6516, acc: 0.7258\n",
            "E2E-ABSA >>> 2022-08-17 16:20:37\n",
            "loss: 0.6498, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 16:20:38\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:39\n",
            "loss: 0.6144, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 16:20:40\n",
            "loss: 0.6349, acc: 0.7387\n",
            "E2E-ABSA >>> 2022-08-17 16:20:41\n",
            "loss: 0.6539, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 16:20:43\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:43\n",
            "loss: 0.7049, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 16:20:44\n",
            "loss: 0.6403, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-08-17 16:20:45\n",
            "loss: 0.6395, acc: 0.7398\n",
            "E2E-ABSA >>> 2022-08-17 16:20:46\n",
            "loss: 0.6475, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:20:47\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:48\n",
            "loss: 0.6653, acc: 0.7143\n",
            "E2E-ABSA >>> 2022-08-17 16:20:49\n",
            "loss: 0.6481, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-08-17 16:20:50\n",
            "loss: 0.6483, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 16:20:52\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:52\n",
            "loss: 0.5934, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:20:53\n",
            "loss: 0.6504, acc: 0.7212\n",
            "E2E-ABSA >>> 2022-08-17 16:20:54\n",
            "loss: 0.6417, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-08-17 16:20:55\n",
            "loss: 0.6451, acc: 0.7296\n",
            "E2E-ABSA >>> 2022-08-17 16:20:56\n",
            ">>> val_acc: 0.6560, val_precision: 0.6560 val_recall: 0.6560, val_f1: 0.6560\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:20:57\n",
            "loss: 0.6483, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 16:20:58\n",
            "loss: 0.6381, acc: 0.7331\n",
            "E2E-ABSA >>> 2022-08-17 16:20:59\n",
            "loss: 0.6428, acc: 0.7304\n",
            "E2E-ABSA >>> 2022-08-17 16:21:00\n",
            "loss: 0.6463, acc: 0.7295\n",
            "E2E-ABSA >>> 2022-08-17 16:21:01\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            "you can download the best model from state_dict/ian_acl14shortdata_val_f1_0.6944\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.6944, test_precision: 0.6944, test_recall: 0.6944, test_f1: 0.6944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **acl14shortdata** dataset on model(**IAN**)\n"
      ],
      "metadata": {
        "id": "SeyAFJHIXjEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name ian --dataset acl14shortdata_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xMbYMy8XjK-",
        "outputId": "044b3e7d-c77d-4b79-8a7a-4fd8c2d19db7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 25457664\n",
            "> n_trainable_params: 2168403, n_nontrainable_params: 4115100\n",
            "> training arguments:\n",
            ">>> model_name: ian\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f2329e3eb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.ian.IAN'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:21:41\n",
            "loss: 1.0663, acc: 0.4319\n",
            "E2E-ABSA >>> 2022-08-17 16:21:43\n",
            "loss: 1.0493, acc: 0.4669\n",
            "E2E-ABSA >>> 2022-08-17 16:21:44\n",
            "loss: 1.0345, acc: 0.4829\n",
            "E2E-ABSA >>> 2022-08-17 16:21:45\n",
            ">>> val_acc: 0.4640, val_precision: 0.4640 val_recall: 0.4640, val_f1: 0.4640\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.464\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:21:46\n",
            "loss: 1.0029, acc: 0.5130\n",
            "E2E-ABSA >>> 2022-08-17 16:21:47\n",
            "loss: 0.9974, acc: 0.5165\n",
            "E2E-ABSA >>> 2022-08-17 16:21:48\n",
            "loss: 0.9992, acc: 0.5131\n",
            "E2E-ABSA >>> 2022-08-17 16:21:50\n",
            "loss: 0.9953, acc: 0.5167\n",
            "E2E-ABSA >>> 2022-08-17 16:21:50\n",
            ">>> val_acc: 0.4832, val_precision: 0.4832 val_recall: 0.4832, val_f1: 0.4832\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.4832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:21:51\n",
            "loss: 0.9939, acc: 0.5150\n",
            "E2E-ABSA >>> 2022-08-17 16:21:53\n",
            "loss: 0.9860, acc: 0.5236\n",
            "E2E-ABSA >>> 2022-08-17 16:21:54\n",
            "loss: 0.9795, acc: 0.5260\n",
            "E2E-ABSA >>> 2022-08-17 16:21:55\n",
            ">>> val_acc: 0.4992, val_precision: 0.4992 val_recall: 0.4992, val_f1: 0.4992\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.4992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:21:56\n",
            "loss: 0.9625, acc: 0.5412\n",
            "E2E-ABSA >>> 2022-08-17 16:21:57\n",
            "loss: 0.9640, acc: 0.5386\n",
            "E2E-ABSA >>> 2022-08-17 16:21:59\n",
            "loss: 0.9587, acc: 0.5428\n",
            "E2E-ABSA >>> 2022-08-17 16:22:00\n",
            "loss: 0.9643, acc: 0.5383\n",
            "E2E-ABSA >>> 2022-08-17 16:22:00\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:02\n",
            "loss: 0.9720, acc: 0.5455\n",
            "E2E-ABSA >>> 2022-08-17 16:22:03\n",
            "loss: 0.9596, acc: 0.5505\n",
            "E2E-ABSA >>> 2022-08-17 16:22:04\n",
            "loss: 0.9593, acc: 0.5462\n",
            "E2E-ABSA >>> 2022-08-17 16:22:05\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:06\n",
            "loss: 0.9638, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-08-17 16:22:07\n",
            "loss: 0.9535, acc: 0.5536\n",
            "E2E-ABSA >>> 2022-08-17 16:22:09\n",
            "loss: 0.9553, acc: 0.5565\n",
            "E2E-ABSA >>> 2022-08-17 16:22:10\n",
            "loss: 0.9520, acc: 0.5557\n",
            "E2E-ABSA >>> 2022-08-17 16:22:10\n",
            ">>> val_acc: 0.5184, val_precision: 0.5184 val_recall: 0.5184, val_f1: 0.5184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:12\n",
            "loss: 0.9305, acc: 0.5817\n",
            "E2E-ABSA >>> 2022-08-17 16:22:13\n",
            "loss: 0.9433, acc: 0.5701\n",
            "E2E-ABSA >>> 2022-08-17 16:22:14\n",
            "loss: 0.9415, acc: 0.5651\n",
            "E2E-ABSA >>> 2022-08-17 16:22:16\n",
            ">>> val_acc: 0.5184, val_precision: 0.5184 val_recall: 0.5184, val_f1: 0.5184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:16\n",
            "loss: 0.9359, acc: 0.5694\n",
            "E2E-ABSA >>> 2022-08-17 16:22:18\n",
            "loss: 0.9511, acc: 0.5671\n",
            "E2E-ABSA >>> 2022-08-17 16:22:19\n",
            "loss: 0.9495, acc: 0.5620\n",
            "E2E-ABSA >>> 2022-08-17 16:22:20\n",
            "loss: 0.9459, acc: 0.5651\n",
            "E2E-ABSA >>> 2022-08-17 16:22:21\n",
            ">>> val_acc: 0.5248, val_precision: 0.5248 val_recall: 0.5248, val_f1: 0.5248\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:22\n",
            "loss: 0.9296, acc: 0.5699\n",
            "E2E-ABSA >>> 2022-08-17 16:22:23\n",
            "loss: 0.9329, acc: 0.5747\n",
            "E2E-ABSA >>> 2022-08-17 16:22:25\n",
            "loss: 0.9344, acc: 0.5717\n",
            "E2E-ABSA >>> 2022-08-17 16:22:26\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:26\n",
            "loss: 0.8771, acc: 0.6367\n",
            "E2E-ABSA >>> 2022-08-17 16:22:28\n",
            "loss: 0.9248, acc: 0.5800\n",
            "E2E-ABSA >>> 2022-08-17 16:22:29\n",
            "loss: 0.9308, acc: 0.5752\n",
            "E2E-ABSA >>> 2022-08-17 16:22:31\n",
            "loss: 0.9372, acc: 0.5742\n",
            "E2E-ABSA >>> 2022-08-17 16:22:31\n",
            ">>> val_acc: 0.5312, val_precision: 0.5312 val_recall: 0.5312, val_f1: 0.5312\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.5312\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:32\n",
            "loss: 0.9384, acc: 0.5633\n",
            "E2E-ABSA >>> 2022-08-17 16:22:34\n",
            "loss: 0.9391, acc: 0.5705\n",
            "E2E-ABSA >>> 2022-08-17 16:22:35\n",
            "loss: 0.9349, acc: 0.5757\n",
            "E2E-ABSA >>> 2022-08-17 16:22:36\n",
            ">>> val_acc: 0.5296, val_precision: 0.5296 val_recall: 0.5296, val_f1: 0.5296\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:37\n",
            "loss: 0.9374, acc: 0.5670\n",
            "E2E-ABSA >>> 2022-08-17 16:22:38\n",
            "loss: 0.9445, acc: 0.5654\n",
            "E2E-ABSA >>> 2022-08-17 16:22:39\n",
            "loss: 0.9358, acc: 0.5735\n",
            "E2E-ABSA >>> 2022-08-17 16:22:41\n",
            "loss: 0.9378, acc: 0.5703\n",
            "E2E-ABSA >>> 2022-08-17 16:22:41\n",
            ">>> val_acc: 0.5312, val_precision: 0.5312 val_recall: 0.5312, val_f1: 0.5312\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:42\n",
            "loss: 0.9114, acc: 0.5789\n",
            "E2E-ABSA >>> 2022-08-17 16:22:44\n",
            "loss: 0.9193, acc: 0.5835\n",
            "E2E-ABSA >>> 2022-08-17 16:22:45\n",
            "loss: 0.9289, acc: 0.5786\n",
            "E2E-ABSA >>> 2022-08-17 16:22:46\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.536\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:47\n",
            "loss: 0.9111, acc: 0.6042\n",
            "E2E-ABSA >>> 2022-08-17 16:22:48\n",
            "loss: 0.9447, acc: 0.5670\n",
            "E2E-ABSA >>> 2022-08-17 16:22:49\n",
            "loss: 0.9348, acc: 0.5717\n",
            "E2E-ABSA >>> 2022-08-17 16:22:51\n",
            "loss: 0.9367, acc: 0.5706\n",
            "E2E-ABSA >>> 2022-08-17 16:22:51\n",
            ">>> val_acc: 0.5328, val_precision: 0.5328 val_recall: 0.5328, val_f1: 0.5328\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:52\n",
            "loss: 0.9201, acc: 0.5955\n",
            "E2E-ABSA >>> 2022-08-17 16:22:54\n",
            "loss: 0.9271, acc: 0.5850\n",
            "E2E-ABSA >>> 2022-08-17 16:22:55\n",
            "loss: 0.9323, acc: 0.5777\n",
            "E2E-ABSA >>> 2022-08-17 16:22:57\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:22:57\n",
            "loss: 0.9402, acc: 0.5875\n",
            "E2E-ABSA >>> 2022-08-17 16:22:58\n",
            "loss: 0.9274, acc: 0.5786\n",
            "E2E-ABSA >>> 2022-08-17 16:23:00\n",
            "loss: 0.9273, acc: 0.5750\n",
            "E2E-ABSA >>> 2022-08-17 16:23:01\n",
            "loss: 0.9290, acc: 0.5773\n",
            "E2E-ABSA >>> 2022-08-17 16:23:02\n",
            ">>> val_acc: 0.5296, val_precision: 0.5296 val_recall: 0.5296, val_f1: 0.5296\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:03\n",
            "loss: 0.9337, acc: 0.5763\n",
            "E2E-ABSA >>> 2022-08-17 16:23:04\n",
            "loss: 0.9294, acc: 0.5800\n",
            "E2E-ABSA >>> 2022-08-17 16:23:05\n",
            "loss: 0.9296, acc: 0.5805\n",
            "E2E-ABSA >>> 2022-08-17 16:23:07\n",
            ">>> val_acc: 0.5376, val_precision: 0.5376 val_recall: 0.5376, val_f1: 0.5376\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.5376\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:07\n",
            "loss: 0.8965, acc: 0.5898\n",
            "E2E-ABSA >>> 2022-08-17 16:23:08\n",
            "loss: 0.9273, acc: 0.5727\n",
            "E2E-ABSA >>> 2022-08-17 16:23:10\n",
            "loss: 0.9329, acc: 0.5700\n",
            "E2E-ABSA >>> 2022-08-17 16:23:11\n",
            "loss: 0.9312, acc: 0.5746\n",
            "E2E-ABSA >>> 2022-08-17 16:23:12\n",
            ">>> val_acc: 0.5376, val_precision: 0.5376 val_recall: 0.5376, val_f1: 0.5376\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:13\n",
            "loss: 0.9533, acc: 0.5615\n",
            "E2E-ABSA >>> 2022-08-17 16:23:14\n",
            "loss: 0.9376, acc: 0.5720\n",
            "E2E-ABSA >>> 2022-08-17 16:23:16\n",
            "loss: 0.9321, acc: 0.5729\n",
            "E2E-ABSA >>> 2022-08-17 16:23:17\n",
            ">>> val_acc: 0.5328, val_precision: 0.5328 val_recall: 0.5328, val_f1: 0.5328\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:17\n",
            "loss: 0.9138, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 16:23:18\n",
            "loss: 0.9384, acc: 0.5664\n",
            "E2E-ABSA >>> 2022-08-17 16:23:20\n",
            "loss: 0.9378, acc: 0.5690\n",
            "E2E-ABSA >>> 2022-08-17 16:23:21\n",
            "loss: 0.9293, acc: 0.5751\n",
            "E2E-ABSA >>> 2022-08-17 16:23:22\n",
            ">>> val_acc: 0.5328, val_precision: 0.5328 val_recall: 0.5328, val_f1: 0.5328\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:23\n",
            "loss: 0.9474, acc: 0.5490\n",
            "E2E-ABSA >>> 2022-08-17 16:23:24\n",
            "loss: 0.9278, acc: 0.5719\n",
            "E2E-ABSA >>> 2022-08-17 16:23:26\n",
            "loss: 0.9296, acc: 0.5709\n",
            "E2E-ABSA >>> 2022-08-17 16:23:27\n",
            ">>> val_acc: 0.5328, val_precision: 0.5328 val_recall: 0.5328, val_f1: 0.5328\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:27\n",
            "loss: 0.9011, acc: 0.6172\n",
            "E2E-ABSA >>> 2022-08-17 16:23:28\n",
            "loss: 0.9285, acc: 0.5747\n",
            "E2E-ABSA >>> 2022-08-17 16:23:30\n",
            "loss: 0.9275, acc: 0.5763\n",
            "E2E-ABSA >>> 2022-08-17 16:23:31\n",
            "loss: 0.9288, acc: 0.5755\n",
            "E2E-ABSA >>> 2022-08-17 16:23:32\n",
            ">>> val_acc: 0.5376, val_precision: 0.5376 val_recall: 0.5376, val_f1: 0.5376\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:33\n",
            "loss: 0.9357, acc: 0.5681\n",
            "E2E-ABSA >>> 2022-08-17 16:23:34\n",
            "loss: 0.9474, acc: 0.5589\n",
            "E2E-ABSA >>> 2022-08-17 16:23:36\n",
            "loss: 0.9385, acc: 0.5684\n",
            "E2E-ABSA >>> 2022-08-17 16:23:37\n",
            ">>> val_acc: 0.5376, val_precision: 0.5376 val_recall: 0.5376, val_f1: 0.5376\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:37\n",
            "loss: 0.9306, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 16:23:39\n",
            "loss: 0.9325, acc: 0.5739\n",
            "E2E-ABSA >>> 2022-08-17 16:23:40\n",
            "loss: 0.9255, acc: 0.5806\n",
            "E2E-ABSA >>> 2022-08-17 16:23:41\n",
            "loss: 0.9239, acc: 0.5798\n",
            "E2E-ABSA >>> 2022-08-17 16:23:42\n",
            ">>> val_acc: 0.5376, val_precision: 0.5376 val_recall: 0.5376, val_f1: 0.5376\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:43\n",
            "loss: 0.9142, acc: 0.5817\n",
            "E2E-ABSA >>> 2022-08-17 16:23:44\n",
            "loss: 0.9209, acc: 0.5794\n",
            "E2E-ABSA >>> 2022-08-17 16:23:46\n",
            "loss: 0.9186, acc: 0.5828\n",
            "E2E-ABSA >>> 2022-08-17 16:23:47\n",
            "loss: 0.9268, acc: 0.5762\n",
            "E2E-ABSA >>> 2022-08-17 16:23:47\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:49\n",
            "loss: 0.9193, acc: 0.5844\n",
            "E2E-ABSA >>> 2022-08-17 16:23:50\n",
            "loss: 0.9230, acc: 0.5763\n",
            "E2E-ABSA >>> 2022-08-17 16:23:51\n",
            "loss: 0.9258, acc: 0.5735\n",
            "E2E-ABSA >>> 2022-08-17 16:23:52\n",
            ">>> val_acc: 0.5392, val_precision: 0.5392 val_recall: 0.5392, val_f1: 0.5392\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.5392\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:53\n",
            "loss: 0.9420, acc: 0.5755\n",
            "E2E-ABSA >>> 2022-08-17 16:23:54\n",
            "loss: 0.9401, acc: 0.5633\n",
            "E2E-ABSA >>> 2022-08-17 16:23:56\n",
            "loss: 0.9307, acc: 0.5711\n",
            "E2E-ABSA >>> 2022-08-17 16:23:57\n",
            "loss: 0.9262, acc: 0.5749\n",
            "E2E-ABSA >>> 2022-08-17 16:23:57\n",
            ">>> val_acc: 0.5344, val_precision: 0.5344 val_recall: 0.5344, val_f1: 0.5344\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:23:59\n",
            "loss: 0.9201, acc: 0.5736\n",
            "E2E-ABSA >>> 2022-08-17 16:24:00\n",
            "loss: 0.9212, acc: 0.5788\n",
            "E2E-ABSA >>> 2022-08-17 16:24:02\n",
            "loss: 0.9279, acc: 0.5777\n",
            "E2E-ABSA >>> 2022-08-17 16:24:03\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:03\n",
            "loss: 0.9525, acc: 0.5611\n",
            "E2E-ABSA >>> 2022-08-17 16:24:05\n",
            "loss: 0.9382, acc: 0.5673\n",
            "E2E-ABSA >>> 2022-08-17 16:24:06\n",
            "loss: 0.9322, acc: 0.5717\n",
            "E2E-ABSA >>> 2022-08-17 16:24:07\n",
            "loss: 0.9246, acc: 0.5765\n",
            "E2E-ABSA >>> 2022-08-17 16:24:08\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:09\n",
            "loss: 0.9277, acc: 0.5761\n",
            "E2E-ABSA >>> 2022-08-17 16:24:10\n",
            "loss: 0.9318, acc: 0.5710\n",
            "E2E-ABSA >>> 2022-08-17 16:24:12\n",
            "loss: 0.9293, acc: 0.5715\n",
            "E2E-ABSA >>> 2022-08-17 16:24:13\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:13\n",
            "loss: 0.9603, acc: 0.5484\n",
            "E2E-ABSA >>> 2022-08-17 16:24:15\n",
            "loss: 0.9321, acc: 0.5723\n",
            "E2E-ABSA >>> 2022-08-17 16:24:16\n",
            "loss: 0.9285, acc: 0.5734\n",
            "E2E-ABSA >>> 2022-08-17 16:24:17\n",
            "loss: 0.9254, acc: 0.5754\n",
            "E2E-ABSA >>> 2022-08-17 16:24:18\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:19\n",
            "loss: 0.9248, acc: 0.5760\n",
            "E2E-ABSA >>> 2022-08-17 16:24:20\n",
            "loss: 0.9236, acc: 0.5795\n",
            "E2E-ABSA >>> 2022-08-17 16:24:22\n",
            "loss: 0.9265, acc: 0.5775\n",
            "E2E-ABSA >>> 2022-08-17 16:24:23\n",
            ">>> val_acc: 0.5408, val_precision: 0.5408 val_recall: 0.5408, val_f1: 0.5408\n",
            ">> saved: state_dict/ian_acl14shortdata_know_val_f1_0.5408\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:23\n",
            "loss: 0.9304, acc: 0.5920\n",
            "E2E-ABSA >>> 2022-08-17 16:24:25\n",
            "loss: 0.9238, acc: 0.5786\n",
            "E2E-ABSA >>> 2022-08-17 16:24:26\n",
            "loss: 0.9275, acc: 0.5744\n",
            "E2E-ABSA >>> 2022-08-17 16:24:27\n",
            "loss: 0.9236, acc: 0.5772\n",
            "E2E-ABSA >>> 2022-08-17 16:24:28\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:29\n",
            "loss: 0.9213, acc: 0.5811\n",
            "E2E-ABSA >>> 2022-08-17 16:24:30\n",
            "loss: 0.9252, acc: 0.5751\n",
            "E2E-ABSA >>> 2022-08-17 16:24:32\n",
            "loss: 0.9280, acc: 0.5706\n",
            "E2E-ABSA >>> 2022-08-17 16:24:33\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:33\n",
            "loss: 0.9066, acc: 0.5879\n",
            "E2E-ABSA >>> 2022-08-17 16:24:35\n",
            "loss: 0.9265, acc: 0.5724\n",
            "E2E-ABSA >>> 2022-08-17 16:24:36\n",
            "loss: 0.9281, acc: 0.5730\n",
            "E2E-ABSA >>> 2022-08-17 16:24:37\n",
            "loss: 0.9245, acc: 0.5766\n",
            "E2E-ABSA >>> 2022-08-17 16:24:38\n",
            ">>> val_acc: 0.5408, val_precision: 0.5408 val_recall: 0.5408, val_f1: 0.5408\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:39\n",
            "loss: 0.9142, acc: 0.5844\n",
            "E2E-ABSA >>> 2022-08-17 16:24:41\n",
            "loss: 0.9317, acc: 0.5687\n",
            "E2E-ABSA >>> 2022-08-17 16:24:42\n",
            "loss: 0.9271, acc: 0.5748\n",
            "E2E-ABSA >>> 2022-08-17 16:24:43\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:44\n",
            "loss: 0.9537, acc: 0.5513\n",
            "E2E-ABSA >>> 2022-08-17 16:24:45\n",
            "loss: 0.9254, acc: 0.5703\n",
            "E2E-ABSA >>> 2022-08-17 16:24:46\n",
            "loss: 0.9190, acc: 0.5787\n",
            "E2E-ABSA >>> 2022-08-17 16:24:48\n",
            "loss: 0.9239, acc: 0.5766\n",
            "E2E-ABSA >>> 2022-08-17 16:24:48\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:49\n",
            "loss: 0.9154, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 16:24:51\n",
            "loss: 0.9206, acc: 0.5874\n",
            "E2E-ABSA >>> 2022-08-17 16:24:52\n",
            "loss: 0.9267, acc: 0.5779\n",
            "E2E-ABSA >>> 2022-08-17 16:24:53\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:54\n",
            "loss: 0.9312, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 16:24:55\n",
            "loss: 0.9138, acc: 0.5847\n",
            "E2E-ABSA >>> 2022-08-17 16:24:56\n",
            "loss: 0.9212, acc: 0.5806\n",
            "E2E-ABSA >>> 2022-08-17 16:24:58\n",
            "loss: 0.9244, acc: 0.5760\n",
            "E2E-ABSA >>> 2022-08-17 16:24:58\n",
            ">>> val_acc: 0.5408, val_precision: 0.5408 val_recall: 0.5408, val_f1: 0.5408\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:24:59\n",
            "loss: 0.9307, acc: 0.5686\n",
            "E2E-ABSA >>> 2022-08-17 16:25:01\n",
            "loss: 0.9220, acc: 0.5778\n",
            "E2E-ABSA >>> 2022-08-17 16:25:02\n",
            "loss: 0.9256, acc: 0.5749\n",
            "E2E-ABSA >>> 2022-08-17 16:25:03\n",
            ">>> val_acc: 0.5344, val_precision: 0.5344 val_recall: 0.5344, val_f1: 0.5344\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:04\n",
            "loss: 0.9517, acc: 0.5531\n",
            "E2E-ABSA >>> 2022-08-17 16:25:05\n",
            "loss: 0.9177, acc: 0.5760\n",
            "E2E-ABSA >>> 2022-08-17 16:25:06\n",
            "loss: 0.9238, acc: 0.5747\n",
            "E2E-ABSA >>> 2022-08-17 16:25:08\n",
            "loss: 0.9226, acc: 0.5777\n",
            "E2E-ABSA >>> 2022-08-17 16:25:09\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:09\n",
            "loss: 0.9326, acc: 0.5790\n",
            "E2E-ABSA >>> 2022-08-17 16:25:11\n",
            "loss: 0.9242, acc: 0.5789\n",
            "E2E-ABSA >>> 2022-08-17 16:25:12\n",
            "loss: 0.9246, acc: 0.5770\n",
            "E2E-ABSA >>> 2022-08-17 16:25:14\n",
            ">>> val_acc: 0.5344, val_precision: 0.5344 val_recall: 0.5344, val_f1: 0.5344\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:14\n",
            "loss: 0.9255, acc: 0.5664\n",
            "E2E-ABSA >>> 2022-08-17 16:25:15\n",
            "loss: 0.9121, acc: 0.5927\n",
            "E2E-ABSA >>> 2022-08-17 16:25:17\n",
            "loss: 0.9168, acc: 0.5874\n",
            "E2E-ABSA >>> 2022-08-17 16:25:18\n",
            "loss: 0.9228, acc: 0.5777\n",
            "E2E-ABSA >>> 2022-08-17 16:25:19\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:20\n",
            "loss: 0.9057, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 16:25:21\n",
            "loss: 0.9150, acc: 0.5770\n",
            "E2E-ABSA >>> 2022-08-17 16:25:22\n",
            "loss: 0.9213, acc: 0.5779\n",
            "E2E-ABSA >>> 2022-08-17 16:25:24\n",
            ">>> val_acc: 0.5408, val_precision: 0.5408 val_recall: 0.5408, val_f1: 0.5408\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:24\n",
            "loss: 0.9865, acc: 0.5365\n",
            "E2E-ABSA >>> 2022-08-17 16:25:25\n",
            "loss: 0.9142, acc: 0.5859\n",
            "E2E-ABSA >>> 2022-08-17 16:25:27\n",
            "loss: 0.9184, acc: 0.5814\n",
            "E2E-ABSA >>> 2022-08-17 16:25:28\n",
            "loss: 0.9198, acc: 0.5789\n",
            "E2E-ABSA >>> 2022-08-17 16:25:29\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:30\n",
            "loss: 0.9336, acc: 0.5719\n",
            "E2E-ABSA >>> 2022-08-17 16:25:31\n",
            "loss: 0.9238, acc: 0.5734\n",
            "E2E-ABSA >>> 2022-08-17 16:25:32\n",
            "loss: 0.9240, acc: 0.5769\n",
            "E2E-ABSA >>> 2022-08-17 16:25:34\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:34\n",
            "loss: 0.9188, acc: 0.5859\n",
            "E2E-ABSA >>> 2022-08-17 16:25:35\n",
            "loss: 0.9115, acc: 0.5891\n",
            "E2E-ABSA >>> 2022-08-17 16:25:37\n",
            "loss: 0.9157, acc: 0.5838\n",
            "E2E-ABSA >>> 2022-08-17 16:25:38\n",
            "loss: 0.9162, acc: 0.5826\n",
            "E2E-ABSA >>> 2022-08-17 16:25:39\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:40\n",
            "loss: 0.9352, acc: 0.5725\n",
            "E2E-ABSA >>> 2022-08-17 16:25:41\n",
            "loss: 0.9216, acc: 0.5733\n",
            "E2E-ABSA >>> 2022-08-17 16:25:42\n",
            "loss: 0.9202, acc: 0.5823\n",
            "E2E-ABSA >>> 2022-08-17 16:25:44\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:44\n",
            "loss: 1.0252, acc: 0.5000\n",
            "E2E-ABSA >>> 2022-08-17 16:25:45\n",
            "loss: 0.9163, acc: 0.5883\n",
            "E2E-ABSA >>> 2022-08-17 16:25:47\n",
            "loss: 0.9168, acc: 0.5846\n",
            "E2E-ABSA >>> 2022-08-17 16:25:48\n",
            "loss: 0.9226, acc: 0.5806\n",
            "E2E-ABSA >>> 2022-08-17 16:25:49\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:50\n",
            "loss: 0.9175, acc: 0.5913\n",
            "E2E-ABSA >>> 2022-08-17 16:25:51\n",
            "loss: 0.9338, acc: 0.5687\n",
            "E2E-ABSA >>> 2022-08-17 16:25:52\n",
            "loss: 0.9242, acc: 0.5756\n",
            "E2E-ABSA >>> 2022-08-17 16:25:54\n",
            "loss: 0.9229, acc: 0.5773\n",
            "E2E-ABSA >>> 2022-08-17 16:25:54\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:25:55\n",
            "loss: 0.9226, acc: 0.5813\n",
            "E2E-ABSA >>> 2022-08-17 16:25:57\n",
            "loss: 0.9250, acc: 0.5722\n",
            "E2E-ABSA >>> 2022-08-17 16:25:58\n",
            "loss: 0.9213, acc: 0.5765\n",
            "E2E-ABSA >>> 2022-08-17 16:25:59\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:00\n",
            "loss: 0.9400, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 16:26:01\n",
            "loss: 0.9416, acc: 0.5642\n",
            "E2E-ABSA >>> 2022-08-17 16:26:03\n",
            "loss: 0.9274, acc: 0.5733\n",
            "E2E-ABSA >>> 2022-08-17 16:26:04\n",
            "loss: 0.9228, acc: 0.5774\n",
            "E2E-ABSA >>> 2022-08-17 16:26:04\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:06\n",
            "loss: 0.9059, acc: 0.6029\n",
            "E2E-ABSA >>> 2022-08-17 16:26:07\n",
            "loss: 0.9162, acc: 0.5851\n",
            "E2E-ABSA >>> 2022-08-17 16:26:08\n",
            "loss: 0.9214, acc: 0.5771\n",
            "E2E-ABSA >>> 2022-08-17 16:26:09\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:10\n",
            "loss: 0.9361, acc: 0.5767\n",
            "E2E-ABSA >>> 2022-08-17 16:26:11\n",
            "loss: 0.9236, acc: 0.5794\n",
            "E2E-ABSA >>> 2022-08-17 16:26:13\n",
            "loss: 0.9179, acc: 0.5815\n",
            "E2E-ABSA >>> 2022-08-17 16:26:14\n",
            "loss: 0.9205, acc: 0.5787\n",
            "E2E-ABSA >>> 2022-08-17 16:26:14\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:16\n",
            "loss: 0.9355, acc: 0.5659\n",
            "E2E-ABSA >>> 2022-08-17 16:26:17\n",
            "loss: 0.9322, acc: 0.5693\n",
            "E2E-ABSA >>> 2022-08-17 16:26:18\n",
            "loss: 0.9224, acc: 0.5775\n",
            "E2E-ABSA >>> 2022-08-17 16:26:19\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:20\n",
            "loss: 0.8936, acc: 0.6031\n",
            "E2E-ABSA >>> 2022-08-17 16:26:21\n",
            "loss: 0.9192, acc: 0.5853\n",
            "E2E-ABSA >>> 2022-08-17 16:26:23\n",
            "loss: 0.9184, acc: 0.5794\n",
            "E2E-ABSA >>> 2022-08-17 16:26:24\n",
            "loss: 0.9211, acc: 0.5772\n",
            "E2E-ABSA >>> 2022-08-17 16:26:25\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:26\n",
            "loss: 0.9227, acc: 0.5788\n",
            "E2E-ABSA >>> 2022-08-17 16:26:27\n",
            "loss: 0.9198, acc: 0.5788\n",
            "E2E-ABSA >>> 2022-08-17 16:26:29\n",
            "loss: 0.9221, acc: 0.5777\n",
            "E2E-ABSA >>> 2022-08-17 16:26:30\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:30\n",
            "loss: 0.9365, acc: 0.5729\n",
            "E2E-ABSA >>> 2022-08-17 16:26:32\n",
            "loss: 0.9331, acc: 0.5680\n",
            "E2E-ABSA >>> 2022-08-17 16:26:33\n",
            "loss: 0.9207, acc: 0.5810\n",
            "E2E-ABSA >>> 2022-08-17 16:26:34\n",
            "loss: 0.9217, acc: 0.5783\n",
            "E2E-ABSA >>> 2022-08-17 16:26:35\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:36\n",
            "loss: 0.8893, acc: 0.6057\n",
            "E2E-ABSA >>> 2022-08-17 16:26:37\n",
            "loss: 0.9047, acc: 0.5904\n",
            "E2E-ABSA >>> 2022-08-17 16:26:39\n",
            "loss: 0.9170, acc: 0.5775\n",
            "E2E-ABSA >>> 2022-08-17 16:26:40\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:40\n",
            "loss: 0.9369, acc: 0.5625\n",
            "E2E-ABSA >>> 2022-08-17 16:26:42\n",
            "loss: 0.9207, acc: 0.5810\n",
            "E2E-ABSA >>> 2022-08-17 16:26:43\n",
            "loss: 0.9214, acc: 0.5779\n",
            "E2E-ABSA >>> 2022-08-17 16:26:45\n",
            "loss: 0.9238, acc: 0.5761\n",
            "E2E-ABSA >>> 2022-08-17 16:26:45\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:46\n",
            "loss: 0.9162, acc: 0.5758\n",
            "E2E-ABSA >>> 2022-08-17 16:26:48\n",
            "loss: 0.9145, acc: 0.5813\n",
            "E2E-ABSA >>> 2022-08-17 16:26:49\n",
            "loss: 0.9196, acc: 0.5783\n",
            "E2E-ABSA >>> 2022-08-17 16:26:50\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            "E2E-ABSA >>> 2022-08-17 16:26:51\n",
            "loss: 0.9596, acc: 0.5335\n",
            "E2E-ABSA >>> 2022-08-17 16:26:52\n",
            "loss: 0.9292, acc: 0.5674\n",
            "E2E-ABSA >>> 2022-08-17 16:26:53\n",
            "loss: 0.9292, acc: 0.5721\n",
            "E2E-ABSA >>> 2022-08-17 16:26:55\n",
            "loss: 0.9247, acc: 0.5774\n",
            "E2E-ABSA >>> 2022-08-17 16:26:55\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            "E2E-ABSA >>> 2022-08-17 16:26:55\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.5408, val_precision: 0.5408 val_recall: 0.5408, val_f1: 0.5408\n",
            "you can download the best model from state_dict/ian_acl14shortdata_know_val_f1_0.5408\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  aspect_len = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/content/DictionaryFused-E2E-ABSA/models/ian.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  text_raw_len = torch.tensor(text_raw_len, dtype=torch.float).to(self.opt.device)\n",
            ">>> test_acc: 0.5408, test_precision: 0.5408, test_recall: 0.5408, test_f1: 0.5408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **acl14shortdata** dataset on model(**MEMNET**)\n"
      ],
      "metadata": {
        "id": "iMGxJyvzXjP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset acl14shortdata --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfeFN38gXjVy",
        "outputId": "1eafe3c0-e73a-4f5c-a7ae-e4166126d0b1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 16769536\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 3828600\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f7f2b629b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:35\n",
            "loss: 1.0269, acc: 0.4981\n",
            "E2E-ABSA >>> 2022-08-17 16:27:36\n",
            "loss: 1.0059, acc: 0.5109\n",
            "E2E-ABSA >>> 2022-08-17 16:27:37\n",
            "loss: 0.9822, acc: 0.5290\n",
            "E2E-ABSA >>> 2022-08-17 16:27:37\n",
            ">>> val_acc: 0.5440, val_precision: 0.5440 val_recall: 0.5440, val_f1: 0.5440\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:38\n",
            "loss: 0.8630, acc: 0.6146\n",
            "E2E-ABSA >>> 2022-08-17 16:27:38\n",
            "loss: 0.8900, acc: 0.5933\n",
            "E2E-ABSA >>> 2022-08-17 16:27:39\n",
            "loss: 0.8852, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 16:27:40\n",
            "loss: 0.8820, acc: 0.5950\n",
            "E2E-ABSA >>> 2022-08-17 16:27:40\n",
            ">>> val_acc: 0.5824, val_precision: 0.5824 val_recall: 0.5824, val_f1: 0.5824\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.5824\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:41\n",
            "loss: 0.8173, acc: 0.6322\n",
            "E2E-ABSA >>> 2022-08-17 16:27:41\n",
            "loss: 0.8278, acc: 0.6237\n",
            "E2E-ABSA >>> 2022-08-17 16:27:42\n",
            "loss: 0.8298, acc: 0.6263\n",
            "E2E-ABSA >>> 2022-08-17 16:27:43\n",
            ">>> val_acc: 0.6048, val_precision: 0.6048 val_recall: 0.6048, val_f1: 0.6048\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6048\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:43\n",
            "loss: 0.7854, acc: 0.6520\n",
            "E2E-ABSA >>> 2022-08-17 16:27:44\n",
            "loss: 0.7976, acc: 0.6510\n",
            "E2E-ABSA >>> 2022-08-17 16:27:45\n",
            "loss: 0.7956, acc: 0.6473\n",
            "E2E-ABSA >>> 2022-08-17 16:27:45\n",
            "loss: 0.7871, acc: 0.6555\n",
            "E2E-ABSA >>> 2022-08-17 16:27:45\n",
            ">>> val_acc: 0.6032, val_precision: 0.6032 val_recall: 0.6032, val_f1: 0.6032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:46\n",
            "loss: 0.7763, acc: 0.6522\n",
            "E2E-ABSA >>> 2022-08-17 16:27:47\n",
            "loss: 0.7746, acc: 0.6559\n",
            "E2E-ABSA >>> 2022-08-17 16:27:48\n",
            "loss: 0.7646, acc: 0.6635\n",
            "E2E-ABSA >>> 2022-08-17 16:27:48\n",
            ">>> val_acc: 0.6384, val_precision: 0.6384 val_recall: 0.6384, val_f1: 0.6384\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6384\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:48\n",
            "loss: 0.7347, acc: 0.6953\n",
            "E2E-ABSA >>> 2022-08-17 16:27:49\n",
            "loss: 0.7477, acc: 0.6808\n",
            "E2E-ABSA >>> 2022-08-17 16:27:50\n",
            "loss: 0.7465, acc: 0.6766\n",
            "E2E-ABSA >>> 2022-08-17 16:27:51\n",
            "loss: 0.7448, acc: 0.6796\n",
            "E2E-ABSA >>> 2022-08-17 16:27:51\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.648\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:52\n",
            "loss: 0.7369, acc: 0.6761\n",
            "E2E-ABSA >>> 2022-08-17 16:27:52\n",
            "loss: 0.7331, acc: 0.6855\n",
            "E2E-ABSA >>> 2022-08-17 16:27:53\n",
            "loss: 0.7394, acc: 0.6808\n",
            "E2E-ABSA >>> 2022-08-17 16:27:54\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:54\n",
            "loss: 0.7056, acc: 0.6962\n",
            "E2E-ABSA >>> 2022-08-17 16:27:55\n",
            "loss: 0.7165, acc: 0.6962\n",
            "E2E-ABSA >>> 2022-08-17 16:27:55\n",
            "loss: 0.7171, acc: 0.6925\n",
            "E2E-ABSA >>> 2022-08-17 16:27:56\n",
            "loss: 0.7235, acc: 0.6869\n",
            "E2E-ABSA >>> 2022-08-17 16:27:56\n",
            ">>> val_acc: 0.6432, val_precision: 0.6432 val_recall: 0.6432, val_f1: 0.6432\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:57\n",
            "loss: 0.7022, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 16:27:58\n",
            "loss: 0.7206, acc: 0.6858\n",
            "E2E-ABSA >>> 2022-08-17 16:27:58\n",
            "loss: 0.7206, acc: 0.6842\n",
            "E2E-ABSA >>> 2022-08-17 16:27:59\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:27:59\n",
            "loss: 0.6916, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 16:28:00\n",
            "loss: 0.7145, acc: 0.6913\n",
            "E2E-ABSA >>> 2022-08-17 16:28:01\n",
            "loss: 0.7113, acc: 0.6929\n",
            "E2E-ABSA >>> 2022-08-17 16:28:01\n",
            "loss: 0.7082, acc: 0.6926\n",
            "E2E-ABSA >>> 2022-08-17 16:28:02\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:02\n",
            "loss: 0.7286, acc: 0.6734\n",
            "E2E-ABSA >>> 2022-08-17 16:28:03\n",
            "loss: 0.7194, acc: 0.6833\n",
            "E2E-ABSA >>> 2022-08-17 16:28:04\n",
            "loss: 0.7139, acc: 0.6900\n",
            "E2E-ABSA >>> 2022-08-17 16:28:04\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:05\n",
            "loss: 0.6694, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 16:28:05\n",
            "loss: 0.7108, acc: 0.6958\n",
            "E2E-ABSA >>> 2022-08-17 16:28:06\n",
            "loss: 0.7082, acc: 0.6930\n",
            "E2E-ABSA >>> 2022-08-17 16:28:07\n",
            "loss: 0.7017, acc: 0.6974\n",
            "E2E-ABSA >>> 2022-08-17 16:28:07\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:08\n",
            "loss: 0.7160, acc: 0.6965\n",
            "E2E-ABSA >>> 2022-08-17 16:28:08\n",
            "loss: 0.6982, acc: 0.7074\n",
            "E2E-ABSA >>> 2022-08-17 16:28:09\n",
            "loss: 0.6956, acc: 0.7052\n",
            "E2E-ABSA >>> 2022-08-17 16:28:10\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:10\n",
            "loss: 0.7295, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 16:28:11\n",
            "loss: 0.7044, acc: 0.6951\n",
            "E2E-ABSA >>> 2022-08-17 16:28:11\n",
            "loss: 0.6935, acc: 0.6978\n",
            "E2E-ABSA >>> 2022-08-17 16:28:12\n",
            "loss: 0.6918, acc: 0.7035\n",
            "E2E-ABSA >>> 2022-08-17 16:28:12\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:13\n",
            "loss: 0.6840, acc: 0.6988\n",
            "E2E-ABSA >>> 2022-08-17 16:28:14\n",
            "loss: 0.6960, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:28:14\n",
            "loss: 0.6907, acc: 0.7004\n",
            "E2E-ABSA >>> 2022-08-17 16:28:15\n",
            ">>> val_acc: 0.6576, val_precision: 0.6576 val_recall: 0.6576, val_f1: 0.6576\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:15\n",
            "loss: 0.7033, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 16:28:16\n",
            "loss: 0.6765, acc: 0.7099\n",
            "E2E-ABSA >>> 2022-08-17 16:28:17\n",
            "loss: 0.6803, acc: 0.7068\n",
            "E2E-ABSA >>> 2022-08-17 16:28:17\n",
            "loss: 0.6888, acc: 0.7008\n",
            "E2E-ABSA >>> 2022-08-17 16:28:18\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:18\n",
            "loss: 0.6944, acc: 0.6967\n",
            "E2E-ABSA >>> 2022-08-17 16:28:19\n",
            "loss: 0.6826, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 16:28:20\n",
            "loss: 0.6813, acc: 0.7071\n",
            "E2E-ABSA >>> 2022-08-17 16:28:20\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:21\n",
            "loss: 0.5977, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 16:28:21\n",
            "loss: 0.6611, acc: 0.7171\n",
            "E2E-ABSA >>> 2022-08-17 16:28:22\n",
            "loss: 0.6728, acc: 0.7130\n",
            "E2E-ABSA >>> 2022-08-17 16:28:23\n",
            "loss: 0.6791, acc: 0.7053\n",
            "E2E-ABSA >>> 2022-08-17 16:28:23\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:24\n",
            "loss: 0.6841, acc: 0.6992\n",
            "E2E-ABSA >>> 2022-08-17 16:28:24\n",
            "loss: 0.6868, acc: 0.7016\n",
            "E2E-ABSA >>> 2022-08-17 16:28:25\n",
            "loss: 0.6844, acc: 0.7050\n",
            "E2E-ABSA >>> 2022-08-17 16:28:26\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:26\n",
            "loss: 0.7174, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-08-17 16:28:27\n",
            "loss: 0.6896, acc: 0.7003\n",
            "E2E-ABSA >>> 2022-08-17 16:28:27\n",
            "loss: 0.6796, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-08-17 16:28:28\n",
            "loss: 0.6750, acc: 0.7075\n",
            "E2E-ABSA >>> 2022-08-17 16:28:28\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:29\n",
            "loss: 0.6643, acc: 0.7333\n",
            "E2E-ABSA >>> 2022-08-17 16:28:30\n",
            "loss: 0.6544, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 16:28:30\n",
            "loss: 0.6683, acc: 0.7113\n",
            "E2E-ABSA >>> 2022-08-17 16:28:31\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:31\n",
            "loss: 0.5804, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 16:28:32\n",
            "loss: 0.6457, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-08-17 16:28:33\n",
            "loss: 0.6693, acc: 0.7097\n",
            "E2E-ABSA >>> 2022-08-17 16:28:33\n",
            "loss: 0.6749, acc: 0.7037\n",
            "E2E-ABSA >>> 2022-08-17 16:28:34\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:34\n",
            "loss: 0.6782, acc: 0.7065\n",
            "E2E-ABSA >>> 2022-08-17 16:28:35\n",
            "loss: 0.6817, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-08-17 16:28:36\n",
            "loss: 0.6725, acc: 0.7129\n",
            "E2E-ABSA >>> 2022-08-17 16:28:36\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:36\n",
            "loss: 0.5935, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:28:37\n",
            "loss: 0.6330, acc: 0.7314\n",
            "E2E-ABSA >>> 2022-08-17 16:28:38\n",
            "loss: 0.6550, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 16:28:39\n",
            "loss: 0.6623, acc: 0.7165\n",
            "E2E-ABSA >>> 2022-08-17 16:28:39\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:40\n",
            "loss: 0.6455, acc: 0.7272\n",
            "E2E-ABSA >>> 2022-08-17 16:28:40\n",
            "loss: 0.6687, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 16:28:41\n",
            "loss: 0.6704, acc: 0.7106\n",
            "E2E-ABSA >>> 2022-08-17 16:28:42\n",
            "loss: 0.6684, acc: 0.7119\n",
            "E2E-ABSA >>> 2022-08-17 16:28:42\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:43\n",
            "loss: 0.6544, acc: 0.7206\n",
            "E2E-ABSA >>> 2022-08-17 16:28:43\n",
            "loss: 0.6721, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 16:28:44\n",
            "loss: 0.6682, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 16:28:45\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:45\n",
            "loss: 0.6779, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 16:28:46\n",
            "loss: 0.6533, acc: 0.7192\n",
            "E2E-ABSA >>> 2022-08-17 16:28:46\n",
            "loss: 0.6685, acc: 0.7102\n",
            "E2E-ABSA >>> 2022-08-17 16:28:47\n",
            "loss: 0.6662, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-08-17 16:28:47\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:48\n",
            "loss: 0.6676, acc: 0.7103\n",
            "E2E-ABSA >>> 2022-08-17 16:28:49\n",
            "loss: 0.6607, acc: 0.7194\n",
            "E2E-ABSA >>> 2022-08-17 16:28:50\n",
            "loss: 0.6629, acc: 0.7177\n",
            "E2E-ABSA >>> 2022-08-17 16:28:50\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:50\n",
            "loss: 0.6151, acc: 0.7457\n",
            "E2E-ABSA >>> 2022-08-17 16:28:51\n",
            "loss: 0.6352, acc: 0.7235\n",
            "E2E-ABSA >>> 2022-08-17 16:28:52\n",
            "loss: 0.6573, acc: 0.7147\n",
            "E2E-ABSA >>> 2022-08-17 16:28:53\n",
            "loss: 0.6626, acc: 0.7151\n",
            "E2E-ABSA >>> 2022-08-17 16:28:53\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:53\n",
            "loss: 0.6337, acc: 0.7378\n",
            "E2E-ABSA >>> 2022-08-17 16:28:54\n",
            "loss: 0.6685, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-08-17 16:28:55\n",
            "loss: 0.6598, acc: 0.7217\n",
            "E2E-ABSA >>> 2022-08-17 16:28:55\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:56\n",
            "loss: 0.7045, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 16:28:56\n",
            "loss: 0.6692, acc: 0.7138\n",
            "E2E-ABSA >>> 2022-08-17 16:28:57\n",
            "loss: 0.6588, acc: 0.7174\n",
            "E2E-ABSA >>> 2022-08-17 16:28:58\n",
            "loss: 0.6631, acc: 0.7134\n",
            "E2E-ABSA >>> 2022-08-17 16:28:58\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:28:59\n",
            "loss: 0.6715, acc: 0.7060\n",
            "E2E-ABSA >>> 2022-08-17 16:29:00\n",
            "loss: 0.6544, acc: 0.7237\n",
            "E2E-ABSA >>> 2022-08-17 16:29:00\n",
            "loss: 0.6590, acc: 0.7174\n",
            "E2E-ABSA >>> 2022-08-17 16:29:01\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:01\n",
            "loss: 0.6056, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 16:29:02\n",
            "loss: 0.6639, acc: 0.7146\n",
            "E2E-ABSA >>> 2022-08-17 16:29:03\n",
            "loss: 0.6587, acc: 0.7174\n",
            "E2E-ABSA >>> 2022-08-17 16:29:03\n",
            "loss: 0.6564, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 16:29:04\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:04\n",
            "loss: 0.6718, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 16:29:05\n",
            "loss: 0.6669, acc: 0.7069\n",
            "E2E-ABSA >>> 2022-08-17 16:29:06\n",
            "loss: 0.6663, acc: 0.7077\n",
            "E2E-ABSA >>> 2022-08-17 16:29:06\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:06\n",
            "loss: 0.6496, acc: 0.7363\n",
            "E2E-ABSA >>> 2022-08-17 16:29:07\n",
            "loss: 0.6388, acc: 0.7315\n",
            "E2E-ABSA >>> 2022-08-17 16:29:08\n",
            "loss: 0.6495, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 16:29:09\n",
            "loss: 0.6558, acc: 0.7221\n",
            "E2E-ABSA >>> 2022-08-17 16:29:09\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:10\n",
            "loss: 0.6353, acc: 0.7359\n",
            "E2E-ABSA >>> 2022-08-17 16:29:10\n",
            "loss: 0.6526, acc: 0.7198\n",
            "E2E-ABSA >>> 2022-08-17 16:29:11\n",
            "loss: 0.6555, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 16:29:12\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:12\n",
            "loss: 0.6214, acc: 0.7411\n",
            "E2E-ABSA >>> 2022-08-17 16:29:13\n",
            "loss: 0.6574, acc: 0.7148\n",
            "E2E-ABSA >>> 2022-08-17 16:29:13\n",
            "loss: 0.6543, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 16:29:14\n",
            "loss: 0.6583, acc: 0.7157\n",
            "E2E-ABSA >>> 2022-08-17 16:29:14\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:15\n",
            "loss: 0.6488, acc: 0.7237\n",
            "E2E-ABSA >>> 2022-08-17 16:29:16\n",
            "loss: 0.6636, acc: 0.7074\n",
            "E2E-ABSA >>> 2022-08-17 16:29:16\n",
            "loss: 0.6560, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 16:29:17\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:17\n",
            "loss: 0.6995, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-08-17 16:29:18\n",
            "loss: 0.6360, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-08-17 16:29:19\n",
            "loss: 0.6533, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 16:29:19\n",
            "loss: 0.6556, acc: 0.7218\n",
            "E2E-ABSA >>> 2022-08-17 16:29:20\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:20\n",
            "loss: 0.6423, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 16:29:21\n",
            "loss: 0.6464, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 16:29:22\n",
            "loss: 0.6526, acc: 0.7213\n",
            "E2E-ABSA >>> 2022-08-17 16:29:22\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:23\n",
            "loss: 0.6803, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 16:29:23\n",
            "loss: 0.6452, acc: 0.7193\n",
            "E2E-ABSA >>> 2022-08-17 16:29:24\n",
            "loss: 0.6484, acc: 0.7196\n",
            "E2E-ABSA >>> 2022-08-17 16:29:25\n",
            "loss: 0.6578, acc: 0.7191\n",
            "E2E-ABSA >>> 2022-08-17 16:29:26\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:26\n",
            "loss: 0.6215, acc: 0.7353\n",
            "E2E-ABSA >>> 2022-08-17 16:29:27\n",
            "loss: 0.6469, acc: 0.7225\n",
            "E2E-ABSA >>> 2022-08-17 16:29:27\n",
            "loss: 0.6432, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-08-17 16:29:28\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:28\n",
            "loss: 0.6335, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 16:29:29\n",
            "loss: 0.6470, acc: 0.7252\n",
            "E2E-ABSA >>> 2022-08-17 16:29:30\n",
            "loss: 0.6547, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 16:29:30\n",
            "loss: 0.6528, acc: 0.7209\n",
            "E2E-ABSA >>> 2022-08-17 16:29:31\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:31\n",
            "loss: 0.6204, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-08-17 16:29:32\n",
            "loss: 0.6453, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 16:29:33\n",
            "loss: 0.6556, acc: 0.7228\n",
            "E2E-ABSA >>> 2022-08-17 16:29:34\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:34\n",
            "loss: 0.6264, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 16:29:34\n",
            "loss: 0.6442, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 16:29:35\n",
            "loss: 0.6530, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 16:29:36\n",
            "loss: 0.6476, acc: 0.7216\n",
            "E2E-ABSA >>> 2022-08-17 16:29:36\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:37\n",
            "loss: 0.6321, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 16:29:37\n",
            "loss: 0.6478, acc: 0.7223\n",
            "E2E-ABSA >>> 2022-08-17 16:29:38\n",
            "loss: 0.6424, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-08-17 16:29:39\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:39\n",
            "loss: 0.6673, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-08-17 16:29:40\n",
            "loss: 0.6468, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-08-17 16:29:40\n",
            "loss: 0.6427, acc: 0.7215\n",
            "E2E-ABSA >>> 2022-08-17 16:29:41\n",
            "loss: 0.6467, acc: 0.7234\n",
            "E2E-ABSA >>> 2022-08-17 16:29:42\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:42\n",
            "loss: 0.6804, acc: 0.6920\n",
            "E2E-ABSA >>> 2022-08-17 16:29:43\n",
            "loss: 0.6548, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 16:29:43\n",
            "loss: 0.6526, acc: 0.7200\n",
            "E2E-ABSA >>> 2022-08-17 16:29:44\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:44\n",
            "loss: 0.6901, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:29:45\n",
            "loss: 0.6407, acc: 0.7302\n",
            "E2E-ABSA >>> 2022-08-17 16:29:46\n",
            "loss: 0.6317, acc: 0.7310\n",
            "E2E-ABSA >>> 2022-08-17 16:29:46\n",
            "loss: 0.6468, acc: 0.7231\n",
            "E2E-ABSA >>> 2022-08-17 16:29:47\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:47\n",
            "loss: 0.6774, acc: 0.7043\n",
            "E2E-ABSA >>> 2022-08-17 16:29:48\n",
            "loss: 0.6668, acc: 0.7056\n",
            "E2E-ABSA >>> 2022-08-17 16:29:49\n",
            "loss: 0.6552, acc: 0.7190\n",
            "E2E-ABSA >>> 2022-08-17 16:29:49\n",
            "loss: 0.6447, acc: 0.7258\n",
            "E2E-ABSA >>> 2022-08-17 16:29:49\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:50\n",
            "loss: 0.6436, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 16:29:51\n",
            "loss: 0.6433, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 16:29:52\n",
            "loss: 0.6367, acc: 0.7265\n",
            "E2E-ABSA >>> 2022-08-17 16:29:52\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:53\n",
            "loss: 0.6756, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 16:29:53\n",
            "loss: 0.6407, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-08-17 16:29:54\n",
            "loss: 0.6420, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 16:29:55\n",
            "loss: 0.6428, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-08-17 16:29:55\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:56\n",
            "loss: 0.6162, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:29:56\n",
            "loss: 0.6400, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 16:29:57\n",
            "loss: 0.6448, acc: 0.7211\n",
            "E2E-ABSA >>> 2022-08-17 16:29:57\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:29:58\n",
            "loss: 0.6521, acc: 0.7259\n",
            "E2E-ABSA >>> 2022-08-17 16:29:59\n",
            "loss: 0.6484, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-08-17 16:29:59\n",
            "loss: 0.6426, acc: 0.7221\n",
            "E2E-ABSA >>> 2022-08-17 16:30:00\n",
            "loss: 0.6449, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-08-17 16:30:00\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:01\n",
            "loss: 0.6504, acc: 0.7215\n",
            "E2E-ABSA >>> 2022-08-17 16:30:02\n",
            "loss: 0.6474, acc: 0.7197\n",
            "E2E-ABSA >>> 2022-08-17 16:30:02\n",
            "loss: 0.6441, acc: 0.7224\n",
            "E2E-ABSA >>> 2022-08-17 16:30:03\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:03\n",
            "loss: 0.6831, acc: 0.7078\n",
            "E2E-ABSA >>> 2022-08-17 16:30:04\n",
            "loss: 0.6588, acc: 0.7192\n",
            "E2E-ABSA >>> 2022-08-17 16:30:05\n",
            "loss: 0.6486, acc: 0.7198\n",
            "E2E-ABSA >>> 2022-08-17 16:30:05\n",
            "loss: 0.6452, acc: 0.7246\n",
            "E2E-ABSA >>> 2022-08-17 16:30:06\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:06\n",
            "loss: 0.6409, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:30:07\n",
            "loss: 0.6419, acc: 0.7251\n",
            "E2E-ABSA >>> 2022-08-17 16:30:08\n",
            "loss: 0.6415, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-08-17 16:30:08\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:08\n",
            "loss: 0.6771, acc: 0.7049\n",
            "E2E-ABSA >>> 2022-08-17 16:30:09\n",
            "loss: 0.6530, acc: 0.7293\n",
            "E2E-ABSA >>> 2022-08-17 16:30:10\n",
            "loss: 0.6589, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:30:11\n",
            "loss: 0.6460, acc: 0.7234\n",
            "E2E-ABSA >>> 2022-08-17 16:30:11\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:11\n",
            "loss: 0.6620, acc: 0.7180\n",
            "E2E-ABSA >>> 2022-08-17 16:30:12\n",
            "loss: 0.6412, acc: 0.7303\n",
            "E2E-ABSA >>> 2022-08-17 16:30:13\n",
            "loss: 0.6357, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-08-17 16:30:14\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:14\n",
            "loss: 0.6102, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-08-17 16:30:15\n",
            "loss: 0.6441, acc: 0.7263\n",
            "E2E-ABSA >>> 2022-08-17 16:30:15\n",
            "loss: 0.6464, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-08-17 16:30:16\n",
            "loss: 0.6412, acc: 0.7255\n",
            "E2E-ABSA >>> 2022-08-17 16:30:16\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:17\n",
            "loss: 0.6339, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 16:30:18\n",
            "loss: 0.6476, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 16:30:18\n",
            "loss: 0.6496, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 16:30:19\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:19\n",
            "loss: 0.6605, acc: 0.7478\n",
            "E2E-ABSA >>> 2022-08-17 16:30:20\n",
            "loss: 0.6587, acc: 0.7144\n",
            "E2E-ABSA >>> 2022-08-17 16:30:21\n",
            "loss: 0.6444, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 16:30:21\n",
            "loss: 0.6374, acc: 0.7300\n",
            "E2E-ABSA >>> 2022-08-17 16:30:22\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:22\n",
            "loss: 0.6367, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-08-17 16:30:23\n",
            "loss: 0.6271, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 16:30:24\n",
            "loss: 0.6323, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:30:24\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:24\n",
            "loss: 0.6177, acc: 0.7526\n",
            "E2E-ABSA >>> 2022-08-17 16:30:25\n",
            "loss: 0.6328, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:30:26\n",
            "loss: 0.6336, acc: 0.7327\n",
            "E2E-ABSA >>> 2022-08-17 16:30:27\n",
            "loss: 0.6363, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 16:30:27\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:27\n",
            "loss: 0.6223, acc: 0.7309\n",
            "E2E-ABSA >>> 2022-08-17 16:30:28\n",
            "loss: 0.6199, acc: 0.7376\n",
            "E2E-ABSA >>> 2022-08-17 16:30:29\n",
            "loss: 0.6360, acc: 0.7291\n",
            "E2E-ABSA >>> 2022-08-17 16:30:30\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:30\n",
            "loss: 0.6851, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 16:30:30\n",
            "loss: 0.6466, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:30:31\n",
            "loss: 0.6439, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-08-17 16:30:32\n",
            "loss: 0.6402, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-08-17 16:30:32\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:33\n",
            "loss: 0.6374, acc: 0.7178\n",
            "E2E-ABSA >>> 2022-08-17 16:30:33\n",
            "loss: 0.6326, acc: 0.7236\n",
            "E2E-ABSA >>> 2022-08-17 16:30:34\n",
            "loss: 0.6273, acc: 0.7292\n",
            "E2E-ABSA >>> 2022-08-17 16:30:35\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:35\n",
            "loss: 0.6027, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:30:36\n",
            "loss: 0.6308, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:30:37\n",
            "loss: 0.6377, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 16:30:37\n",
            "loss: 0.6376, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 16:30:38\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:38\n",
            "loss: 0.6179, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:30:39\n",
            "loss: 0.6379, acc: 0.7279\n",
            "E2E-ABSA >>> 2022-08-17 16:30:40\n",
            "loss: 0.6407, acc: 0.7294\n",
            "E2E-ABSA >>> 2022-08-17 16:30:40\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:40\n",
            "loss: 0.6948, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-08-17 16:30:41\n",
            "loss: 0.6648, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-08-17 16:30:42\n",
            "loss: 0.6288, acc: 0.7409\n",
            "E2E-ABSA >>> 2022-08-17 16:30:43\n",
            "loss: 0.6339, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 16:30:43\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:43\n",
            "loss: 0.6265, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 16:30:44\n",
            "loss: 0.6349, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-08-17 16:30:45\n",
            "loss: 0.6394, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 16:30:46\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:46\n",
            "loss: 0.6317, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 16:30:46\n",
            "loss: 0.6236, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 16:30:47\n",
            "loss: 0.6384, acc: 0.7284\n",
            "E2E-ABSA >>> 2022-08-17 16:30:48\n",
            "loss: 0.6371, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 16:30:48\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:49\n",
            "loss: 0.6318, acc: 0.7366\n",
            "E2E-ABSA >>> 2022-08-17 16:30:50\n",
            "loss: 0.6448, acc: 0.7183\n",
            "E2E-ABSA >>> 2022-08-17 16:30:50\n",
            "loss: 0.6372, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 16:30:51\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:51\n",
            "loss: 0.6362, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:30:52\n",
            "loss: 0.6087, acc: 0.7440\n",
            "E2E-ABSA >>> 2022-08-17 16:30:53\n",
            "loss: 0.6148, acc: 0.7381\n",
            "E2E-ABSA >>> 2022-08-17 16:30:53\n",
            "loss: 0.6308, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-08-17 16:30:54\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:54\n",
            "loss: 0.6656, acc: 0.7139\n",
            "E2E-ABSA >>> 2022-08-17 16:30:55\n",
            "loss: 0.6408, acc: 0.7225\n",
            "E2E-ABSA >>> 2022-08-17 16:30:56\n",
            "loss: 0.6356, acc: 0.7287\n",
            "E2E-ABSA >>> 2022-08-17 16:30:56\n",
            "loss: 0.6352, acc: 0.7290\n",
            "E2E-ABSA >>> 2022-08-17 16:30:56\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:57\n",
            "loss: 0.6163, acc: 0.7362\n",
            "E2E-ABSA >>> 2022-08-17 16:30:58\n",
            "loss: 0.6220, acc: 0.7319\n",
            "E2E-ABSA >>> 2022-08-17 16:30:59\n",
            "loss: 0.6318, acc: 0.7312\n",
            "E2E-ABSA >>> 2022-08-17 16:30:59\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:30:59\n",
            "loss: 0.6107, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-08-17 16:31:00\n",
            "loss: 0.6245, acc: 0.7390\n",
            "E2E-ABSA >>> 2022-08-17 16:31:01\n",
            "loss: 0.6340, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 16:31:02\n",
            "loss: 0.6344, acc: 0.7295\n",
            "E2E-ABSA >>> 2022-08-17 16:31:02\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:02\n",
            "loss: 0.6025, acc: 0.7526\n",
            "E2E-ABSA >>> 2022-08-17 16:31:03\n",
            "loss: 0.6244, acc: 0.7414\n",
            "E2E-ABSA >>> 2022-08-17 16:31:04\n",
            "loss: 0.6309, acc: 0.7361\n",
            "E2E-ABSA >>> 2022-08-17 16:31:04\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:05\n",
            "loss: 0.6111, acc: 0.7429\n",
            "E2E-ABSA >>> 2022-08-17 16:31:05\n",
            "loss: 0.6265, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 16:31:06\n",
            "loss: 0.6356, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-08-17 16:31:07\n",
            "loss: 0.6334, acc: 0.7335\n",
            "E2E-ABSA >>> 2022-08-17 16:31:07\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:08\n",
            "loss: 0.6083, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 16:31:08\n",
            "loss: 0.6301, acc: 0.7340\n",
            "E2E-ABSA >>> 2022-08-17 16:31:09\n",
            "loss: 0.6293, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 16:31:10\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:10\n",
            "loss: 0.6339, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-08-17 16:31:11\n",
            "loss: 0.6222, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 16:31:11\n",
            "loss: 0.6300, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 16:31:12\n",
            "loss: 0.6339, acc: 0.7311\n",
            "E2E-ABSA >>> 2022-08-17 16:31:12\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:13\n",
            "loss: 0.6361, acc: 0.7180\n",
            "E2E-ABSA >>> 2022-08-17 16:31:14\n",
            "loss: 0.6368, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:31:14\n",
            "loss: 0.6359, acc: 0.7276\n",
            "E2E-ABSA >>> 2022-08-17 16:31:15\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:15\n",
            "loss: 0.6237, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:31:16\n",
            "loss: 0.6113, acc: 0.7486\n",
            "E2E-ABSA >>> 2022-08-17 16:31:17\n",
            "loss: 0.6281, acc: 0.7333\n",
            "E2E-ABSA >>> 2022-08-17 16:31:17\n",
            "loss: 0.6312, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 16:31:18\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:18\n",
            "loss: 0.6153, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:31:19\n",
            "loss: 0.6422, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 16:31:20\n",
            "loss: 0.6329, acc: 0.7313\n",
            "E2E-ABSA >>> 2022-08-17 16:31:20\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:21\n",
            "loss: 0.5697, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 16:31:21\n",
            "loss: 0.5994, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:31:22\n",
            "loss: 0.6272, acc: 0.7349\n",
            "E2E-ABSA >>> 2022-08-17 16:31:23\n",
            "loss: 0.6300, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-08-17 16:31:23\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:24\n",
            "loss: 0.6240, acc: 0.7289\n",
            "E2E-ABSA >>> 2022-08-17 16:31:24\n",
            "loss: 0.6238, acc: 0.7351\n",
            "E2E-ABSA >>> 2022-08-17 16:31:25\n",
            "loss: 0.6321, acc: 0.7288\n",
            "E2E-ABSA >>> 2022-08-17 16:31:26\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:26\n",
            "loss: 0.6496, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:31:27\n",
            "loss: 0.6450, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:31:27\n",
            "loss: 0.6387, acc: 0.7223\n",
            "E2E-ABSA >>> 2022-08-17 16:31:28\n",
            "loss: 0.6347, acc: 0.7264\n",
            "E2E-ABSA >>> 2022-08-17 16:31:28\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:29\n",
            "loss: 0.6465, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 16:31:30\n",
            "loss: 0.6351, acc: 0.7408\n",
            "E2E-ABSA >>> 2022-08-17 16:31:30\n",
            "loss: 0.6308, acc: 0.7378\n",
            "E2E-ABSA >>> 2022-08-17 16:31:31\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:31\n",
            "loss: 0.6275, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:31:32\n",
            "loss: 0.6394, acc: 0.7283\n",
            "E2E-ABSA >>> 2022-08-17 16:31:33\n",
            "loss: 0.6367, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:31:33\n",
            "loss: 0.6276, acc: 0.7334\n",
            "E2E-ABSA >>> 2022-08-17 16:31:34\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:34\n",
            "loss: 0.6133, acc: 0.7387\n",
            "E2E-ABSA >>> 2022-08-17 16:31:35\n",
            "loss: 0.6312, acc: 0.7271\n",
            "E2E-ABSA >>> 2022-08-17 16:31:36\n",
            "loss: 0.6293, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 16:31:36\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:36\n",
            "loss: 0.5995, acc: 0.7750\n",
            "E2E-ABSA >>> 2022-08-17 16:31:37\n",
            "loss: 0.6255, acc: 0.7417\n",
            "E2E-ABSA >>> 2022-08-17 16:31:38\n",
            "loss: 0.6347, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 16:31:39\n",
            "loss: 0.6345, acc: 0.7291\n",
            "E2E-ABSA >>> 2022-08-17 16:31:39\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:39\n",
            "loss: 0.6339, acc: 0.7123\n",
            "E2E-ABSA >>> 2022-08-17 16:31:40\n",
            "loss: 0.6336, acc: 0.7299\n",
            "E2E-ABSA >>> 2022-08-17 16:31:41\n",
            "loss: 0.6294, acc: 0.7332\n",
            "E2E-ABSA >>> 2022-08-17 16:31:42\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:42\n",
            "loss: 0.6657, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 16:31:43\n",
            "loss: 0.6362, acc: 0.7225\n",
            "E2E-ABSA >>> 2022-08-17 16:31:43\n",
            "loss: 0.6346, acc: 0.7260\n",
            "E2E-ABSA >>> 2022-08-17 16:31:44\n",
            "loss: 0.6296, acc: 0.7320\n",
            "E2E-ABSA >>> 2022-08-17 16:31:44\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:45\n",
            "loss: 0.6252, acc: 0.7334\n",
            "E2E-ABSA >>> 2022-08-17 16:31:46\n",
            "loss: 0.6190, acc: 0.7424\n",
            "E2E-ABSA >>> 2022-08-17 16:31:46\n",
            "loss: 0.6227, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-08-17 16:31:47\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:47\n",
            "loss: 0.6645, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:31:48\n",
            "loss: 0.6297, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 16:31:49\n",
            "loss: 0.6278, acc: 0.7356\n",
            "E2E-ABSA >>> 2022-08-17 16:31:50\n",
            "loss: 0.6272, acc: 0.7334\n",
            "E2E-ABSA >>> 2022-08-17 16:31:50\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:51\n",
            "loss: 0.6393, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 16:31:51\n",
            "loss: 0.6317, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 16:31:52\n",
            "loss: 0.6235, acc: 0.7315\n",
            "E2E-ABSA >>> 2022-08-17 16:31:53\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:53\n",
            "loss: 0.5871, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 16:31:54\n",
            "loss: 0.6363, acc: 0.7367\n",
            "E2E-ABSA >>> 2022-08-17 16:31:54\n",
            "loss: 0.6368, acc: 0.7263\n",
            "E2E-ABSA >>> 2022-08-17 16:31:55\n",
            "loss: 0.6352, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-08-17 16:31:55\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">> saved: state_dict/memnet_acl14shortdata_val_f1_0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:56\n",
            "loss: 0.6106, acc: 0.7600\n",
            "E2E-ABSA >>> 2022-08-17 16:31:57\n",
            "loss: 0.6232, acc: 0.7380\n",
            "E2E-ABSA >>> 2022-08-17 16:31:57\n",
            "loss: 0.6205, acc: 0.7373\n",
            "E2E-ABSA >>> 2022-08-17 16:31:58\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:31:58\n",
            "loss: 0.6721, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:31:59\n",
            "loss: 0.5985, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 16:32:00\n",
            "loss: 0.6098, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 16:32:00\n",
            "loss: 0.6273, acc: 0.7338\n",
            "E2E-ABSA >>> 2022-08-17 16:32:01\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:32:01\n",
            "loss: 0.6145, acc: 0.7536\n",
            "E2E-ABSA >>> 2022-08-17 16:32:02\n",
            "loss: 0.6282, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 16:32:03\n",
            "loss: 0.6315, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-08-17 16:32:03\n",
            "loss: 0.6267, acc: 0.7336\n",
            "E2E-ABSA >>> 2022-08-17 16:32:04\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            "you can download the best model from state_dict/memnet_acl14shortdata_val_f1_0.6928\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.6928, test_precision: 0.6928, test_recall: 0.6928, test_f1: 0.6928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **acl14shortdata** dataset on model(**MEMNET**)\n"
      ],
      "metadata": {
        "id": "5CRPy6TUXjdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name memnet --dataset acl14shortdata_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB9jVmjXXjhr",
        "outputId": "364e4c7c-a2a1-4832-964e-c159fc90fc34"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 18232320\n",
            "> n_trainable_params: 362703, n_nontrainable_params: 4115100\n",
            "> training arguments:\n",
            ">>> model_name: memnet\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f1b53dbfb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.memnet.MemNet'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['context_indices', 'aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:32:43\n",
            "loss: 1.0348, acc: 0.5012\n",
            "E2E-ABSA >>> 2022-08-17 16:32:44\n",
            "loss: 1.0179, acc: 0.5069\n",
            "E2E-ABSA >>> 2022-08-17 16:32:45\n",
            "loss: 1.0014, acc: 0.5177\n",
            "E2E-ABSA >>> 2022-08-17 16:32:45\n",
            ">>> val_acc: 0.5232, val_precision: 0.5232 val_recall: 0.5232, val_f1: 0.5232\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.5232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:32:46\n",
            "loss: 0.9058, acc: 0.5885\n",
            "E2E-ABSA >>> 2022-08-17 16:32:46\n",
            "loss: 0.9394, acc: 0.5688\n",
            "E2E-ABSA >>> 2022-08-17 16:32:47\n",
            "loss: 0.9393, acc: 0.5643\n",
            "E2E-ABSA >>> 2022-08-17 16:32:48\n",
            "loss: 0.9398, acc: 0.5654\n",
            "E2E-ABSA >>> 2022-08-17 16:32:48\n",
            ">>> val_acc: 0.5264, val_precision: 0.5264 val_recall: 0.5264, val_f1: 0.5264\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.5264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:32:49\n",
            "loss: 0.8933, acc: 0.5957\n",
            "E2E-ABSA >>> 2022-08-17 16:32:49\n",
            "loss: 0.9055, acc: 0.5848\n",
            "E2E-ABSA >>> 2022-08-17 16:32:50\n",
            "loss: 0.9131, acc: 0.5750\n",
            "E2E-ABSA >>> 2022-08-17 16:32:51\n",
            ">>> val_acc: 0.5360, val_precision: 0.5360 val_recall: 0.5360, val_f1: 0.5360\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.536\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:32:51\n",
            "loss: 0.8827, acc: 0.5895\n",
            "E2E-ABSA >>> 2022-08-17 16:32:52\n",
            "loss: 0.9038, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 16:32:52\n",
            "loss: 0.8964, acc: 0.5832\n",
            "E2E-ABSA >>> 2022-08-17 16:32:53\n",
            "loss: 0.8868, acc: 0.5930\n",
            "E2E-ABSA >>> 2022-08-17 16:32:53\n",
            ">>> val_acc: 0.5392, val_precision: 0.5392 val_recall: 0.5392, val_f1: 0.5392\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.5392\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:32:54\n",
            "loss: 0.8714, acc: 0.5992\n",
            "E2E-ABSA >>> 2022-08-17 16:32:55\n",
            "loss: 0.8684, acc: 0.6025\n",
            "E2E-ABSA >>> 2022-08-17 16:32:56\n",
            "loss: 0.8578, acc: 0.6156\n",
            "E2E-ABSA >>> 2022-08-17 16:32:56\n",
            ">>> val_acc: 0.5824, val_precision: 0.5824 val_recall: 0.5824, val_f1: 0.5824\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.5824\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:32:56\n",
            "loss: 0.8197, acc: 0.6312\n",
            "E2E-ABSA >>> 2022-08-17 16:32:57\n",
            "loss: 0.8351, acc: 0.6228\n",
            "E2E-ABSA >>> 2022-08-17 16:32:58\n",
            "loss: 0.8294, acc: 0.6279\n",
            "E2E-ABSA >>> 2022-08-17 16:32:59\n",
            "loss: 0.8261, acc: 0.6272\n",
            "E2E-ABSA >>> 2022-08-17 16:32:59\n",
            ">>> val_acc: 0.6032, val_precision: 0.6032 val_recall: 0.6032, val_f1: 0.6032\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6032\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:00\n",
            "loss: 0.8061, acc: 0.6420\n",
            "E2E-ABSA >>> 2022-08-17 16:33:00\n",
            "loss: 0.7991, acc: 0.6503\n",
            "E2E-ABSA >>> 2022-08-17 16:33:01\n",
            "loss: 0.8030, acc: 0.6441\n",
            "E2E-ABSA >>> 2022-08-17 16:33:02\n",
            ">>> val_acc: 0.6256, val_precision: 0.6256 val_recall: 0.6256, val_f1: 0.6256\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6256\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:02\n",
            "loss: 0.7533, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-08-17 16:33:03\n",
            "loss: 0.7677, acc: 0.6682\n",
            "E2E-ABSA >>> 2022-08-17 16:33:03\n",
            "loss: 0.7685, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-08-17 16:33:04\n",
            "loss: 0.7715, acc: 0.6661\n",
            "E2E-ABSA >>> 2022-08-17 16:33:04\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:05\n",
            "loss: 0.7404, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-08-17 16:33:06\n",
            "loss: 0.7591, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 16:33:06\n",
            "loss: 0.7574, acc: 0.6695\n",
            "E2E-ABSA >>> 2022-08-17 16:33:07\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:07\n",
            "loss: 0.7129, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 16:33:08\n",
            "loss: 0.7401, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 16:33:09\n",
            "loss: 0.7404, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-08-17 16:33:09\n",
            "loss: 0.7382, acc: 0.6811\n",
            "E2E-ABSA >>> 2022-08-17 16:33:10\n",
            ">>> val_acc: 0.6576, val_precision: 0.6576 val_recall: 0.6576, val_f1: 0.6576\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6576\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:10\n",
            "loss: 0.7490, acc: 0.6852\n",
            "E2E-ABSA >>> 2022-08-17 16:33:11\n",
            "loss: 0.7443, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-08-17 16:33:12\n",
            "loss: 0.7377, acc: 0.6888\n",
            "E2E-ABSA >>> 2022-08-17 16:33:12\n",
            ">>> val_acc: 0.6640, val_precision: 0.6640 val_recall: 0.6640, val_f1: 0.6640\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.664\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:13\n",
            "loss: 0.6939, acc: 0.7366\n",
            "E2E-ABSA >>> 2022-08-17 16:33:13\n",
            "loss: 0.7351, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:33:14\n",
            "loss: 0.7295, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:33:15\n",
            "loss: 0.7198, acc: 0.6904\n",
            "E2E-ABSA >>> 2022-08-17 16:33:15\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:16\n",
            "loss: 0.7334, acc: 0.6768\n",
            "E2E-ABSA >>> 2022-08-17 16:33:17\n",
            "loss: 0.7136, acc: 0.6971\n",
            "E2E-ABSA >>> 2022-08-17 16:33:17\n",
            "loss: 0.7121, acc: 0.6966\n",
            "E2E-ABSA >>> 2022-08-17 16:33:18\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:18\n",
            "loss: 0.7182, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:33:19\n",
            "loss: 0.7137, acc: 0.6925\n",
            "E2E-ABSA >>> 2022-08-17 16:33:20\n",
            "loss: 0.7078, acc: 0.6964\n",
            "E2E-ABSA >>> 2022-08-17 16:33:20\n",
            "loss: 0.7073, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-08-17 16:33:21\n",
            ">>> val_acc: 0.6544, val_precision: 0.6544 val_recall: 0.6544, val_f1: 0.6544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:21\n",
            "loss: 0.7106, acc: 0.6997\n",
            "E2E-ABSA >>> 2022-08-17 16:33:22\n",
            "loss: 0.7177, acc: 0.6908\n",
            "E2E-ABSA >>> 2022-08-17 16:33:23\n",
            "loss: 0.7085, acc: 0.6983\n",
            "E2E-ABSA >>> 2022-08-17 16:33:23\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:24\n",
            "loss: 0.7177, acc: 0.6937\n",
            "E2E-ABSA >>> 2022-08-17 16:33:24\n",
            "loss: 0.6875, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 16:33:25\n",
            "loss: 0.6957, acc: 0.6972\n",
            "E2E-ABSA >>> 2022-08-17 16:33:26\n",
            "loss: 0.7005, acc: 0.6949\n",
            "E2E-ABSA >>> 2022-08-17 16:33:26\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:27\n",
            "loss: 0.7118, acc: 0.6976\n",
            "E2E-ABSA >>> 2022-08-17 16:33:27\n",
            "loss: 0.6938, acc: 0.7065\n",
            "E2E-ABSA >>> 2022-08-17 16:33:28\n",
            "loss: 0.6897, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 16:33:29\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:29\n",
            "loss: 0.6303, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:33:30\n",
            "loss: 0.6797, acc: 0.7112\n",
            "E2E-ABSA >>> 2022-08-17 16:33:31\n",
            "loss: 0.6876, acc: 0.7080\n",
            "E2E-ABSA >>> 2022-08-17 16:33:31\n",
            "loss: 0.6897, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:33:32\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:32\n",
            "loss: 0.6995, acc: 0.7051\n",
            "E2E-ABSA >>> 2022-08-17 16:33:33\n",
            "loss: 0.6987, acc: 0.7066\n",
            "E2E-ABSA >>> 2022-08-17 16:33:34\n",
            "loss: 0.6937, acc: 0.7048\n",
            "E2E-ABSA >>> 2022-08-17 16:33:34\n",
            ">>> val_acc: 0.6672, val_precision: 0.6672 val_recall: 0.6672, val_f1: 0.6672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:34\n",
            "loss: 0.7349, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-08-17 16:33:35\n",
            "loss: 0.7026, acc: 0.7020\n",
            "E2E-ABSA >>> 2022-08-17 16:33:36\n",
            "loss: 0.6920, acc: 0.7028\n",
            "E2E-ABSA >>> 2022-08-17 16:33:37\n",
            "loss: 0.6864, acc: 0.7065\n",
            "E2E-ABSA >>> 2022-08-17 16:33:37\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:37\n",
            "loss: 0.6856, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 16:33:38\n",
            "loss: 0.6632, acc: 0.7195\n",
            "E2E-ABSA >>> 2022-08-17 16:33:39\n",
            "loss: 0.6777, acc: 0.7089\n",
            "E2E-ABSA >>> 2022-08-17 16:33:40\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:40\n",
            "loss: 0.5758, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 16:33:40\n",
            "loss: 0.6598, acc: 0.7193\n",
            "E2E-ABSA >>> 2022-08-17 16:33:41\n",
            "loss: 0.6796, acc: 0.7076\n",
            "E2E-ABSA >>> 2022-08-17 16:33:42\n",
            "loss: 0.6823, acc: 0.7054\n",
            "E2E-ABSA >>> 2022-08-17 16:33:42\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:43\n",
            "loss: 0.6960, acc: 0.7087\n",
            "E2E-ABSA >>> 2022-08-17 16:33:44\n",
            "loss: 0.6907, acc: 0.7047\n",
            "E2E-ABSA >>> 2022-08-17 16:33:44\n",
            "loss: 0.6816, acc: 0.7085\n",
            "E2E-ABSA >>> 2022-08-17 16:33:45\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:45\n",
            "loss: 0.5989, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:33:46\n",
            "loss: 0.6446, acc: 0.7254\n",
            "E2E-ABSA >>> 2022-08-17 16:33:47\n",
            "loss: 0.6639, acc: 0.7203\n",
            "E2E-ABSA >>> 2022-08-17 16:33:47\n",
            "loss: 0.6679, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 16:33:48\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:48\n",
            "loss: 0.6593, acc: 0.7103\n",
            "E2E-ABSA >>> 2022-08-17 16:33:49\n",
            "loss: 0.6765, acc: 0.7035\n",
            "E2E-ABSA >>> 2022-08-17 16:33:50\n",
            "loss: 0.6792, acc: 0.7071\n",
            "E2E-ABSA >>> 2022-08-17 16:33:50\n",
            "loss: 0.6757, acc: 0.7078\n",
            "E2E-ABSA >>> 2022-08-17 16:33:50\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:51\n",
            "loss: 0.6595, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-08-17 16:33:52\n",
            "loss: 0.6833, acc: 0.7150\n",
            "E2E-ABSA >>> 2022-08-17 16:33:53\n",
            "loss: 0.6771, acc: 0.7127\n",
            "E2E-ABSA >>> 2022-08-17 16:33:53\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:54\n",
            "loss: 0.6678, acc: 0.7174\n",
            "E2E-ABSA >>> 2022-08-17 16:33:54\n",
            "loss: 0.6557, acc: 0.7276\n",
            "E2E-ABSA >>> 2022-08-17 16:33:55\n",
            "loss: 0.6723, acc: 0.7137\n",
            "E2E-ABSA >>> 2022-08-17 16:33:56\n",
            "loss: 0.6730, acc: 0.7092\n",
            "E2E-ABSA >>> 2022-08-17 16:33:56\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:57\n",
            "loss: 0.6642, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-08-17 16:33:57\n",
            "loss: 0.6628, acc: 0.7156\n",
            "E2E-ABSA >>> 2022-08-17 16:33:58\n",
            "loss: 0.6666, acc: 0.7168\n",
            "E2E-ABSA >>> 2022-08-17 16:33:59\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:33:59\n",
            "loss: 0.6273, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-08-17 16:34:00\n",
            "loss: 0.6442, acc: 0.7227\n",
            "E2E-ABSA >>> 2022-08-17 16:34:00\n",
            "loss: 0.6661, acc: 0.7116\n",
            "E2E-ABSA >>> 2022-08-17 16:34:01\n",
            "loss: 0.6694, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-08-17 16:34:01\n",
            ">>> val_acc: 0.6752, val_precision: 0.6752 val_recall: 0.6752, val_f1: 0.6752\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:02\n",
            "loss: 0.6382, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-08-17 16:34:03\n",
            "loss: 0.6769, acc: 0.7132\n",
            "E2E-ABSA >>> 2022-08-17 16:34:03\n",
            "loss: 0.6691, acc: 0.7202\n",
            "E2E-ABSA >>> 2022-08-17 16:34:04\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:04\n",
            "loss: 0.7082, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 16:34:05\n",
            "loss: 0.6749, acc: 0.7183\n",
            "E2E-ABSA >>> 2022-08-17 16:34:06\n",
            "loss: 0.6644, acc: 0.7195\n",
            "E2E-ABSA >>> 2022-08-17 16:34:06\n",
            "loss: 0.6679, acc: 0.7182\n",
            "E2E-ABSA >>> 2022-08-17 16:34:07\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:07\n",
            "loss: 0.6804, acc: 0.6939\n",
            "E2E-ABSA >>> 2022-08-17 16:34:08\n",
            "loss: 0.6558, acc: 0.7194\n",
            "E2E-ABSA >>> 2022-08-17 16:34:09\n",
            "loss: 0.6638, acc: 0.7144\n",
            "E2E-ABSA >>> 2022-08-17 16:34:09\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:09\n",
            "loss: 0.6154, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:34:10\n",
            "loss: 0.6692, acc: 0.7128\n",
            "E2E-ABSA >>> 2022-08-17 16:34:11\n",
            "loss: 0.6662, acc: 0.7172\n",
            "E2E-ABSA >>> 2022-08-17 16:34:12\n",
            "loss: 0.6629, acc: 0.7180\n",
            "E2E-ABSA >>> 2022-08-17 16:34:12\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:13\n",
            "loss: 0.6775, acc: 0.7039\n",
            "E2E-ABSA >>> 2022-08-17 16:34:14\n",
            "loss: 0.6698, acc: 0.7133\n",
            "E2E-ABSA >>> 2022-08-17 16:34:14\n",
            "loss: 0.6709, acc: 0.7126\n",
            "E2E-ABSA >>> 2022-08-17 16:34:15\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:15\n",
            "loss: 0.6576, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-08-17 16:34:16\n",
            "loss: 0.6445, acc: 0.7382\n",
            "E2E-ABSA >>> 2022-08-17 16:34:17\n",
            "loss: 0.6578, acc: 0.7233\n",
            "E2E-ABSA >>> 2022-08-17 16:34:17\n",
            "loss: 0.6623, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:34:18\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:18\n",
            "loss: 0.6341, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 16:34:19\n",
            "loss: 0.6542, acc: 0.7285\n",
            "E2E-ABSA >>> 2022-08-17 16:34:20\n",
            "loss: 0.6583, acc: 0.7239\n",
            "E2E-ABSA >>> 2022-08-17 16:34:20\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:21\n",
            "loss: 0.6215, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:34:21\n",
            "loss: 0.6569, acc: 0.7236\n",
            "E2E-ABSA >>> 2022-08-17 16:34:22\n",
            "loss: 0.6567, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:34:23\n",
            "loss: 0.6613, acc: 0.7165\n",
            "E2E-ABSA >>> 2022-08-17 16:34:23\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:24\n",
            "loss: 0.6554, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-08-17 16:34:24\n",
            "loss: 0.6712, acc: 0.7092\n",
            "E2E-ABSA >>> 2022-08-17 16:34:25\n",
            "loss: 0.6627, acc: 0.7167\n",
            "E2E-ABSA >>> 2022-08-17 16:34:26\n",
            ">>> val_acc: 0.6704, val_precision: 0.6704 val_recall: 0.6704, val_f1: 0.6704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:26\n",
            "loss: 0.6922, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:34:27\n",
            "loss: 0.6434, acc: 0.7268\n",
            "E2E-ABSA >>> 2022-08-17 16:34:27\n",
            "loss: 0.6591, acc: 0.7249\n",
            "E2E-ABSA >>> 2022-08-17 16:34:28\n",
            "loss: 0.6599, acc: 0.7242\n",
            "E2E-ABSA >>> 2022-08-17 16:34:28\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:29\n",
            "loss: 0.6472, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 16:34:30\n",
            "loss: 0.6505, acc: 0.7220\n",
            "E2E-ABSA >>> 2022-08-17 16:34:30\n",
            "loss: 0.6547, acc: 0.7229\n",
            "E2E-ABSA >>> 2022-08-17 16:34:31\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:31\n",
            "loss: 0.6991, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:34:32\n",
            "loss: 0.6509, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 16:34:33\n",
            "loss: 0.6545, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 16:34:33\n",
            "loss: 0.6631, acc: 0.7225\n",
            "E2E-ABSA >>> 2022-08-17 16:34:34\n",
            ">>> val_acc: 0.6864, val_precision: 0.6864 val_recall: 0.6864, val_f1: 0.6864\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:34\n",
            "loss: 0.6258, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-08-17 16:34:35\n",
            "loss: 0.6470, acc: 0.7281\n",
            "E2E-ABSA >>> 2022-08-17 16:34:36\n",
            "loss: 0.6440, acc: 0.7290\n",
            "E2E-ABSA >>> 2022-08-17 16:34:37\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:37\n",
            "loss: 0.6511, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 16:34:37\n",
            "loss: 0.6564, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:34:38\n",
            "loss: 0.6629, acc: 0.7173\n",
            "E2E-ABSA >>> 2022-08-17 16:34:39\n",
            "loss: 0.6557, acc: 0.7219\n",
            "E2E-ABSA >>> 2022-08-17 16:34:39\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:40\n",
            "loss: 0.6133, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 16:34:40\n",
            "loss: 0.6461, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-08-17 16:34:41\n",
            "loss: 0.6584, acc: 0.7230\n",
            "E2E-ABSA >>> 2022-08-17 16:34:42\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:42\n",
            "loss: 0.6483, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:34:43\n",
            "loss: 0.6443, acc: 0.7316\n",
            "E2E-ABSA >>> 2022-08-17 16:34:44\n",
            "loss: 0.6543, acc: 0.7214\n",
            "E2E-ABSA >>> 2022-08-17 16:34:44\n",
            "loss: 0.6516, acc: 0.7274\n",
            "E2E-ABSA >>> 2022-08-17 16:34:45\n",
            ">>> val_acc: 0.6816, val_precision: 0.6816 val_recall: 0.6816, val_f1: 0.6816\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:45\n",
            "loss: 0.6341, acc: 0.7427\n",
            "E2E-ABSA >>> 2022-08-17 16:34:46\n",
            "loss: 0.6513, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:34:47\n",
            "loss: 0.6453, acc: 0.7353\n",
            "E2E-ABSA >>> 2022-08-17 16:34:47\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:47\n",
            "loss: 0.6604, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:34:48\n",
            "loss: 0.6524, acc: 0.7245\n",
            "E2E-ABSA >>> 2022-08-17 16:34:49\n",
            "loss: 0.6469, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:34:50\n",
            "loss: 0.6525, acc: 0.7232\n",
            "E2E-ABSA >>> 2022-08-17 16:34:50\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:50\n",
            "loss: 0.6791, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:34:51\n",
            "loss: 0.6522, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 16:34:52\n",
            "loss: 0.6526, acc: 0.7256\n",
            "E2E-ABSA >>> 2022-08-17 16:34:53\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:53\n",
            "loss: 0.7485, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 16:34:53\n",
            "loss: 0.6479, acc: 0.7356\n",
            "E2E-ABSA >>> 2022-08-17 16:34:54\n",
            "loss: 0.6361, acc: 0.7362\n",
            "E2E-ABSA >>> 2022-08-17 16:34:55\n",
            "loss: 0.6498, acc: 0.7305\n",
            "E2E-ABSA >>> 2022-08-17 16:34:55\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:56\n",
            "loss: 0.6881, acc: 0.7091\n",
            "E2E-ABSA >>> 2022-08-17 16:34:57\n",
            "loss: 0.6727, acc: 0.7060\n",
            "E2E-ABSA >>> 2022-08-17 16:34:57\n",
            "loss: 0.6602, acc: 0.7205\n",
            "E2E-ABSA >>> 2022-08-17 16:34:58\n",
            "loss: 0.6479, acc: 0.7268\n",
            "E2E-ABSA >>> 2022-08-17 16:34:58\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:34:59\n",
            "loss: 0.6437, acc: 0.7244\n",
            "E2E-ABSA >>> 2022-08-17 16:35:00\n",
            "loss: 0.6444, acc: 0.7228\n",
            "E2E-ABSA >>> 2022-08-17 16:35:00\n",
            "loss: 0.6391, acc: 0.7308\n",
            "E2E-ABSA >>> 2022-08-17 16:35:01\n",
            ">>> val_acc: 0.6864, val_precision: 0.6864 val_recall: 0.6864, val_f1: 0.6864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:01\n",
            "loss: 0.6646, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:35:02\n",
            "loss: 0.6373, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:35:03\n",
            "loss: 0.6438, acc: 0.7298\n",
            "E2E-ABSA >>> 2022-08-17 16:35:03\n",
            "loss: 0.6447, acc: 0.7306\n",
            "E2E-ABSA >>> 2022-08-17 16:35:03\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:04\n",
            "loss: 0.6161, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:35:05\n",
            "loss: 0.6390, acc: 0.7321\n",
            "E2E-ABSA >>> 2022-08-17 16:35:06\n",
            "loss: 0.6458, acc: 0.7266\n",
            "E2E-ABSA >>> 2022-08-17 16:35:06\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:06\n",
            "loss: 0.6477, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-08-17 16:35:07\n",
            "loss: 0.6464, acc: 0.7253\n",
            "E2E-ABSA >>> 2022-08-17 16:35:08\n",
            "loss: 0.6408, acc: 0.7282\n",
            "E2E-ABSA >>> 2022-08-17 16:35:09\n",
            "loss: 0.6465, acc: 0.7287\n",
            "E2E-ABSA >>> 2022-08-17 16:35:09\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:09\n",
            "loss: 0.6553, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 16:35:10\n",
            "loss: 0.6510, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-08-17 16:35:11\n",
            "loss: 0.6481, acc: 0.7243\n",
            "E2E-ABSA >>> 2022-08-17 16:35:11\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:12\n",
            "loss: 0.6766, acc: 0.7016\n",
            "E2E-ABSA >>> 2022-08-17 16:35:12\n",
            "loss: 0.6611, acc: 0.7241\n",
            "E2E-ABSA >>> 2022-08-17 16:35:13\n",
            "loss: 0.6553, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-08-17 16:35:14\n",
            "loss: 0.6474, acc: 0.7307\n",
            "E2E-ABSA >>> 2022-08-17 16:35:14\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:15\n",
            "loss: 0.6379, acc: 0.7379\n",
            "E2E-ABSA >>> 2022-08-17 16:35:16\n",
            "loss: 0.6414, acc: 0.7304\n",
            "E2E-ABSA >>> 2022-08-17 16:35:16\n",
            "loss: 0.6446, acc: 0.7294\n",
            "E2E-ABSA >>> 2022-08-17 16:35:17\n",
            ">>> val_acc: 0.6688, val_precision: 0.6688 val_recall: 0.6688, val_f1: 0.6688\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:17\n",
            "loss: 0.6647, acc: 0.7222\n",
            "E2E-ABSA >>> 2022-08-17 16:35:18\n",
            "loss: 0.6527, acc: 0.7275\n",
            "E2E-ABSA >>> 2022-08-17 16:35:19\n",
            "loss: 0.6558, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 16:35:19\n",
            "loss: 0.6458, acc: 0.7294\n",
            "E2E-ABSA >>> 2022-08-17 16:35:20\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:20\n",
            "loss: 0.6612, acc: 0.7314\n",
            "E2E-ABSA >>> 2022-08-17 16:35:21\n",
            "loss: 0.6416, acc: 0.7357\n",
            "E2E-ABSA >>> 2022-08-17 16:35:22\n",
            "loss: 0.6366, acc: 0.7348\n",
            "E2E-ABSA >>> 2022-08-17 16:35:22\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:23\n",
            "loss: 0.6099, acc: 0.7324\n",
            "E2E-ABSA >>> 2022-08-17 16:35:23\n",
            "loss: 0.6460, acc: 0.7277\n",
            "E2E-ABSA >>> 2022-08-17 16:35:24\n",
            "loss: 0.6468, acc: 0.7263\n",
            "E2E-ABSA >>> 2022-08-17 16:35:25\n",
            "loss: 0.6439, acc: 0.7295\n",
            "E2E-ABSA >>> 2022-08-17 16:35:25\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:26\n",
            "loss: 0.6330, acc: 0.7328\n",
            "E2E-ABSA >>> 2022-08-17 16:35:26\n",
            "loss: 0.6481, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 16:35:27\n",
            "loss: 0.6493, acc: 0.7263\n",
            "E2E-ABSA >>> 2022-08-17 16:35:28\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:28\n",
            "loss: 0.6572, acc: 0.7433\n",
            "E2E-ABSA >>> 2022-08-17 16:35:29\n",
            "loss: 0.6598, acc: 0.7178\n",
            "E2E-ABSA >>> 2022-08-17 16:35:29\n",
            "loss: 0.6443, acc: 0.7248\n",
            "E2E-ABSA >>> 2022-08-17 16:35:30\n",
            "loss: 0.6392, acc: 0.7279\n",
            "E2E-ABSA >>> 2022-08-17 16:35:30\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:31\n",
            "loss: 0.6494, acc: 0.7360\n",
            "E2E-ABSA >>> 2022-08-17 16:35:32\n",
            "loss: 0.6386, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 16:35:32\n",
            "loss: 0.6359, acc: 0.7355\n",
            "E2E-ABSA >>> 2022-08-17 16:35:33\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:33\n",
            "loss: 0.6225, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 16:35:34\n",
            "loss: 0.6388, acc: 0.7384\n",
            "E2E-ABSA >>> 2022-08-17 16:35:35\n",
            "loss: 0.6356, acc: 0.7360\n",
            "E2E-ABSA >>> 2022-08-17 16:35:35\n",
            "loss: 0.6368, acc: 0.7342\n",
            "E2E-ABSA >>> 2022-08-17 16:35:36\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:36\n",
            "loss: 0.6314, acc: 0.7378\n",
            "E2E-ABSA >>> 2022-08-17 16:35:37\n",
            "loss: 0.6204, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-08-17 16:35:38\n",
            "loss: 0.6350, acc: 0.7335\n",
            "E2E-ABSA >>> 2022-08-17 16:35:38\n",
            ">>> val_acc: 0.6992, val_precision: 0.6992 val_recall: 0.6992, val_f1: 0.6992\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.6992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:39\n",
            "loss: 0.6938, acc: 0.7031\n",
            "E2E-ABSA >>> 2022-08-17 16:35:39\n",
            "loss: 0.6524, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-08-17 16:35:40\n",
            "loss: 0.6446, acc: 0.7267\n",
            "E2E-ABSA >>> 2022-08-17 16:35:41\n",
            "loss: 0.6377, acc: 0.7326\n",
            "E2E-ABSA >>> 2022-08-17 16:35:41\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:42\n",
            "loss: 0.6275, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-08-17 16:35:42\n",
            "loss: 0.6266, acc: 0.7325\n",
            "E2E-ABSA >>> 2022-08-17 16:35:43\n",
            "loss: 0.6250, acc: 0.7358\n",
            "E2E-ABSA >>> 2022-08-17 16:35:44\n",
            ">>> val_acc: 0.6880, val_precision: 0.6880 val_recall: 0.6880, val_f1: 0.6880\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:44\n",
            "loss: 0.5997, acc: 0.7383\n",
            "E2E-ABSA >>> 2022-08-17 16:35:45\n",
            "loss: 0.6325, acc: 0.7295\n",
            "E2E-ABSA >>> 2022-08-17 16:35:45\n",
            "loss: 0.6348, acc: 0.7297\n",
            "E2E-ABSA >>> 2022-08-17 16:35:46\n",
            "loss: 0.6357, acc: 0.7286\n",
            "E2E-ABSA >>> 2022-08-17 16:35:47\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:47\n",
            "loss: 0.6182, acc: 0.7549\n",
            "E2E-ABSA >>> 2022-08-17 16:35:48\n",
            "loss: 0.6366, acc: 0.7355\n",
            "E2E-ABSA >>> 2022-08-17 16:35:49\n",
            "loss: 0.6391, acc: 0.7330\n",
            "E2E-ABSA >>> 2022-08-17 16:35:49\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:49\n",
            "loss: 0.6687, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 16:35:50\n",
            "loss: 0.6574, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:35:51\n",
            "loss: 0.6265, acc: 0.7376\n",
            "E2E-ABSA >>> 2022-08-17 16:35:52\n",
            "loss: 0.6300, acc: 0.7356\n",
            "E2E-ABSA >>> 2022-08-17 16:35:52\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:52\n",
            "loss: 0.6294, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 16:35:53\n",
            "loss: 0.6285, acc: 0.7352\n",
            "E2E-ABSA >>> 2022-08-17 16:35:54\n",
            "loss: 0.6333, acc: 0.7339\n",
            "E2E-ABSA >>> 2022-08-17 16:35:55\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:55\n",
            "loss: 0.6432, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 16:35:55\n",
            "loss: 0.6254, acc: 0.7425\n",
            "E2E-ABSA >>> 2022-08-17 16:35:56\n",
            "loss: 0.6356, acc: 0.7368\n",
            "E2E-ABSA >>> 2022-08-17 16:35:57\n",
            "loss: 0.6334, acc: 0.7368\n",
            "E2E-ABSA >>> 2022-08-17 16:35:57\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:35:58\n",
            "loss: 0.6383, acc: 0.7366\n",
            "E2E-ABSA >>> 2022-08-17 16:35:59\n",
            "loss: 0.6383, acc: 0.7348\n",
            "E2E-ABSA >>> 2022-08-17 16:35:59\n",
            "loss: 0.6286, acc: 0.7395\n",
            "E2E-ABSA >>> 2022-08-17 16:36:00\n",
            ">>> val_acc: 0.6992, val_precision: 0.6992 val_recall: 0.6992, val_f1: 0.6992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:00\n",
            "loss: 0.6526, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 16:36:01\n",
            "loss: 0.6109, acc: 0.7506\n",
            "E2E-ABSA >>> 2022-08-17 16:36:02\n",
            "loss: 0.6136, acc: 0.7494\n",
            "E2E-ABSA >>> 2022-08-17 16:36:02\n",
            "loss: 0.6268, acc: 0.7385\n",
            "E2E-ABSA >>> 2022-08-17 16:36:03\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:03\n",
            "loss: 0.6645, acc: 0.7175\n",
            "E2E-ABSA >>> 2022-08-17 16:36:04\n",
            "loss: 0.6347, acc: 0.7340\n",
            "E2E-ABSA >>> 2022-08-17 16:36:05\n",
            "loss: 0.6290, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 16:36:05\n",
            "loss: 0.6275, acc: 0.7405\n",
            "E2E-ABSA >>> 2022-08-17 16:36:06\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:06\n",
            "loss: 0.6101, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 16:36:07\n",
            "loss: 0.6147, acc: 0.7409\n",
            "E2E-ABSA >>> 2022-08-17 16:36:08\n",
            "loss: 0.6262, acc: 0.7362\n",
            "E2E-ABSA >>> 2022-08-17 16:36:08\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:09\n",
            "loss: 0.5905, acc: 0.7643\n",
            "E2E-ABSA >>> 2022-08-17 16:36:09\n",
            "loss: 0.6097, acc: 0.7504\n",
            "E2E-ABSA >>> 2022-08-17 16:36:10\n",
            "loss: 0.6213, acc: 0.7402\n",
            "E2E-ABSA >>> 2022-08-17 16:36:11\n",
            "loss: 0.6264, acc: 0.7387\n",
            "E2E-ABSA >>> 2022-08-17 16:36:11\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:12\n",
            "loss: 0.5923, acc: 0.7546\n",
            "E2E-ABSA >>> 2022-08-17 16:36:12\n",
            "loss: 0.6197, acc: 0.7404\n",
            "E2E-ABSA >>> 2022-08-17 16:36:13\n",
            "loss: 0.6245, acc: 0.7399\n",
            "E2E-ABSA >>> 2022-08-17 16:36:14\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:14\n",
            "loss: 0.6053, acc: 0.7514\n",
            "E2E-ABSA >>> 2022-08-17 16:36:15\n",
            "loss: 0.6132, acc: 0.7496\n",
            "E2E-ABSA >>> 2022-08-17 16:36:16\n",
            "loss: 0.6207, acc: 0.7418\n",
            "E2E-ABSA >>> 2022-08-17 16:36:16\n",
            "loss: 0.6237, acc: 0.7400\n",
            "E2E-ABSA >>> 2022-08-17 16:36:16\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:17\n",
            "loss: 0.5966, acc: 0.7595\n",
            "E2E-ABSA >>> 2022-08-17 16:36:18\n",
            "loss: 0.6213, acc: 0.7435\n",
            "E2E-ABSA >>> 2022-08-17 16:36:19\n",
            "loss: 0.6187, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-08-17 16:36:19\n",
            ">>> val_acc: 0.6992, val_precision: 0.6992 val_recall: 0.6992, val_f1: 0.6992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:19\n",
            "loss: 0.6326, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 16:36:20\n",
            "loss: 0.6172, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 16:36:21\n",
            "loss: 0.6243, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-08-17 16:36:22\n",
            "loss: 0.6250, acc: 0.7406\n",
            "E2E-ABSA >>> 2022-08-17 16:36:22\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:22\n",
            "loss: 0.6196, acc: 0.7472\n",
            "E2E-ABSA >>> 2022-08-17 16:36:23\n",
            "loss: 0.6236, acc: 0.7377\n",
            "E2E-ABSA >>> 2022-08-17 16:36:24\n",
            "loss: 0.6221, acc: 0.7437\n",
            "E2E-ABSA >>> 2022-08-17 16:36:25\n",
            ">>> val_acc: 0.6864, val_precision: 0.6864 val_recall: 0.6864, val_f1: 0.6864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:25\n",
            "loss: 0.6149, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:36:26\n",
            "loss: 0.5984, acc: 0.7486\n",
            "E2E-ABSA >>> 2022-08-17 16:36:26\n",
            "loss: 0.6134, acc: 0.7391\n",
            "E2E-ABSA >>> 2022-08-17 16:36:27\n",
            "loss: 0.6187, acc: 0.7405\n",
            "E2E-ABSA >>> 2022-08-17 16:36:27\n",
            ">>> val_acc: 0.6944, val_precision: 0.6944 val_recall: 0.6944, val_f1: 0.6944\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:28\n",
            "loss: 0.6142, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 16:36:29\n",
            "loss: 0.6334, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 16:36:29\n",
            "loss: 0.6242, acc: 0.7401\n",
            "E2E-ABSA >>> 2022-08-17 16:36:30\n",
            ">>> val_acc: 0.6992, val_precision: 0.6992 val_recall: 0.6992, val_f1: 0.6992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:30\n",
            "loss: 0.5711, acc: 0.7578\n",
            "E2E-ABSA >>> 2022-08-17 16:36:31\n",
            "loss: 0.5937, acc: 0.7571\n",
            "E2E-ABSA >>> 2022-08-17 16:36:32\n",
            "loss: 0.6156, acc: 0.7446\n",
            "E2E-ABSA >>> 2022-08-17 16:36:32\n",
            "loss: 0.6159, acc: 0.7445\n",
            "E2E-ABSA >>> 2022-08-17 16:36:33\n",
            ">>> val_acc: 0.7056, val_precision: 0.7056 val_recall: 0.7056, val_f1: 0.7056\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:33\n",
            "loss: 0.6039, acc: 0.7516\n",
            "E2E-ABSA >>> 2022-08-17 16:36:34\n",
            "loss: 0.6128, acc: 0.7472\n",
            "E2E-ABSA >>> 2022-08-17 16:36:35\n",
            "loss: 0.6192, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 16:36:35\n",
            ">>> val_acc: 0.7072, val_precision: 0.7072 val_recall: 0.7072, val_f1: 0.7072\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.7072\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:36\n",
            "loss: 0.6211, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:36:37\n",
            "loss: 0.6306, acc: 0.7329\n",
            "E2E-ABSA >>> 2022-08-17 16:36:37\n",
            "loss: 0.6244, acc: 0.7388\n",
            "E2E-ABSA >>> 2022-08-17 16:36:38\n",
            "loss: 0.6215, acc: 0.7389\n",
            "E2E-ABSA >>> 2022-08-17 16:36:38\n",
            ">>> val_acc: 0.7072, val_precision: 0.7072 val_recall: 0.7072, val_f1: 0.7072\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:39\n",
            "loss: 0.6337, acc: 0.7451\n",
            "E2E-ABSA >>> 2022-08-17 16:36:40\n",
            "loss: 0.6248, acc: 0.7447\n",
            "E2E-ABSA >>> 2022-08-17 16:36:40\n",
            "loss: 0.6185, acc: 0.7466\n",
            "E2E-ABSA >>> 2022-08-17 16:36:41\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:41\n",
            "loss: 0.6178, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 16:36:42\n",
            "loss: 0.6173, acc: 0.7485\n",
            "E2E-ABSA >>> 2022-08-17 16:36:43\n",
            "loss: 0.6201, acc: 0.7461\n",
            "E2E-ABSA >>> 2022-08-17 16:36:44\n",
            "loss: 0.6136, acc: 0.7473\n",
            "E2E-ABSA >>> 2022-08-17 16:36:44\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:44\n",
            "loss: 0.6081, acc: 0.7387\n",
            "E2E-ABSA >>> 2022-08-17 16:36:45\n",
            "loss: 0.6144, acc: 0.7424\n",
            "E2E-ABSA >>> 2022-08-17 16:36:46\n",
            "loss: 0.6129, acc: 0.7401\n",
            "E2E-ABSA >>> 2022-08-17 16:36:47\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:47\n",
            "loss: 0.5867, acc: 0.7719\n",
            "E2E-ABSA >>> 2022-08-17 16:36:47\n",
            "loss: 0.6041, acc: 0.7620\n",
            "E2E-ABSA >>> 2022-08-17 16:36:48\n",
            "loss: 0.6242, acc: 0.7472\n",
            "E2E-ABSA >>> 2022-08-17 16:36:49\n",
            "loss: 0.6193, acc: 0.7420\n",
            "E2E-ABSA >>> 2022-08-17 16:36:49\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:50\n",
            "loss: 0.6181, acc: 0.7261\n",
            "E2E-ABSA >>> 2022-08-17 16:36:50\n",
            "loss: 0.6141, acc: 0.7388\n",
            "E2E-ABSA >>> 2022-08-17 16:36:51\n",
            "loss: 0.6158, acc: 0.7395\n",
            "E2E-ABSA >>> 2022-08-17 16:36:52\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:52\n",
            "loss: 0.6584, acc: 0.7070\n",
            "E2E-ABSA >>> 2022-08-17 16:36:53\n",
            "loss: 0.6271, acc: 0.7371\n",
            "E2E-ABSA >>> 2022-08-17 16:36:54\n",
            "loss: 0.6219, acc: 0.7364\n",
            "E2E-ABSA >>> 2022-08-17 16:36:54\n",
            "loss: 0.6132, acc: 0.7437\n",
            "E2E-ABSA >>> 2022-08-17 16:36:55\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:55\n",
            "loss: 0.6096, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-08-17 16:36:56\n",
            "loss: 0.6100, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-08-17 16:36:57\n",
            "loss: 0.6103, acc: 0.7446\n",
            "E2E-ABSA >>> 2022-08-17 16:36:57\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:36:58\n",
            "loss: 0.6325, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 16:36:58\n",
            "loss: 0.6150, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-08-17 16:36:59\n",
            "loss: 0.6136, acc: 0.7373\n",
            "E2E-ABSA >>> 2022-08-17 16:37:00\n",
            "loss: 0.6112, acc: 0.7420\n",
            "E2E-ABSA >>> 2022-08-17 16:37:00\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:37:01\n",
            "loss: 0.6205, acc: 0.7323\n",
            "E2E-ABSA >>> 2022-08-17 16:37:01\n",
            "loss: 0.6112, acc: 0.7395\n",
            "E2E-ABSA >>> 2022-08-17 16:37:02\n",
            "loss: 0.6059, acc: 0.7478\n",
            "E2E-ABSA >>> 2022-08-17 16:37:03\n",
            ">>> val_acc: 0.7072, val_precision: 0.7072 val_recall: 0.7072, val_f1: 0.7072\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:37:03\n",
            "loss: 0.5667, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 16:37:04\n",
            "loss: 0.6235, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-08-17 16:37:04\n",
            "loss: 0.6199, acc: 0.7413\n",
            "E2E-ABSA >>> 2022-08-17 16:37:05\n",
            "loss: 0.6160, acc: 0.7441\n",
            "E2E-ABSA >>> 2022-08-17 16:37:06\n",
            ">>> val_acc: 0.7136, val_precision: 0.7136 val_recall: 0.7136, val_f1: 0.7136\n",
            ">> saved: state_dict/memnet_acl14shortdata_know_val_f1_0.7136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:37:06\n",
            "loss: 0.5959, acc: 0.7701\n",
            "E2E-ABSA >>> 2022-08-17 16:37:07\n",
            "loss: 0.6055, acc: 0.7556\n",
            "E2E-ABSA >>> 2022-08-17 16:37:08\n",
            "loss: 0.5985, acc: 0.7510\n",
            "E2E-ABSA >>> 2022-08-17 16:37:08\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:37:08\n",
            "loss: 0.6344, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 16:37:09\n",
            "loss: 0.5870, acc: 0.7584\n",
            "E2E-ABSA >>> 2022-08-17 16:37:10\n",
            "loss: 0.5958, acc: 0.7580\n",
            "E2E-ABSA >>> 2022-08-17 16:37:11\n",
            "loss: 0.6087, acc: 0.7447\n",
            "E2E-ABSA >>> 2022-08-17 16:37:11\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "E2E-ABSA >>> 2022-08-17 16:37:12\n",
            "loss: 0.5935, acc: 0.7476\n",
            "E2E-ABSA >>> 2022-08-17 16:37:12\n",
            "loss: 0.6048, acc: 0.7488\n",
            "E2E-ABSA >>> 2022-08-17 16:37:13\n",
            "loss: 0.6083, acc: 0.7483\n",
            "E2E-ABSA >>> 2022-08-17 16:37:14\n",
            "loss: 0.6057, acc: 0.7480\n",
            "E2E-ABSA >>> 2022-08-17 16:37:14\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            "you can download the best model from state_dict/memnet_acl14shortdata_know_val_f1_0.7136\n",
            "/content/DictionaryFused-E2E-ABSA/models/memnet.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float).to(self.opt.device)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            ">>> test_acc: 0.7136, test_precision: 0.7136, test_recall: 0.7136, test_f1: 0.7136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training **acl14shortdata** dataset on model(**CABASC**)\n"
      ],
      "metadata": {
        "id": "RWHbziD0Xjm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset acl14shortdata --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwpnIfN9Xjsh",
        "outputId": "301a2f9c-8d34-4f30-a0a3-200c88b3ee8c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  state_dict\n",
            "deberta_abas.ipynb  layers\t      README.md    train.py\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 4720.\n",
            "> testing dataset count: 520.\n",
            "cuda memory allocated: 21485568\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 3828600\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f5cdc82cb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 16:37:55\n",
            "loss: 1.0968, acc: 0.3706\n",
            "E2E-ABSA >>> 2022-08-17 16:37:58\n",
            "loss: 1.0657, acc: 0.4247\n",
            "E2E-ABSA >>> 2022-08-17 16:38:01\n",
            ">>> val_acc: 0.4596, val_precision: 0.4596 val_recall: 0.4596, val_f1: 0.4596\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.4596\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:01\n",
            "loss: 0.9944, acc: 0.5250\n",
            "E2E-ABSA >>> 2022-08-17 16:38:04\n",
            "loss: 0.9917, acc: 0.5125\n",
            "E2E-ABSA >>> 2022-08-17 16:38:06\n",
            "loss: 0.9840, acc: 0.5171\n",
            "E2E-ABSA >>> 2022-08-17 16:38:09\n",
            ">>> val_acc: 0.4981, val_precision: 0.4981 val_recall: 0.4981, val_f1: 0.4981\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.4981\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:09\n",
            "loss: 0.9445, acc: 0.5375\n",
            "E2E-ABSA >>> 2022-08-17 16:38:12\n",
            "loss: 0.9421, acc: 0.5455\n",
            "E2E-ABSA >>> 2022-08-17 16:38:14\n",
            "loss: 0.9338, acc: 0.5542\n",
            "E2E-ABSA >>> 2022-08-17 16:38:17\n",
            ">>> val_acc: 0.5327, val_precision: 0.5327 val_recall: 0.5327, val_f1: 0.5327\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:17\n",
            "loss: 0.8900, acc: 0.5833\n",
            "E2E-ABSA >>> 2022-08-17 16:38:20\n",
            "loss: 0.9196, acc: 0.5641\n",
            "E2E-ABSA >>> 2022-08-17 16:38:22\n",
            "loss: 0.9019, acc: 0.5811\n",
            "E2E-ABSA >>> 2022-08-17 16:38:25\n",
            ">>> val_acc: 0.5462, val_precision: 0.5462 val_recall: 0.5462, val_f1: 0.5462\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5462\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:26\n",
            "loss: 0.8962, acc: 0.5844\n",
            "E2E-ABSA >>> 2022-08-17 16:38:28\n",
            "loss: 0.8766, acc: 0.5927\n",
            "E2E-ABSA >>> 2022-08-17 16:38:31\n",
            "loss: 0.8790, acc: 0.5898\n",
            "E2E-ABSA >>> 2022-08-17 16:38:33\n",
            ">>> val_acc: 0.5519, val_precision: 0.5519 val_recall: 0.5519, val_f1: 0.5519\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5519\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:34\n",
            "loss: 0.8811, acc: 0.5925\n",
            "E2E-ABSA >>> 2022-08-17 16:38:37\n",
            "loss: 0.8812, acc: 0.5850\n",
            "E2E-ABSA >>> 2022-08-17 16:38:39\n",
            "loss: 0.8663, acc: 0.5994\n",
            "E2E-ABSA >>> 2022-08-17 16:38:41\n",
            ">>> val_acc: 0.5462, val_precision: 0.5462 val_recall: 0.5462, val_f1: 0.5462\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:42\n",
            "loss: 0.9028, acc: 0.5563\n",
            "E2E-ABSA >>> 2022-08-17 16:38:45\n",
            "loss: 0.8627, acc: 0.5981\n",
            "E2E-ABSA >>> 2022-08-17 16:38:47\n",
            "loss: 0.8550, acc: 0.6062\n",
            "E2E-ABSA >>> 2022-08-17 16:38:50\n",
            ">>> val_acc: 0.5654, val_precision: 0.5654 val_recall: 0.5654, val_f1: 0.5654\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5654\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:50\n",
            "loss: 0.8478, acc: 0.6214\n",
            "E2E-ABSA >>> 2022-08-17 16:38:53\n",
            "loss: 0.8581, acc: 0.6139\n",
            "E2E-ABSA >>> 2022-08-17 16:38:56\n",
            "loss: 0.8538, acc: 0.6066\n",
            "E2E-ABSA >>> 2022-08-17 16:38:58\n",
            ">>> val_acc: 0.5442, val_precision: 0.5442 val_recall: 0.5442, val_f1: 0.5442\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 16:38:59\n",
            "loss: 0.8282, acc: 0.6312\n",
            "E2E-ABSA >>> 2022-08-17 16:39:02\n",
            "loss: 0.8343, acc: 0.6254\n",
            "E2E-ABSA >>> 2022-08-17 16:39:04\n",
            "loss: 0.8402, acc: 0.6156\n",
            "E2E-ABSA >>> 2022-08-17 16:39:06\n",
            ">>> val_acc: 0.5827, val_precision: 0.5827 val_recall: 0.5827, val_f1: 0.5827\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 16:39:07\n",
            "loss: 0.8557, acc: 0.5986\n",
            "E2E-ABSA >>> 2022-08-17 16:39:10\n",
            "loss: 0.8503, acc: 0.6065\n",
            "E2E-ABSA >>> 2022-08-17 16:39:12\n",
            "loss: 0.8436, acc: 0.6125\n",
            "E2E-ABSA >>> 2022-08-17 16:39:14\n",
            ">>> val_acc: 0.5615, val_precision: 0.5615 val_recall: 0.5615, val_f1: 0.5615\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 16:39:15\n",
            "loss: 0.8217, acc: 0.6238\n",
            "E2E-ABSA >>> 2022-08-17 16:39:18\n",
            "loss: 0.8327, acc: 0.6233\n",
            "E2E-ABSA >>> 2022-08-17 16:39:20\n",
            "loss: 0.8356, acc: 0.6188\n",
            "E2E-ABSA >>> 2022-08-17 16:39:22\n",
            ">>> val_acc: 0.5635, val_precision: 0.5635 val_recall: 0.5635, val_f1: 0.5635\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 16:39:23\n",
            "loss: 0.7992, acc: 0.6455\n",
            "E2E-ABSA >>> 2022-08-17 16:39:26\n",
            "loss: 0.8364, acc: 0.6214\n",
            "E2E-ABSA >>> 2022-08-17 16:39:28\n",
            "loss: 0.8301, acc: 0.6225\n",
            "E2E-ABSA >>> 2022-08-17 16:39:30\n",
            ">>> val_acc: 0.5904, val_precision: 0.5904 val_recall: 0.5904, val_f1: 0.5904\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5904\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 16:39:31\n",
            "loss: 0.8397, acc: 0.6302\n",
            "E2E-ABSA >>> 2022-08-17 16:39:34\n",
            "loss: 0.8283, acc: 0.6262\n",
            "E2E-ABSA >>> 2022-08-17 16:39:36\n",
            "loss: 0.8243, acc: 0.6288\n",
            "E2E-ABSA >>> 2022-08-17 16:39:38\n",
            ">>> val_acc: 0.5769, val_precision: 0.5769 val_recall: 0.5769, val_f1: 0.5769\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 16:39:39\n",
            "loss: 0.8411, acc: 0.6279\n",
            "E2E-ABSA >>> 2022-08-17 16:39:42\n",
            "loss: 0.8323, acc: 0.6288\n",
            "E2E-ABSA >>> 2022-08-17 16:39:44\n",
            "loss: 0.8241, acc: 0.6292\n",
            "E2E-ABSA >>> 2022-08-17 16:39:46\n",
            ">>> val_acc: 0.5885, val_precision: 0.5885 val_recall: 0.5885, val_f1: 0.5885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 16:39:47\n",
            "loss: 0.8044, acc: 0.6330\n",
            "E2E-ABSA >>> 2022-08-17 16:39:50\n",
            "loss: 0.8154, acc: 0.6338\n",
            "E2E-ABSA >>> 2022-08-17 16:39:52\n",
            "loss: 0.8132, acc: 0.6347\n",
            "E2E-ABSA >>> 2022-08-17 16:39:54\n",
            ">>> val_acc: 0.5827, val_precision: 0.5827 val_recall: 0.5827, val_f1: 0.5827\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 16:39:55\n",
            "loss: 0.8224, acc: 0.6342\n",
            "E2E-ABSA >>> 2022-08-17 16:39:58\n",
            "loss: 0.8129, acc: 0.6332\n",
            "E2E-ABSA >>> 2022-08-17 16:40:00\n",
            "loss: 0.8150, acc: 0.6330\n",
            "E2E-ABSA >>> 2022-08-17 16:40:02\n",
            ">>> val_acc: 0.5923, val_precision: 0.5923 val_recall: 0.5923, val_f1: 0.5923\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5923\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:04\n",
            "loss: 0.8047, acc: 0.6414\n",
            "E2E-ABSA >>> 2022-08-17 16:40:06\n",
            "loss: 0.8047, acc: 0.6413\n",
            "E2E-ABSA >>> 2022-08-17 16:40:09\n",
            "loss: 0.8096, acc: 0.6371\n",
            "E2E-ABSA >>> 2022-08-17 16:40:10\n",
            ">>> val_acc: 0.5846, val_precision: 0.5846 val_recall: 0.5846, val_f1: 0.5846\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:12\n",
            "loss: 0.7916, acc: 0.6456\n",
            "E2E-ABSA >>> 2022-08-17 16:40:14\n",
            "loss: 0.8041, acc: 0.6392\n",
            "E2E-ABSA >>> 2022-08-17 16:40:17\n",
            "loss: 0.8101, acc: 0.6351\n",
            "E2E-ABSA >>> 2022-08-17 16:40:17\n",
            ">>> val_acc: 0.5962, val_precision: 0.5962 val_recall: 0.5962, val_f1: 0.5962\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5962\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:20\n",
            "loss: 0.8154, acc: 0.6299\n",
            "E2E-ABSA >>> 2022-08-17 16:40:22\n",
            "loss: 0.8011, acc: 0.6378\n",
            "E2E-ABSA >>> 2022-08-17 16:40:25\n",
            "loss: 0.8072, acc: 0.6373\n",
            "E2E-ABSA >>> 2022-08-17 16:40:25\n",
            ">>> val_acc: 0.5981, val_precision: 0.5981 val_recall: 0.5981, val_f1: 0.5981\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.5981\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:28\n",
            "loss: 0.8025, acc: 0.6401\n",
            "E2E-ABSA >>> 2022-08-17 16:40:30\n",
            "loss: 0.8032, acc: 0.6378\n",
            "E2E-ABSA >>> 2022-08-17 16:40:33\n",
            "loss: 0.8044, acc: 0.6379\n",
            "E2E-ABSA >>> 2022-08-17 16:40:33\n",
            ">>> val_acc: 0.6019, val_precision: 0.6019 val_recall: 0.6019, val_f1: 0.6019\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6019\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:36\n",
            "loss: 0.8032, acc: 0.6488\n",
            "E2E-ABSA >>> 2022-08-17 16:40:38\n",
            "loss: 0.7954, acc: 0.6506\n",
            "E2E-ABSA >>> 2022-08-17 16:40:41\n",
            ">>> val_acc: 0.6000, val_precision: 0.6000 val_recall: 0.6000, val_f1: 0.6000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:41\n",
            "loss: 0.8636, acc: 0.5875\n",
            "E2E-ABSA >>> 2022-08-17 16:40:44\n",
            "loss: 0.8041, acc: 0.6375\n",
            "E2E-ABSA >>> 2022-08-17 16:40:46\n",
            "loss: 0.8053, acc: 0.6393\n",
            "E2E-ABSA >>> 2022-08-17 16:40:49\n",
            ">>> val_acc: 0.5865, val_precision: 0.5865 val_recall: 0.5865, val_f1: 0.5865\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:49\n",
            "loss: 0.7980, acc: 0.6125\n",
            "E2E-ABSA >>> 2022-08-17 16:40:52\n",
            "loss: 0.8137, acc: 0.6188\n",
            "E2E-ABSA >>> 2022-08-17 16:40:54\n",
            "loss: 0.7982, acc: 0.6330\n",
            "E2E-ABSA >>> 2022-08-17 16:40:57\n",
            ">>> val_acc: 0.6077, val_precision: 0.6077 val_recall: 0.6077, val_f1: 0.6077\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6077\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 16:40:57\n",
            "loss: 0.7983, acc: 0.6208\n",
            "E2E-ABSA >>> 2022-08-17 16:41:00\n",
            "loss: 0.7911, acc: 0.6429\n",
            "E2E-ABSA >>> 2022-08-17 16:41:02\n",
            "loss: 0.7971, acc: 0.6369\n",
            "E2E-ABSA >>> 2022-08-17 16:41:05\n",
            ">>> val_acc: 0.6077, val_precision: 0.6077 val_recall: 0.6077, val_f1: 0.6077\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 16:41:05\n",
            "loss: 0.8257, acc: 0.6000\n",
            "E2E-ABSA >>> 2022-08-17 16:41:08\n",
            "loss: 0.7876, acc: 0.6443\n",
            "E2E-ABSA >>> 2022-08-17 16:41:10\n",
            "loss: 0.7915, acc: 0.6443\n",
            "E2E-ABSA >>> 2022-08-17 16:41:13\n",
            ">>> val_acc: 0.6038, val_precision: 0.6038 val_recall: 0.6038, val_f1: 0.6038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 16:41:13\n",
            "loss: 0.7856, acc: 0.6425\n",
            "E2E-ABSA >>> 2022-08-17 16:41:16\n",
            "loss: 0.8115, acc: 0.6305\n",
            "E2E-ABSA >>> 2022-08-17 16:41:18\n",
            "loss: 0.7972, acc: 0.6378\n",
            "E2E-ABSA >>> 2022-08-17 16:41:21\n",
            ">>> val_acc: 0.6000, val_precision: 0.6000 val_recall: 0.6000, val_f1: 0.6000\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 16:41:22\n",
            "loss: 0.7700, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-08-17 16:41:24\n",
            "loss: 0.7787, acc: 0.6428\n",
            "E2E-ABSA >>> 2022-08-17 16:41:27\n",
            "loss: 0.7867, acc: 0.6413\n",
            "E2E-ABSA >>> 2022-08-17 16:41:29\n",
            ">>> val_acc: 0.6058, val_precision: 0.6058 val_recall: 0.6058, val_f1: 0.6058\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 16:41:30\n",
            "loss: 0.7432, acc: 0.7018\n",
            "E2E-ABSA >>> 2022-08-17 16:41:32\n",
            "loss: 0.7951, acc: 0.6491\n",
            "E2E-ABSA >>> 2022-08-17 16:41:35\n",
            "loss: 0.7918, acc: 0.6452\n",
            "E2E-ABSA >>> 2022-08-17 16:41:37\n",
            ">>> val_acc: 0.6135, val_precision: 0.6135 val_recall: 0.6135, val_f1: 0.6135\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6135\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 16:41:38\n",
            "loss: 0.7936, acc: 0.6344\n",
            "E2E-ABSA >>> 2022-08-17 16:41:40\n",
            "loss: 0.7881, acc: 0.6496\n",
            "E2E-ABSA >>> 2022-08-17 16:41:43\n",
            "loss: 0.7866, acc: 0.6464\n",
            "E2E-ABSA >>> 2022-08-17 16:41:45\n",
            ">>> val_acc: 0.5981, val_precision: 0.5981 val_recall: 0.5981, val_f1: 0.5981\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 16:41:46\n",
            "loss: 0.7967, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-08-17 16:41:49\n",
            "loss: 0.7747, acc: 0.6582\n",
            "E2E-ABSA >>> 2022-08-17 16:41:51\n",
            "loss: 0.7820, acc: 0.6482\n",
            "E2E-ABSA >>> 2022-08-17 16:41:53\n",
            ">>> val_acc: 0.6096, val_precision: 0.6096 val_recall: 0.6096, val_f1: 0.6096\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 16:41:54\n",
            "loss: 0.8169, acc: 0.6188\n",
            "E2E-ABSA >>> 2022-08-17 16:41:57\n",
            "loss: 0.7958, acc: 0.6362\n",
            "E2E-ABSA >>> 2022-08-17 16:41:59\n",
            "loss: 0.7879, acc: 0.6435\n",
            "E2E-ABSA >>> 2022-08-17 16:42:01\n",
            ">>> val_acc: 0.6096, val_precision: 0.6096 val_recall: 0.6096, val_f1: 0.6096\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:02\n",
            "loss: 0.7767, acc: 0.6580\n",
            "E2E-ABSA >>> 2022-08-17 16:42:05\n",
            "loss: 0.7805, acc: 0.6464\n",
            "E2E-ABSA >>> 2022-08-17 16:42:07\n",
            "loss: 0.7833, acc: 0.6490\n",
            "E2E-ABSA >>> 2022-08-17 16:42:09\n",
            ">>> val_acc: 0.6154, val_precision: 0.6154 val_recall: 0.6154, val_f1: 0.6154\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6154\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:10\n",
            "loss: 0.7666, acc: 0.6469\n",
            "E2E-ABSA >>> 2022-08-17 16:42:13\n",
            "loss: 0.7866, acc: 0.6395\n",
            "E2E-ABSA >>> 2022-08-17 16:42:15\n",
            "loss: 0.7847, acc: 0.6478\n",
            "E2E-ABSA >>> 2022-08-17 16:42:17\n",
            ">>> val_acc: 0.6135, val_precision: 0.6135 val_recall: 0.6135, val_f1: 0.6135\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:18\n",
            "loss: 0.8001, acc: 0.6462\n",
            "E2E-ABSA >>> 2022-08-17 16:42:21\n",
            "loss: 0.7899, acc: 0.6443\n",
            "E2E-ABSA >>> 2022-08-17 16:42:23\n",
            "loss: 0.7819, acc: 0.6491\n",
            "E2E-ABSA >>> 2022-08-17 16:42:24\n",
            ">>> val_acc: 0.6077, val_precision: 0.6077 val_recall: 0.6077, val_f1: 0.6077\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:26\n",
            "loss: 0.7768, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:42:29\n",
            "loss: 0.7770, acc: 0.6537\n",
            "E2E-ABSA >>> 2022-08-17 16:42:31\n",
            "loss: 0.7798, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-08-17 16:42:32\n",
            ">>> val_acc: 0.6154, val_precision: 0.6154 val_recall: 0.6154, val_f1: 0.6154\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:34\n",
            "loss: 0.7884, acc: 0.6383\n",
            "E2E-ABSA >>> 2022-08-17 16:42:37\n",
            "loss: 0.7966, acc: 0.6318\n",
            "E2E-ABSA >>> 2022-08-17 16:42:39\n",
            "loss: 0.7842, acc: 0.6443\n",
            "E2E-ABSA >>> 2022-08-17 16:42:40\n",
            ">>> val_acc: 0.6115, val_precision: 0.6115 val_recall: 0.6115, val_f1: 0.6115\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:42\n",
            "loss: 0.7943, acc: 0.6344\n",
            "E2E-ABSA >>> 2022-08-17 16:42:45\n",
            "loss: 0.7874, acc: 0.6434\n",
            "E2E-ABSA >>> 2022-08-17 16:42:47\n",
            "loss: 0.7804, acc: 0.6471\n",
            "E2E-ABSA >>> 2022-08-17 16:42:48\n",
            ">>> val_acc: 0.6135, val_precision: 0.6135 val_recall: 0.6135, val_f1: 0.6135\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:50\n",
            "loss: 0.7798, acc: 0.6449\n",
            "E2E-ABSA >>> 2022-08-17 16:42:53\n",
            "loss: 0.7881, acc: 0.6372\n",
            "E2E-ABSA >>> 2022-08-17 16:42:55\n",
            "loss: 0.7823, acc: 0.6450\n",
            "E2E-ABSA >>> 2022-08-17 16:42:56\n",
            ">>> val_acc: 0.6135, val_precision: 0.6135 val_recall: 0.6135, val_f1: 0.6135\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 16:42:59\n",
            "loss: 0.7800, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 16:43:01\n",
            "loss: 0.7719, acc: 0.6533\n",
            "E2E-ABSA >>> 2022-08-17 16:43:04\n",
            "loss: 0.7786, acc: 0.6489\n",
            "E2E-ABSA >>> 2022-08-17 16:43:04\n",
            ">>> val_acc: 0.6173, val_precision: 0.6173 val_recall: 0.6173, val_f1: 0.6173\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6173\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 16:43:07\n",
            "loss: 0.7831, acc: 0.6388\n",
            "E2E-ABSA >>> 2022-08-17 16:43:09\n",
            "loss: 0.7806, acc: 0.6494\n",
            "E2E-ABSA >>> 2022-08-17 16:43:12\n",
            "loss: 0.7781, acc: 0.6492\n",
            "E2E-ABSA >>> 2022-08-17 16:43:12\n",
            ">>> val_acc: 0.6173, val_precision: 0.6173 val_recall: 0.6173, val_f1: 0.6173\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 16:43:15\n",
            "loss: 0.7726, acc: 0.6550\n",
            "E2E-ABSA >>> 2022-08-17 16:43:17\n",
            "loss: 0.7725, acc: 0.6559\n",
            "E2E-ABSA >>> 2022-08-17 16:43:20\n",
            ">>> val_acc: 0.6154, val_precision: 0.6154 val_recall: 0.6154, val_f1: 0.6154\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 16:43:20\n",
            "loss: 0.7821, acc: 0.6375\n",
            "E2E-ABSA >>> 2022-08-17 16:43:23\n",
            "loss: 0.7761, acc: 0.6482\n",
            "E2E-ABSA >>> 2022-08-17 16:43:25\n",
            "loss: 0.7734, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-08-17 16:43:28\n",
            ">>> val_acc: 0.6135, val_precision: 0.6135 val_recall: 0.6135, val_f1: 0.6135\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 16:43:28\n",
            "loss: 0.7648, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 16:43:31\n",
            "loss: 0.7745, acc: 0.6449\n",
            "E2E-ABSA >>> 2022-08-17 16:43:33\n",
            "loss: 0.7704, acc: 0.6512\n",
            "E2E-ABSA >>> 2022-08-17 16:43:36\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 16:43:36\n",
            "loss: 0.7903, acc: 0.6417\n",
            "E2E-ABSA >>> 2022-08-17 16:43:39\n",
            "loss: 0.7776, acc: 0.6516\n",
            "E2E-ABSA >>> 2022-08-17 16:43:41\n",
            "loss: 0.7779, acc: 0.6424\n",
            "E2E-ABSA >>> 2022-08-17 16:43:44\n",
            ">>> val_acc: 0.6154, val_precision: 0.6154 val_recall: 0.6154, val_f1: 0.6154\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 16:43:45\n",
            "loss: 0.7865, acc: 0.6781\n",
            "E2E-ABSA >>> 2022-08-17 16:43:47\n",
            "loss: 0.7651, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-08-17 16:43:50\n",
            "loss: 0.7722, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-08-17 16:43:52\n",
            ">>> val_acc: 0.6135, val_precision: 0.6135 val_recall: 0.6135, val_f1: 0.6135\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 16:43:53\n",
            "loss: 0.7315, acc: 0.6800\n",
            "E2E-ABSA >>> 2022-08-17 16:43:55\n",
            "loss: 0.7709, acc: 0.6475\n",
            "E2E-ABSA >>> 2022-08-17 16:43:58\n",
            "loss: 0.7751, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-08-17 16:44:00\n",
            ">>> val_acc: 0.6135, val_precision: 0.6135 val_recall: 0.6135, val_f1: 0.6135\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:01\n",
            "loss: 0.7899, acc: 0.6208\n",
            "E2E-ABSA >>> 2022-08-17 16:44:03\n",
            "loss: 0.7747, acc: 0.6466\n",
            "E2E-ABSA >>> 2022-08-17 16:44:06\n",
            "loss: 0.7689, acc: 0.6497\n",
            "E2E-ABSA >>> 2022-08-17 16:44:08\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:09\n",
            "loss: 0.7716, acc: 0.6411\n",
            "E2E-ABSA >>> 2022-08-17 16:44:11\n",
            "loss: 0.7685, acc: 0.6574\n",
            "E2E-ABSA >>> 2022-08-17 16:44:14\n",
            "loss: 0.7753, acc: 0.6532\n",
            "E2E-ABSA >>> 2022-08-17 16:44:16\n",
            ">>> val_acc: 0.6154, val_precision: 0.6154 val_recall: 0.6154, val_f1: 0.6154\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:17\n",
            "loss: 0.7498, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 16:44:19\n",
            "loss: 0.7625, acc: 0.6527\n",
            "E2E-ABSA >>> 2022-08-17 16:44:22\n",
            "loss: 0.7634, acc: 0.6539\n",
            "E2E-ABSA >>> 2022-08-17 16:44:24\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:25\n",
            "loss: 0.7501, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 16:44:27\n",
            "loss: 0.7698, acc: 0.6534\n",
            "E2E-ABSA >>> 2022-08-17 16:44:30\n",
            "loss: 0.7713, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-08-17 16:44:32\n",
            ">>> val_acc: 0.6212, val_precision: 0.6212 val_recall: 0.6212, val_f1: 0.6212\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6212\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:33\n",
            "loss: 0.7487, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 16:44:35\n",
            "loss: 0.7622, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-08-17 16:44:38\n",
            "loss: 0.7708, acc: 0.6560\n",
            "E2E-ABSA >>> 2022-08-17 16:44:40\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:41\n",
            "loss: 0.7480, acc: 0.6636\n",
            "E2E-ABSA >>> 2022-08-17 16:44:43\n",
            "loss: 0.7547, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-08-17 16:44:46\n",
            "loss: 0.7642, acc: 0.6591\n",
            "E2E-ABSA >>> 2022-08-17 16:44:47\n",
            ">>> val_acc: 0.6212, val_precision: 0.6212 val_recall: 0.6212, val_f1: 0.6212\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:49\n",
            "loss: 0.7826, acc: 0.6385\n",
            "E2E-ABSA >>> 2022-08-17 16:44:51\n",
            "loss: 0.7699, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-08-17 16:44:54\n",
            "loss: 0.7702, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-08-17 16:44:55\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 16:44:57\n",
            "loss: 0.8021, acc: 0.6365\n",
            "E2E-ABSA >>> 2022-08-17 16:44:59\n",
            "loss: 0.7693, acc: 0.6598\n",
            "E2E-ABSA >>> 2022-08-17 16:45:02\n",
            "loss: 0.7692, acc: 0.6559\n",
            "E2E-ABSA >>> 2022-08-17 16:45:03\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:05\n",
            "loss: 0.7631, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:45:07\n",
            "loss: 0.7683, acc: 0.6599\n",
            "E2E-ABSA >>> 2022-08-17 16:45:10\n",
            "loss: 0.7682, acc: 0.6565\n",
            "E2E-ABSA >>> 2022-08-17 16:45:11\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:13\n",
            "loss: 0.7831, acc: 0.6475\n",
            "E2E-ABSA >>> 2022-08-17 16:45:15\n",
            "loss: 0.7809, acc: 0.6479\n",
            "E2E-ABSA >>> 2022-08-17 16:45:18\n",
            "loss: 0.7683, acc: 0.6557\n",
            "E2E-ABSA >>> 2022-08-17 16:45:19\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:21\n",
            "loss: 0.7814, acc: 0.6508\n",
            "E2E-ABSA >>> 2022-08-17 16:45:23\n",
            "loss: 0.7674, acc: 0.6559\n",
            "E2E-ABSA >>> 2022-08-17 16:45:26\n",
            "loss: 0.7636, acc: 0.6538\n",
            "E2E-ABSA >>> 2022-08-17 16:45:27\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:29\n",
            "loss: 0.7605, acc: 0.6551\n",
            "E2E-ABSA >>> 2022-08-17 16:45:31\n",
            "loss: 0.7694, acc: 0.6473\n",
            "E2E-ABSA >>> 2022-08-17 16:45:34\n",
            "loss: 0.7673, acc: 0.6498\n",
            "E2E-ABSA >>> 2022-08-17 16:45:35\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:37\n",
            "loss: 0.7634, acc: 0.6542\n",
            "E2E-ABSA >>> 2022-08-17 16:45:39\n",
            "loss: 0.7726, acc: 0.6503\n",
            "E2E-ABSA >>> 2022-08-17 16:45:42\n",
            "loss: 0.7676, acc: 0.6567\n",
            "E2E-ABSA >>> 2022-08-17 16:45:43\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.625\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:45\n",
            "loss: 0.7623, acc: 0.6559\n",
            "E2E-ABSA >>> 2022-08-17 16:45:47\n",
            "loss: 0.7610, acc: 0.6558\n",
            "E2E-ABSA >>> 2022-08-17 16:45:50\n",
            "loss: 0.7659, acc: 0.6542\n",
            "E2E-ABSA >>> 2022-08-17 16:45:50\n",
            ">>> val_acc: 0.6269, val_precision: 0.6269 val_recall: 0.6269, val_f1: 0.6269\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6269\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:53\n",
            "loss: 0.7793, acc: 0.6381\n",
            "E2E-ABSA >>> 2022-08-17 16:45:55\n",
            "loss: 0.7693, acc: 0.6538\n",
            "E2E-ABSA >>> 2022-08-17 16:45:58\n",
            ">>> val_acc: 0.6269, val_precision: 0.6269 val_recall: 0.6269, val_f1: 0.6269\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 16:45:58\n",
            "loss: 0.8039, acc: 0.7375\n",
            "E2E-ABSA >>> 2022-08-17 16:46:01\n",
            "loss: 0.7507, acc: 0.6744\n",
            "E2E-ABSA >>> 2022-08-17 16:46:03\n",
            "loss: 0.7618, acc: 0.6616\n",
            "E2E-ABSA >>> 2022-08-17 16:46:07\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 16:46:07\n",
            "loss: 0.8047, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 16:46:09\n",
            "loss: 0.7598, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-08-17 16:46:12\n",
            "loss: 0.7657, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-08-17 16:46:14\n",
            ">>> val_acc: 0.6154, val_precision: 0.6154 val_recall: 0.6154, val_f1: 0.6154\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 16:46:15\n",
            "loss: 0.7263, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:46:17\n",
            "loss: 0.7634, acc: 0.6478\n",
            "E2E-ABSA >>> 2022-08-17 16:46:20\n",
            "loss: 0.7587, acc: 0.6596\n",
            "E2E-ABSA >>> 2022-08-17 16:46:22\n",
            ">>> val_acc: 0.6192, val_precision: 0.6192 val_recall: 0.6192, val_f1: 0.6192\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 16:46:23\n",
            "loss: 0.7561, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 16:46:25\n",
            "loss: 0.7516, acc: 0.6630\n",
            "E2E-ABSA >>> 2022-08-17 16:46:28\n",
            "loss: 0.7598, acc: 0.6594\n",
            "E2E-ABSA >>> 2022-08-17 16:46:30\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 16:46:31\n",
            "loss: 0.7268, acc: 0.6525\n",
            "E2E-ABSA >>> 2022-08-17 16:46:33\n",
            "loss: 0.7700, acc: 0.6525\n",
            "E2E-ABSA >>> 2022-08-17 16:46:36\n",
            "loss: 0.7642, acc: 0.6592\n",
            "E2E-ABSA >>> 2022-08-17 16:46:38\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 16:46:39\n",
            "loss: 0.7602, acc: 0.6854\n",
            "E2E-ABSA >>> 2022-08-17 16:46:41\n",
            "loss: 0.7686, acc: 0.6553\n",
            "E2E-ABSA >>> 2022-08-17 16:46:44\n",
            "loss: 0.7605, acc: 0.6592\n",
            "E2E-ABSA >>> 2022-08-17 16:46:46\n",
            ">>> val_acc: 0.6212, val_precision: 0.6212 val_recall: 0.6212, val_f1: 0.6212\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 16:46:47\n",
            "loss: 0.7853, acc: 0.6429\n",
            "E2E-ABSA >>> 2022-08-17 16:46:49\n",
            "loss: 0.7653, acc: 0.6606\n",
            "E2E-ABSA >>> 2022-08-17 16:46:52\n",
            "loss: 0.7618, acc: 0.6593\n",
            "E2E-ABSA >>> 2022-08-17 16:46:54\n",
            ">>> val_acc: 0.6212, val_precision: 0.6212 val_recall: 0.6212, val_f1: 0.6212\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 16:46:55\n",
            "loss: 0.7577, acc: 0.6547\n",
            "E2E-ABSA >>> 2022-08-17 16:46:57\n",
            "loss: 0.7613, acc: 0.6598\n",
            "E2E-ABSA >>> 2022-08-17 16:47:00\n",
            "loss: 0.7582, acc: 0.6630\n",
            "E2E-ABSA >>> 2022-08-17 16:47:02\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:03\n",
            "loss: 0.7978, acc: 0.6431\n",
            "E2E-ABSA >>> 2022-08-17 16:47:05\n",
            "loss: 0.7799, acc: 0.6466\n",
            "E2E-ABSA >>> 2022-08-17 16:47:08\n",
            "loss: 0.7636, acc: 0.6554\n",
            "E2E-ABSA >>> 2022-08-17 16:47:10\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:11\n",
            "loss: 0.7725, acc: 0.6575\n",
            "E2E-ABSA >>> 2022-08-17 16:47:13\n",
            "loss: 0.7497, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 16:47:16\n",
            "loss: 0.7591, acc: 0.6595\n",
            "E2E-ABSA >>> 2022-08-17 16:47:17\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:19\n",
            "loss: 0.7515, acc: 0.6648\n",
            "E2E-ABSA >>> 2022-08-17 16:47:21\n",
            "loss: 0.7562, acc: 0.6617\n",
            "E2E-ABSA >>> 2022-08-17 16:47:24\n",
            "loss: 0.7617, acc: 0.6586\n",
            "E2E-ABSA >>> 2022-08-17 16:47:25\n",
            ">>> val_acc: 0.6212, val_precision: 0.6212 val_recall: 0.6212, val_f1: 0.6212\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:27\n",
            "loss: 0.7531, acc: 0.6740\n",
            "E2E-ABSA >>> 2022-08-17 16:47:29\n",
            "loss: 0.7546, acc: 0.6633\n",
            "E2E-ABSA >>> 2022-08-17 16:47:32\n",
            "loss: 0.7570, acc: 0.6577\n",
            "E2E-ABSA >>> 2022-08-17 16:47:33\n",
            ">>> val_acc: 0.6212, val_precision: 0.6212 val_recall: 0.6212, val_f1: 0.6212\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:35\n",
            "loss: 0.7777, acc: 0.6529\n",
            "E2E-ABSA >>> 2022-08-17 16:47:37\n",
            "loss: 0.7661, acc: 0.6644\n",
            "E2E-ABSA >>> 2022-08-17 16:47:40\n",
            "loss: 0.7626, acc: 0.6606\n",
            "E2E-ABSA >>> 2022-08-17 16:47:41\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:43\n",
            "loss: 0.7533, acc: 0.6661\n",
            "E2E-ABSA >>> 2022-08-17 16:47:45\n",
            "loss: 0.7608, acc: 0.6599\n",
            "E2E-ABSA >>> 2022-08-17 16:47:48\n",
            "loss: 0.7645, acc: 0.6590\n",
            "E2E-ABSA >>> 2022-08-17 16:47:49\n",
            ">>> val_acc: 0.6269, val_precision: 0.6269 val_recall: 0.6269, val_f1: 0.6269\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:51\n",
            "loss: 0.7507, acc: 0.6767\n",
            "E2E-ABSA >>> 2022-08-17 16:47:53\n",
            "loss: 0.7662, acc: 0.6654\n",
            "E2E-ABSA >>> 2022-08-17 16:47:56\n",
            "loss: 0.7596, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-08-17 16:47:57\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 16:47:59\n",
            "loss: 0.7658, acc: 0.6547\n",
            "E2E-ABSA >>> 2022-08-17 16:48:01\n",
            "loss: 0.7611, acc: 0.6628\n",
            "E2E-ABSA >>> 2022-08-17 16:48:04\n",
            "loss: 0.7611, acc: 0.6600\n",
            "E2E-ABSA >>> 2022-08-17 16:48:05\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 16:48:07\n",
            "loss: 0.7611, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:48:09\n",
            "loss: 0.7579, acc: 0.6632\n",
            "E2E-ABSA >>> 2022-08-17 16:48:12\n",
            "loss: 0.7578, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-08-17 16:48:13\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 16:48:15\n",
            "loss: 0.7538, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:48:17\n",
            "loss: 0.7542, acc: 0.6635\n",
            "E2E-ABSA >>> 2022-08-17 16:48:20\n",
            "loss: 0.7579, acc: 0.6586\n",
            "E2E-ABSA >>> 2022-08-17 16:48:20\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 16:48:23\n",
            "loss: 0.7527, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-08-17 16:48:25\n",
            "loss: 0.7603, acc: 0.6654\n",
            "E2E-ABSA >>> 2022-08-17 16:48:28\n",
            "loss: 0.7581, acc: 0.6657\n",
            "E2E-ABSA >>> 2022-08-17 16:48:28\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 16:48:31\n",
            "loss: 0.7554, acc: 0.6519\n",
            "E2E-ABSA >>> 2022-08-17 16:48:34\n",
            "loss: 0.7574, acc: 0.6587\n",
            "E2E-ABSA >>> 2022-08-17 16:48:37\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 16:48:37\n",
            "loss: 0.7319, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 16:48:39\n",
            "loss: 0.7461, acc: 0.6661\n",
            "E2E-ABSA >>> 2022-08-17 16:48:42\n",
            "loss: 0.7599, acc: 0.6668\n",
            "E2E-ABSA >>> 2022-08-17 16:48:44\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 16:48:45\n",
            "loss: 0.7294, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 16:48:47\n",
            "loss: 0.7509, acc: 0.6602\n",
            "E2E-ABSA >>> 2022-08-17 16:48:50\n",
            "loss: 0.7545, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-08-17 16:48:52\n",
            ">>> val_acc: 0.6327, val_precision: 0.6327 val_recall: 0.6327, val_f1: 0.6327\n",
            ">> saved: state_dict/cabasc_acl14shortdata_val_f1_0.6327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 16:48:53\n",
            "loss: 0.6993, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 16:48:55\n",
            "loss: 0.7592, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-08-17 16:48:58\n",
            "loss: 0.7513, acc: 0.6689\n",
            "E2E-ABSA >>> 2022-08-17 16:49:00\n",
            ">>> val_acc: 0.6269, val_precision: 0.6269 val_recall: 0.6269, val_f1: 0.6269\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:01\n",
            "loss: 0.7551, acc: 0.6531\n",
            "E2E-ABSA >>> 2022-08-17 16:49:03\n",
            "loss: 0.7416, acc: 0.6729\n",
            "E2E-ABSA >>> 2022-08-17 16:49:06\n",
            "loss: 0.7571, acc: 0.6651\n",
            "E2E-ABSA >>> 2022-08-17 16:49:08\n",
            ">>> val_acc: 0.6269, val_precision: 0.6269 val_recall: 0.6269, val_f1: 0.6269\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:09\n",
            "loss: 0.7324, acc: 0.6700\n",
            "E2E-ABSA >>> 2022-08-17 16:49:11\n",
            "loss: 0.7423, acc: 0.6760\n",
            "E2E-ABSA >>> 2022-08-17 16:49:14\n",
            "loss: 0.7559, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:49:16\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:17\n",
            "loss: 0.7543, acc: 0.6729\n",
            "E2E-ABSA >>> 2022-08-17 16:49:19\n",
            "loss: 0.7554, acc: 0.6620\n",
            "E2E-ABSA >>> 2022-08-17 16:49:22\n",
            "loss: 0.7553, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:49:24\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:25\n",
            "loss: 0.7633, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-08-17 16:49:27\n",
            "loss: 0.7469, acc: 0.6727\n",
            "E2E-ABSA >>> 2022-08-17 16:49:30\n",
            "loss: 0.7557, acc: 0.6665\n",
            "E2E-ABSA >>> 2022-08-17 16:49:32\n",
            ">>> val_acc: 0.6212, val_precision: 0.6212 val_recall: 0.6212, val_f1: 0.6212\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:33\n",
            "loss: 0.7547, acc: 0.6547\n",
            "E2E-ABSA >>> 2022-08-17 16:49:35\n",
            "loss: 0.7523, acc: 0.6612\n",
            "E2E-ABSA >>> 2022-08-17 16:49:38\n",
            "loss: 0.7554, acc: 0.6617\n",
            "E2E-ABSA >>> 2022-08-17 16:49:40\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:41\n",
            "loss: 0.7544, acc: 0.6639\n",
            "E2E-ABSA >>> 2022-08-17 16:49:43\n",
            "loss: 0.7569, acc: 0.6690\n",
            "E2E-ABSA >>> 2022-08-17 16:49:46\n",
            "loss: 0.7554, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-08-17 16:49:48\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:49\n",
            "loss: 0.7586, acc: 0.6587\n",
            "E2E-ABSA >>> 2022-08-17 16:49:51\n",
            "loss: 0.7648, acc: 0.6583\n",
            "E2E-ABSA >>> 2022-08-17 16:49:54\n",
            "loss: 0.7596, acc: 0.6600\n",
            "E2E-ABSA >>> 2022-08-17 16:49:55\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 16:49:57\n",
            "loss: 0.7604, acc: 0.6534\n",
            "E2E-ABSA >>> 2022-08-17 16:49:59\n",
            "loss: 0.7448, acc: 0.6718\n",
            "E2E-ABSA >>> 2022-08-17 16:50:02\n",
            "loss: 0.7515, acc: 0.6647\n",
            "E2E-ABSA >>> 2022-08-17 16:50:03\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n",
            "E2E-ABSA >>> 2022-08-17 16:50:05\n",
            "loss: 0.7608, acc: 0.6750\n",
            "E2E-ABSA >>> 2022-08-17 16:50:07\n",
            "loss: 0.7563, acc: 0.6656\n",
            "E2E-ABSA >>> 2022-08-17 16:50:10\n",
            "loss: 0.7557, acc: 0.6637\n",
            "E2E-ABSA >>> 2022-08-17 16:50:11\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 93.\n",
            "E2E-ABSA >>> 2022-08-17 16:50:13\n",
            "loss: 0.7598, acc: 0.6567\n",
            "E2E-ABSA >>> 2022-08-17 16:50:15\n",
            "loss: 0.7649, acc: 0.6598\n",
            "E2E-ABSA >>> 2022-08-17 16:50:18\n",
            "loss: 0.7580, acc: 0.6637\n",
            "E2E-ABSA >>> 2022-08-17 16:50:19\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 94.\n",
            "E2E-ABSA >>> 2022-08-17 16:50:21\n",
            "loss: 0.7577, acc: 0.6598\n",
            "E2E-ABSA >>> 2022-08-17 16:50:23\n",
            "loss: 0.7421, acc: 0.6706\n",
            "E2E-ABSA >>> 2022-08-17 16:50:26\n",
            "loss: 0.7524, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-08-17 16:50:27\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 95.\n",
            "E2E-ABSA >>> 2022-08-17 16:50:29\n",
            "loss: 0.7641, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 16:50:31\n",
            "loss: 0.7570, acc: 0.6636\n",
            "E2E-ABSA >>> 2022-08-17 16:50:34\n",
            "loss: 0.7543, acc: 0.6650\n",
            "E2E-ABSA >>> 2022-08-17 16:50:35\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 96.\n",
            "E2E-ABSA >>> 2022-08-17 16:50:37\n",
            "loss: 0.7610, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-08-17 16:50:39\n",
            "loss: 0.7446, acc: 0.6656\n",
            "E2E-ABSA >>> 2022-08-17 16:50:41\n",
            "loss: 0.7505, acc: 0.6643\n",
            "E2E-ABSA >>> 2022-08-17 16:50:42\n",
            ">>> val_acc: 0.6231, val_precision: 0.6231 val_recall: 0.6231, val_f1: 0.6231\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 97.\n",
            "E2E-ABSA >>> 2022-08-17 16:50:45\n",
            "loss: 0.7357, acc: 0.6809\n",
            "E2E-ABSA >>> 2022-08-17 16:50:47\n",
            "loss: 0.7448, acc: 0.6723\n",
            "E2E-ABSA >>> 2022-08-17 16:50:50\n",
            "loss: 0.7506, acc: 0.6660\n",
            "E2E-ABSA >>> 2022-08-17 16:50:51\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 98.\n",
            "E2E-ABSA >>> 2022-08-17 16:50:53\n",
            "loss: 0.7541, acc: 0.6660\n",
            "E2E-ABSA >>> 2022-08-17 16:50:55\n",
            "loss: 0.7613, acc: 0.6635\n",
            "E2E-ABSA >>> 2022-08-17 16:50:58\n",
            "loss: 0.7538, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-08-17 16:50:59\n",
            ">>> val_acc: 0.6250, val_precision: 0.6250 val_recall: 0.6250, val_f1: 0.6250\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 99.\n",
            "E2E-ABSA >>> 2022-08-17 16:51:01\n",
            "loss: 0.7439, acc: 0.6724\n",
            "E2E-ABSA >>> 2022-08-17 16:51:03\n",
            "loss: 0.7538, acc: 0.6638\n",
            "E2E-ABSA >>> 2022-08-17 16:51:06\n",
            "loss: 0.7536, acc: 0.6642\n",
            "E2E-ABSA >>> 2022-08-17 16:51:06\n",
            ">>> val_acc: 0.6269, val_precision: 0.6269 val_recall: 0.6269, val_f1: 0.6269\n",
            "you can download the best model from state_dict/cabasc_acl14shortdata_val_f1_0.6327\n",
            ">>> test_acc: 0.6327, test_precision: 0.6327, test_recall: 0.6327, test_f1: 0.6327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 增加字典知识后：Training **acl14shortdata** dataset on model(**CABASC**)\n"
      ],
      "metadata": {
        "id": "EXZVsmNUXy13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && pwd && ls && python3 train.py --model_name cabasc --dataset acl14shortdata_know --embed_dim 300 --patience 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJkUxSOIXy8w",
        "outputId": "c58736c6-3616-4d19-d6a6-3d0dab1262a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/DictionaryFused-E2E-ABSA\n",
            "datasets\t    glove_embeddings  models\t   requirements.txt\n",
            "data_utils.py\t    infer_example.py  __pycache__  train.py\n",
            "deberta_abas.ipynb  layers\t      README.md\n",
            ">>> 使用设备:cuda 训练.\n",
            "加载预训练向量...\n",
            ">>> 使用 ./glove_embeddings/glove.42B.300d.txt 作为预训练单词的向量.\n",
            "预训练向量加载完毕.\n",
            "> training dataset count: 4917.\n",
            "> testing dataset count: 539.\n",
            "cuda memory allocated: 22565888\n",
            "> n_trainable_params: 1446005, n_nontrainable_params: 4115100\n",
            "> training arguments:\n",
            ">>> model_name: cabasc\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f2845cc1b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 30\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.cabasc.Cabasc'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 16:51:51\n",
            "loss: 1.0893, acc: 0.3856\n",
            "E2E-ABSA >>> 2022-08-17 16:51:57\n",
            "loss: 1.0588, acc: 0.4425\n",
            "E2E-ABSA >>> 2022-08-17 16:52:02\n",
            "loss: 1.0516, acc: 0.4567\n",
            "E2E-ABSA >>> 2022-08-17 16:52:04\n",
            ">>> val_acc: 0.4601, val_precision: 0.4601 val_recall: 0.4601, val_f1: 0.4601\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.4601\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 16:52:09\n",
            "loss: 1.0085, acc: 0.5190\n",
            "E2E-ABSA >>> 2022-08-17 16:52:15\n",
            "loss: 1.0107, acc: 0.5091\n",
            "E2E-ABSA >>> 2022-08-17 16:52:20\n",
            "loss: 1.0062, acc: 0.5056\n",
            "E2E-ABSA >>> 2022-08-17 16:52:22\n",
            ">>> val_acc: 0.4638, val_precision: 0.4638 val_recall: 0.4638, val_f1: 0.4638\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.4638\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 16:52:27\n",
            "loss: 0.9610, acc: 0.5387\n",
            "E2E-ABSA >>> 2022-08-17 16:52:33\n",
            "loss: 0.9547, acc: 0.5394\n",
            "E2E-ABSA >>> 2022-08-17 16:52:38\n",
            "loss: 0.9379, acc: 0.5541\n",
            "E2E-ABSA >>> 2022-08-17 16:52:41\n",
            ">>> val_acc: 0.5603, val_precision: 0.5603 val_recall: 0.5603, val_f1: 0.5603\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5603\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 16:52:45\n",
            "loss: 0.8908, acc: 0.5995\n",
            "E2E-ABSA >>> 2022-08-17 16:52:51\n",
            "loss: 0.8741, acc: 0.6147\n",
            "E2E-ABSA >>> 2022-08-17 16:52:56\n",
            "loss: 0.8823, acc: 0.5994\n",
            "E2E-ABSA >>> 2022-08-17 16:52:59\n",
            ">>> val_acc: 0.5547, val_precision: 0.5547 val_recall: 0.5547, val_f1: 0.5547\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 16:53:03\n",
            "loss: 0.8621, acc: 0.6158\n",
            "E2E-ABSA >>> 2022-08-17 16:53:09\n",
            "loss: 0.8617, acc: 0.6034\n",
            "E2E-ABSA >>> 2022-08-17 16:53:15\n",
            "loss: 0.8605, acc: 0.6035\n",
            "E2E-ABSA >>> 2022-08-17 16:53:18\n",
            ">>> val_acc: 0.5640, val_precision: 0.5640 val_recall: 0.5640, val_f1: 0.5640\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.564\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 16:53:22\n",
            "loss: 0.8679, acc: 0.6031\n",
            "E2E-ABSA >>> 2022-08-17 16:53:27\n",
            "loss: 0.8522, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 16:53:33\n",
            "loss: 0.8476, acc: 0.6147\n",
            "E2E-ABSA >>> 2022-08-17 16:53:37\n",
            ">>> val_acc: 0.5659, val_precision: 0.5659 val_recall: 0.5659, val_f1: 0.5659\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 16:53:40\n",
            "loss: 0.8465, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 16:53:45\n",
            "loss: 0.8345, acc: 0.6271\n",
            "E2E-ABSA >>> 2022-08-17 16:53:51\n",
            "loss: 0.8366, acc: 0.6220\n",
            "E2E-ABSA >>> 2022-08-17 16:53:55\n",
            ">>> val_acc: 0.5696, val_precision: 0.5696 val_recall: 0.5696, val_f1: 0.5696\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5696\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 16:53:58\n",
            "loss: 0.8566, acc: 0.6051\n",
            "E2E-ABSA >>> 2022-08-17 16:54:03\n",
            "loss: 0.8232, acc: 0.6267\n",
            "E2E-ABSA >>> 2022-08-17 16:54:09\n",
            "loss: 0.8285, acc: 0.6217\n",
            "E2E-ABSA >>> 2022-08-17 16:54:14\n",
            ">>> val_acc: 0.5733, val_precision: 0.5733 val_recall: 0.5733, val_f1: 0.5733\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5733\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 16:54:16\n",
            "loss: 0.8224, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 16:54:21\n",
            "loss: 0.8214, acc: 0.6333\n",
            "E2E-ABSA >>> 2022-08-17 16:54:27\n",
            "loss: 0.8226, acc: 0.6290\n",
            "E2E-ABSA >>> 2022-08-17 16:54:32\n",
            ">>> val_acc: 0.5788, val_precision: 0.5788 val_recall: 0.5788, val_f1: 0.5788\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5788\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 16:54:34\n",
            "loss: 0.8232, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 16:54:40\n",
            "loss: 0.8267, acc: 0.6333\n",
            "E2E-ABSA >>> 2022-08-17 16:54:45\n",
            "loss: 0.8233, acc: 0.6297\n",
            "E2E-ABSA >>> 2022-08-17 16:54:51\n",
            ">>> val_acc: 0.5881, val_precision: 0.5881 val_recall: 0.5881, val_f1: 0.5881\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 16:54:52\n",
            "loss: 0.8075, acc: 0.6312\n",
            "E2E-ABSA >>> 2022-08-17 16:54:58\n",
            "loss: 0.8157, acc: 0.6297\n",
            "E2E-ABSA >>> 2022-08-17 16:55:03\n",
            "loss: 0.8142, acc: 0.6347\n",
            "E2E-ABSA >>> 2022-08-17 16:55:09\n",
            ">>> val_acc: 0.5881, val_precision: 0.5881 val_recall: 0.5881, val_f1: 0.5881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 16:55:10\n",
            "loss: 0.8663, acc: 0.5677\n",
            "E2E-ABSA >>> 2022-08-17 16:55:16\n",
            "loss: 0.8082, acc: 0.6390\n",
            "E2E-ABSA >>> 2022-08-17 16:55:21\n",
            "loss: 0.8102, acc: 0.6368\n",
            "E2E-ABSA >>> 2022-08-17 16:55:28\n",
            ">>> val_acc: 0.5937, val_precision: 0.5937 val_recall: 0.5937, val_f1: 0.5937\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5937\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 16:55:28\n",
            "loss: 0.8029, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 16:55:34\n",
            "loss: 0.7927, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 16:55:40\n",
            "loss: 0.7921, acc: 0.6569\n",
            "E2E-ABSA >>> 2022-08-17 16:55:45\n",
            "loss: 0.8014, acc: 0.6435\n",
            "E2E-ABSA >>> 2022-08-17 16:55:47\n",
            ">>> val_acc: 0.5900, val_precision: 0.5900 val_recall: 0.5900, val_f1: 0.5900\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 16:55:52\n",
            "loss: 0.8023, acc: 0.6419\n",
            "E2E-ABSA >>> 2022-08-17 16:55:58\n",
            "loss: 0.8008, acc: 0.6413\n",
            "E2E-ABSA >>> 2022-08-17 16:56:03\n",
            "loss: 0.7963, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-08-17 16:56:05\n",
            ">>> val_acc: 0.5863, val_precision: 0.5863 val_recall: 0.5863, val_f1: 0.5863\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 16:56:10\n",
            "loss: 0.8075, acc: 0.6307\n",
            "E2E-ABSA >>> 2022-08-17 16:56:16\n",
            "loss: 0.7899, acc: 0.6449\n",
            "E2E-ABSA >>> 2022-08-17 16:56:21\n",
            "loss: 0.7947, acc: 0.6469\n",
            "E2E-ABSA >>> 2022-08-17 16:56:24\n",
            ">>> val_acc: 0.5863, val_precision: 0.5863 val_recall: 0.5863, val_f1: 0.5863\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 16:56:28\n",
            "loss: 0.7908, acc: 0.6430\n",
            "E2E-ABSA >>> 2022-08-17 16:56:34\n",
            "loss: 0.7904, acc: 0.6486\n",
            "E2E-ABSA >>> 2022-08-17 16:56:39\n",
            "loss: 0.7933, acc: 0.6464\n",
            "E2E-ABSA >>> 2022-08-17 16:56:42\n",
            ">>> val_acc: 0.5937, val_precision: 0.5937 val_recall: 0.5937, val_f1: 0.5937\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 16:56:46\n",
            "loss: 0.7682, acc: 0.6649\n",
            "E2E-ABSA >>> 2022-08-17 16:56:52\n",
            "loss: 0.7799, acc: 0.6548\n",
            "E2E-ABSA >>> 2022-08-17 16:56:57\n",
            "loss: 0.7866, acc: 0.6503\n",
            "E2E-ABSA >>> 2022-08-17 16:57:01\n",
            ">>> val_acc: 0.5974, val_precision: 0.5974 val_recall: 0.5974, val_f1: 0.5974\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.5974\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 16:57:04\n",
            "loss: 0.7984, acc: 0.6455\n",
            "E2E-ABSA >>> 2022-08-17 16:57:10\n",
            "loss: 0.7822, acc: 0.6543\n",
            "E2E-ABSA >>> 2022-08-17 16:57:15\n",
            "loss: 0.7815, acc: 0.6558\n",
            "E2E-ABSA >>> 2022-08-17 16:57:19\n",
            ">>> val_acc: 0.5918, val_precision: 0.5918 val_recall: 0.5918, val_f1: 0.5918\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 16:57:22\n",
            "loss: 0.7764, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 16:57:28\n",
            "loss: 0.7818, acc: 0.6522\n",
            "E2E-ABSA >>> 2022-08-17 16:57:33\n",
            "loss: 0.7809, acc: 0.6497\n",
            "E2E-ABSA >>> 2022-08-17 16:57:38\n",
            ">>> val_acc: 0.6067, val_precision: 0.6067 val_recall: 0.6067, val_f1: 0.6067\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6067\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 16:57:40\n",
            "loss: 0.7870, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-08-17 16:57:46\n",
            "loss: 0.7822, acc: 0.6486\n",
            "E2E-ABSA >>> 2022-08-17 16:57:51\n",
            "loss: 0.7770, acc: 0.6517\n",
            "E2E-ABSA >>> 2022-08-17 16:57:56\n",
            ">>> val_acc: 0.5993, val_precision: 0.5993 val_recall: 0.5993, val_f1: 0.5993\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 16:57:59\n",
            "loss: 0.8228, acc: 0.6219\n",
            "E2E-ABSA >>> 2022-08-17 16:58:04\n",
            "loss: 0.7820, acc: 0.6442\n",
            "E2E-ABSA >>> 2022-08-17 16:58:10\n",
            "loss: 0.7819, acc: 0.6518\n",
            "E2E-ABSA >>> 2022-08-17 16:58:15\n",
            ">>> val_acc: 0.5918, val_precision: 0.5918 val_recall: 0.5918, val_f1: 0.5918\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 16:58:17\n",
            "loss: 0.7957, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 16:58:22\n",
            "loss: 0.7709, acc: 0.6605\n",
            "E2E-ABSA >>> 2022-08-17 16:58:28\n",
            "loss: 0.7709, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 16:58:33\n",
            ">>> val_acc: 0.6067, val_precision: 0.6067 val_recall: 0.6067, val_f1: 0.6067\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-08-17 16:58:35\n",
            "loss: 0.7668, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-08-17 16:58:40\n",
            "loss: 0.7884, acc: 0.6467\n",
            "E2E-ABSA >>> 2022-08-17 16:58:46\n",
            "loss: 0.7699, acc: 0.6557\n",
            "E2E-ABSA >>> 2022-08-17 16:58:52\n",
            ">>> val_acc: 0.5974, val_precision: 0.5974 val_recall: 0.5974, val_f1: 0.5974\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-08-17 16:58:53\n",
            "loss: 0.7840, acc: 0.6367\n",
            "E2E-ABSA >>> 2022-08-17 16:58:58\n",
            "loss: 0.7818, acc: 0.6503\n",
            "E2E-ABSA >>> 2022-08-17 16:59:04\n",
            "loss: 0.7736, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 16:59:10\n",
            ">>> val_acc: 0.6085, val_precision: 0.6085 val_recall: 0.6085, val_f1: 0.6085\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-08-17 16:59:11\n",
            "loss: 0.7658, acc: 0.6797\n",
            "E2E-ABSA >>> 2022-08-17 16:59:16\n",
            "loss: 0.7602, acc: 0.6730\n",
            "E2E-ABSA >>> 2022-08-17 16:59:22\n",
            "loss: 0.7754, acc: 0.6593\n",
            "E2E-ABSA >>> 2022-08-17 16:59:27\n",
            "loss: 0.7711, acc: 0.6565\n",
            "E2E-ABSA >>> 2022-08-17 16:59:29\n",
            ">>> val_acc: 0.5974, val_precision: 0.5974 val_recall: 0.5974, val_f1: 0.5974\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-08-17 16:59:34\n",
            "loss: 0.7827, acc: 0.6394\n",
            "E2E-ABSA >>> 2022-08-17 16:59:40\n",
            "loss: 0.7707, acc: 0.6556\n",
            "E2E-ABSA >>> 2022-08-17 16:59:45\n",
            "loss: 0.7706, acc: 0.6544\n",
            "E2E-ABSA >>> 2022-08-17 16:59:47\n",
            ">>> val_acc: 0.6085, val_precision: 0.6085 val_recall: 0.6085, val_f1: 0.6085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-08-17 16:59:52\n",
            "loss: 0.7530, acc: 0.6678\n",
            "E2E-ABSA >>> 2022-08-17 16:59:58\n",
            "loss: 0.7589, acc: 0.6628\n",
            "E2E-ABSA >>> 2022-08-17 17:00:03\n",
            "loss: 0.7666, acc: 0.6590\n",
            "E2E-ABSA >>> 2022-08-17 17:00:06\n",
            ">>> val_acc: 0.6067, val_precision: 0.6067 val_recall: 0.6067, val_f1: 0.6067\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-08-17 17:00:10\n",
            "loss: 0.7626, acc: 0.6496\n",
            "E2E-ABSA >>> 2022-08-17 17:00:16\n",
            "loss: 0.7661, acc: 0.6566\n",
            "E2E-ABSA >>> 2022-08-17 17:00:22\n",
            "loss: 0.7671, acc: 0.6574\n",
            "E2E-ABSA >>> 2022-08-17 17:00:25\n",
            ">>> val_acc: 0.6141, val_precision: 0.6141 val_recall: 0.6141, val_f1: 0.6141\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6141\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-08-17 17:00:29\n",
            "loss: 0.7640, acc: 0.6513\n",
            "E2E-ABSA >>> 2022-08-17 17:00:34\n",
            "loss: 0.7621, acc: 0.6591\n",
            "E2E-ABSA >>> 2022-08-17 17:00:40\n",
            "loss: 0.7661, acc: 0.6558\n",
            "E2E-ABSA >>> 2022-08-17 17:00:43\n",
            ">>> val_acc: 0.6067, val_precision: 0.6067 val_recall: 0.6067, val_f1: 0.6067\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-08-17 17:00:47\n",
            "loss: 0.7457, acc: 0.6801\n",
            "E2E-ABSA >>> 2022-08-17 17:00:52\n",
            "loss: 0.7584, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-08-17 17:00:58\n",
            "loss: 0.7649, acc: 0.6572\n",
            "E2E-ABSA >>> 2022-08-17 17:01:02\n",
            ">>> val_acc: 0.6011, val_precision: 0.6011 val_recall: 0.6011, val_f1: 0.6011\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-08-17 17:01:05\n",
            "loss: 0.7902, acc: 0.6448\n",
            "E2E-ABSA >>> 2022-08-17 17:01:10\n",
            "loss: 0.7640, acc: 0.6605\n",
            "E2E-ABSA >>> 2022-08-17 17:01:16\n",
            "loss: 0.7584, acc: 0.6618\n",
            "E2E-ABSA >>> 2022-08-17 17:01:20\n",
            ">>> val_acc: 0.6178, val_precision: 0.6178 val_recall: 0.6178, val_f1: 0.6178\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6178\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-08-17 17:01:23\n",
            "loss: 0.7510, acc: 0.6611\n",
            "E2E-ABSA >>> 2022-08-17 17:01:28\n",
            "loss: 0.7525, acc: 0.6690\n",
            "E2E-ABSA >>> 2022-08-17 17:01:34\n",
            "loss: 0.7555, acc: 0.6629\n",
            "E2E-ABSA >>> 2022-08-17 17:01:39\n",
            ">>> val_acc: 0.6178, val_precision: 0.6178 val_recall: 0.6178, val_f1: 0.6178\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-08-17 17:01:41\n",
            "loss: 0.7337, acc: 0.6790\n",
            "E2E-ABSA >>> 2022-08-17 17:01:47\n",
            "loss: 0.7485, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-08-17 17:01:52\n",
            "loss: 0.7628, acc: 0.6557\n",
            "E2E-ABSA >>> 2022-08-17 17:01:57\n",
            ">>> val_acc: 0.6197, val_precision: 0.6197 val_recall: 0.6197, val_f1: 0.6197\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6197\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-08-17 17:01:59\n",
            "loss: 0.6925, acc: 0.7170\n",
            "E2E-ABSA >>> 2022-08-17 17:02:05\n",
            "loss: 0.7371, acc: 0.6737\n",
            "E2E-ABSA >>> 2022-08-17 17:02:10\n",
            "loss: 0.7536, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-08-17 17:02:15\n",
            ">>> val_acc: 0.6085, val_precision: 0.6085 val_recall: 0.6085, val_f1: 0.6085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-08-17 17:02:17\n",
            "loss: 0.7381, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-08-17 17:02:22\n",
            "loss: 0.7685, acc: 0.6479\n",
            "E2E-ABSA >>> 2022-08-17 17:02:28\n",
            "loss: 0.7592, acc: 0.6557\n",
            "E2E-ABSA >>> 2022-08-17 17:02:34\n",
            ">>> val_acc: 0.6160, val_precision: 0.6160 val_recall: 0.6160, val_f1: 0.6160\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-08-17 17:02:35\n",
            "loss: 0.7990, acc: 0.6500\n",
            "E2E-ABSA >>> 2022-08-17 17:02:40\n",
            "loss: 0.7649, acc: 0.6656\n",
            "E2E-ABSA >>> 2022-08-17 17:02:46\n",
            "loss: 0.7616, acc: 0.6585\n",
            "E2E-ABSA >>> 2022-08-17 17:02:53\n",
            ">>> val_acc: 0.6122, val_precision: 0.6122 val_recall: 0.6122, val_f1: 0.6122\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-08-17 17:02:53\n",
            "loss: 0.7635, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 17:02:59\n",
            "loss: 0.7517, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-08-17 17:03:04\n",
            "loss: 0.7459, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-08-17 17:03:11\n",
            ">>> val_acc: 0.6178, val_precision: 0.6178 val_recall: 0.6178, val_f1: 0.6178\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-08-17 17:03:11\n",
            "loss: 0.8668, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 17:03:17\n",
            "loss: 0.7444, acc: 0.6689\n",
            "E2E-ABSA >>> 2022-08-17 17:03:22\n",
            "loss: 0.7490, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-08-17 17:03:28\n",
            "loss: 0.7576, acc: 0.6595\n",
            "E2E-ABSA >>> 2022-08-17 17:03:29\n",
            ">>> val_acc: 0.6197, val_precision: 0.6197 val_recall: 0.6197, val_f1: 0.6197\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-08-17 17:03:35\n",
            "loss: 0.7301, acc: 0.6784\n",
            "E2E-ABSA >>> 2022-08-17 17:03:40\n",
            "loss: 0.7495, acc: 0.6617\n",
            "E2E-ABSA >>> 2022-08-17 17:03:46\n",
            "loss: 0.7543, acc: 0.6626\n",
            "E2E-ABSA >>> 2022-08-17 17:03:48\n",
            ">>> val_acc: 0.6215, val_precision: 0.6215 val_recall: 0.6215, val_f1: 0.6215\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6215\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-08-17 17:03:53\n",
            "loss: 0.7631, acc: 0.6548\n",
            "E2E-ABSA >>> 2022-08-17 17:03:58\n",
            "loss: 0.7638, acc: 0.6572\n",
            "E2E-ABSA >>> 2022-08-17 17:04:04\n",
            "loss: 0.7595, acc: 0.6608\n",
            "E2E-ABSA >>> 2022-08-17 17:04:06\n",
            ">>> val_acc: 0.6234, val_precision: 0.6234 val_recall: 0.6234, val_f1: 0.6234\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6234\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-08-17 17:04:11\n",
            "loss: 0.7241, acc: 0.6930\n",
            "E2E-ABSA >>> 2022-08-17 17:04:16\n",
            "loss: 0.7427, acc: 0.6694\n",
            "E2E-ABSA >>> 2022-08-17 17:04:22\n",
            "loss: 0.7560, acc: 0.6612\n",
            "E2E-ABSA >>> 2022-08-17 17:04:25\n",
            ">>> val_acc: 0.6271, val_precision: 0.6271 val_recall: 0.6271, val_f1: 0.6271\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6271\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-08-17 17:04:29\n",
            "loss: 0.7770, acc: 0.6415\n",
            "E2E-ABSA >>> 2022-08-17 17:04:35\n",
            "loss: 0.7631, acc: 0.6552\n",
            "E2E-ABSA >>> 2022-08-17 17:04:40\n",
            "loss: 0.7521, acc: 0.6604\n",
            "E2E-ABSA >>> 2022-08-17 17:04:44\n",
            ">>> val_acc: 0.6197, val_precision: 0.6197 val_recall: 0.6197, val_f1: 0.6197\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-08-17 17:04:47\n",
            "loss: 0.7563, acc: 0.6689\n",
            "E2E-ABSA >>> 2022-08-17 17:04:53\n",
            "loss: 0.7474, acc: 0.6677\n",
            "E2E-ABSA >>> 2022-08-17 17:04:58\n",
            "loss: 0.7485, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-08-17 17:05:02\n",
            ">>> val_acc: 0.6178, val_precision: 0.6178 val_recall: 0.6178, val_f1: 0.6178\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-08-17 17:05:05\n",
            "loss: 0.7631, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-08-17 17:05:11\n",
            "loss: 0.7569, acc: 0.6651\n",
            "E2E-ABSA >>> 2022-08-17 17:05:16\n",
            "loss: 0.7526, acc: 0.6614\n",
            "E2E-ABSA >>> 2022-08-17 17:05:21\n",
            ">>> val_acc: 0.6141, val_precision: 0.6141 val_recall: 0.6141, val_f1: 0.6141\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-08-17 17:05:23\n",
            "loss: 0.7219, acc: 0.6862\n",
            "E2E-ABSA >>> 2022-08-17 17:05:29\n",
            "loss: 0.7364, acc: 0.6791\n",
            "E2E-ABSA >>> 2022-08-17 17:05:34\n",
            "loss: 0.7469, acc: 0.6709\n",
            "E2E-ABSA >>> 2022-08-17 17:05:39\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-08-17 17:05:42\n",
            "loss: 0.7620, acc: 0.6531\n",
            "E2E-ABSA >>> 2022-08-17 17:05:47\n",
            "loss: 0.7559, acc: 0.6612\n",
            "E2E-ABSA >>> 2022-08-17 17:05:53\n",
            "loss: 0.7513, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-08-17 17:05:58\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-08-17 17:06:00\n",
            "loss: 0.7212, acc: 0.6738\n",
            "E2E-ABSA >>> 2022-08-17 17:06:05\n",
            "loss: 0.7271, acc: 0.6738\n",
            "E2E-ABSA >>> 2022-08-17 17:06:11\n",
            "loss: 0.7440, acc: 0.6659\n",
            "E2E-ABSA >>> 2022-08-17 17:06:16\n",
            ">>> val_acc: 0.6327, val_precision: 0.6327 val_recall: 0.6327, val_f1: 0.6327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-08-17 17:06:18\n",
            "loss: 0.7098, acc: 0.6979\n",
            "E2E-ABSA >>> 2022-08-17 17:06:23\n",
            "loss: 0.7428, acc: 0.6623\n",
            "E2E-ABSA >>> 2022-08-17 17:06:29\n",
            "loss: 0.7514, acc: 0.6616\n",
            "E2E-ABSA >>> 2022-08-17 17:06:35\n",
            ">>> val_acc: 0.6215, val_precision: 0.6215 val_recall: 0.6215, val_f1: 0.6215\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-08-17 17:06:36\n",
            "loss: 0.8231, acc: 0.5938\n",
            "E2E-ABSA >>> 2022-08-17 17:06:41\n",
            "loss: 0.7597, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-08-17 17:06:47\n",
            "loss: 0.7453, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-08-17 17:06:53\n",
            ">>> val_acc: 0.6271, val_precision: 0.6271 val_recall: 0.6271, val_f1: 0.6271\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-08-17 17:06:54\n",
            "loss: 0.7738, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-08-17 17:06:59\n",
            "loss: 0.7324, acc: 0.6777\n",
            "E2E-ABSA >>> 2022-08-17 17:07:05\n",
            "loss: 0.7451, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-08-17 17:07:10\n",
            "loss: 0.7491, acc: 0.6638\n",
            "E2E-ABSA >>> 2022-08-17 17:07:12\n",
            ">>> val_acc: 0.6327, val_precision: 0.6327 val_recall: 0.6327, val_f1: 0.6327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-08-17 17:07:17\n",
            "loss: 0.7364, acc: 0.6737\n",
            "E2E-ABSA >>> 2022-08-17 17:07:23\n",
            "loss: 0.7452, acc: 0.6625\n",
            "E2E-ABSA >>> 2022-08-17 17:07:28\n",
            "loss: 0.7469, acc: 0.6646\n",
            "E2E-ABSA >>> 2022-08-17 17:07:30\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-08-17 17:07:35\n",
            "loss: 0.7208, acc: 0.6827\n",
            "E2E-ABSA >>> 2022-08-17 17:07:41\n",
            "loss: 0.7344, acc: 0.6761\n",
            "E2E-ABSA >>> 2022-08-17 17:07:46\n",
            "loss: 0.7468, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 17:07:49\n",
            ">>> val_acc: 0.6252, val_precision: 0.6252 val_recall: 0.6252, val_f1: 0.6252\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-08-17 17:07:53\n",
            "loss: 0.7403, acc: 0.6607\n",
            "E2E-ABSA >>> 2022-08-17 17:07:59\n",
            "loss: 0.7489, acc: 0.6634\n",
            "E2E-ABSA >>> 2022-08-17 17:08:04\n",
            "loss: 0.7472, acc: 0.6657\n",
            "E2E-ABSA >>> 2022-08-17 17:08:07\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-08-17 17:08:11\n",
            "loss: 0.7289, acc: 0.6883\n",
            "E2E-ABSA >>> 2022-08-17 17:08:17\n",
            "loss: 0.7436, acc: 0.6683\n",
            "E2E-ABSA >>> 2022-08-17 17:08:22\n",
            "loss: 0.7483, acc: 0.6626\n",
            "E2E-ABSA >>> 2022-08-17 17:08:26\n",
            ">>> val_acc: 0.6178, val_precision: 0.6178 val_recall: 0.6178, val_f1: 0.6178\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-08-17 17:08:29\n",
            "loss: 0.7834, acc: 0.6333\n",
            "E2E-ABSA >>> 2022-08-17 17:08:35\n",
            "loss: 0.7568, acc: 0.6589\n",
            "E2E-ABSA >>> 2022-08-17 17:08:40\n",
            "loss: 0.7446, acc: 0.6637\n",
            "E2E-ABSA >>> 2022-08-17 17:08:44\n",
            ">>> val_acc: 0.6327, val_precision: 0.6327 val_recall: 0.6327, val_f1: 0.6327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-08-17 17:08:47\n",
            "loss: 0.7673, acc: 0.6490\n",
            "E2E-ABSA >>> 2022-08-17 17:08:53\n",
            "loss: 0.7526, acc: 0.6605\n",
            "E2E-ABSA >>> 2022-08-17 17:08:58\n",
            "loss: 0.7447, acc: 0.6666\n",
            "E2E-ABSA >>> 2022-08-17 17:09:02\n",
            ">>> val_acc: 0.6271, val_precision: 0.6271 val_recall: 0.6271, val_f1: 0.6271\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-08-17 17:09:05\n",
            "loss: 0.7778, acc: 0.6502\n",
            "E2E-ABSA >>> 2022-08-17 17:09:11\n",
            "loss: 0.7551, acc: 0.6620\n",
            "E2E-ABSA >>> 2022-08-17 17:09:16\n",
            "loss: 0.7471, acc: 0.6615\n",
            "E2E-ABSA >>> 2022-08-17 17:09:21\n",
            ">>> val_acc: 0.6271, val_precision: 0.6271 val_recall: 0.6271, val_f1: 0.6271\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-08-17 17:09:23\n",
            "loss: 0.7440, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 17:09:29\n",
            "loss: 0.7357, acc: 0.6745\n",
            "E2E-ABSA >>> 2022-08-17 17:09:34\n",
            "loss: 0.7422, acc: 0.6703\n",
            "E2E-ABSA >>> 2022-08-17 17:09:39\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-08-17 17:09:41\n",
            "loss: 0.7306, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-08-17 17:09:47\n",
            "loss: 0.7423, acc: 0.6668\n",
            "E2E-ABSA >>> 2022-08-17 17:09:53\n",
            "loss: 0.7427, acc: 0.6653\n",
            "E2E-ABSA >>> 2022-08-17 17:09:58\n",
            ">>> val_acc: 0.6327, val_precision: 0.6327 val_recall: 0.6327, val_f1: 0.6327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-08-17 17:10:00\n",
            "loss: 0.7518, acc: 0.6786\n",
            "E2E-ABSA >>> 2022-08-17 17:10:05\n",
            "loss: 0.7423, acc: 0.6689\n",
            "E2E-ABSA >>> 2022-08-17 17:10:11\n",
            "loss: 0.7404, acc: 0.6683\n",
            "E2E-ABSA >>> 2022-08-17 17:10:17\n",
            ">>> val_acc: 0.6327, val_precision: 0.6327 val_recall: 0.6327, val_f1: 0.6327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-08-17 17:10:18\n",
            "loss: 0.7116, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 17:10:23\n",
            "loss: 0.7518, acc: 0.6724\n",
            "E2E-ABSA >>> 2022-08-17 17:10:29\n",
            "loss: 0.7475, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-08-17 17:10:35\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-08-17 17:10:36\n",
            "loss: 0.7621, acc: 0.6771\n",
            "E2E-ABSA >>> 2022-08-17 17:10:41\n",
            "loss: 0.7583, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-08-17 17:10:47\n",
            "loss: 0.7358, acc: 0.6728\n",
            "E2E-ABSA >>> 2022-08-17 17:10:53\n",
            ">>> val_acc: 0.6271, val_precision: 0.6271 val_recall: 0.6271, val_f1: 0.6271\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-08-17 17:10:54\n",
            "loss: 0.5689, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 17:10:59\n",
            "loss: 0.7341, acc: 0.6779\n",
            "E2E-ABSA >>> 2022-08-17 17:11:05\n",
            "loss: 0.7448, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-08-17 17:11:10\n",
            "loss: 0.7417, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-08-17 17:11:12\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-08-17 17:11:17\n",
            "loss: 0.7093, acc: 0.6901\n",
            "E2E-ABSA >>> 2022-08-17 17:11:23\n",
            "loss: 0.7372, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-08-17 17:11:28\n",
            "loss: 0.7405, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-08-17 17:11:30\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-08-17 17:11:35\n",
            "loss: 0.7218, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-08-17 17:11:41\n",
            "loss: 0.7451, acc: 0.6712\n",
            "E2E-ABSA >>> 2022-08-17 17:11:46\n",
            "loss: 0.7447, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-08-17 17:11:49\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-08-17 17:11:53\n",
            "loss: 0.7225, acc: 0.6852\n",
            "E2E-ABSA >>> 2022-08-17 17:11:59\n",
            "loss: 0.7281, acc: 0.6726\n",
            "E2E-ABSA >>> 2022-08-17 17:12:04\n",
            "loss: 0.7357, acc: 0.6708\n",
            "E2E-ABSA >>> 2022-08-17 17:12:07\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 66.\n",
            "E2E-ABSA >>> 2022-08-17 17:12:11\n",
            "loss: 0.7292, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 17:12:17\n",
            "loss: 0.7359, acc: 0.6744\n",
            "E2E-ABSA >>> 2022-08-17 17:12:22\n",
            "loss: 0.7388, acc: 0.6670\n",
            "E2E-ABSA >>> 2022-08-17 17:12:26\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 67.\n",
            "E2E-ABSA >>> 2022-08-17 17:12:29\n",
            "loss: 0.7350, acc: 0.6631\n",
            "E2E-ABSA >>> 2022-08-17 17:12:35\n",
            "loss: 0.7367, acc: 0.6669\n",
            "E2E-ABSA >>> 2022-08-17 17:12:40\n",
            "loss: 0.7397, acc: 0.6690\n",
            "E2E-ABSA >>> 2022-08-17 17:12:44\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 68.\n",
            "E2E-ABSA >>> 2022-08-17 17:12:47\n",
            "loss: 0.7530, acc: 0.6663\n",
            "E2E-ABSA >>> 2022-08-17 17:12:53\n",
            "loss: 0.7256, acc: 0.6775\n",
            "E2E-ABSA >>> 2022-08-17 17:12:58\n",
            "loss: 0.7363, acc: 0.6655\n",
            "E2E-ABSA >>> 2022-08-17 17:13:03\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 69.\n",
            "E2E-ABSA >>> 2022-08-17 17:13:05\n",
            "loss: 0.7484, acc: 0.6680\n",
            "E2E-ABSA >>> 2022-08-17 17:13:11\n",
            "loss: 0.7395, acc: 0.6677\n",
            "E2E-ABSA >>> 2022-08-17 17:13:16\n",
            "loss: 0.7401, acc: 0.6678\n",
            "E2E-ABSA >>> 2022-08-17 17:13:21\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 70.\n",
            "E2E-ABSA >>> 2022-08-17 17:13:23\n",
            "loss: 0.7468, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-08-17 17:13:29\n",
            "loss: 0.7371, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-08-17 17:13:34\n",
            "loss: 0.7402, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 17:13:40\n",
            ">>> val_acc: 0.6308, val_precision: 0.6308 val_recall: 0.6308, val_f1: 0.6308\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 71.\n",
            "E2E-ABSA >>> 2022-08-17 17:13:41\n",
            "loss: 0.7323, acc: 0.6641\n",
            "E2E-ABSA >>> 2022-08-17 17:13:47\n",
            "loss: 0.7313, acc: 0.6728\n",
            "E2E-ABSA >>> 2022-08-17 17:13:52\n",
            "loss: 0.7352, acc: 0.6692\n",
            "E2E-ABSA >>> 2022-08-17 17:13:58\n",
            ">>> val_acc: 0.6382, val_precision: 0.6382 val_recall: 0.6382, val_f1: 0.6382\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6382\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 72.\n",
            "E2E-ABSA >>> 2022-08-17 17:13:59\n",
            "loss: 0.6809, acc: 0.7135\n",
            "E2E-ABSA >>> 2022-08-17 17:14:05\n",
            "loss: 0.7286, acc: 0.6694\n",
            "E2E-ABSA >>> 2022-08-17 17:14:11\n",
            "loss: 0.7351, acc: 0.6671\n",
            "E2E-ABSA >>> 2022-08-17 17:14:17\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 73.\n",
            "E2E-ABSA >>> 2022-08-17 17:14:17\n",
            "loss: 0.7755, acc: 0.6484\n",
            "E2E-ABSA >>> 2022-08-17 17:14:23\n",
            "loss: 0.7252, acc: 0.6751\n",
            "E2E-ABSA >>> 2022-08-17 17:14:28\n",
            "loss: 0.7387, acc: 0.6672\n",
            "E2E-ABSA >>> 2022-08-17 17:14:35\n",
            ">>> val_acc: 0.6382, val_precision: 0.6382 val_recall: 0.6382, val_f1: 0.6382\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 74.\n",
            "E2E-ABSA >>> 2022-08-17 17:14:36\n",
            "loss: 0.7874, acc: 0.6406\n",
            "E2E-ABSA >>> 2022-08-17 17:14:41\n",
            "loss: 0.7392, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-08-17 17:14:47\n",
            "loss: 0.7487, acc: 0.6605\n",
            "E2E-ABSA >>> 2022-08-17 17:14:52\n",
            "loss: 0.7380, acc: 0.6699\n",
            "E2E-ABSA >>> 2022-08-17 17:14:54\n",
            ">>> val_acc: 0.6419, val_precision: 0.6419 val_recall: 0.6419, val_f1: 0.6419\n",
            ">> saved: state_dict/cabasc_acl14shortdata_know_val_f1_0.6419\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 75.\n",
            "E2E-ABSA >>> 2022-08-17 17:14:59\n",
            "loss: 0.7323, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 17:15:05\n",
            "loss: 0.7348, acc: 0.6731\n",
            "E2E-ABSA >>> 2022-08-17 17:15:11\n",
            "loss: 0.7375, acc: 0.6704\n",
            "E2E-ABSA >>> 2022-08-17 17:15:12\n",
            ">>> val_acc: 0.6382, val_precision: 0.6382 val_recall: 0.6382, val_f1: 0.6382\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 76.\n",
            "E2E-ABSA >>> 2022-08-17 17:15:17\n",
            "loss: 0.7259, acc: 0.6943\n",
            "E2E-ABSA >>> 2022-08-17 17:15:23\n",
            "loss: 0.7315, acc: 0.6794\n",
            "E2E-ABSA >>> 2022-08-17 17:15:28\n",
            "loss: 0.7386, acc: 0.6702\n",
            "E2E-ABSA >>> 2022-08-17 17:15:31\n",
            ">>> val_acc: 0.6382, val_precision: 0.6382 val_recall: 0.6382, val_f1: 0.6382\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 77.\n",
            "E2E-ABSA >>> 2022-08-17 17:15:35\n",
            "loss: 0.7266, acc: 0.6585\n",
            "E2E-ABSA >>> 2022-08-17 17:15:41\n",
            "loss: 0.7440, acc: 0.6559\n",
            "E2E-ABSA >>> 2022-08-17 17:15:46\n",
            "loss: 0.7356, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-08-17 17:15:49\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 78.\n",
            "E2E-ABSA >>> 2022-08-17 17:15:53\n",
            "loss: 0.7493, acc: 0.6604\n",
            "E2E-ABSA >>> 2022-08-17 17:15:59\n",
            "loss: 0.7328, acc: 0.6683\n",
            "E2E-ABSA >>> 2022-08-17 17:16:04\n",
            "loss: 0.7356, acc: 0.6685\n",
            "E2E-ABSA >>> 2022-08-17 17:16:07\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 79.\n",
            "E2E-ABSA >>> 2022-08-17 17:16:11\n",
            "loss: 0.7202, acc: 0.6774\n",
            "E2E-ABSA >>> 2022-08-17 17:16:17\n",
            "loss: 0.7277, acc: 0.6815\n",
            "E2E-ABSA >>> 2022-08-17 17:16:22\n",
            "loss: 0.7328, acc: 0.6747\n",
            "E2E-ABSA >>> 2022-08-17 17:16:26\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 80.\n",
            "E2E-ABSA >>> 2022-08-17 17:16:29\n",
            "loss: 0.7352, acc: 0.6687\n",
            "E2E-ABSA >>> 2022-08-17 17:16:35\n",
            "loss: 0.7163, acc: 0.6809\n",
            "E2E-ABSA >>> 2022-08-17 17:16:40\n",
            "loss: 0.7329, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-08-17 17:16:44\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 81.\n",
            "E2E-ABSA >>> 2022-08-17 17:16:47\n",
            "loss: 0.7098, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-08-17 17:16:53\n",
            "loss: 0.7184, acc: 0.6760\n",
            "E2E-ABSA >>> 2022-08-17 17:16:59\n",
            "loss: 0.7252, acc: 0.6756\n",
            "E2E-ABSA >>> 2022-08-17 17:17:03\n",
            ">>> val_acc: 0.6382, val_precision: 0.6382 val_recall: 0.6382, val_f1: 0.6382\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 82.\n",
            "E2E-ABSA >>> 2022-08-17 17:17:06\n",
            "loss: 0.7386, acc: 0.6662\n",
            "E2E-ABSA >>> 2022-08-17 17:17:11\n",
            "loss: 0.7168, acc: 0.6840\n",
            "E2E-ABSA >>> 2022-08-17 17:17:17\n",
            "loss: 0.7314, acc: 0.6737\n",
            "E2E-ABSA >>> 2022-08-17 17:17:21\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 83.\n",
            "E2E-ABSA >>> 2022-08-17 17:17:23\n",
            "loss: 0.7031, acc: 0.6910\n",
            "E2E-ABSA >>> 2022-08-17 17:17:29\n",
            "loss: 0.7224, acc: 0.6778\n",
            "E2E-ABSA >>> 2022-08-17 17:17:35\n",
            "loss: 0.7360, acc: 0.6682\n",
            "E2E-ABSA >>> 2022-08-17 17:17:40\n",
            ">>> val_acc: 0.6364, val_precision: 0.6364 val_recall: 0.6364, val_f1: 0.6364\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 84.\n",
            "E2E-ABSA >>> 2022-08-17 17:17:41\n",
            "loss: 0.7254, acc: 0.6808\n",
            "E2E-ABSA >>> 2022-08-17 17:17:47\n",
            "loss: 0.7330, acc: 0.6802\n",
            "E2E-ABSA >>> 2022-08-17 17:17:52\n",
            "loss: 0.7300, acc: 0.6754\n",
            "E2E-ABSA >>> 2022-08-17 17:17:58\n",
            ">>> val_acc: 0.6271, val_precision: 0.6271 val_recall: 0.6271, val_f1: 0.6271\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 85.\n",
            "E2E-ABSA >>> 2022-08-17 17:17:59\n",
            "loss: 0.6798, acc: 0.7063\n",
            "E2E-ABSA >>> 2022-08-17 17:18:05\n",
            "loss: 0.7233, acc: 0.6714\n",
            "E2E-ABSA >>> 2022-08-17 17:18:10\n",
            "loss: 0.7282, acc: 0.6707\n",
            "E2E-ABSA >>> 2022-08-17 17:18:17\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 86.\n",
            "E2E-ABSA >>> 2022-08-17 17:18:17\n",
            "loss: 0.7273, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-08-17 17:18:23\n",
            "loss: 0.7460, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-08-17 17:18:28\n",
            "loss: 0.7254, acc: 0.6813\n",
            "E2E-ABSA >>> 2022-08-17 17:18:35\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 87.\n",
            "E2E-ABSA >>> 2022-08-17 17:18:35\n",
            "loss: 0.7000, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 17:18:41\n",
            "loss: 0.7502, acc: 0.6629\n",
            "E2E-ABSA >>> 2022-08-17 17:18:46\n",
            "loss: 0.7398, acc: 0.6676\n",
            "E2E-ABSA >>> 2022-08-17 17:18:52\n",
            "loss: 0.7342, acc: 0.6713\n",
            "E2E-ABSA >>> 2022-08-17 17:18:53\n",
            ">>> val_acc: 0.6401, val_precision: 0.6401 val_recall: 0.6401, val_f1: 0.6401\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 88.\n",
            "E2E-ABSA >>> 2022-08-17 17:18:59\n",
            "loss: 0.7188, acc: 0.6895\n",
            "E2E-ABSA >>> 2022-08-17 17:19:04\n",
            "loss: 0.7256, acc: 0.6792\n",
            "E2E-ABSA >>> 2022-08-17 17:19:10\n",
            "loss: 0.7347, acc: 0.6738\n",
            "E2E-ABSA >>> 2022-08-17 17:19:12\n",
            ">>> val_acc: 0.6215, val_precision: 0.6215 val_recall: 0.6215, val_f1: 0.6215\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 89.\n",
            "E2E-ABSA >>> 2022-08-17 17:19:17\n",
            "loss: 0.7288, acc: 0.6882\n",
            "E2E-ABSA >>> 2022-08-17 17:19:22\n",
            "loss: 0.7350, acc: 0.6752\n",
            "E2E-ABSA >>> 2022-08-17 17:19:28\n",
            "loss: 0.7309, acc: 0.6740\n",
            "E2E-ABSA >>> 2022-08-17 17:19:31\n",
            ">>> val_acc: 0.6382, val_precision: 0.6382 val_recall: 0.6382, val_f1: 0.6382\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 90.\n",
            "E2E-ABSA >>> 2022-08-17 17:19:35\n",
            "loss: 0.7524, acc: 0.6711\n",
            "E2E-ABSA >>> 2022-08-17 17:19:40\n",
            "loss: 0.7415, acc: 0.6670\n",
            "E2E-ABSA >>> 2022-08-17 17:19:46\n",
            "loss: 0.7364, acc: 0.6710\n",
            "E2E-ABSA >>> 2022-08-17 17:19:49\n",
            ">>> val_acc: 0.6289, val_precision: 0.6289 val_recall: 0.6289, val_f1: 0.6289\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 91.\n",
            "E2E-ABSA >>> 2022-08-17 17:19:53\n",
            "loss: 0.7354, acc: 0.6571\n",
            "E2E-ABSA >>> 2022-08-17 17:19:58\n",
            "loss: 0.7279, acc: 0.6690\n",
            "E2E-ABSA >>> 2022-08-17 17:20:04\n",
            "loss: 0.7298, acc: 0.6705\n",
            "E2E-ABSA >>> 2022-08-17 17:20:07\n",
            ">>> val_acc: 0.6345, val_precision: 0.6345 val_recall: 0.6345, val_f1: 0.6345\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 92.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 测试五个数据集在Bert_spc上的效果\n",
        "**BERT_BASED - MODEL**"
      ],
      "metadata": {
        "id": "IZLAPmxY9Ytb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "跑Bert前最好检测一下是否cuda可用"
      ],
      "metadata": {
        "id": "yDvO9P7Nn_gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVNLcD8FnxuQ",
        "outputId": "6531e35e-d4fa-4a73-e611-71ea9015a0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Twitter** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "2yChpIT3uJhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset twitter --log_step 20  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9fh_txtuSR6",
        "outputId": "95a7b32b-a0dd-4acb-c724-48ba85e72593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 1687.\n",
            "> testing dataset count: 422.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f887c34ea70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 11:32:45\n",
            "loss: 0.8217, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-20 11:32:51\n",
            "loss: 0.8429, acc: 0.6297\n",
            "E2E-ABSA >>> 2022-05-20 11:32:56\n",
            "loss: 0.8504, acc: 0.6271\n",
            "E2E-ABSA >>> 2022-05-20 11:33:02\n",
            "loss: 0.8493, acc: 0.6391\n",
            "E2E-ABSA >>> 2022-05-20 11:33:08\n",
            "loss: 0.8536, acc: 0.6412\n",
            "E2E-ABSA >>> 2022-05-20 11:33:12\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">> saved: state_dict/bert_spc_twitter_val_f1_0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 11:33:17\n",
            "loss: 0.7814, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-05-20 11:33:23\n",
            "loss: 0.7760, acc: 0.6562\n",
            "E2E-ABSA >>> 2022-05-20 11:33:29\n",
            "loss: 0.7943, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-20 11:33:35\n",
            "loss: 0.8049, acc: 0.6579\n",
            "E2E-ABSA >>> 2022-05-20 11:33:41\n",
            "loss: 0.8235, acc: 0.6456\n",
            "E2E-ABSA >>> 2022-05-20 11:33:47\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 11:33:50\n",
            "loss: 0.6920, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-05-20 11:33:56\n",
            "loss: 0.7707, acc: 0.6652\n",
            "E2E-ABSA >>> 2022-05-20 11:34:02\n",
            "loss: 0.7822, acc: 0.6693\n",
            "E2E-ABSA >>> 2022-05-20 11:34:08\n",
            "loss: 0.7834, acc: 0.6673\n",
            "E2E-ABSA >>> 2022-05-20 11:34:14\n",
            "loss: 0.7965, acc: 0.6591\n",
            "E2E-ABSA >>> 2022-05-20 11:34:22\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 11:34:22\n",
            "loss: 0.8645, acc: 0.6250\n",
            "E2E-ABSA >>> 2022-05-20 11:34:28\n",
            "loss: 0.7784, acc: 0.6818\n",
            "E2E-ABSA >>> 2022-05-20 11:34:34\n",
            "loss: 0.7559, acc: 0.6875\n",
            "E2E-ABSA >>> 2022-05-20 11:34:40\n",
            "loss: 0.7409, acc: 0.6865\n",
            "E2E-ABSA >>> 2022-05-20 11:34:46\n",
            "loss: 0.7562, acc: 0.6784\n",
            "E2E-ABSA >>> 2022-05-20 11:34:52\n",
            "loss: 0.7515, acc: 0.6850\n",
            "E2E-ABSA >>> 2022-05-20 11:34:55\n",
            ">>> val_acc: 0.7014, val_precision: 0.7014 val_recall: 0.7014, val_f1: 0.7014\n",
            ">> saved: state_dict/bert_spc_twitter_val_f1_0.7014\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 11:35:02\n",
            "loss: 0.7472, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-05-20 11:35:08\n",
            "loss: 0.6780, acc: 0.6962\n",
            "E2E-ABSA >>> 2022-05-20 11:35:14\n",
            "loss: 0.7085, acc: 0.6830\n",
            "E2E-ABSA >>> 2022-05-20 11:35:20\n",
            "loss: 0.6960, acc: 0.6933\n",
            "E2E-ABSA >>> 2022-05-20 11:35:26\n",
            "loss: 0.6966, acc: 0.6960\n",
            "E2E-ABSA >>> 2022-05-20 11:35:31\n",
            ">>> val_acc: 0.7204, val_precision: 0.7204 val_recall: 0.7204, val_f1: 0.7204\n",
            ">> saved: state_dict/bert_spc_twitter_val_f1_0.7204\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 11:35:35\n",
            "loss: 0.5394, acc: 0.7875\n",
            "E2E-ABSA >>> 2022-05-20 11:35:41\n",
            "loss: 0.6116, acc: 0.7438\n",
            "E2E-ABSA >>> 2022-05-20 11:35:47\n",
            "loss: 0.5783, acc: 0.7588\n",
            "E2E-ABSA >>> 2022-05-20 11:35:53\n",
            "loss: 0.5910, acc: 0.7482\n",
            "E2E-ABSA >>> 2022-05-20 11:35:59\n",
            "loss: 0.5990, acc: 0.7382\n",
            "E2E-ABSA >>> 2022-05-20 11:36:07\n",
            ">>> val_acc: 0.6588, val_precision: 0.6588 val_recall: 0.6588, val_f1: 0.6588\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 11:36:08\n",
            "loss: 0.4884, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-05-20 11:36:14\n",
            "loss: 0.3998, acc: 0.8307\n",
            "E2E-ABSA >>> 2022-05-20 11:36:20\n",
            "loss: 0.4611, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-20 11:36:26\n",
            "loss: 0.4477, acc: 0.8115\n",
            "E2E-ABSA >>> 2022-05-20 11:36:32\n",
            "loss: 0.4629, acc: 0.8013\n",
            "E2E-ABSA >>> 2022-05-20 11:36:37\n",
            "loss: 0.4384, acc: 0.8137\n",
            "E2E-ABSA >>> 2022-05-20 11:36:41\n",
            ">>> val_acc: 0.7607, val_precision: 0.7607 val_recall: 0.7607, val_f1: 0.7607\n",
            ">> saved: state_dict/bert_spc_twitter_val_f1_0.7607\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 11:36:47\n",
            "loss: 0.2921, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-05-20 11:36:53\n",
            "loss: 0.2782, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-20 11:36:59\n",
            "loss: 0.3079, acc: 0.8879\n",
            "E2E-ABSA >>> 2022-05-20 11:37:05\n",
            "loss: 0.3054, acc: 0.8870\n",
            "E2E-ABSA >>> 2022-05-20 11:37:11\n",
            "loss: 0.3246, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-20 11:37:16\n",
            ">>> val_acc: 0.7512, val_precision: 0.7512 val_recall: 0.7512, val_f1: 0.7512\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 11:37:19\n",
            "loss: 0.1266, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:37:25\n",
            "loss: 0.1120, acc: 0.9668\n",
            "E2E-ABSA >>> 2022-05-20 11:37:31\n",
            "loss: 0.1127, acc: 0.9639\n",
            "E2E-ABSA >>> 2022-05-20 11:37:37\n",
            "loss: 0.1372, acc: 0.9523\n",
            "E2E-ABSA >>> 2022-05-20 11:37:43\n",
            "loss: 0.1605, acc: 0.9409\n",
            "E2E-ABSA >>> 2022-05-20 11:37:50\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 11:37:52\n",
            "loss: 0.0882, acc: 0.9792\n",
            "E2E-ABSA >>> 2022-05-20 11:37:58\n",
            "loss: 0.0814, acc: 0.9736\n",
            "E2E-ABSA >>> 2022-05-20 11:38:04\n",
            "loss: 0.0807, acc: 0.9769\n",
            "E2E-ABSA >>> 2022-05-20 11:38:10\n",
            "loss: 0.1432, acc: 0.9489\n",
            "E2E-ABSA >>> 2022-05-20 11:38:16\n",
            "loss: 0.1728, acc: 0.9368\n",
            "E2E-ABSA >>> 2022-05-20 11:38:21\n",
            "loss: 0.1667, acc: 0.9389\n",
            "E2E-ABSA >>> 2022-05-20 11:38:24\n",
            ">>> val_acc: 0.7014, val_precision: 0.7014 val_recall: 0.7014, val_f1: 0.7014\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 11:38:30\n",
            "loss: 0.0551, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-05-20 11:38:36\n",
            "loss: 0.0978, acc: 0.9703\n",
            "E2E-ABSA >>> 2022-05-20 11:38:42\n",
            "loss: 0.1219, acc: 0.9604\n",
            "E2E-ABSA >>> 2022-05-20 11:38:48\n",
            "loss: 0.1174, acc: 0.9617\n",
            "E2E-ABSA >>> 2022-05-20 11:38:54\n",
            "loss: 0.1216, acc: 0.9581\n",
            "E2E-ABSA >>> 2022-05-20 11:38:58\n",
            ">>> val_acc: 0.7251, val_precision: 0.7251 val_recall: 0.7251, val_f1: 0.7251\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 11:39:02\n",
            "loss: 0.0575, acc: 0.9821\n",
            "E2E-ABSA >>> 2022-05-20 11:39:08\n",
            "loss: 0.0403, acc: 0.9890\n",
            "E2E-ABSA >>> 2022-05-20 11:39:14\n",
            "loss: 0.0484, acc: 0.9826\n",
            "E2E-ABSA >>> 2022-05-20 11:39:20\n",
            "loss: 0.0491, acc: 0.9831\n",
            "E2E-ABSA >>> 2022-05-20 11:39:26\n",
            "loss: 0.0595, acc: 0.9820\n",
            "E2E-ABSA >>> 2022-05-20 11:39:32\n",
            ">>> val_acc: 0.7180, val_precision: 0.7180 val_recall: 0.7180, val_f1: 0.7180\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 11:39:34\n",
            "loss: 0.0112, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:39:40\n",
            "loss: 0.0320, acc: 0.9955\n",
            "E2E-ABSA >>> 2022-05-20 11:39:46\n",
            "loss: 0.0608, acc: 0.9818\n",
            "E2E-ABSA >>> 2022-05-20 11:39:52\n",
            "loss: 0.0572, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:39:58\n",
            "loss: 0.0603, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:40:06\n",
            ">>> val_acc: 0.7180, val_precision: 0.7180 val_recall: 0.7180, val_f1: 0.7180\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 11:40:06\n",
            "loss: 0.1823, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:40:12\n",
            "loss: 0.1190, acc: 0.9744\n",
            "E2E-ABSA >>> 2022-05-20 11:40:18\n",
            "loss: 0.1068, acc: 0.9717\n",
            "E2E-ABSA >>> 2022-05-20 11:40:24\n",
            "loss: 0.0988, acc: 0.9718\n",
            "E2E-ABSA >>> 2022-05-20 11:40:30\n",
            "loss: 0.0901, acc: 0.9748\n",
            "E2E-ABSA >>> 2022-05-20 11:40:36\n",
            "loss: 0.0905, acc: 0.9743\n",
            "E2E-ABSA >>> 2022-05-20 11:40:40\n",
            ">>> val_acc: 0.6374, val_precision: 0.6374 val_recall: 0.6374, val_f1: 0.6374\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 11:40:44\n",
            "loss: 0.0831, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-05-20 11:40:50\n",
            "loss: 0.1164, acc: 0.9635\n",
            "E2E-ABSA >>> 2022-05-20 11:40:56\n",
            "loss: 0.1255, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-05-20 11:41:02\n",
            "loss: 0.1194, acc: 0.9589\n",
            "E2E-ABSA >>> 2022-05-20 11:41:08\n",
            "loss: 0.1033, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-05-20 11:41:14\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 11:41:17\n",
            "loss: 0.0143, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:41:23\n",
            "loss: 0.0300, acc: 0.9917\n",
            "E2E-ABSA >>> 2022-05-20 11:41:29\n",
            "loss: 0.0433, acc: 0.9850\n",
            "E2E-ABSA >>> 2022-05-20 11:41:34\n",
            "loss: 0.0586, acc: 0.9786\n",
            "E2E-ABSA >>> 2022-05-20 11:41:40\n",
            "loss: 0.0741, acc: 0.9764\n",
            "E2E-ABSA >>> 2022-05-20 11:41:48\n",
            ">>> val_acc: 0.7133, val_precision: 0.7133 val_recall: 0.7133, val_f1: 0.7133\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 11:41:49\n",
            "loss: 0.0257, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:41:55\n",
            "loss: 0.0265, acc: 0.9922\n",
            "E2E-ABSA >>> 2022-05-20 11:42:01\n",
            "loss: 0.0411, acc: 0.9830\n",
            "E2E-ABSA >>> 2022-05-20 11:42:07\n",
            "loss: 0.0438, acc: 0.9814\n",
            "E2E-ABSA >>> 2022-05-20 11:42:13\n",
            "loss: 0.0522, acc: 0.9792\n",
            "E2E-ABSA >>> 2022-05-20 11:42:19\n",
            "loss: 0.0536, acc: 0.9796\n",
            "E2E-ABSA >>> 2022-05-20 11:42:22\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 11:42:27\n",
            "loss: 0.0184, acc: 0.9965\n",
            "E2E-ABSA >>> 2022-05-20 11:42:33\n",
            "loss: 0.0503, acc: 0.9786\n",
            "E2E-ABSA >>> 2022-05-20 11:42:39\n",
            "loss: 0.0438, acc: 0.9838\n",
            "E2E-ABSA >>> 2022-05-20 11:42:45\n",
            "loss: 0.0461, acc: 0.9816\n",
            "E2E-ABSA >>> 2022-05-20 11:42:51\n",
            "loss: 0.0477, acc: 0.9809\n",
            "E2E-ABSA >>> 2022-05-20 11:42:56\n",
            ">>> val_acc: 0.6706, val_precision: 0.6706 val_recall: 0.6706, val_f1: 0.6706\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 11:42:59\n",
            "loss: 0.0529, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:43:05\n",
            "loss: 0.0448, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:43:11\n",
            "loss: 0.0523, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:43:17\n",
            "loss: 0.0484, acc: 0.9861\n",
            "E2E-ABSA >>> 2022-05-20 11:43:23\n",
            "loss: 0.0503, acc: 0.9857\n",
            "E2E-ABSA >>> 2022-05-20 11:43:30\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 11:43:32\n",
            "loss: 0.0913, acc: 0.9583\n",
            "E2E-ABSA >>> 2022-05-20 11:43:38\n",
            "loss: 0.0755, acc: 0.9712\n",
            "E2E-ABSA >>> 2022-05-20 11:43:44\n",
            "loss: 0.0709, acc: 0.9728\n",
            "E2E-ABSA >>> 2022-05-20 11:43:50\n",
            "loss: 0.0638, acc: 0.9754\n",
            "E2E-ABSA >>> 2022-05-20 11:43:55\n",
            "loss: 0.0655, acc: 0.9760\n",
            "E2E-ABSA >>> 2022-05-20 11:44:01\n",
            "loss: 0.0578, acc: 0.9793\n",
            "E2E-ABSA >>> 2022-05-20 11:44:04\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 11:44:10\n",
            "loss: 0.0158, acc: 0.9969\n",
            "E2E-ABSA >>> 2022-05-20 11:44:16\n",
            "loss: 0.0162, acc: 0.9969\n",
            "E2E-ABSA >>> 2022-05-20 11:44:22\n",
            "loss: 0.0241, acc: 0.9927\n",
            "E2E-ABSA >>> 2022-05-20 11:44:28\n",
            "loss: 0.0334, acc: 0.9906\n",
            "E2E-ABSA >>> 2022-05-20 11:44:34\n",
            "loss: 0.0598, acc: 0.9806\n",
            "E2E-ABSA >>> 2022-05-20 11:44:38\n",
            ">>> val_acc: 0.6090, val_precision: 0.6090 val_recall: 0.6090, val_f1: 0.6090\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 11:44:42\n",
            "loss: 0.2958, acc: 0.8661\n",
            "E2E-ABSA >>> 2022-05-20 11:44:48\n",
            "loss: 0.1869, acc: 0.9246\n",
            "E2E-ABSA >>> 2022-05-20 11:44:54\n",
            "loss: 0.1413, acc: 0.9444\n",
            "E2E-ABSA >>> 2022-05-20 11:45:00\n",
            "loss: 0.1288, acc: 0.9493\n",
            "E2E-ABSA >>> 2022-05-20 11:45:06\n",
            "loss: 0.1145, acc: 0.9548\n",
            "E2E-ABSA >>> 2022-05-20 11:45:12\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 11:45:14\n",
            "loss: 0.1400, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-05-20 11:45:20\n",
            "loss: 0.0693, acc: 0.9710\n",
            "E2E-ABSA >>> 2022-05-20 11:45:26\n",
            "loss: 0.0629, acc: 0.9766\n",
            "E2E-ABSA >>> 2022-05-20 11:45:32\n",
            "loss: 0.0597, acc: 0.9779\n",
            "E2E-ABSA >>> 2022-05-20 11:45:38\n",
            "loss: 0.0601, acc: 0.9773\n",
            "E2E-ABSA >>> 2022-05-20 11:45:46\n",
            ">>> val_acc: 0.7133, val_precision: 0.7133 val_recall: 0.7133, val_f1: 0.7133\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 11:45:47\n",
            "loss: 0.0143, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:45:53\n",
            "loss: 0.0404, acc: 0.9801\n",
            "E2E-ABSA >>> 2022-05-20 11:45:59\n",
            "loss: 0.0384, acc: 0.9851\n",
            "E2E-ABSA >>> 2022-05-20 11:46:05\n",
            "loss: 0.0388, acc: 0.9839\n",
            "E2E-ABSA >>> 2022-05-20 11:46:11\n",
            "loss: 0.0578, acc: 0.9771\n",
            "E2E-ABSA >>> 2022-05-20 11:46:17\n",
            "loss: 0.0534, acc: 0.9786\n",
            "E2E-ABSA >>> 2022-05-20 11:46:20\n",
            ">>> val_acc: 0.7204, val_precision: 0.7204 val_recall: 0.7204, val_f1: 0.7204\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 11:46:25\n",
            "loss: 0.0276, acc: 0.9961\n",
            "E2E-ABSA >>> 2022-05-20 11:46:31\n",
            "loss: 0.0279, acc: 0.9931\n",
            "E2E-ABSA >>> 2022-05-20 11:46:37\n",
            "loss: 0.0295, acc: 0.9933\n",
            "E2E-ABSA >>> 2022-05-20 11:46:43\n",
            "loss: 0.0356, acc: 0.9918\n",
            "E2E-ABSA >>> 2022-05-20 11:46:49\n",
            "loss: 0.0313, acc: 0.9915\n",
            "E2E-ABSA >>> 2022-05-20 11:46:54\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-20 11:46:57\n",
            "loss: 0.0367, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-05-20 11:47:03\n",
            "loss: 0.0273, acc: 0.9917\n",
            "E2E-ABSA >>> 2022-05-20 11:47:09\n",
            "loss: 0.0214, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 11:47:15\n",
            "loss: 0.0183, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 11:47:21\n",
            "loss: 0.0191, acc: 0.9931\n",
            "E2E-ABSA >>> 2022-05-20 11:47:28\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-20 11:47:29\n",
            "loss: 0.0331, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:47:35\n",
            "loss: 0.1037, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:47:41\n",
            "loss: 0.0860, acc: 0.9716\n",
            "E2E-ABSA >>> 2022-05-20 11:47:47\n",
            "loss: 0.0889, acc: 0.9717\n",
            "E2E-ABSA >>> 2022-05-20 11:47:53\n",
            "loss: 0.0839, acc: 0.9717\n",
            "E2E-ABSA >>> 2022-05-20 11:47:59\n",
            "loss: 0.0904, acc: 0.9681\n",
            "E2E-ABSA >>> 2022-05-20 11:48:02\n",
            ">>> val_acc: 0.7038, val_precision: 0.7038 val_recall: 0.7038, val_f1: 0.7038\n",
            "E2E-ABSA >>> 2022-05-20 11:48:02\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7607, val_precision: 0.7607 val_recall: 0.7607, val_f1: 0.7607\n",
            "you can download the best model from state_dict/bert_spc_twitter_val_f1_0.7607\n",
            ">>> test_acc: 0.7607, test_precision: 0.7607, test_recall: 0.7607, test_f1: 0.7607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后： Training **Twitter** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "RxgICN44uIe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset twitter_know --log_step 20  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTC3yjgvuTMZ",
        "outputId": "4f46e5f6-559e-4cb5-c375-c34167a30064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 1687.\n",
            "> testing dataset count: 422.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f4d8f81ea70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 11:49:39\n",
            "loss: 0.8964, acc: 0.6219\n",
            "E2E-ABSA >>> 2022-05-20 11:49:44\n",
            "loss: 0.8914, acc: 0.6188\n",
            "E2E-ABSA >>> 2022-05-20 11:49:50\n",
            "loss: 0.8847, acc: 0.6125\n",
            "E2E-ABSA >>> 2022-05-20 11:49:56\n",
            "loss: 0.8804, acc: 0.6273\n",
            "E2E-ABSA >>> 2022-05-20 11:50:01\n",
            "loss: 0.8757, acc: 0.6325\n",
            "E2E-ABSA >>> 2022-05-20 11:50:06\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">> saved: state_dict/bert_spc_twitter_know_val_f1_0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 11:50:11\n",
            "loss: 0.8202, acc: 0.6696\n",
            "E2E-ABSA >>> 2022-05-20 11:50:17\n",
            "loss: 0.7789, acc: 0.6599\n",
            "E2E-ABSA >>> 2022-05-20 11:50:23\n",
            "loss: 0.7791, acc: 0.6701\n",
            "E2E-ABSA >>> 2022-05-20 11:50:29\n",
            "loss: 0.7884, acc: 0.6605\n",
            "E2E-ABSA >>> 2022-05-20 11:50:35\n",
            "loss: 0.7931, acc: 0.6569\n",
            "E2E-ABSA >>> 2022-05-20 11:50:42\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">> saved: state_dict/bert_spc_twitter_know_val_f1_0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 11:50:45\n",
            "loss: 0.6248, acc: 0.7344\n",
            "E2E-ABSA >>> 2022-05-20 11:50:51\n",
            "loss: 0.6941, acc: 0.6942\n",
            "E2E-ABSA >>> 2022-05-20 11:50:57\n",
            "loss: 0.6959, acc: 0.7005\n",
            "E2E-ABSA >>> 2022-05-20 11:51:03\n",
            "loss: 0.6968, acc: 0.6893\n",
            "E2E-ABSA >>> 2022-05-20 11:51:09\n",
            "loss: 0.7029, acc: 0.6847\n",
            "E2E-ABSA >>> 2022-05-20 11:51:17\n",
            ">>> val_acc: 0.7322, val_precision: 0.7322 val_recall: 0.7322, val_f1: 0.7322\n",
            ">> saved: state_dict/bert_spc_twitter_know_val_f1_0.7322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 11:51:19\n",
            "loss: 0.6970, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-05-20 11:51:25\n",
            "loss: 0.5816, acc: 0.7528\n",
            "E2E-ABSA >>> 2022-05-20 11:51:31\n",
            "loss: 0.5973, acc: 0.7470\n",
            "E2E-ABSA >>> 2022-05-20 11:51:36\n",
            "loss: 0.5745, acc: 0.7681\n",
            "E2E-ABSA >>> 2022-05-20 11:51:42\n",
            "loss: 0.5715, acc: 0.7660\n",
            "E2E-ABSA >>> 2022-05-20 11:51:48\n",
            "loss: 0.5790, acc: 0.7629\n",
            "E2E-ABSA >>> 2022-05-20 11:51:52\n",
            ">>> val_acc: 0.7796, val_precision: 0.7796 val_recall: 0.7796, val_f1: 0.7796\n",
            ">> saved: state_dict/bert_spc_twitter_know_val_f1_0.7796\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 11:51:58\n",
            "loss: 0.3999, acc: 0.8398\n",
            "E2E-ABSA >>> 2022-05-20 11:52:04\n",
            "loss: 0.3319, acc: 0.8698\n",
            "E2E-ABSA >>> 2022-05-20 11:52:10\n",
            "loss: 0.3438, acc: 0.8661\n",
            "E2E-ABSA >>> 2022-05-20 11:52:16\n",
            "loss: 0.3764, acc: 0.8536\n",
            "E2E-ABSA >>> 2022-05-20 11:52:22\n",
            "loss: 0.3906, acc: 0.8470\n",
            "E2E-ABSA >>> 2022-05-20 11:52:28\n",
            ">>> val_acc: 0.7393, val_precision: 0.7393 val_recall: 0.7393, val_f1: 0.7393\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 11:52:31\n",
            "loss: 0.2288, acc: 0.9313\n",
            "E2E-ABSA >>> 2022-05-20 11:52:37\n",
            "loss: 0.2223, acc: 0.9229\n",
            "E2E-ABSA >>> 2022-05-20 11:52:43\n",
            "loss: 0.2252, acc: 0.9175\n",
            "E2E-ABSA >>> 2022-05-20 11:52:48\n",
            "loss: 0.2272, acc: 0.9187\n",
            "E2E-ABSA >>> 2022-05-20 11:52:54\n",
            "loss: 0.2359, acc: 0.9090\n",
            "E2E-ABSA >>> 2022-05-20 11:53:02\n",
            ">>> val_acc: 0.7630, val_precision: 0.7630 val_recall: 0.7630, val_f1: 0.7630\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 11:53:03\n",
            "loss: 0.1892, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-20 11:53:09\n",
            "loss: 0.1628, acc: 0.9427\n",
            "E2E-ABSA >>> 2022-05-20 11:53:15\n",
            "loss: 0.1551, acc: 0.9503\n",
            "E2E-ABSA >>> 2022-05-20 11:53:21\n",
            "loss: 0.1482, acc: 0.9521\n",
            "E2E-ABSA >>> 2022-05-20 11:53:27\n",
            "loss: 0.1413, acc: 0.9546\n",
            "E2E-ABSA >>> 2022-05-20 11:53:32\n",
            "loss: 0.1357, acc: 0.9549\n",
            "E2E-ABSA >>> 2022-05-20 11:53:36\n",
            ">>> val_acc: 0.7488, val_precision: 0.7488 val_recall: 0.7488, val_f1: 0.7488\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 11:53:41\n",
            "loss: 0.1141, acc: 0.9618\n",
            "E2E-ABSA >>> 2022-05-20 11:53:47\n",
            "loss: 0.1185, acc: 0.9671\n",
            "E2E-ABSA >>> 2022-05-20 11:53:53\n",
            "loss: 0.1170, acc: 0.9655\n",
            "E2E-ABSA >>> 2022-05-20 11:53:59\n",
            "loss: 0.1227, acc: 0.9639\n",
            "E2E-ABSA >>> 2022-05-20 11:54:05\n",
            "loss: 0.1177, acc: 0.9643\n",
            "E2E-ABSA >>> 2022-05-20 11:54:10\n",
            ">>> val_acc: 0.7464, val_precision: 0.7464 val_recall: 0.7464, val_f1: 0.7464\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 11:54:13\n",
            "loss: 0.0616, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:54:19\n",
            "loss: 0.0725, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:54:25\n",
            "loss: 0.0866, acc: 0.9639\n",
            "E2E-ABSA >>> 2022-05-20 11:54:31\n",
            "loss: 0.1093, acc: 0.9566\n",
            "E2E-ABSA >>> 2022-05-20 11:54:37\n",
            "loss: 0.1068, acc: 0.9586\n",
            "E2E-ABSA >>> 2022-05-20 11:54:44\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 11:54:46\n",
            "loss: 0.1233, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:54:52\n",
            "loss: 0.1147, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:54:58\n",
            "loss: 0.1030, acc: 0.9715\n",
            "E2E-ABSA >>> 2022-05-20 11:55:03\n",
            "loss: 0.1058, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:55:09\n",
            "loss: 0.1049, acc: 0.9666\n",
            "E2E-ABSA >>> 2022-05-20 11:55:15\n",
            "loss: 0.1352, acc: 0.9544\n",
            "E2E-ABSA >>> 2022-05-20 11:55:18\n",
            ">>> val_acc: 0.7299, val_precision: 0.7299 val_recall: 0.7299, val_f1: 0.7299\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 11:55:24\n",
            "loss: 0.1321, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-05-20 11:55:30\n",
            "loss: 0.1185, acc: 0.9594\n",
            "E2E-ABSA >>> 2022-05-20 11:55:36\n",
            "loss: 0.1001, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:55:42\n",
            "loss: 0.0867, acc: 0.9727\n",
            "E2E-ABSA >>> 2022-05-20 11:55:48\n",
            "loss: 0.0892, acc: 0.9712\n",
            "E2E-ABSA >>> 2022-05-20 11:55:52\n",
            ">>> val_acc: 0.7654, val_precision: 0.7654 val_recall: 0.7654, val_f1: 0.7654\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 11:55:56\n",
            "loss: 0.0731, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 11:56:02\n",
            "loss: 0.0744, acc: 0.9743\n",
            "E2E-ABSA >>> 2022-05-20 11:56:08\n",
            "loss: 0.0779, acc: 0.9664\n",
            "E2E-ABSA >>> 2022-05-20 11:56:14\n",
            "loss: 0.0767, acc: 0.9679\n",
            "E2E-ABSA >>> 2022-05-20 11:56:20\n",
            "loss: 0.0844, acc: 0.9654\n",
            "E2E-ABSA >>> 2022-05-20 11:56:26\n",
            ">>> val_acc: 0.7583, val_precision: 0.7583 val_recall: 0.7583, val_f1: 0.7583\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 11:56:28\n",
            "loss: 0.0439, acc: 0.9922\n",
            "E2E-ABSA >>> 2022-05-20 11:56:34\n",
            "loss: 0.0637, acc: 0.9821\n",
            "E2E-ABSA >>> 2022-05-20 11:56:40\n",
            "loss: 0.0576, acc: 0.9831\n",
            "E2E-ABSA >>> 2022-05-20 11:56:46\n",
            "loss: 0.0577, acc: 0.9816\n",
            "E2E-ABSA >>> 2022-05-20 11:56:52\n",
            "loss: 0.0574, acc: 0.9808\n",
            "E2E-ABSA >>> 2022-05-20 11:57:00\n",
            ">>> val_acc: 0.7441, val_precision: 0.7441 val_recall: 0.7441, val_f1: 0.7441\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 11:57:00\n",
            "loss: 0.0196, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:57:06\n",
            "loss: 0.0542, acc: 0.9716\n",
            "E2E-ABSA >>> 2022-05-20 11:57:12\n",
            "loss: 0.0731, acc: 0.9732\n",
            "E2E-ABSA >>> 2022-05-20 11:57:18\n",
            "loss: 0.0725, acc: 0.9738\n",
            "E2E-ABSA >>> 2022-05-20 11:57:24\n",
            "loss: 0.0736, acc: 0.9703\n",
            "E2E-ABSA >>> 2022-05-20 11:57:30\n",
            "loss: 0.0876, acc: 0.9694\n",
            "E2E-ABSA >>> 2022-05-20 11:57:34\n",
            ">>> val_acc: 0.7346, val_precision: 0.7346 val_recall: 0.7346, val_f1: 0.7346\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 11:57:38\n",
            "loss: 0.0393, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:57:44\n",
            "loss: 0.0363, acc: 0.9861\n",
            "E2E-ABSA >>> 2022-05-20 11:57:50\n",
            "loss: 0.0343, acc: 0.9877\n",
            "E2E-ABSA >>> 2022-05-20 11:57:56\n",
            "loss: 0.0471, acc: 0.9836\n",
            "E2E-ABSA >>> 2022-05-20 11:58:02\n",
            "loss: 0.0562, acc: 0.9785\n",
            "E2E-ABSA >>> 2022-05-20 11:58:08\n",
            ">>> val_acc: 0.6825, val_precision: 0.6825 val_recall: 0.6825, val_f1: 0.6825\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 11:58:11\n",
            "loss: 0.0658, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-05-20 11:58:17\n",
            "loss: 0.1040, acc: 0.9646\n",
            "E2E-ABSA >>> 2022-05-20 11:58:23\n",
            "loss: 0.0874, acc: 0.9712\n",
            "E2E-ABSA >>> 2022-05-20 11:58:29\n",
            "loss: 0.0825, acc: 0.9732\n",
            "E2E-ABSA >>> 2022-05-20 11:58:35\n",
            "loss: 0.0757, acc: 0.9757\n",
            "E2E-ABSA >>> 2022-05-20 11:58:42\n",
            ">>> val_acc: 0.6872, val_precision: 0.6872 val_recall: 0.6872, val_f1: 0.6872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 11:58:43\n",
            "loss: 0.0258, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:58:49\n",
            "loss: 0.0551, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 11:58:55\n",
            "loss: 0.0538, acc: 0.9801\n",
            "E2E-ABSA >>> 2022-05-20 11:59:01\n",
            "loss: 0.0726, acc: 0.9756\n",
            "E2E-ABSA >>> 2022-05-20 11:59:07\n",
            "loss: 0.0660, acc: 0.9784\n",
            "E2E-ABSA >>> 2022-05-20 11:59:13\n",
            "loss: 0.0733, acc: 0.9754\n",
            "E2E-ABSA >>> 2022-05-20 11:59:16\n",
            ">>> val_acc: 0.7346, val_precision: 0.7346 val_recall: 0.7346, val_f1: 0.7346\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 11:59:21\n",
            "loss: 0.0865, acc: 0.9722\n",
            "E2E-ABSA >>> 2022-05-20 11:59:27\n",
            "loss: 0.0748, acc: 0.9753\n",
            "E2E-ABSA >>> 2022-05-20 11:59:33\n",
            "loss: 0.0543, acc: 0.9828\n",
            "E2E-ABSA >>> 2022-05-20 11:59:39\n",
            "loss: 0.0513, acc: 0.9832\n",
            "E2E-ABSA >>> 2022-05-20 11:59:45\n",
            "loss: 0.0649, acc: 0.9777\n",
            "E2E-ABSA >>> 2022-05-20 11:59:50\n",
            ">>> val_acc: 0.7322, val_precision: 0.7322 val_recall: 0.7322, val_f1: 0.7322\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 11:59:54\n",
            "loss: 0.0719, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 12:00:00\n",
            "loss: 0.1158, acc: 0.9551\n",
            "E2E-ABSA >>> 2022-05-20 12:00:06\n",
            "loss: 0.1051, acc: 0.9591\n",
            "E2E-ABSA >>> 2022-05-20 12:00:11\n",
            "loss: 0.0938, acc: 0.9644\n",
            "E2E-ABSA >>> 2022-05-20 12:00:17\n",
            "loss: 0.0890, acc: 0.9667\n",
            "E2E-ABSA >>> 2022-05-20 12:00:24\n",
            ">>> val_acc: 0.7109, val_precision: 0.7109 val_recall: 0.7109, val_f1: 0.7109\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 12:00:26\n",
            "loss: 0.1053, acc: 0.9583\n",
            "E2E-ABSA >>> 2022-05-20 12:00:32\n",
            "loss: 0.0767, acc: 0.9663\n",
            "E2E-ABSA >>> 2022-05-20 12:00:38\n",
            "loss: 0.0632, acc: 0.9742\n",
            "E2E-ABSA >>> 2022-05-20 12:00:44\n",
            "loss: 0.0585, acc: 0.9773\n",
            "E2E-ABSA >>> 2022-05-20 12:00:50\n",
            "loss: 0.0520, acc: 0.9797\n",
            "E2E-ABSA >>> 2022-05-20 12:00:55\n",
            "loss: 0.0557, acc: 0.9787\n",
            "E2E-ABSA >>> 2022-05-20 12:00:58\n",
            ">>> val_acc: 0.7393, val_precision: 0.7393 val_recall: 0.7393, val_f1: 0.7393\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 12:01:04\n",
            "loss: 0.1374, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-05-20 12:01:10\n",
            "loss: 0.1549, acc: 0.9578\n",
            "E2E-ABSA >>> 2022-05-20 12:01:16\n",
            "loss: 0.1340, acc: 0.9604\n",
            "E2E-ABSA >>> 2022-05-20 12:01:22\n",
            "loss: 0.1164, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-05-20 12:01:28\n",
            "loss: 0.1033, acc: 0.9675\n",
            "E2E-ABSA >>> 2022-05-20 12:01:32\n",
            ">>> val_acc: 0.7014, val_precision: 0.7014 val_recall: 0.7014, val_f1: 0.7014\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 12:01:36\n",
            "loss: 0.0623, acc: 0.9777\n",
            "E2E-ABSA >>> 2022-05-20 12:01:42\n",
            "loss: 0.0511, acc: 0.9816\n",
            "E2E-ABSA >>> 2022-05-20 12:01:48\n",
            "loss: 0.0476, acc: 0.9838\n",
            "E2E-ABSA >>> 2022-05-20 12:01:54\n",
            "loss: 0.0415, acc: 0.9848\n",
            "E2E-ABSA >>> 2022-05-20 12:02:00\n",
            "loss: 0.0442, acc: 0.9834\n",
            "E2E-ABSA >>> 2022-05-20 12:02:06\n",
            ">>> val_acc: 0.7156, val_precision: 0.7156 val_recall: 0.7156, val_f1: 0.7156\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 12:02:08\n",
            "loss: 0.0140, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 12:02:14\n",
            "loss: 0.0199, acc: 0.9978\n",
            "E2E-ABSA >>> 2022-05-20 12:02:20\n",
            "loss: 0.0273, acc: 0.9909\n",
            "E2E-ABSA >>> 2022-05-20 12:02:26\n",
            "loss: 0.0357, acc: 0.9871\n",
            "E2E-ABSA >>> 2022-05-20 12:02:32\n",
            "loss: 0.0440, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 12:02:40\n",
            ">>> val_acc: 0.7085, val_precision: 0.7085 val_recall: 0.7085, val_f1: 0.7085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 12:02:41\n",
            "loss: 0.0786, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:02:47\n",
            "loss: 0.0676, acc: 0.9744\n",
            "E2E-ABSA >>> 2022-05-20 12:02:53\n",
            "loss: 0.0462, acc: 0.9851\n",
            "E2E-ABSA >>> 2022-05-20 12:02:59\n",
            "loss: 0.0432, acc: 0.9839\n",
            "E2E-ABSA >>> 2022-05-20 12:03:05\n",
            "loss: 0.0466, acc: 0.9809\n",
            "E2E-ABSA >>> 2022-05-20 12:03:11\n",
            "loss: 0.0671, acc: 0.9737\n",
            "E2E-ABSA >>> 2022-05-20 12:03:14\n",
            ">>> val_acc: 0.7464, val_precision: 0.7464 val_recall: 0.7464, val_f1: 0.7464\n",
            "E2E-ABSA >>> 2022-05-20 12:03:14\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7796, val_precision: 0.7796 val_recall: 0.7796, val_f1: 0.7796\n",
            "you can download the best model from state_dict/bert_spc_twitter_know_val_f1_0.7796\n",
            ">>> test_acc: 0.7796, test_precision: 0.7796, test_recall: 0.7796, test_f1: 0.7796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **SemEval2014** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "XBi2jlRjhhao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset SemEval2014 --log_step 20  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "id": "SFBosKd6gVM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37bc09b-5a09-46bb-820f-7212d72174e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 3102.\n",
            "> testing dataset count: 331.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f029d35ea70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 12:04:42\n",
            "loss: 1.0573, acc: 0.5156\n",
            "E2E-ABSA >>> 2022-05-20 12:04:47\n",
            "loss: 1.0461, acc: 0.5109\n",
            "E2E-ABSA >>> 2022-05-20 12:04:53\n",
            "loss: 1.0337, acc: 0.5188\n",
            "E2E-ABSA >>> 2022-05-20 12:04:59\n",
            "loss: 1.0209, acc: 0.5344\n",
            "E2E-ABSA >>> 2022-05-20 12:05:05\n",
            "loss: 1.0189, acc: 0.5400\n",
            "E2E-ABSA >>> 2022-05-20 12:05:10\n",
            "loss: 1.0088, acc: 0.5443\n",
            "E2E-ABSA >>> 2022-05-20 12:05:16\n",
            "loss: 0.9898, acc: 0.5567\n",
            "E2E-ABSA >>> 2022-05-20 12:05:22\n",
            "loss: 0.9835, acc: 0.5598\n",
            "E2E-ABSA >>> 2022-05-20 12:05:28\n",
            "loss: 0.9550, acc: 0.5788\n",
            "E2E-ABSA >>> 2022-05-20 12:05:35\n",
            ">>> val_acc: 0.7674, val_precision: 0.7674 val_recall: 0.7674, val_f1: 0.7674\n",
            ">> saved: state_dict/bert_spc_SemEval2014_val_f1_0.7674\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 12:05:38\n",
            "loss: 0.7629, acc: 0.6667\n",
            "E2E-ABSA >>> 2022-05-20 12:05:44\n",
            "loss: 0.6902, acc: 0.6923\n",
            "E2E-ABSA >>> 2022-05-20 12:05:50\n",
            "loss: 0.6392, acc: 0.7269\n",
            "E2E-ABSA >>> 2022-05-20 12:05:56\n",
            "loss: 0.6073, acc: 0.7434\n",
            "E2E-ABSA >>> 2022-05-20 12:06:02\n",
            "loss: 0.6018, acc: 0.7544\n",
            "E2E-ABSA >>> 2022-05-20 12:06:08\n",
            "loss: 0.5820, acc: 0.7618\n",
            "E2E-ABSA >>> 2022-05-20 12:06:14\n",
            "loss: 0.5681, acc: 0.7654\n",
            "E2E-ABSA >>> 2022-05-20 12:06:20\n",
            "loss: 0.5732, acc: 0.7603\n",
            "E2E-ABSA >>> 2022-05-20 12:06:26\n",
            "loss: 0.5677, acc: 0.7617\n",
            "E2E-ABSA >>> 2022-05-20 12:06:32\n",
            "loss: 0.5717, acc: 0.7597\n",
            "E2E-ABSA >>> 2022-05-20 12:06:36\n",
            ">>> val_acc: 0.7674, val_precision: 0.7674 val_recall: 0.7674, val_f1: 0.7674\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 12:06:39\n",
            "loss: 0.4427, acc: 0.8698\n",
            "E2E-ABSA >>> 2022-05-20 12:06:45\n",
            "loss: 0.4470, acc: 0.8398\n",
            "E2E-ABSA >>> 2022-05-20 12:06:51\n",
            "loss: 0.4136, acc: 0.8413\n",
            "E2E-ABSA >>> 2022-05-20 12:06:57\n",
            "loss: 0.4189, acc: 0.8429\n",
            "E2E-ABSA >>> 2022-05-20 12:07:03\n",
            "loss: 0.4120, acc: 0.8451\n",
            "E2E-ABSA >>> 2022-05-20 12:07:09\n",
            "loss: 0.4216, acc: 0.8421\n",
            "E2E-ABSA >>> 2022-05-20 12:07:15\n",
            "loss: 0.4177, acc: 0.8414\n",
            "E2E-ABSA >>> 2022-05-20 12:07:21\n",
            "loss: 0.4137, acc: 0.8400\n",
            "E2E-ABSA >>> 2022-05-20 12:07:27\n",
            "loss: 0.4195, acc: 0.8365\n",
            "E2E-ABSA >>> 2022-05-20 12:07:33\n",
            "loss: 0.4122, acc: 0.8392\n",
            "E2E-ABSA >>> 2022-05-20 12:07:36\n",
            ">>> val_acc: 0.7734, val_precision: 0.7734 val_recall: 0.7734, val_f1: 0.7734\n",
            ">> saved: state_dict/bert_spc_SemEval2014_val_f1_0.7734\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 12:07:42\n",
            "loss: 0.2706, acc: 0.8889\n",
            "E2E-ABSA >>> 2022-05-20 12:07:48\n",
            "loss: 0.2242, acc: 0.9145\n",
            "E2E-ABSA >>> 2022-05-20 12:07:54\n",
            "loss: 0.2412, acc: 0.9149\n",
            "E2E-ABSA >>> 2022-05-20 12:08:00\n",
            "loss: 0.2357, acc: 0.9143\n",
            "E2E-ABSA >>> 2022-05-20 12:08:06\n",
            "loss: 0.2567, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-20 12:08:12\n",
            "loss: 0.2675, acc: 0.9010\n",
            "E2E-ABSA >>> 2022-05-20 12:08:18\n",
            "loss: 0.2847, acc: 0.8931\n",
            "E2E-ABSA >>> 2022-05-20 12:08:24\n",
            "loss: 0.2941, acc: 0.8881\n",
            "E2E-ABSA >>> 2022-05-20 12:08:30\n",
            "loss: 0.3008, acc: 0.8831\n",
            "E2E-ABSA >>> 2022-05-20 12:08:36\n",
            ">>> val_acc: 0.7825, val_precision: 0.7825 val_recall: 0.7825, val_f1: 0.7825\n",
            ">> saved: state_dict/bert_spc_SemEval2014_val_f1_0.7825\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 12:08:39\n",
            "loss: 0.1169, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:08:45\n",
            "loss: 0.1675, acc: 0.9401\n",
            "E2E-ABSA >>> 2022-05-20 12:08:51\n",
            "loss: 0.1554, acc: 0.9418\n",
            "E2E-ABSA >>> 2022-05-20 12:08:57\n",
            "loss: 0.1732, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-20 12:09:03\n",
            "loss: 0.1861, acc: 0.9315\n",
            "E2E-ABSA >>> 2022-05-20 12:09:09\n",
            "loss: 0.2014, acc: 0.9273\n",
            "E2E-ABSA >>> 2022-05-20 12:09:15\n",
            "loss: 0.2176, acc: 0.9194\n",
            "E2E-ABSA >>> 2022-05-20 12:09:21\n",
            "loss: 0.2216, acc: 0.9171\n",
            "E2E-ABSA >>> 2022-05-20 12:09:27\n",
            "loss: 0.2226, acc: 0.9154\n",
            "E2E-ABSA >>> 2022-05-20 12:09:33\n",
            "loss: 0.2221, acc: 0.9154\n",
            "E2E-ABSA >>> 2022-05-20 12:09:38\n",
            ">>> val_acc: 0.7674, val_precision: 0.7674 val_recall: 0.7674, val_f1: 0.7674\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 12:09:41\n",
            "loss: 0.1009, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-05-20 12:09:47\n",
            "loss: 0.0898, acc: 0.9729\n",
            "E2E-ABSA >>> 2022-05-20 12:09:53\n",
            "loss: 0.1262, acc: 0.9600\n",
            "E2E-ABSA >>> 2022-05-20 12:09:59\n",
            "loss: 0.1371, acc: 0.9554\n",
            "E2E-ABSA >>> 2022-05-20 12:10:04\n",
            "loss: 0.1424, acc: 0.9528\n",
            "E2E-ABSA >>> 2022-05-20 12:10:10\n",
            "loss: 0.1433, acc: 0.9517\n",
            "E2E-ABSA >>> 2022-05-20 12:10:16\n",
            "loss: 0.1581, acc: 0.9437\n",
            "E2E-ABSA >>> 2022-05-20 12:10:22\n",
            "loss: 0.1587, acc: 0.9417\n",
            "E2E-ABSA >>> 2022-05-20 12:10:28\n",
            "loss: 0.1643, acc: 0.9404\n",
            "E2E-ABSA >>> 2022-05-20 12:10:34\n",
            "loss: 0.1705, acc: 0.9378\n",
            "E2E-ABSA >>> 2022-05-20 12:10:37\n",
            ">>> val_acc: 0.7764, val_precision: 0.7764 val_recall: 0.7764, val_f1: 0.7764\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 12:10:42\n",
            "loss: 0.1652, acc: 0.9492\n",
            "E2E-ABSA >>> 2022-05-20 12:10:48\n",
            "loss: 0.2514, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-05-20 12:10:54\n",
            "loss: 0.2301, acc: 0.9252\n",
            "E2E-ABSA >>> 2022-05-20 12:11:00\n",
            "loss: 0.2129, acc: 0.9285\n",
            "E2E-ABSA >>> 2022-05-20 12:11:06\n",
            "loss: 0.2026, acc: 0.9329\n",
            "E2E-ABSA >>> 2022-05-20 12:11:12\n",
            "loss: 0.1937, acc: 0.9353\n",
            "E2E-ABSA >>> 2022-05-20 12:11:18\n",
            "loss: 0.1819, acc: 0.9403\n",
            "E2E-ABSA >>> 2022-05-20 12:11:24\n",
            "loss: 0.1806, acc: 0.9387\n",
            "E2E-ABSA >>> 2022-05-20 12:11:30\n",
            "loss: 0.1781, acc: 0.9393\n",
            "E2E-ABSA >>> 2022-05-20 12:11:37\n",
            ">>> val_acc: 0.7734, val_precision: 0.7734 val_recall: 0.7734, val_f1: 0.7734\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 12:11:37\n",
            "loss: 0.0802, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 12:11:43\n",
            "loss: 0.1036, acc: 0.9773\n",
            "E2E-ABSA >>> 2022-05-20 12:11:49\n",
            "loss: 0.0787, acc: 0.9821\n",
            "E2E-ABSA >>> 2022-05-20 12:11:55\n",
            "loss: 0.0723, acc: 0.9819\n",
            "E2E-ABSA >>> 2022-05-20 12:12:01\n",
            "loss: 0.0829, acc: 0.9771\n",
            "E2E-ABSA >>> 2022-05-20 12:12:07\n",
            "loss: 0.0898, acc: 0.9730\n",
            "E2E-ABSA >>> 2022-05-20 12:12:13\n",
            "loss: 0.1125, acc: 0.9631\n",
            "E2E-ABSA >>> 2022-05-20 12:12:19\n",
            "loss: 0.1290, acc: 0.9564\n",
            "E2E-ABSA >>> 2022-05-20 12:12:25\n",
            "loss: 0.1348, acc: 0.9533\n",
            "E2E-ABSA >>> 2022-05-20 12:12:31\n",
            "loss: 0.1306, acc: 0.9550\n",
            "E2E-ABSA >>> 2022-05-20 12:12:36\n",
            ">>> val_acc: 0.7795, val_precision: 0.7795 val_recall: 0.7795, val_f1: 0.7795\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 12:12:39\n",
            "loss: 0.1002, acc: 0.9609\n",
            "E2E-ABSA >>> 2022-05-20 12:12:45\n",
            "loss: 0.1024, acc: 0.9710\n",
            "E2E-ABSA >>> 2022-05-20 12:12:51\n",
            "loss: 0.1056, acc: 0.9701\n",
            "E2E-ABSA >>> 2022-05-20 12:12:57\n",
            "loss: 0.1121, acc: 0.9632\n",
            "E2E-ABSA >>> 2022-05-20 12:13:03\n",
            "loss: 0.1138, acc: 0.9631\n",
            "E2E-ABSA >>> 2022-05-20 12:13:09\n",
            "loss: 0.1108, acc: 0.9630\n",
            "E2E-ABSA >>> 2022-05-20 12:13:15\n",
            "loss: 0.1135, acc: 0.9614\n",
            "E2E-ABSA >>> 2022-05-20 12:13:20\n",
            "loss: 0.1218, acc: 0.9590\n",
            "E2E-ABSA >>> 2022-05-20 12:13:26\n",
            "loss: 0.1318, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-05-20 12:13:32\n",
            "loss: 0.1340, acc: 0.9518\n",
            "E2E-ABSA >>> 2022-05-20 12:13:36\n",
            ">>> val_acc: 0.7583, val_precision: 0.7583 val_recall: 0.7583, val_f1: 0.7583\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 12:13:40\n",
            "loss: 0.1187, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:13:46\n",
            "loss: 0.1171, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:13:52\n",
            "loss: 0.1038, acc: 0.9711\n",
            "E2E-ABSA >>> 2022-05-20 12:13:58\n",
            "loss: 0.1291, acc: 0.9578\n",
            "E2E-ABSA >>> 2022-05-20 12:14:04\n",
            "loss: 0.1339, acc: 0.9548\n",
            "E2E-ABSA >>> 2022-05-20 12:14:10\n",
            "loss: 0.1336, acc: 0.9539\n",
            "E2E-ABSA >>> 2022-05-20 12:14:16\n",
            "loss: 0.1261, acc: 0.9580\n",
            "E2E-ABSA >>> 2022-05-20 12:14:22\n",
            "loss: 0.1314, acc: 0.9562\n",
            "E2E-ABSA >>> 2022-05-20 12:14:28\n",
            "loss: 0.1299, acc: 0.9569\n",
            "E2E-ABSA >>> 2022-05-20 12:14:34\n",
            "loss: 0.1282, acc: 0.9578\n",
            "E2E-ABSA >>> 2022-05-20 12:14:36\n",
            ">>> val_acc: 0.7613, val_precision: 0.7613 val_recall: 0.7613, val_f1: 0.7613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 12:14:42\n",
            "loss: 0.0736, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:14:48\n",
            "loss: 0.1105, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-05-20 12:14:54\n",
            "loss: 0.1043, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-05-20 12:15:00\n",
            "loss: 0.0997, acc: 0.9617\n",
            "E2E-ABSA >>> 2022-05-20 12:15:06\n",
            "loss: 0.1080, acc: 0.9619\n",
            "E2E-ABSA >>> 2022-05-20 12:15:12\n",
            "loss: 0.1144, acc: 0.9583\n",
            "E2E-ABSA >>> 2022-05-20 12:15:18\n",
            "loss: 0.1213, acc: 0.9567\n",
            "E2E-ABSA >>> 2022-05-20 12:15:24\n",
            "loss: 0.1176, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-05-20 12:15:30\n",
            "loss: 0.1165, acc: 0.9573\n",
            "E2E-ABSA >>> 2022-05-20 12:15:36\n",
            ">>> val_acc: 0.7462, val_precision: 0.7462 val_recall: 0.7462, val_f1: 0.7462\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 12:15:38\n",
            "loss: 0.1469, acc: 0.9792\n",
            "E2E-ABSA >>> 2022-05-20 12:15:43\n",
            "loss: 0.1097, acc: 0.9639\n",
            "E2E-ABSA >>> 2022-05-20 12:15:49\n",
            "loss: 0.0792, acc: 0.9755\n",
            "E2E-ABSA >>> 2022-05-20 12:15:55\n",
            "loss: 0.0749, acc: 0.9754\n",
            "E2E-ABSA >>> 2022-05-20 12:16:01\n",
            "loss: 0.0831, acc: 0.9724\n",
            "E2E-ABSA >>> 2022-05-20 12:16:07\n",
            "loss: 0.0950, acc: 0.9682\n",
            "E2E-ABSA >>> 2022-05-20 12:16:13\n",
            "loss: 0.1525, acc: 0.9514\n",
            "E2E-ABSA >>> 2022-05-20 12:16:19\n",
            "loss: 0.1502, acc: 0.9533\n",
            "E2E-ABSA >>> 2022-05-20 12:16:25\n",
            "loss: 0.1492, acc: 0.9514\n",
            "E2E-ABSA >>> 2022-05-20 12:16:31\n",
            "loss: 0.1481, acc: 0.9496\n",
            "E2E-ABSA >>> 2022-05-20 12:16:35\n",
            ">>> val_acc: 0.7613, val_precision: 0.7613 val_recall: 0.7613, val_f1: 0.7613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 12:16:39\n",
            "loss: 0.1070, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:16:45\n",
            "loss: 0.1466, acc: 0.9551\n",
            "E2E-ABSA >>> 2022-05-20 12:16:51\n",
            "loss: 0.1304, acc: 0.9579\n",
            "E2E-ABSA >>> 2022-05-20 12:16:57\n",
            "loss: 0.1245, acc: 0.9609\n",
            "E2E-ABSA >>> 2022-05-20 12:17:03\n",
            "loss: 0.1167, acc: 0.9620\n",
            "E2E-ABSA >>> 2022-05-20 12:17:09\n",
            "loss: 0.1215, acc: 0.9581\n",
            "E2E-ABSA >>> 2022-05-20 12:17:14\n",
            "loss: 0.1265, acc: 0.9550\n",
            "E2E-ABSA >>> 2022-05-20 12:17:20\n",
            "loss: 0.1266, acc: 0.9552\n",
            "E2E-ABSA >>> 2022-05-20 12:17:26\n",
            "loss: 0.1285, acc: 0.9549\n",
            "E2E-ABSA >>> 2022-05-20 12:17:32\n",
            "loss: 0.1244, acc: 0.9564\n",
            "E2E-ABSA >>> 2022-05-20 12:17:35\n",
            ">>> val_acc: 0.7613, val_precision: 0.7613 val_recall: 0.7613, val_f1: 0.7613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 12:17:40\n",
            "loss: 0.0495, acc: 0.9826\n",
            "E2E-ABSA >>> 2022-05-20 12:17:46\n",
            "loss: 0.0565, acc: 0.9819\n",
            "E2E-ABSA >>> 2022-05-20 12:17:52\n",
            "loss: 0.0674, acc: 0.9752\n",
            "E2E-ABSA >>> 2022-05-20 12:17:58\n",
            "loss: 0.0838, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:18:04\n",
            "loss: 0.0906, acc: 0.9662\n",
            "E2E-ABSA >>> 2022-05-20 12:18:10\n",
            "loss: 0.0904, acc: 0.9677\n",
            "E2E-ABSA >>> 2022-05-20 12:18:16\n",
            "loss: 0.0933, acc: 0.9674\n",
            "E2E-ABSA >>> 2022-05-20 12:18:22\n",
            "loss: 0.0963, acc: 0.9664\n",
            "E2E-ABSA >>> 2022-05-20 12:18:28\n",
            "loss: 0.0949, acc: 0.9666\n",
            "E2E-ABSA >>> 2022-05-20 12:18:34\n",
            ">>> val_acc: 0.7402, val_precision: 0.7402 val_recall: 0.7402, val_f1: 0.7402\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 12:18:36\n",
            "loss: 0.0548, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 12:18:41\n",
            "loss: 0.0997, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:18:47\n",
            "loss: 0.0958, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:18:53\n",
            "loss: 0.1080, acc: 0.9629\n",
            "E2E-ABSA >>> 2022-05-20 12:18:59\n",
            "loss: 0.1129, acc: 0.9621\n",
            "E2E-ABSA >>> 2022-05-20 12:19:05\n",
            "loss: 0.0989, acc: 0.9681\n",
            "E2E-ABSA >>> 2022-05-20 12:19:11\n",
            "loss: 0.0961, acc: 0.9698\n",
            "E2E-ABSA >>> 2022-05-20 12:19:17\n",
            "loss: 0.1041, acc: 0.9657\n",
            "E2E-ABSA >>> 2022-05-20 12:19:23\n",
            "loss: 0.1003, acc: 0.9657\n",
            "E2E-ABSA >>> 2022-05-20 12:19:29\n",
            "loss: 0.1087, acc: 0.9606\n",
            "E2E-ABSA >>> 2022-05-20 12:19:34\n",
            ">>> val_acc: 0.7402, val_precision: 0.7402 val_recall: 0.7402, val_f1: 0.7402\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 12:19:37\n",
            "loss: 0.0801, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-05-20 12:19:43\n",
            "loss: 0.0615, acc: 0.9854\n",
            "E2E-ABSA >>> 2022-05-20 12:19:49\n",
            "loss: 0.0685, acc: 0.9788\n",
            "E2E-ABSA >>> 2022-05-20 12:19:55\n",
            "loss: 0.1007, acc: 0.9661\n",
            "E2E-ABSA >>> 2022-05-20 12:20:00\n",
            "loss: 0.1046, acc: 0.9660\n",
            "E2E-ABSA >>> 2022-05-20 12:20:06\n",
            "loss: 0.1090, acc: 0.9642\n",
            "E2E-ABSA >>> 2022-05-20 12:20:12\n",
            "loss: 0.1521, acc: 0.9462\n",
            "E2E-ABSA >>> 2022-05-20 12:20:18\n",
            "loss: 0.1681, acc: 0.9383\n",
            "E2E-ABSA >>> 2022-05-20 12:20:24\n",
            "loss: 0.1700, acc: 0.9382\n",
            "E2E-ABSA >>> 2022-05-20 12:20:30\n",
            "loss: 0.1678, acc: 0.9411\n",
            "E2E-ABSA >>> 2022-05-20 12:20:33\n",
            ">>> val_acc: 0.7613, val_precision: 0.7613 val_recall: 0.7613, val_f1: 0.7613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 12:20:38\n",
            "loss: 0.0559, acc: 0.9805\n",
            "E2E-ABSA >>> 2022-05-20 12:20:44\n",
            "loss: 0.0447, acc: 0.9878\n",
            "E2E-ABSA >>> 2022-05-20 12:20:50\n",
            "loss: 0.0623, acc: 0.9799\n",
            "E2E-ABSA >>> 2022-05-20 12:20:56\n",
            "loss: 0.0960, acc: 0.9671\n",
            "E2E-ABSA >>> 2022-05-20 12:21:02\n",
            "loss: 0.1135, acc: 0.9583\n",
            "E2E-ABSA >>> 2022-05-20 12:21:08\n",
            "loss: 0.1118, acc: 0.9607\n",
            "E2E-ABSA >>> 2022-05-20 12:21:14\n",
            "loss: 0.1062, acc: 0.9632\n",
            "E2E-ABSA >>> 2022-05-20 12:21:20\n",
            "loss: 0.1011, acc: 0.9647\n",
            "E2E-ABSA >>> 2022-05-20 12:21:26\n",
            "loss: 0.1071, acc: 0.9627\n",
            "E2E-ABSA >>> 2022-05-20 12:21:33\n",
            ">>> val_acc: 0.7613, val_precision: 0.7613 val_recall: 0.7613, val_f1: 0.7613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 12:21:34\n",
            "loss: 0.0359, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 12:21:40\n",
            "loss: 0.0685, acc: 0.9830\n",
            "E2E-ABSA >>> 2022-05-20 12:21:46\n",
            "loss: 0.0649, acc: 0.9792\n",
            "E2E-ABSA >>> 2022-05-20 12:21:52\n",
            "loss: 0.0604, acc: 0.9788\n",
            "E2E-ABSA >>> 2022-05-20 12:21:57\n",
            "loss: 0.0601, acc: 0.9794\n",
            "E2E-ABSA >>> 2022-05-20 12:22:03\n",
            "loss: 0.0593, acc: 0.9798\n",
            "E2E-ABSA >>> 2022-05-20 12:22:09\n",
            "loss: 0.0732, acc: 0.9759\n",
            "E2E-ABSA >>> 2022-05-20 12:22:15\n",
            "loss: 0.0820, acc: 0.9727\n",
            "E2E-ABSA >>> 2022-05-20 12:22:21\n",
            "loss: 0.0917, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:22:27\n",
            "loss: 0.0951, acc: 0.9677\n",
            "E2E-ABSA >>> 2022-05-20 12:22:33\n",
            ">>> val_acc: 0.7341, val_precision: 0.7341 val_recall: 0.7341, val_f1: 0.7341\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 12:22:35\n",
            "loss: 0.0799, acc: 0.9609\n",
            "E2E-ABSA >>> 2022-05-20 12:22:41\n",
            "loss: 0.0698, acc: 0.9777\n",
            "E2E-ABSA >>> 2022-05-20 12:22:47\n",
            "loss: 0.0547, acc: 0.9831\n",
            "E2E-ABSA >>> 2022-05-20 12:22:53\n",
            "loss: 0.0675, acc: 0.9770\n",
            "E2E-ABSA >>> 2022-05-20 12:22:59\n",
            "loss: 0.0756, acc: 0.9744\n",
            "E2E-ABSA >>> 2022-05-20 12:23:05\n",
            "loss: 0.0817, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 12:23:11\n",
            "loss: 0.0946, acc: 0.9678\n",
            "E2E-ABSA >>> 2022-05-20 12:23:17\n",
            "loss: 0.0928, acc: 0.9683\n",
            "E2E-ABSA >>> 2022-05-20 12:23:22\n",
            "loss: 0.0983, acc: 0.9665\n",
            "E2E-ABSA >>> 2022-05-20 12:23:28\n",
            "loss: 0.0979, acc: 0.9671\n",
            "E2E-ABSA >>> 2022-05-20 12:23:32\n",
            ">>> val_acc: 0.7221, val_precision: 0.7221 val_recall: 0.7221, val_f1: 0.7221\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 12:23:36\n",
            "loss: 0.0716, acc: 0.9777\n",
            "E2E-ABSA >>> 2022-05-20 12:23:42\n",
            "loss: 0.1389, acc: 0.9559\n",
            "E2E-ABSA >>> 2022-05-20 12:23:48\n",
            "loss: 0.1317, acc: 0.9560\n",
            "E2E-ABSA >>> 2022-05-20 12:23:54\n",
            "loss: 0.1503, acc: 0.9485\n",
            "E2E-ABSA >>> 2022-05-20 12:24:00\n",
            "loss: 0.1400, acc: 0.9535\n",
            "E2E-ABSA >>> 2022-05-20 12:24:06\n",
            "loss: 0.1311, acc: 0.9567\n",
            "E2E-ABSA >>> 2022-05-20 12:24:12\n",
            "loss: 0.1273, acc: 0.9566\n",
            "E2E-ABSA >>> 2022-05-20 12:24:18\n",
            "loss: 0.1270, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-05-20 12:24:24\n",
            "loss: 0.1301, acc: 0.9573\n",
            "E2E-ABSA >>> 2022-05-20 12:24:30\n",
            "loss: 0.1250, acc: 0.9587\n",
            "E2E-ABSA >>> 2022-05-20 12:24:32\n",
            ">>> val_acc: 0.7523, val_precision: 0.7523 val_recall: 0.7523, val_f1: 0.7523\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 12:24:38\n",
            "loss: 0.0620, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-05-20 12:24:44\n",
            "loss: 0.0712, acc: 0.9766\n",
            "E2E-ABSA >>> 2022-05-20 12:24:50\n",
            "loss: 0.0795, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 12:24:56\n",
            "loss: 0.0953, acc: 0.9680\n",
            "E2E-ABSA >>> 2022-05-20 12:25:02\n",
            "loss: 0.0911, acc: 0.9694\n",
            "E2E-ABSA >>> 2022-05-20 12:25:08\n",
            "loss: 0.0904, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:25:14\n",
            "loss: 0.0855, acc: 0.9705\n",
            "E2E-ABSA >>> 2022-05-20 12:25:20\n",
            "loss: 0.0854, acc: 0.9703\n",
            "E2E-ABSA >>> 2022-05-20 12:25:26\n",
            "loss: 0.0910, acc: 0.9691\n",
            "E2E-ABSA >>> 2022-05-20 12:25:32\n",
            ">>> val_acc: 0.7372, val_precision: 0.7372 val_recall: 0.7372, val_f1: 0.7372\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 12:25:34\n",
            "loss: 0.1910, acc: 0.9271\n",
            "E2E-ABSA >>> 2022-05-20 12:25:40\n",
            "loss: 0.1389, acc: 0.9471\n",
            "E2E-ABSA >>> 2022-05-20 12:25:45\n",
            "loss: 0.1140, acc: 0.9565\n",
            "E2E-ABSA >>> 2022-05-20 12:25:51\n",
            "loss: 0.0983, acc: 0.9621\n",
            "E2E-ABSA >>> 2022-05-20 12:25:57\n",
            "loss: 0.0910, acc: 0.9658\n",
            "E2E-ABSA >>> 2022-05-20 12:26:03\n",
            "loss: 0.0954, acc: 0.9634\n",
            "E2E-ABSA >>> 2022-05-20 12:26:09\n",
            "loss: 0.0883, acc: 0.9663\n",
            "E2E-ABSA >>> 2022-05-20 12:26:15\n",
            "loss: 0.0850, acc: 0.9675\n",
            "E2E-ABSA >>> 2022-05-20 12:26:21\n",
            "loss: 0.0874, acc: 0.9654\n",
            "E2E-ABSA >>> 2022-05-20 12:26:27\n",
            "loss: 0.0908, acc: 0.9654\n",
            "E2E-ABSA >>> 2022-05-20 12:26:31\n",
            ">>> val_acc: 0.7281, val_precision: 0.7281 val_recall: 0.7281, val_f1: 0.7281\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 12:26:35\n",
            "loss: 0.0790, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 12:26:41\n",
            "loss: 0.0903, acc: 0.9707\n",
            "E2E-ABSA >>> 2022-05-20 12:26:47\n",
            "loss: 0.0923, acc: 0.9663\n",
            "E2E-ABSA >>> 2022-05-20 12:26:53\n",
            "loss: 0.1091, acc: 0.9601\n",
            "E2E-ABSA >>> 2022-05-20 12:26:59\n",
            "loss: 0.1028, acc: 0.9647\n",
            "E2E-ABSA >>> 2022-05-20 12:27:05\n",
            "loss: 0.0914, acc: 0.9699\n",
            "E2E-ABSA >>> 2022-05-20 12:27:11\n",
            "loss: 0.0899, acc: 0.9697\n",
            "E2E-ABSA >>> 2022-05-20 12:27:16\n",
            "loss: 0.0902, acc: 0.9692\n",
            "E2E-ABSA >>> 2022-05-20 12:27:22\n",
            "loss: 0.0974, acc: 0.9673\n",
            "E2E-ABSA >>> 2022-05-20 12:27:28\n",
            "loss: 0.1045, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-05-20 12:27:31\n",
            ">>> val_acc: 0.7100, val_precision: 0.7100 val_recall: 0.7100, val_f1: 0.7100\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 12:27:36\n",
            "loss: 0.0693, acc: 0.9826\n",
            "E2E-ABSA >>> 2022-05-20 12:27:42\n",
            "loss: 0.1163, acc: 0.9638\n",
            "E2E-ABSA >>> 2022-05-20 12:27:48\n",
            "loss: 0.1094, acc: 0.9655\n",
            "E2E-ABSA >>> 2022-05-20 12:27:54\n",
            "loss: 0.1051, acc: 0.9671\n",
            "E2E-ABSA >>> 2022-05-20 12:28:00\n",
            "loss: 0.1028, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:28:06\n",
            "loss: 0.0928, acc: 0.9714\n",
            "E2E-ABSA >>> 2022-05-20 12:28:12\n",
            "loss: 0.0897, acc: 0.9715\n",
            "E2E-ABSA >>> 2022-05-20 12:28:18\n",
            "loss: 0.0917, acc: 0.9695\n",
            "E2E-ABSA >>> 2022-05-20 12:28:24\n",
            "loss: 0.0969, acc: 0.9663\n",
            "E2E-ABSA >>> 2022-05-20 12:28:31\n",
            ">>> val_acc: 0.7462, val_precision: 0.7462 val_recall: 0.7462, val_f1: 0.7462\n",
            "E2E-ABSA >>> 2022-05-20 12:28:31\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7825, val_precision: 0.7825 val_recall: 0.7825, val_f1: 0.7825\n",
            "you can download the best model from state_dict/bert_spc_SemEval2014_val_f1_0.7825\n",
            ">>> test_acc: 0.7825, test_precision: 0.7825, test_recall: 0.7825, test_f1: 0.7825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **SemEval2014** dataset on model**(Bert_spc)**"
      ],
      "metadata": {
        "id": "DyAwLQsihgkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset SemEval2014_know --log_step 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osaCQQ4eiLgF",
        "outputId": "30168adf-1aa2-4150-a47a-3cb2105b6d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 3104.\n",
            "> testing dataset count: 331.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f3c8e238a70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 12:30:01\n",
            "loss: 1.0000, acc: 0.5456\n",
            "E2E-ABSA >>> 2022-05-20 12:30:32\n",
            ">>> val_acc: 0.7583, val_precision: 0.7583 val_recall: 0.7583, val_f1: 0.7583\n",
            ">> saved: state_dict/bert_spc_SemEval2014_know_val_f1_0.7583\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 12:30:35\n",
            "loss: 0.6667, acc: 0.7396\n",
            "E2E-ABSA >>> 2022-05-20 12:31:04\n",
            "loss: 0.5983, acc: 0.7512\n",
            "E2E-ABSA >>> 2022-05-20 12:31:32\n",
            ">>> val_acc: 0.7946, val_precision: 0.7946 val_recall: 0.7946, val_f1: 0.7946\n",
            ">> saved: state_dict/bert_spc_SemEval2014_know_val_f1_0.7946\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 12:31:37\n",
            "loss: 0.4026, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-05-20 12:32:07\n",
            "loss: 0.3841, acc: 0.8527\n",
            "E2E-ABSA >>> 2022-05-20 12:32:34\n",
            ">>> val_acc: 0.8459, val_precision: 0.8459 val_recall: 0.8459, val_f1: 0.8459\n",
            ">> saved: state_dict/bert_spc_SemEval2014_know_val_f1_0.8459\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 12:32:40\n",
            "loss: 0.2348, acc: 0.9167\n",
            "E2E-ABSA >>> 2022-05-20 12:33:10\n",
            "loss: 0.2535, acc: 0.8999\n",
            "E2E-ABSA >>> 2022-05-20 12:33:35\n",
            ">>> val_acc: 0.8278, val_precision: 0.8278 val_recall: 0.8278, val_f1: 0.8278\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 12:33:42\n",
            "loss: 0.1284, acc: 0.9661\n",
            "E2E-ABSA >>> 2022-05-20 12:34:12\n",
            "loss: 0.1679, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-20 12:34:35\n",
            ">>> val_acc: 0.8218, val_precision: 0.8218 val_recall: 0.8218, val_f1: 0.8218\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 12:34:44\n",
            "loss: 0.1533, acc: 0.9396\n",
            "E2E-ABSA >>> 2022-05-20 12:35:13\n",
            "loss: 0.1434, acc: 0.9519\n",
            "E2E-ABSA >>> 2022-05-20 12:35:34\n",
            ">>> val_acc: 0.8278, val_precision: 0.8278 val_recall: 0.8278, val_f1: 0.8278\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 12:35:45\n",
            "loss: 0.0813, acc: 0.9722\n",
            "E2E-ABSA >>> 2022-05-20 12:36:15\n",
            "loss: 0.1429, acc: 0.9513\n",
            "E2E-ABSA >>> 2022-05-20 12:36:34\n",
            ">>> val_acc: 0.8550, val_precision: 0.8550 val_recall: 0.8550, val_f1: 0.8550\n",
            ">> saved: state_dict/bert_spc_SemEval2014_know_val_f1_0.855\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 12:36:48\n",
            "loss: 0.0909, acc: 0.9717\n",
            "E2E-ABSA >>> 2022-05-20 12:37:18\n",
            "loss: 0.1029, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-05-20 12:37:35\n",
            ">>> val_acc: 0.8278, val_precision: 0.8278 val_recall: 0.8278, val_f1: 0.8278\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 12:37:50\n",
            "loss: 0.0935, acc: 0.9714\n",
            "E2E-ABSA >>> 2022-05-20 12:38:20\n",
            "loss: 0.0976, acc: 0.9649\n",
            "E2E-ABSA >>> 2022-05-20 12:38:35\n",
            ">>> val_acc: 0.8248, val_precision: 0.8248 val_recall: 0.8248, val_f1: 0.8248\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 12:38:51\n",
            "loss: 0.0923, acc: 0.9595\n",
            "E2E-ABSA >>> 2022-05-20 12:39:21\n",
            "loss: 0.0908, acc: 0.9651\n",
            "E2E-ABSA >>> 2022-05-20 12:39:35\n",
            ">>> val_acc: 0.7795, val_precision: 0.7795 val_recall: 0.7795, val_f1: 0.7795\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 12:39:53\n",
            "loss: 0.0780, acc: 0.9719\n",
            "E2E-ABSA >>> 2022-05-20 12:40:23\n",
            "loss: 0.0891, acc: 0.9668\n",
            "E2E-ABSA >>> 2022-05-20 12:40:35\n",
            ">>> val_acc: 0.7734, val_precision: 0.7734 val_recall: 0.7734, val_f1: 0.7734\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 12:40:54\n",
            "loss: 0.0778, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 12:41:24\n",
            "loss: 0.1247, acc: 0.9533\n",
            "E2E-ABSA >>> 2022-05-20 12:41:35\n",
            ">>> val_acc: 0.8006, val_precision: 0.8006 val_recall: 0.8006, val_f1: 0.8006\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 12:41:56\n",
            "loss: 0.0816, acc: 0.9696\n",
            "E2E-ABSA >>> 2022-05-20 12:42:26\n",
            "loss: 0.0990, acc: 0.9658\n",
            "E2E-ABSA >>> 2022-05-20 12:42:34\n",
            ">>> val_acc: 0.8187, val_precision: 0.8187 val_recall: 0.8187, val_f1: 0.8187\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 12:42:58\n",
            "loss: 0.1176, acc: 0.9599\n",
            "E2E-ABSA >>> 2022-05-20 12:43:27\n",
            "loss: 0.1053, acc: 0.9628\n",
            "E2E-ABSA >>> 2022-05-20 12:43:34\n",
            ">>> val_acc: 0.7855, val_precision: 0.7855 val_recall: 0.7855, val_f1: 0.7855\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 12:43:59\n",
            "loss: 0.0784, acc: 0.9710\n",
            "E2E-ABSA >>> 2022-05-20 12:44:29\n",
            "loss: 0.1022, acc: 0.9660\n",
            "E2E-ABSA >>> 2022-05-20 12:44:34\n",
            ">>> val_acc: 0.8066, val_precision: 0.8066 val_recall: 0.8066, val_f1: 0.8066\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 12:45:01\n",
            "loss: 0.0903, acc: 0.9729\n",
            "E2E-ABSA >>> 2022-05-20 12:45:30\n",
            "loss: 0.1123, acc: 0.9618\n",
            "E2E-ABSA >>> 2022-05-20 12:45:34\n",
            ">>> val_acc: 0.8187, val_precision: 0.8187 val_recall: 0.8187, val_f1: 0.8187\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 12:46:02\n",
            "loss: 0.1517, acc: 0.9460\n",
            "E2E-ABSA >>> 2022-05-20 12:46:34\n",
            ">>> val_acc: 0.8036, val_precision: 0.8036 val_recall: 0.8036, val_f1: 0.8036\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 12:46:34\n",
            "loss: 0.0124, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 12:47:04\n",
            "loss: 0.1017, acc: 0.9626\n",
            "E2E-ABSA >>> 2022-05-20 12:47:33\n",
            ">>> val_acc: 0.7795, val_precision: 0.7795 val_recall: 0.7795, val_f1: 0.7795\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 12:47:36\n",
            "loss: 0.0747, acc: 0.9766\n",
            "E2E-ABSA >>> 2022-05-20 12:48:05\n",
            "loss: 0.1225, acc: 0.9572\n",
            "E2E-ABSA >>> 2022-05-20 12:48:33\n",
            ">>> val_acc: 0.7674, val_precision: 0.7674 val_recall: 0.7674, val_f1: 0.7674\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 12:48:37\n",
            "loss: 0.0709, acc: 0.9777\n",
            "E2E-ABSA >>> 2022-05-20 12:49:07\n",
            "loss: 0.0719, acc: 0.9764\n",
            "E2E-ABSA >>> 2022-05-20 12:49:33\n",
            ">>> val_acc: 0.7885, val_precision: 0.7885 val_recall: 0.7885, val_f1: 0.7885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 12:49:39\n",
            "loss: 0.0715, acc: 0.9719\n",
            "E2E-ABSA >>> 2022-05-20 12:50:09\n",
            "loss: 0.0848, acc: 0.9724\n",
            "E2E-ABSA >>> 2022-05-20 12:50:33\n",
            ">>> val_acc: 0.8036, val_precision: 0.8036 val_recall: 0.8036, val_f1: 0.8036\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 12:50:40\n",
            "loss: 0.0671, acc: 0.9808\n",
            "E2E-ABSA >>> 2022-05-20 12:51:10\n",
            "loss: 0.0884, acc: 0.9727\n",
            "E2E-ABSA >>> 2022-05-20 12:51:32\n",
            ">>> val_acc: 0.7885, val_precision: 0.7885 val_recall: 0.7885, val_f1: 0.7885\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 12:51:42\n",
            "loss: 0.0396, acc: 0.9922\n",
            "E2E-ABSA >>> 2022-05-20 12:52:12\n",
            "loss: 0.0772, acc: 0.9697\n",
            "E2E-ABSA >>> 2022-05-20 12:52:32\n",
            ">>> val_acc: 0.8036, val_precision: 0.8036 val_recall: 0.8036, val_f1: 0.8036\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 12:52:43\n",
            "loss: 0.0300, acc: 0.9901\n",
            "E2E-ABSA >>> 2022-05-20 12:53:13\n",
            "loss: 0.0790, acc: 0.9706\n",
            "E2E-ABSA >>> 2022-05-20 12:53:32\n",
            ">>> val_acc: 0.7492, val_precision: 0.7492 val_recall: 0.7492, val_f1: 0.7492\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 12:53:45\n",
            "loss: 0.0871, acc: 0.9759\n",
            "E2E-ABSA >>> 2022-05-20 12:54:15\n",
            "loss: 0.0874, acc: 0.9683\n",
            "E2E-ABSA >>> 2022-05-20 12:54:32\n",
            ">>> val_acc: 0.7613, val_precision: 0.7613 val_recall: 0.7613, val_f1: 0.7613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-20 12:54:47\n",
            "loss: 0.0597, acc: 0.9825\n",
            "E2E-ABSA >>> 2022-05-20 12:55:17\n",
            "loss: 0.0940, acc: 0.9675\n",
            "E2E-ABSA >>> 2022-05-20 12:55:32\n",
            ">>> val_acc: 0.7734, val_precision: 0.7734 val_recall: 0.7734, val_f1: 0.7734\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-20 12:55:48\n",
            "loss: 0.0511, acc: 0.9833\n",
            "E2E-ABSA >>> 2022-05-20 12:56:18\n",
            "loss: 0.0580, acc: 0.9796\n",
            "E2E-ABSA >>> 2022-05-20 12:56:31\n",
            ">>> val_acc: 0.7795, val_precision: 0.7795 val_recall: 0.7795, val_f1: 0.7795\n",
            "E2E-ABSA >>> 2022-05-20 12:56:31\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8550, val_precision: 0.8550 val_recall: 0.8550, val_f1: 0.8550\n",
            "you can download the best model from state_dict/bert_spc_SemEval2014_know_val_f1_0.855\n",
            ">>> test_acc: 0.8550, test_precision: 0.8550, test_recall: 0.8550, test_f1: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **SemEval2015** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "SC_nFN5QGUUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset SemEval2015 --log_step 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfL7T5SEwCYn",
        "outputId": "7e8072b0-172c-4d97-e4db-ba1f387c9b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fede65f6a70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 11:19:16\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/bert_spc_SemEval2015_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 11:19:32\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 11:19:34\n",
            "loss: 0.4966, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-20 11:19:47\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 11:20:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 11:20:06\n",
            "loss: 0.5101, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-20 11:20:17\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">> saved: state_dict/bert_spc_SemEval2015_val_f1_0.878\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 11:20:32\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 11:20:37\n",
            "loss: 0.1608, acc: 0.9514\n",
            "E2E-ABSA >>> 2022-05-20 11:20:46\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 11:21:01\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 11:21:08\n",
            "loss: 0.0328, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:21:15\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 11:21:30\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 11:21:39\n",
            "loss: 0.0458, acc: 0.9896\n",
            "E2E-ABSA >>> 2022-05-20 11:21:45\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 11:21:59\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 11:22:10\n",
            "loss: 0.0079, acc: 0.9983\n",
            "E2E-ABSA >>> 2022-05-20 11:22:13\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 11:22:28\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 11:22:40\n",
            "loss: 0.0192, acc: 0.9970\n",
            "E2E-ABSA >>> 2022-05-20 11:22:42\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">> saved: state_dict/bert_spc_SemEval2015_val_f1_0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 11:22:58\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 11:23:13\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 11:23:13\n",
            "loss: 0.0072, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:23:27\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 11:23:42\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 11:23:44\n",
            "loss: 0.0017, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:23:56\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 11:24:11\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 11:24:14\n",
            "loss: 0.0031, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:24:25\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 11:24:39\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 11:24:45\n",
            "loss: 0.0470, acc: 0.9901\n",
            "E2E-ABSA >>> 2022-05-20 11:24:54\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 11:25:08\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-20 11:25:16\n",
            "loss: 0.0261, acc: 0.9950\n",
            "E2E-ABSA >>> 2022-05-20 11:25:23\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-20 11:25:37\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-20 11:25:46\n",
            "loss: 0.0436, acc: 0.9859\n",
            "E2E-ABSA >>> 2022-05-20 11:25:52\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-20 11:26:06\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">> saved: state_dict/bert_spc_SemEval2015_val_f1_0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-20 11:26:18\n",
            "loss: 0.0154, acc: 0.9932\n",
            "E2E-ABSA >>> 2022-05-20 11:26:22\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-20 11:26:36\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-20 11:26:49\n",
            "loss: 0.0015, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:26:51\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-20 11:27:05\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-20 11:27:20\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-20 11:27:20\n",
            "loss: 0.0005, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:27:34\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-20 11:27:48\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-20 11:27:51\n",
            "loss: 0.0440, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 11:28:03\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-20 11:28:17\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-20 11:28:22\n",
            "loss: 0.0526, acc: 0.9866\n",
            "E2E-ABSA >>> 2022-05-20 11:28:32\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-20 11:28:46\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-20 11:28:52\n",
            "loss: 0.0020, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:29:01\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-20 11:29:15\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-20 11:29:23\n",
            "loss: 0.0016, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:29:30\n",
            ">>> val_acc: 0.7927, val_precision: 0.7927 val_recall: 0.7927, val_f1: 0.7927\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-20 11:29:44\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-20 11:29:53\n",
            "loss: 0.0615, acc: 0.9824\n",
            "E2E-ABSA >>> 2022-05-20 11:29:58\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-20 11:30:13\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-20 11:30:24\n",
            "loss: 0.0065, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:30:27\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-20 11:30:42\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-20 11:30:55\n",
            "loss: 0.0265, acc: 0.9943\n",
            "E2E-ABSA >>> 2022-05-20 11:30:56\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            "E2E-ABSA >>> 2022-05-20 11:30:56\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            "you can download the best model from state_dict/bert_spc_SemEval2015_val_f1_0.9146\n",
            ">>> test_acc: 0.9146, test_precision: 0.9146, test_recall: 0.9146, test_f1: 0.9146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **SemEval2015** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "01RauAGlGWCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset SemEval2015_know --log_step 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZK6muB2Gf-O",
        "outputId": "e9b2935c-98da-4a00-b05b-cc83261df71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f3f6cfb7a70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 11:02:46\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/bert_spc_SemEval2015_know_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 11:03:01\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 11:03:03\n",
            "loss: 0.5089, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-20 11:03:15\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 11:03:29\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 11:03:32\n",
            "loss: 0.5593, acc: 0.8073\n",
            "E2E-ABSA >>> 2022-05-20 11:03:43\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 11:03:58\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">> saved: state_dict/bert_spc_SemEval2015_know_val_f1_0.878\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 11:04:05\n",
            "loss: 0.3439, acc: 0.8854\n",
            "E2E-ABSA >>> 2022-05-20 11:04:15\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 11:04:29\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 11:04:36\n",
            "loss: 0.0894, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 11:04:43\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 11:04:58\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 11:05:07\n",
            "loss: 0.0311, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 11:05:12\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 11:05:27\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 11:05:38\n",
            "loss: 0.0482, acc: 0.9878\n",
            "E2E-ABSA >>> 2022-05-20 11:05:41\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 11:05:56\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 11:06:08\n",
            "loss: 0.0149, acc: 0.9955\n",
            "E2E-ABSA >>> 2022-05-20 11:06:10\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 11:06:25\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 11:06:39\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 11:06:40\n",
            "loss: 0.0009, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:06:54\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 11:07:08\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 11:07:10\n",
            "loss: 0.0031, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:07:23\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 11:07:37\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">> saved: state_dict/bert_spc_SemEval2015_know_val_f1_0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 11:07:42\n",
            "loss: 0.0359, acc: 0.9856\n",
            "E2E-ABSA >>> 2022-05-20 11:07:53\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 11:08:08\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 11:08:13\n",
            "loss: 0.0344, acc: 0.9934\n",
            "E2E-ABSA >>> 2022-05-20 11:08:22\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 11:08:37\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-20 11:08:44\n",
            "loss: 0.0022, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:08:51\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-20 11:09:06\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-20 11:09:15\n",
            "loss: 0.2238, acc: 0.9073\n",
            "E2E-ABSA >>> 2022-05-20 11:09:20\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-20 11:09:35\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-20 11:09:46\n",
            "loss: 0.0213, acc: 0.9916\n",
            "E2E-ABSA >>> 2022-05-20 11:09:49\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-20 11:10:04\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-20 11:10:17\n",
            "loss: 0.0076, acc: 0.9985\n",
            "E2E-ABSA >>> 2022-05-20 11:10:18\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-20 11:10:33\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-20 11:10:47\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">> saved: state_dict/bert_spc_SemEval2015_know_val_f1_0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-20 11:10:49\n",
            "loss: 0.0061, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:11:03\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-20 11:11:17\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-20 11:11:20\n",
            "loss: 0.0008, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:11:32\n",
            ">>> val_acc: 0.9268, val_precision: 0.9268 val_recall: 0.9268, val_f1: 0.9268\n",
            ">> saved: state_dict/bert_spc_SemEval2015_know_val_f1_0.9268\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-20 11:11:48\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-20 11:11:52\n",
            "loss: 0.0078, acc: 0.9955\n",
            "E2E-ABSA >>> 2022-05-20 11:12:02\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-20 11:12:17\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-20 11:12:23\n",
            "loss: 0.0603, acc: 0.9781\n",
            "E2E-ABSA >>> 2022-05-20 11:12:31\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-20 11:12:46\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-20 11:12:54\n",
            "loss: 0.0655, acc: 0.9760\n",
            "E2E-ABSA >>> 2022-05-20 11:13:00\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-20 11:13:15\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-20 11:13:24\n",
            "loss: 0.0066, acc: 0.9961\n",
            "E2E-ABSA >>> 2022-05-20 11:13:29\n",
            ">>> val_acc: 0.9268, val_precision: 0.9268 val_recall: 0.9268, val_f1: 0.9268\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-20 11:13:44\n",
            ">>> val_acc: 0.9390, val_precision: 0.9390 val_recall: 0.9390, val_f1: 0.9390\n",
            ">> saved: state_dict/bert_spc_SemEval2015_know_val_f1_0.939\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-20 11:13:56\n",
            "loss: 0.0017, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:13:59\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-20 11:14:14\n",
            ">>> val_acc: 0.9268, val_precision: 0.9268 val_recall: 0.9268, val_f1: 0.9268\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 48.\n",
            "E2E-ABSA >>> 2022-05-20 11:14:27\n",
            "loss: 0.0021, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:14:28\n",
            ">>> val_acc: 0.9268, val_precision: 0.9268 val_recall: 0.9268, val_f1: 0.9268\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 49.\n",
            "E2E-ABSA >>> 2022-05-20 11:14:43\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 50.\n",
            "E2E-ABSA >>> 2022-05-20 11:14:57\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 51.\n",
            "E2E-ABSA >>> 2022-05-20 11:14:58\n",
            "loss: 0.0008, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:15:12\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 52.\n",
            "E2E-ABSA >>> 2022-05-20 11:15:26\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 53.\n",
            "E2E-ABSA >>> 2022-05-20 11:15:29\n",
            "loss: 0.1324, acc: 0.9444\n",
            "E2E-ABSA >>> 2022-05-20 11:15:41\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 54.\n",
            "E2E-ABSA >>> 2022-05-20 11:15:55\n",
            ">>> val_acc: 0.8171, val_precision: 0.8171 val_recall: 0.8171, val_f1: 0.8171\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 55.\n",
            "E2E-ABSA >>> 2022-05-20 11:16:00\n",
            "loss: 0.0420, acc: 0.9958\n",
            "E2E-ABSA >>> 2022-05-20 11:16:10\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 56.\n",
            "E2E-ABSA >>> 2022-05-20 11:16:24\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 57.\n",
            "E2E-ABSA >>> 2022-05-20 11:16:31\n",
            "loss: 0.0068, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:16:39\n",
            ">>> val_acc: 0.9146, val_precision: 0.9146 val_recall: 0.9146, val_f1: 0.9146\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 58.\n",
            "E2E-ABSA >>> 2022-05-20 11:16:53\n",
            ">>> val_acc: 0.9268, val_precision: 0.9268 val_recall: 0.9268, val_f1: 0.9268\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 59.\n",
            "E2E-ABSA >>> 2022-05-20 11:17:01\n",
            "loss: 0.0018, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 11:17:08\n",
            ">>> val_acc: 0.8537, val_precision: 0.8537 val_recall: 0.8537, val_f1: 0.8537\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 60.\n",
            "E2E-ABSA >>> 2022-05-20 11:17:22\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 61.\n",
            "E2E-ABSA >>> 2022-05-20 11:17:32\n",
            "loss: 0.0102, acc: 0.9981\n",
            "E2E-ABSA >>> 2022-05-20 11:17:37\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 62.\n",
            "E2E-ABSA >>> 2022-05-20 11:17:51\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 63.\n",
            "E2E-ABSA >>> 2022-05-20 11:18:03\n",
            "loss: 0.0089, acc: 0.9984\n",
            "E2E-ABSA >>> 2022-05-20 11:18:05\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 64.\n",
            "E2E-ABSA >>> 2022-05-20 11:18:20\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 65.\n",
            "E2E-ABSA >>> 2022-05-20 11:18:33\n",
            "loss: 0.0795, acc: 0.9806\n",
            "E2E-ABSA >>> 2022-05-20 11:18:34\n",
            ">>> val_acc: 0.8902, val_precision: 0.8902 val_recall: 0.8902, val_f1: 0.8902\n",
            "E2E-ABSA >>> 2022-05-20 11:18:34\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.9390, val_precision: 0.9390 val_recall: 0.9390, val_f1: 0.9390\n",
            "you can download the best model from state_dict/bert_spc_SemEval2015_know_val_f1_0.939\n",
            ">>> test_acc: 0.9390, test_precision: 0.9390, test_recall: 0.9390, test_f1: 0.9390\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **SemEval2016** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "_lDuDOIzG6CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset SemEval2016 --log_step 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgzyeTOCGinX",
        "outputId": "2b61d797-2f28-4320-96b5-4d668844183d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 1113.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f13fe884a70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 12:58:39\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 12:58:49\n",
            "loss: 0.7044, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-05-20 12:59:01\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 12:59:20\n",
            "loss: 0.6908, acc: 0.7240\n",
            "E2E-ABSA >>> 2022-05-20 12:59:24\n",
            ">>> val_acc: 0.7797, val_precision: 0.7797 val_recall: 0.7797, val_f1: 0.7797\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.7797\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 12:59:47\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.822\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 12:59:54\n",
            "loss: 0.5209, acc: 0.8000\n",
            "E2E-ABSA >>> 2022-05-20 13:00:10\n",
            ">>> val_acc: 0.7458, val_precision: 0.7458 val_recall: 0.7458, val_f1: 0.7458\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 13:00:24\n",
            "loss: 0.5042, acc: 0.8075\n",
            "E2E-ABSA >>> 2022-05-20 13:00:31\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 13:00:52\n",
            ">>> val_acc: 0.7881, val_precision: 0.7881 val_recall: 0.7881, val_f1: 0.7881\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 13:00:55\n",
            "loss: 0.4646, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-05-20 13:01:13\n",
            ">>> val_acc: 0.7966, val_precision: 0.7966 val_recall: 0.7966, val_f1: 0.7966\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 13:01:25\n",
            "loss: 0.3651, acc: 0.8781\n",
            "E2E-ABSA >>> 2022-05-20 13:01:35\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 13:01:55\n",
            "loss: 0.3368, acc: 0.8832\n",
            "E2E-ABSA >>> 2022-05-20 13:01:56\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 13:02:17\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 13:02:26\n",
            "loss: 0.2657, acc: 0.9021\n",
            "E2E-ABSA >>> 2022-05-20 13:02:39\n",
            ">>> val_acc: 0.8136, val_precision: 0.8136 val_recall: 0.8136, val_f1: 0.8136\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 13:02:57\n",
            "loss: 0.2745, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-20 13:03:00\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.839\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 13:03:23\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 13:03:30\n",
            "loss: 0.1612, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-05-20 13:03:46\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 13:04:00\n",
            "loss: 0.1881, acc: 0.9337\n",
            "E2E-ABSA >>> 2022-05-20 13:04:07\n",
            ">>> val_acc: 0.8305, val_precision: 0.8305 val_recall: 0.8305, val_f1: 0.8305\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 13:04:28\n",
            ">>> val_acc: 0.8559, val_precision: 0.8559 val_recall: 0.8559, val_f1: 0.8559\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.8559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 13:04:33\n",
            "loss: 0.1115, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-05-20 13:04:51\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 13:05:04\n",
            "loss: 0.0936, acc: 0.9734\n",
            "E2E-ABSA >>> 2022-05-20 13:05:14\n",
            ">>> val_acc: 0.8051, val_precision: 0.8051 val_recall: 0.8051, val_f1: 0.8051\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 13:05:34\n",
            "loss: 0.1038, acc: 0.9596\n",
            "E2E-ABSA >>> 2022-05-20 13:05:35\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 13:05:57\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">> saved: state_dict/bert_spc_SemEval2016_val_f1_0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 13:06:07\n",
            "loss: 0.0527, acc: 0.9792\n",
            "E2E-ABSA >>> 2022-05-20 13:06:19\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 13:06:37\n",
            "loss: 0.0838, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 13:06:41\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 13:07:02\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 13:07:08\n",
            "loss: 0.0506, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-05-20 13:07:24\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-20 13:07:38\n",
            "loss: 0.0361, acc: 0.9888\n",
            "E2E-ABSA >>> 2022-05-20 13:07:45\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-20 13:08:06\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-20 13:08:09\n",
            "loss: 0.0118, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 13:08:28\n",
            ">>> val_acc: 0.8559, val_precision: 0.8559 val_recall: 0.8559, val_f1: 0.8559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-20 13:08:40\n",
            "loss: 0.0594, acc: 0.9766\n",
            "E2E-ABSA >>> 2022-05-20 13:08:49\n",
            ">>> val_acc: 0.8559, val_precision: 0.8559 val_recall: 0.8559, val_f1: 0.8559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-20 13:09:10\n",
            "loss: 0.0477, acc: 0.9802\n",
            "E2E-ABSA >>> 2022-05-20 13:09:10\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-20 13:09:32\n",
            ">>> val_acc: 0.8559, val_precision: 0.8559 val_recall: 0.8559, val_f1: 0.8559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-20 13:09:41\n",
            "loss: 0.0080, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 13:09:53\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-20 13:10:11\n",
            "loss: 0.0162, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 13:10:15\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-20 13:10:36\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-20 13:10:42\n",
            "loss: 0.0330, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 13:10:58\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-20 13:11:12\n",
            "loss: 0.0818, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-05-20 13:11:19\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-20 13:11:40\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-20 13:11:43\n",
            "loss: 0.0025, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 13:12:02\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-20 13:12:14\n",
            "loss: 0.0284, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 13:12:23\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-20 13:12:44\n",
            "loss: 0.0043, acc: 0.9991\n",
            "E2E-ABSA >>> 2022-05-20 13:12:45\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-20 13:13:06\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            "E2E-ABSA >>> 2022-05-20 13:13:06\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            "you can download the best model from state_dict/bert_spc_SemEval2016_val_f1_0.8729\n",
            ">>> test_acc: 0.8729, test_precision: 0.8729, test_recall: 0.8729, test_f1: 0.8729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **SemEval2016** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "8NAujbDMG-Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset SemEval2016_know --log_step 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVHx2kCxGi6r",
        "outputId": "0070524e-5c47-4797-de00-822fb179a352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 1113.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fdc863e4a70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 13:14:25\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/bert_spc_SemEval2016_know_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 13:14:35\n",
            "loss: 0.7065, acc: 0.7250\n",
            "E2E-ABSA >>> 2022-05-20 13:14:47\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 13:15:06\n",
            "loss: 0.6929, acc: 0.7354\n",
            "E2E-ABSA >>> 2022-05-20 13:15:10\n",
            ">>> val_acc: 0.7373, val_precision: 0.7373 val_recall: 0.7373, val_f1: 0.7373\n",
            ">> saved: state_dict/bert_spc_SemEval2016_know_val_f1_0.7373\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 13:15:32\n",
            ">>> val_acc: 0.7627, val_precision: 0.7627 val_recall: 0.7627, val_f1: 0.7627\n",
            ">> saved: state_dict/bert_spc_SemEval2016_know_val_f1_0.7627\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 13:15:40\n",
            "loss: 0.5158, acc: 0.8031\n",
            "E2E-ABSA >>> 2022-05-20 13:15:55\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">> saved: state_dict/bert_spc_SemEval2016_know_val_f1_0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 13:16:11\n",
            "loss: 0.2357, acc: 0.9175\n",
            "E2E-ABSA >>> 2022-05-20 13:16:17\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 13:16:39\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 13:16:42\n",
            "loss: 0.0828, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 13:17:01\n",
            ">>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            ">> saved: state_dict/bert_spc_SemEval2016_know_val_f1_0.9153\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 13:17:14\n",
            "loss: 0.0944, acc: 0.9672\n",
            "E2E-ABSA >>> 2022-05-20 13:17:23\n",
            ">>> val_acc: 0.8983, val_precision: 0.8983 val_recall: 0.8983, val_f1: 0.8983\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 13:17:44\n",
            "loss: 0.0574, acc: 0.9820\n",
            "E2E-ABSA >>> 2022-05-20 13:17:45\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 13:18:06\n",
            ">>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 13:18:15\n",
            "loss: 0.0224, acc: 0.9917\n",
            "E2E-ABSA >>> 2022-05-20 13:18:28\n",
            ">>> val_acc: 0.8983, val_precision: 0.8983 val_recall: 0.8983, val_f1: 0.8983\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 13:18:46\n",
            "loss: 0.0422, acc: 0.9833\n",
            "E2E-ABSA >>> 2022-05-20 13:18:50\n",
            ">>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 13:19:11\n",
            ">>> val_acc: 0.8814, val_precision: 0.8814 val_recall: 0.8814, val_f1: 0.8814\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 13:19:17\n",
            "loss: 0.0639, acc: 0.9781\n",
            "E2E-ABSA >>> 2022-05-20 13:19:32\n",
            ">>> val_acc: 0.8983, val_precision: 0.8983 val_recall: 0.8983, val_f1: 0.8983\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 13:19:47\n",
            "loss: 0.0255, acc: 0.9888\n",
            "E2E-ABSA >>> 2022-05-20 13:19:54\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 13:20:15\n",
            ">>> val_acc: 0.8814, val_precision: 0.8814 val_recall: 0.8814, val_f1: 0.8814\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 13:20:18\n",
            "loss: 0.0104, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 13:20:37\n",
            ">>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 13:20:49\n",
            "loss: 0.0171, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 13:20:58\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 13:21:19\n",
            "loss: 0.0426, acc: 0.9865\n",
            "E2E-ABSA >>> 2022-05-20 13:21:20\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 13:21:41\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 13:21:50\n",
            "loss: 0.0119, acc: 0.9958\n",
            "E2E-ABSA >>> 2022-05-20 13:22:03\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 13:22:21\n",
            "loss: 0.0306, acc: 0.9906\n",
            "E2E-ABSA >>> 2022-05-20 13:22:24\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 13:22:46\n",
            ">>> val_acc: 0.8814, val_precision: 0.8814 val_recall: 0.8814, val_f1: 0.8814\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 13:22:52\n",
            "loss: 0.0106, acc: 0.9969\n",
            "E2E-ABSA >>> 2022-05-20 13:23:07\n",
            ">>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 25.\n",
            "E2E-ABSA >>> 2022-05-20 13:23:22\n",
            "loss: 0.1036, acc: 0.9613\n",
            "E2E-ABSA >>> 2022-05-20 13:23:29\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 26.\n",
            "E2E-ABSA >>> 2022-05-20 13:23:50\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 27.\n",
            "E2E-ABSA >>> 2022-05-20 13:23:53\n",
            "loss: 0.0580, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-05-20 13:24:12\n",
            ">>> val_acc: 0.9237, val_precision: 0.9237 val_recall: 0.9237, val_f1: 0.9237\n",
            ">> saved: state_dict/bert_spc_SemEval2016_know_val_f1_0.9237\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 28.\n",
            "E2E-ABSA >>> 2022-05-20 13:24:25\n",
            "loss: 0.0338, acc: 0.9906\n",
            "E2E-ABSA >>> 2022-05-20 13:24:34\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 29.\n",
            "E2E-ABSA >>> 2022-05-20 13:24:55\n",
            "loss: 0.0286, acc: 0.9919\n",
            "E2E-ABSA >>> 2022-05-20 13:24:56\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 30.\n",
            "E2E-ABSA >>> 2022-05-20 13:25:17\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 31.\n",
            "E2E-ABSA >>> 2022-05-20 13:25:26\n",
            "loss: 0.0057, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 13:25:39\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 32.\n",
            "E2E-ABSA >>> 2022-05-20 13:25:57\n",
            "loss: 0.0152, acc: 0.9958\n",
            "E2E-ABSA >>> 2022-05-20 13:26:00\n",
            ">>> val_acc: 0.8814, val_precision: 0.8814 val_recall: 0.8814, val_f1: 0.8814\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 33.\n",
            "E2E-ABSA >>> 2022-05-20 13:26:22\n",
            ">>> val_acc: 0.8559, val_precision: 0.8559 val_recall: 0.8559, val_f1: 0.8559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 34.\n",
            "E2E-ABSA >>> 2022-05-20 13:26:28\n",
            "loss: 0.0644, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-05-20 13:26:43\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 35.\n",
            "E2E-ABSA >>> 2022-05-20 13:26:58\n",
            "loss: 0.0168, acc: 0.9950\n",
            "E2E-ABSA >>> 2022-05-20 13:27:05\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 36.\n",
            "E2E-ABSA >>> 2022-05-20 13:27:26\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 37.\n",
            "E2E-ABSA >>> 2022-05-20 13:27:29\n",
            "loss: 0.0569, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-05-20 13:27:48\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 38.\n",
            "E2E-ABSA >>> 2022-05-20 13:28:00\n",
            "loss: 0.0375, acc: 0.9906\n",
            "E2E-ABSA >>> 2022-05-20 13:28:09\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 39.\n",
            "E2E-ABSA >>> 2022-05-20 13:28:30\n",
            "loss: 0.0306, acc: 0.9919\n",
            "E2E-ABSA >>> 2022-05-20 13:28:31\n",
            ">>> val_acc: 0.8814, val_precision: 0.8814 val_recall: 0.8814, val_f1: 0.8814\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 40.\n",
            "E2E-ABSA >>> 2022-05-20 13:28:52\n",
            ">>> val_acc: 0.8475, val_precision: 0.8475 val_recall: 0.8475, val_f1: 0.8475\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 41.\n",
            "E2E-ABSA >>> 2022-05-20 13:29:01\n",
            "loss: 0.0498, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-05-20 13:29:14\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 42.\n",
            "E2E-ABSA >>> 2022-05-20 13:29:31\n",
            "loss: 0.0123, acc: 0.9990\n",
            "E2E-ABSA >>> 2022-05-20 13:29:35\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 43.\n",
            "E2E-ABSA >>> 2022-05-20 13:29:57\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 44.\n",
            "E2E-ABSA >>> 2022-05-20 13:30:02\n",
            "loss: 0.0029, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-05-20 13:30:18\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 45.\n",
            "E2E-ABSA >>> 2022-05-20 13:30:33\n",
            "loss: 0.0238, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-05-20 13:30:39\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 46.\n",
            "E2E-ABSA >>> 2022-05-20 13:31:01\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 47.\n",
            "E2E-ABSA >>> 2022-05-20 13:31:04\n",
            "loss: 0.0446, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-05-20 13:31:22\n",
            ">>> val_acc: 0.8390, val_precision: 0.8390 val_recall: 0.8390, val_f1: 0.8390\n",
            "E2E-ABSA >>> 2022-05-20 13:31:22\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.9237, val_precision: 0.9237 val_recall: 0.9237, val_f1: 0.9237\n",
            "you can download the best model from state_dict/bert_spc_SemEval2016_know_val_f1_0.9237\n",
            ">>> test_acc: 0.9237, test_precision: 0.9237, test_recall: 0.9237, test_f1: 0.9237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **acl14shortdata** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "zLbbeQCAHAL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset acl14shortdata --log_step 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLZsZ7XtGjRW",
        "outputId": "ed134016-0441-4a74-81f4-57c2b7dd8d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f7a4ef24a70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 13:33:05\n",
            "loss: 1.0786, acc: 0.4863\n",
            "E2E-ABSA >>> 2022-05-20 13:33:36\n",
            "loss: 1.0585, acc: 0.4794\n",
            "E2E-ABSA >>> 2022-05-20 13:34:06\n",
            "loss: 1.0160, acc: 0.5046\n",
            "E2E-ABSA >>> 2022-05-20 13:34:25\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_val_f1_0.648\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 13:34:40\n",
            "loss: 0.7024, acc: 0.6927\n",
            "E2E-ABSA >>> 2022-05-20 13:35:10\n",
            "loss: 0.6982, acc: 0.6968\n",
            "E2E-ABSA >>> 2022-05-20 13:35:40\n",
            "loss: 0.6970, acc: 0.6935\n",
            "E2E-ABSA >>> 2022-05-20 13:36:10\n",
            "loss: 0.6981, acc: 0.6956\n",
            "E2E-ABSA >>> 2022-05-20 13:36:15\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_val_f1_0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 13:36:44\n",
            "loss: 0.5784, acc: 0.7559\n",
            "E2E-ABSA >>> 2022-05-20 13:37:14\n",
            "loss: 0.5787, acc: 0.7545\n",
            "E2E-ABSA >>> 2022-05-20 13:37:44\n",
            "loss: 0.5907, acc: 0.7454\n",
            "E2E-ABSA >>> 2022-05-20 13:38:05\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_val_f1_0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 13:38:19\n",
            "loss: 0.4971, acc: 0.8153\n",
            "E2E-ABSA >>> 2022-05-20 13:38:49\n",
            "loss: 0.5107, acc: 0.7912\n",
            "E2E-ABSA >>> 2022-05-20 13:39:19\n",
            "loss: 0.5194, acc: 0.7830\n",
            "E2E-ABSA >>> 2022-05-20 13:39:49\n",
            "loss: 0.5285, acc: 0.7793\n",
            "E2E-ABSA >>> 2022-05-20 13:39:55\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_val_f1_0.696\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 13:40:24\n",
            "loss: 0.4424, acc: 0.8329\n",
            "E2E-ABSA >>> 2022-05-20 13:40:53\n",
            "loss: 0.4452, acc: 0.8255\n",
            "E2E-ABSA >>> 2022-05-20 13:41:23\n",
            "loss: 0.4525, acc: 0.8161\n",
            "E2E-ABSA >>> 2022-05-20 13:41:45\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_val_f1_0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 13:41:58\n",
            "loss: 0.2878, acc: 0.8953\n",
            "E2E-ABSA >>> 2022-05-20 13:42:28\n",
            "loss: 0.3621, acc: 0.8589\n",
            "E2E-ABSA >>> 2022-05-20 13:42:58\n",
            "loss: 0.3993, acc: 0.8453\n",
            "E2E-ABSA >>> 2022-05-20 13:43:27\n",
            "loss: 0.4138, acc: 0.8342\n",
            "E2E-ABSA >>> 2022-05-20 13:43:35\n",
            ">>> val_acc: 0.6912, val_precision: 0.6912 val_recall: 0.6912, val_f1: 0.6912\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 13:44:01\n",
            "loss: 0.3198, acc: 0.8714\n",
            "E2E-ABSA >>> 2022-05-20 13:44:31\n",
            "loss: 0.3610, acc: 0.8614\n",
            "E2E-ABSA >>> 2022-05-20 13:45:00\n",
            "loss: 0.3819, acc: 0.8520\n",
            "E2E-ABSA >>> 2022-05-20 13:45:23\n",
            ">>> val_acc: 0.6608, val_precision: 0.6608 val_recall: 0.6608, val_f1: 0.6608\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 13:45:34\n",
            "loss: 0.2813, acc: 0.8889\n",
            "E2E-ABSA >>> 2022-05-20 13:46:03\n",
            "loss: 0.3222, acc: 0.8745\n",
            "E2E-ABSA >>> 2022-05-20 13:46:33\n",
            "loss: 0.3583, acc: 0.8604\n",
            "E2E-ABSA >>> 2022-05-20 13:47:03\n",
            "loss: 0.3728, acc: 0.8540\n",
            "E2E-ABSA >>> 2022-05-20 13:47:11\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 13:47:36\n",
            "loss: 0.2617, acc: 0.8936\n",
            "E2E-ABSA >>> 2022-05-20 13:48:06\n",
            "loss: 0.2899, acc: 0.8869\n",
            "E2E-ABSA >>> 2022-05-20 13:48:36\n",
            "loss: 0.3170, acc: 0.8790\n",
            "E2E-ABSA >>> 2022-05-20 13:49:00\n",
            ">>> val_acc: 0.6864, val_precision: 0.6864 val_recall: 0.6864, val_f1: 0.6864\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 13:49:09\n",
            "loss: 0.2265, acc: 0.9160\n",
            "E2E-ABSA >>> 2022-05-20 13:49:39\n",
            "loss: 0.2676, acc: 0.9010\n",
            "E2E-ABSA >>> 2022-05-20 13:50:09\n",
            "loss: 0.2851, acc: 0.8920\n",
            "E2E-ABSA >>> 2022-05-20 13:50:39\n",
            "loss: 0.2920, acc: 0.8878\n",
            "E2E-ABSA >>> 2022-05-20 13:50:48\n",
            ">>> val_acc: 0.6848, val_precision: 0.6848 val_recall: 0.6848, val_f1: 0.6848\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 13:51:12\n",
            "loss: 0.1851, acc: 0.9297\n",
            "E2E-ABSA >>> 2022-05-20 13:51:42\n",
            "loss: 0.2121, acc: 0.9226\n",
            "E2E-ABSA >>> 2022-05-20 13:52:11\n",
            "loss: 0.2508, acc: 0.9065\n",
            "E2E-ABSA >>> 2022-05-20 13:52:36\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 13:52:45\n",
            "loss: 0.1597, acc: 0.9442\n",
            "E2E-ABSA >>> 2022-05-20 13:53:14\n",
            "loss: 0.2075, acc: 0.9204\n",
            "E2E-ABSA >>> 2022-05-20 13:53:44\n",
            "loss: 0.2058, acc: 0.9221\n",
            "E2E-ABSA >>> 2022-05-20 13:54:14\n",
            "loss: 0.2303, acc: 0.9131\n",
            "E2E-ABSA >>> 2022-05-20 13:54:25\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 13:54:47\n",
            "loss: 0.2219, acc: 0.9169\n",
            "E2E-ABSA >>> 2022-05-20 13:55:17\n",
            "loss: 0.2167, acc: 0.9180\n",
            "E2E-ABSA >>> 2022-05-20 13:55:47\n",
            "loss: 0.2251, acc: 0.9151\n",
            "E2E-ABSA >>> 2022-05-20 13:56:13\n",
            ">>> val_acc: 0.6208, val_precision: 0.6208 val_recall: 0.6208, val_f1: 0.6208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 13:56:20\n",
            "loss: 0.1656, acc: 0.9505\n",
            "E2E-ABSA >>> 2022-05-20 13:56:50\n",
            "loss: 0.2039, acc: 0.9249\n",
            "E2E-ABSA >>> 2022-05-20 13:57:20\n",
            "loss: 0.2164, acc: 0.9202\n",
            "E2E-ABSA >>> 2022-05-20 13:57:49\n",
            "loss: 0.2138, acc: 0.9203\n",
            "E2E-ABSA >>> 2022-05-20 13:58:01\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 13:58:23\n",
            "loss: 0.1231, acc: 0.9549\n",
            "E2E-ABSA >>> 2022-05-20 13:58:52\n",
            "loss: 0.1501, acc: 0.9451\n",
            "E2E-ABSA >>> 2022-05-20 13:59:22\n",
            "loss: 0.1563, acc: 0.9416\n",
            "E2E-ABSA >>> 2022-05-20 13:59:50\n",
            ">>> val_acc: 0.6544, val_precision: 0.6544 val_recall: 0.6544, val_f1: 0.6544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 13:59:56\n",
            "loss: 0.1253, acc: 0.9469\n",
            "E2E-ABSA >>> 2022-05-20 14:00:25\n",
            "loss: 0.1505, acc: 0.9432\n",
            "E2E-ABSA >>> 2022-05-20 14:00:55\n",
            "loss: 0.1632, acc: 0.9426\n",
            "E2E-ABSA >>> 2022-05-20 14:01:24\n",
            "loss: 0.1710, acc: 0.9373\n",
            "E2E-ABSA >>> 2022-05-20 14:01:38\n",
            ">>> val_acc: 0.6592, val_precision: 0.6592 val_recall: 0.6592, val_f1: 0.6592\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 14:01:58\n",
            "loss: 0.1325, acc: 0.9513\n",
            "E2E-ABSA >>> 2022-05-20 14:02:28\n",
            "loss: 0.1221, acc: 0.9587\n",
            "E2E-ABSA >>> 2022-05-20 14:02:57\n",
            "loss: 0.1338, acc: 0.9522\n",
            "E2E-ABSA >>> 2022-05-20 14:03:26\n",
            ">>> val_acc: 0.6208, val_precision: 0.6208 val_recall: 0.6208, val_f1: 0.6208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 14:03:31\n",
            "loss: 0.0555, acc: 0.9883\n",
            "E2E-ABSA >>> 2022-05-20 14:04:01\n",
            "loss: 0.0896, acc: 0.9731\n",
            "E2E-ABSA >>> 2022-05-20 14:04:30\n",
            "loss: 0.1165, acc: 0.9638\n",
            "E2E-ABSA >>> 2022-05-20 14:05:00\n",
            "loss: 0.1246, acc: 0.9593\n",
            "E2E-ABSA >>> 2022-05-20 14:05:15\n",
            ">>> val_acc: 0.6304, val_precision: 0.6304 val_recall: 0.6304, val_f1: 0.6304\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 14:05:34\n",
            "loss: 0.0929, acc: 0.9756\n",
            "E2E-ABSA >>> 2022-05-20 14:06:03\n",
            "loss: 0.1236, acc: 0.9615\n",
            "E2E-ABSA >>> 2022-05-20 14:06:33\n",
            "loss: 0.1319, acc: 0.9550\n",
            "E2E-ABSA >>> 2022-05-20 14:07:03\n",
            ">>> val_acc: 0.6240, val_precision: 0.6240 val_recall: 0.6240, val_f1: 0.6240\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 14:07:06\n",
            "loss: 0.1076, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 14:07:36\n",
            "loss: 0.1090, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-05-20 14:08:06\n",
            "loss: 0.1169, acc: 0.9620\n",
            "E2E-ABSA >>> 2022-05-20 14:08:35\n",
            "loss: 0.1301, acc: 0.9545\n",
            "E2E-ABSA >>> 2022-05-20 14:08:51\n",
            ">>> val_acc: 0.6512, val_precision: 0.6512 val_recall: 0.6512, val_f1: 0.6512\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 14:09:09\n",
            "loss: 0.0714, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 14:09:39\n",
            "loss: 0.1032, acc: 0.9641\n",
            "E2E-ABSA >>> 2022-05-20 14:10:08\n",
            "loss: 0.1066, acc: 0.9639\n",
            "E2E-ABSA >>> 2022-05-20 14:10:39\n",
            ">>> val_acc: 0.6208, val_precision: 0.6208 val_recall: 0.6208, val_f1: 0.6208\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 14:10:42\n",
            "loss: 0.0631, acc: 0.9766\n",
            "E2E-ABSA >>> 2022-05-20 14:11:12\n",
            "loss: 0.0755, acc: 0.9745\n",
            "E2E-ABSA >>> 2022-05-20 14:11:41\n",
            "loss: 0.0963, acc: 0.9660\n",
            "E2E-ABSA >>> 2022-05-20 14:12:11\n",
            "loss: 0.0976, acc: 0.9649\n",
            "E2E-ABSA >>> 2022-05-20 14:12:28\n",
            ">>> val_acc: 0.5792, val_precision: 0.5792 val_recall: 0.5792, val_f1: 0.5792\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 14:12:45\n",
            "loss: 0.1151, acc: 0.9609\n",
            "E2E-ABSA >>> 2022-05-20 14:13:14\n",
            "loss: 0.1064, acc: 0.9627\n",
            "E2E-ABSA >>> 2022-05-20 14:13:44\n",
            "loss: 0.1141, acc: 0.9617\n",
            "E2E-ABSA >>> 2022-05-20 14:14:16\n",
            ">>> val_acc: 0.6352, val_precision: 0.6352 val_recall: 0.6352, val_f1: 0.6352\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 23.\n",
            "E2E-ABSA >>> 2022-05-20 14:14:18\n",
            "loss: 0.0660, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-05-20 14:14:47\n",
            "loss: 0.0932, acc: 0.9675\n",
            "E2E-ABSA >>> 2022-05-20 14:15:17\n",
            "loss: 0.1032, acc: 0.9620\n",
            "E2E-ABSA >>> 2022-05-20 14:15:47\n",
            "loss: 0.1043, acc: 0.9616\n",
            "E2E-ABSA >>> 2022-05-20 14:16:05\n",
            ">>> val_acc: 0.6176, val_precision: 0.6176 val_recall: 0.6176, val_f1: 0.6176\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 24.\n",
            "E2E-ABSA >>> 2022-05-20 14:16:20\n",
            "loss: 0.0767, acc: 0.9736\n",
            "E2E-ABSA >>> 2022-05-20 14:16:50\n",
            "loss: 0.0779, acc: 0.9741\n",
            "E2E-ABSA >>> 2022-05-20 14:17:20\n",
            "loss: 0.1076, acc: 0.9625\n",
            "E2E-ABSA >>> 2022-05-20 14:17:49\n",
            "loss: 0.1062, acc: 0.9634\n",
            "E2E-ABSA >>> 2022-05-20 14:17:53\n",
            ">>> val_acc: 0.6288, val_precision: 0.6288 val_recall: 0.6288, val_f1: 0.6288\n",
            "E2E-ABSA >>> 2022-05-20 14:17:53\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            "you can download the best model from state_dict/bert_spc_acl14shortdata_val_f1_0.7088\n",
            ">>> test_acc: 0.7088, test_precision: 0.7088, test_recall: 0.7088, test_f1: 0.7088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后：Training **acl14shortdata** dataset on model**(bert_spc)**"
      ],
      "metadata": {
        "id": "-HoH3AZ0HEEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name bert_spc --dataset acl14shortdata_know --log_step 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npYo7gqBGkWk",
        "outputId": "a59421f9-e64f-47a3-cfab-028909c03d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "加载Bert...\n",
            "Bert加载完毕.\n",
            ">>> 使用设备:cuda 训练.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 439075328\n",
            "> n_trainable_params: 109484547, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: bert_spc\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7ff3dc0cca70>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 100\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.bert_spc.BERT_SPC'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-05-20 14:19:27\n",
            "loss: 1.0580, acc: 0.4913\n",
            "E2E-ABSA >>> 2022-05-20 14:19:58\n",
            "loss: 1.0176, acc: 0.5069\n",
            "E2E-ABSA >>> 2022-05-20 14:20:28\n",
            "loss: 0.9401, acc: 0.5546\n",
            "E2E-ABSA >>> 2022-05-20 14:20:47\n",
            ">>> val_acc: 0.7040, val_precision: 0.7040 val_recall: 0.7040, val_f1: 0.7040\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_know_val_f1_0.704\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-05-20 14:21:02\n",
            "loss: 0.5943, acc: 0.7513\n",
            "E2E-ABSA >>> 2022-05-20 14:21:32\n",
            "loss: 0.5909, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-05-20 14:22:02\n",
            "loss: 0.5945, acc: 0.7475\n",
            "E2E-ABSA >>> 2022-05-20 14:22:31\n",
            "loss: 0.5974, acc: 0.7453\n",
            "E2E-ABSA >>> 2022-05-20 14:22:36\n",
            ">>> val_acc: 0.7392, val_precision: 0.7392 val_recall: 0.7392, val_f1: 0.7392\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_know_val_f1_0.7392\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-05-20 14:23:06\n",
            "loss: 0.4776, acc: 0.8027\n",
            "E2E-ABSA >>> 2022-05-20 14:23:36\n",
            "loss: 0.4860, acc: 0.8010\n",
            "E2E-ABSA >>> 2022-05-20 14:24:06\n",
            "loss: 0.4882, acc: 0.7990\n",
            "E2E-ABSA >>> 2022-05-20 14:24:27\n",
            ">>> val_acc: 0.7504, val_precision: 0.7504 val_recall: 0.7504, val_f1: 0.7504\n",
            ">> saved: state_dict/bert_spc_acl14shortdata_know_val_f1_0.7504\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-05-20 14:24:41\n",
            "loss: 0.4050, acc: 0.8523\n",
            "E2E-ABSA >>> 2022-05-20 14:25:11\n",
            "loss: 0.4112, acc: 0.8359\n",
            "E2E-ABSA >>> 2022-05-20 14:25:41\n",
            "loss: 0.4263, acc: 0.8266\n",
            "E2E-ABSA >>> 2022-05-20 14:26:10\n",
            "loss: 0.4296, acc: 0.8223\n",
            "E2E-ABSA >>> 2022-05-20 14:26:17\n",
            ">>> val_acc: 0.7456, val_precision: 0.7456 val_recall: 0.7456, val_f1: 0.7456\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-05-20 14:26:44\n",
            "loss: 0.3590, acc: 0.8587\n",
            "E2E-ABSA >>> 2022-05-20 14:27:14\n",
            "loss: 0.3592, acc: 0.8551\n",
            "E2E-ABSA >>> 2022-05-20 14:27:44\n",
            "loss: 0.3644, acc: 0.8525\n",
            "E2E-ABSA >>> 2022-05-20 14:28:05\n",
            ">>> val_acc: 0.7232, val_precision: 0.7232 val_recall: 0.7232, val_f1: 0.7232\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-05-20 14:28:17\n",
            "loss: 0.2387, acc: 0.9250\n",
            "E2E-ABSA >>> 2022-05-20 14:28:47\n",
            "loss: 0.3002, acc: 0.8929\n",
            "E2E-ABSA >>> 2022-05-20 14:29:17\n",
            "loss: 0.3209, acc: 0.8839\n",
            "E2E-ABSA >>> 2022-05-20 14:29:46\n",
            "loss: 0.3308, acc: 0.8778\n",
            "E2E-ABSA >>> 2022-05-20 14:29:54\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-05-20 14:30:20\n",
            "loss: 0.2570, acc: 0.9041\n",
            "E2E-ABSA >>> 2022-05-20 14:30:50\n",
            "loss: 0.2988, acc: 0.8870\n",
            "E2E-ABSA >>> 2022-05-20 14:31:20\n",
            "loss: 0.3001, acc: 0.8882\n",
            "E2E-ABSA >>> 2022-05-20 14:31:42\n",
            ">>> val_acc: 0.7296, val_precision: 0.7296 val_recall: 0.7296, val_f1: 0.7296\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-05-20 14:31:53\n",
            "loss: 0.2397, acc: 0.9045\n",
            "E2E-ABSA >>> 2022-05-20 14:32:23\n",
            "loss: 0.2547, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-05-20 14:32:53\n",
            "loss: 0.2832, acc: 0.8922\n",
            "E2E-ABSA >>> 2022-05-20 14:33:22\n",
            "loss: 0.2920, acc: 0.8901\n",
            "E2E-ABSA >>> 2022-05-20 14:33:31\n",
            ">>> val_acc: 0.7104, val_precision: 0.7104 val_recall: 0.7104, val_f1: 0.7104\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-05-20 14:33:56\n",
            "loss: 0.2546, acc: 0.9040\n",
            "E2E-ABSA >>> 2022-05-20 14:34:26\n",
            "loss: 0.2711, acc: 0.8988\n",
            "E2E-ABSA >>> 2022-05-20 14:34:56\n",
            "loss: 0.3045, acc: 0.8864\n",
            "E2E-ABSA >>> 2022-05-20 14:35:20\n",
            ">>> val_acc: 0.7248, val_precision: 0.7248 val_recall: 0.7248, val_f1: 0.7248\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-05-20 14:35:29\n",
            "loss: 0.1535, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-05-20 14:35:59\n",
            "loss: 0.2194, acc: 0.9247\n",
            "E2E-ABSA >>> 2022-05-20 14:36:29\n",
            "loss: 0.2385, acc: 0.9154\n",
            "E2E-ABSA >>> 2022-05-20 14:36:59\n",
            "loss: 0.2520, acc: 0.9070\n",
            "E2E-ABSA >>> 2022-05-20 14:37:08\n",
            ">>> val_acc: 0.7264, val_precision: 0.7264 val_recall: 0.7264, val_f1: 0.7264\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-05-20 14:37:32\n",
            "loss: 0.1931, acc: 0.9281\n",
            "E2E-ABSA >>> 2022-05-20 14:38:02\n",
            "loss: 0.2123, acc: 0.9187\n",
            "E2E-ABSA >>> 2022-05-20 14:38:32\n",
            "loss: 0.2203, acc: 0.9163\n",
            "E2E-ABSA >>> 2022-05-20 14:38:57\n",
            ">>> val_acc: 0.6976, val_precision: 0.6976 val_recall: 0.6976, val_f1: 0.6976\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-05-20 14:39:05\n",
            "loss: 0.1545, acc: 0.9487\n",
            "E2E-ABSA >>> 2022-05-20 14:39:35\n",
            "loss: 0.1685, acc: 0.9424\n",
            "E2E-ABSA >>> 2022-05-20 14:40:05\n",
            "loss: 0.1816, acc: 0.9370\n",
            "E2E-ABSA >>> 2022-05-20 14:40:35\n",
            "loss: 0.1968, acc: 0.9284\n",
            "E2E-ABSA >>> 2022-05-20 14:40:46\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-05-20 14:41:08\n",
            "loss: 0.1935, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-05-20 14:41:38\n",
            "loss: 0.2069, acc: 0.9205\n",
            "E2E-ABSA >>> 2022-05-20 14:42:08\n",
            "loss: 0.2039, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-05-20 14:42:34\n",
            ">>> val_acc: 0.7184, val_precision: 0.7184 val_recall: 0.7184, val_f1: 0.7184\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-05-20 14:42:41\n",
            "loss: 0.0871, acc: 0.9766\n",
            "E2E-ABSA >>> 2022-05-20 14:43:11\n",
            "loss: 0.1542, acc: 0.9430\n",
            "E2E-ABSA >>> 2022-05-20 14:43:41\n",
            "loss: 0.1693, acc: 0.9350\n",
            "E2E-ABSA >>> 2022-05-20 14:44:10\n",
            "loss: 0.1705, acc: 0.9363\n",
            "E2E-ABSA >>> 2022-05-20 14:44:23\n",
            ">>> val_acc: 0.7024, val_precision: 0.7024 val_recall: 0.7024, val_f1: 0.7024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-05-20 14:44:44\n",
            "loss: 0.1119, acc: 0.9583\n",
            "E2E-ABSA >>> 2022-05-20 14:45:14\n",
            "loss: 0.1339, acc: 0.9502\n",
            "E2E-ABSA >>> 2022-05-20 14:45:44\n",
            "loss: 0.1482, acc: 0.9442\n",
            "E2E-ABSA >>> 2022-05-20 14:46:11\n",
            ">>> val_acc: 0.6992, val_precision: 0.6992 val_recall: 0.6992, val_f1: 0.6992\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-05-20 14:46:17\n",
            "loss: 0.0958, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-05-20 14:46:47\n",
            "loss: 0.1374, acc: 0.9521\n",
            "E2E-ABSA >>> 2022-05-20 14:47:16\n",
            "loss: 0.1486, acc: 0.9477\n",
            "E2E-ABSA >>> 2022-05-20 14:47:46\n",
            "loss: 0.1650, acc: 0.9410\n",
            "E2E-ABSA >>> 2022-05-20 14:48:00\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-05-20 14:48:20\n",
            "loss: 0.1095, acc: 0.9577\n",
            "E2E-ABSA >>> 2022-05-20 14:48:49\n",
            "loss: 0.1238, acc: 0.9535\n",
            "E2E-ABSA >>> 2022-05-20 14:49:19\n",
            "loss: 0.1253, acc: 0.9541\n",
            "E2E-ABSA >>> 2022-05-20 14:49:48\n",
            ">>> val_acc: 0.6720, val_precision: 0.6720 val_recall: 0.6720, val_f1: 0.6720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-05-20 14:49:53\n",
            "loss: 0.0669, acc: 0.9727\n",
            "E2E-ABSA >>> 2022-05-20 14:50:22\n",
            "loss: 0.0909, acc: 0.9693\n",
            "E2E-ABSA >>> 2022-05-20 14:50:52\n",
            "loss: 0.1086, acc: 0.9627\n",
            "E2E-ABSA >>> 2022-05-20 14:51:22\n",
            "loss: 0.1135, acc: 0.9618\n",
            "E2E-ABSA >>> 2022-05-20 14:51:36\n",
            ">>> val_acc: 0.6896, val_precision: 0.6896 val_recall: 0.6896, val_f1: 0.6896\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-05-20 14:51:55\n",
            "loss: 0.1224, acc: 0.9629\n",
            "E2E-ABSA >>> 2022-05-20 14:52:25\n",
            "loss: 0.1174, acc: 0.9607\n",
            "E2E-ABSA >>> 2022-05-20 14:52:54\n",
            "loss: 0.1375, acc: 0.9534\n",
            "E2E-ABSA >>> 2022-05-20 14:53:24\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-05-20 14:53:28\n",
            "loss: 0.1100, acc: 0.9635\n",
            "E2E-ABSA >>> 2022-05-20 14:53:58\n",
            "loss: 0.0953, acc: 0.9676\n",
            "E2E-ABSA >>> 2022-05-20 14:54:27\n",
            "loss: 0.1039, acc: 0.9640\n",
            "E2E-ABSA >>> 2022-05-20 14:54:57\n",
            "loss: 0.1169, acc: 0.9567\n",
            "E2E-ABSA >>> 2022-05-20 14:55:13\n",
            ">>> val_acc: 0.6672, val_precision: 0.6672 val_recall: 0.6672, val_f1: 0.6672\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-05-20 14:55:31\n",
            "loss: 0.0835, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-05-20 14:56:01\n",
            "loss: 0.0878, acc: 0.9723\n",
            "E2E-ABSA >>> 2022-05-20 14:56:30\n",
            "loss: 0.0914, acc: 0.9719\n",
            "E2E-ABSA >>> 2022-05-20 14:57:01\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-05-20 14:57:04\n",
            "loss: 0.1283, acc: 0.9609\n",
            "E2E-ABSA >>> 2022-05-20 14:57:33\n",
            "loss: 0.0944, acc: 0.9676\n",
            "E2E-ABSA >>> 2022-05-20 14:58:03\n",
            "loss: 0.0986, acc: 0.9654\n",
            "E2E-ABSA >>> 2022-05-20 14:58:33\n",
            "loss: 0.0959, acc: 0.9667\n",
            "E2E-ABSA >>> 2022-05-20 14:58:50\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 22.\n",
            "E2E-ABSA >>> 2022-05-20 14:59:06\n",
            "loss: 0.0613, acc: 0.9788\n",
            "E2E-ABSA >>> 2022-05-20 14:59:36\n",
            "loss: 0.0653, acc: 0.9788\n",
            "E2E-ABSA >>> 2022-05-20 15:00:06\n",
            "loss: 0.0988, acc: 0.9668\n",
            "E2E-ABSA >>> 2022-05-20 15:00:38\n",
            ">>> val_acc: 0.6768, val_precision: 0.6768 val_recall: 0.6768, val_f1: 0.6768\n",
            "E2E-ABSA >>> 2022-05-20 15:00:38\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7504, val_precision: 0.7504 val_recall: 0.7504, val_f1: 0.7504\n",
            "you can download the best model from state_dict/bert_spc_acl14shortdata_know_val_f1_0.7504\n",
            ">>> test_acc: 0.7504, test_precision: 0.7504, test_recall: 0.7504, test_f1: 0.7504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "跑Bert前最好检测一下是否cuda可用"
      ],
      "metadata": {
        "id": "S-Ef3wlp6YNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "6mj7FJaS6YVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **Twitter** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "xSig3kfd6Ya_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset twitter --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk6N8EGK6YhG",
        "outputId": "7e964bc1-b8ca-424f-f27f-a878fac1cad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 1664.\n",
            "> testing dataset count: 419.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: twitter\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fdb14826b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/train.tsv', 'test': './datasets/twitter/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 11:05:42\n",
            "loss: 0.9026, acc: 0.6375\n",
            "E2E-ABSA >>> 2022-08-17 11:05:59\n",
            "loss: 0.8761, acc: 0.6172\n",
            "E2E-ABSA >>> 2022-08-17 11:06:15\n",
            "loss: 0.8486, acc: 0.6333\n",
            "E2E-ABSA >>> 2022-08-17 11:06:31\n",
            "loss: 0.8431, acc: 0.6367\n",
            "E2E-ABSA >>> 2022-08-17 11:06:46\n",
            "loss: 0.8179, acc: 0.6475\n",
            "E2E-ABSA >>> 2022-08-17 11:06:57\n",
            ">>> val_acc: 0.7064, val_precision: 0.7064 val_recall: 0.7064, val_f1: 0.7064\n",
            ">> saved: state_dict/lcf_bert_twitter_val_f1_0.7064\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 11:07:11\n",
            "loss: 0.7385, acc: 0.6719\n",
            "E2E-ABSA >>> 2022-08-17 11:07:27\n",
            "loss: 0.7412, acc: 0.6753\n",
            "E2E-ABSA >>> 2022-08-17 11:07:43\n",
            "loss: 0.6896, acc: 0.7109\n",
            "E2E-ABSA >>> 2022-08-17 11:07:59\n",
            "loss: 0.6937, acc: 0.7122\n",
            "E2E-ABSA >>> 2022-08-17 11:08:15\n",
            "loss: 0.6944, acc: 0.7057\n",
            "E2E-ABSA >>> 2022-08-17 11:08:28\n",
            ">>> val_acc: 0.7661, val_precision: 0.7661 val_recall: 0.7661, val_f1: 0.7661\n",
            ">> saved: state_dict/lcf_bert_twitter_val_f1_0.7661\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 11:08:39\n",
            "loss: 0.5676, acc: 0.7604\n",
            "E2E-ABSA >>> 2022-08-17 11:08:55\n",
            "loss: 0.5260, acc: 0.7910\n",
            "E2E-ABSA >>> 2022-08-17 11:09:11\n",
            "loss: 0.4897, acc: 0.7981\n",
            "E2E-ABSA >>> 2022-08-17 11:09:27\n",
            "loss: 0.4983, acc: 0.7969\n",
            "E2E-ABSA >>> 2022-08-17 11:09:43\n",
            "loss: 0.5164, acc: 0.7785\n",
            "E2E-ABSA >>> 2022-08-17 11:10:00\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 11:10:06\n",
            "loss: 0.3751, acc: 0.8672\n",
            "E2E-ABSA >>> 2022-08-17 11:10:22\n",
            "loss: 0.3034, acc: 0.8839\n",
            "E2E-ABSA >>> 2022-08-17 11:10:38\n",
            "loss: 0.3398, acc: 0.8646\n",
            "E2E-ABSA >>> 2022-08-17 11:10:54\n",
            "loss: 0.3397, acc: 0.8676\n",
            "E2E-ABSA >>> 2022-08-17 11:11:10\n",
            "loss: 0.3629, acc: 0.8544\n",
            "E2E-ABSA >>> 2022-08-17 11:11:30\n",
            ">>> val_acc: 0.7422, val_precision: 0.7422 val_recall: 0.7422, val_f1: 0.7422\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 11:11:33\n",
            "loss: 0.2669, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-08-17 11:11:49\n",
            "loss: 0.2071, acc: 0.9271\n",
            "E2E-ABSA >>> 2022-08-17 11:12:05\n",
            "loss: 0.1894, acc: 0.9276\n",
            "E2E-ABSA >>> 2022-08-17 11:12:21\n",
            "loss: 0.1937, acc: 0.9229\n",
            "E2E-ABSA >>> 2022-08-17 11:12:37\n",
            "loss: 0.1963, acc: 0.9204\n",
            "E2E-ABSA >>> 2022-08-17 11:12:53\n",
            "loss: 0.2085, acc: 0.9171\n",
            "E2E-ABSA >>> 2022-08-17 11:13:00\n",
            ">>> val_acc: 0.7327, val_precision: 0.7327 val_recall: 0.7327, val_f1: 0.7327\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 11:13:16\n",
            "loss: 0.1602, acc: 0.9469\n",
            "E2E-ABSA >>> 2022-08-17 11:13:32\n",
            "loss: 0.1306, acc: 0.9594\n",
            "E2E-ABSA >>> 2022-08-17 11:13:48\n",
            "loss: 0.1511, acc: 0.9479\n",
            "E2E-ABSA >>> 2022-08-17 11:14:04\n",
            "loss: 0.1641, acc: 0.9414\n",
            "E2E-ABSA >>> 2022-08-17 11:14:20\n",
            "loss: 0.1617, acc: 0.9413\n",
            "E2E-ABSA >>> 2022-08-17 11:14:30\n",
            ">>> val_acc: 0.6325, val_precision: 0.6325 val_recall: 0.6325, val_f1: 0.6325\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 11:14:43\n",
            "loss: 0.0891, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-08-17 11:14:59\n",
            "loss: 0.1093, acc: 0.9514\n",
            "E2E-ABSA >>> 2022-08-17 11:15:15\n",
            "loss: 0.1117, acc: 0.9554\n",
            "E2E-ABSA >>> 2022-08-17 11:15:31\n",
            "loss: 0.1073, acc: 0.9589\n",
            "E2E-ABSA >>> 2022-08-17 11:15:47\n",
            "loss: 0.1166, acc: 0.9583\n",
            "E2E-ABSA >>> 2022-08-17 11:16:00\n",
            ">>> val_acc: 0.7064, val_precision: 0.7064 val_recall: 0.7064, val_f1: 0.7064\n",
            "E2E-ABSA >>> 2022-08-17 11:16:00\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7661, val_precision: 0.7661 val_recall: 0.7661, val_f1: 0.7661\n",
            "you can download the best model from state_dict/lcf_bert_twitter_val_f1_0.7661\n",
            ">>> test_acc: 0.7661, test_precision: 0.7661, test_recall: 0.7661, test_f1: 0.7661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后： Training **Twitter** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "5JkjOHbY6YnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset twitter_know --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcvKpO9O6YuB",
        "outputId": "9b1af439-0dea-47c0-c2f3-5a093a1d6c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 1664.\n",
            "> testing dataset count: 419.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: twitter_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f02da371b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/twitter/output_know/train.tsv', 'test': './datasets/twitter/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 11:16:50\n",
            "loss: 0.8955, acc: 0.6531\n",
            "E2E-ABSA >>> 2022-08-17 11:17:07\n",
            "loss: 0.8818, acc: 0.6094\n",
            "E2E-ABSA >>> 2022-08-17 11:17:23\n",
            "loss: 0.8522, acc: 0.6281\n",
            "E2E-ABSA >>> 2022-08-17 11:17:39\n",
            "loss: 0.8493, acc: 0.6328\n",
            "E2E-ABSA >>> 2022-08-17 11:17:55\n",
            "loss: 0.8272, acc: 0.6438\n",
            "E2E-ABSA >>> 2022-08-17 11:18:05\n",
            ">>> val_acc: 0.6874, val_precision: 0.6874 val_recall: 0.6874, val_f1: 0.6874\n",
            ">> saved: state_dict/lcf_bert_twitter_know_val_f1_0.6874\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 11:18:19\n",
            "loss: 0.7172, acc: 0.6523\n",
            "E2E-ABSA >>> 2022-08-17 11:18:35\n",
            "loss: 0.7308, acc: 0.6684\n",
            "E2E-ABSA >>> 2022-08-17 11:18:51\n",
            "loss: 0.6921, acc: 0.6975\n",
            "E2E-ABSA >>> 2022-08-17 11:19:07\n",
            "loss: 0.7011, acc: 0.6965\n",
            "E2E-ABSA >>> 2022-08-17 11:19:23\n",
            "loss: 0.7058, acc: 0.6908\n",
            "E2E-ABSA >>> 2022-08-17 11:19:37\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">> saved: state_dict/lcf_bert_twitter_know_val_f1_0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 11:19:48\n",
            "loss: 0.5584, acc: 0.7656\n",
            "E2E-ABSA >>> 2022-08-17 11:20:04\n",
            "loss: 0.5200, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 11:20:20\n",
            "loss: 0.4876, acc: 0.8017\n",
            "E2E-ABSA >>> 2022-08-17 11:20:35\n",
            "loss: 0.4951, acc: 0.8038\n",
            "E2E-ABSA >>> 2022-08-17 11:20:51\n",
            "loss: 0.4975, acc: 0.7996\n",
            "E2E-ABSA >>> 2022-08-17 11:21:08\n",
            ">>> val_acc: 0.7852, val_precision: 0.7852 val_recall: 0.7852, val_f1: 0.7852\n",
            ">> saved: state_dict/lcf_bert_twitter_know_val_f1_0.7852\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 11:21:16\n",
            "loss: 0.3830, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 11:21:32\n",
            "loss: 0.3399, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 11:21:48\n",
            "loss: 0.3603, acc: 0.8529\n",
            "E2E-ABSA >>> 2022-08-17 11:22:04\n",
            "loss: 0.3424, acc: 0.8631\n",
            "E2E-ABSA >>> 2022-08-17 11:22:20\n",
            "loss: 0.3456, acc: 0.8587\n",
            "E2E-ABSA >>> 2022-08-17 11:22:40\n",
            ">>> val_acc: 0.7637, val_precision: 0.7637 val_recall: 0.7637, val_f1: 0.7637\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 11:22:43\n",
            "loss: 0.2910, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 11:22:59\n",
            "loss: 0.2357, acc: 0.9271\n",
            "E2E-ABSA >>> 2022-08-17 11:23:15\n",
            "loss: 0.2320, acc: 0.9176\n",
            "E2E-ABSA >>> 2022-08-17 11:23:31\n",
            "loss: 0.2197, acc: 0.9189\n",
            "E2E-ABSA >>> 2022-08-17 11:23:47\n",
            "loss: 0.2238, acc: 0.9152\n",
            "E2E-ABSA >>> 2022-08-17 11:24:03\n",
            "loss: 0.2186, acc: 0.9141\n",
            "E2E-ABSA >>> 2022-08-17 11:24:10\n",
            ">>> val_acc: 0.7351, val_precision: 0.7351 val_recall: 0.7351, val_f1: 0.7351\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 11:24:26\n",
            "loss: 0.1319, acc: 0.9594\n",
            "E2E-ABSA >>> 2022-08-17 11:24:42\n",
            "loss: 0.1188, acc: 0.9594\n",
            "E2E-ABSA >>> 2022-08-17 11:24:58\n",
            "loss: 0.1484, acc: 0.9479\n",
            "E2E-ABSA >>> 2022-08-17 11:25:14\n",
            "loss: 0.1648, acc: 0.9422\n",
            "E2E-ABSA >>> 2022-08-17 11:25:30\n",
            "loss: 0.1670, acc: 0.9400\n",
            "E2E-ABSA >>> 2022-08-17 11:25:40\n",
            ">>> val_acc: 0.7613, val_precision: 0.7613 val_recall: 0.7613, val_f1: 0.7613\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 11:25:53\n",
            "loss: 0.1000, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 11:26:09\n",
            "loss: 0.0862, acc: 0.9722\n",
            "E2E-ABSA >>> 2022-08-17 11:26:25\n",
            "loss: 0.0839, acc: 0.9699\n",
            "E2E-ABSA >>> 2022-08-17 11:26:41\n",
            "loss: 0.0847, acc: 0.9712\n",
            "E2E-ABSA >>> 2022-08-17 11:26:57\n",
            "loss: 0.0924, acc: 0.9694\n",
            "E2E-ABSA >>> 2022-08-17 11:27:10\n",
            ">>> val_acc: 0.7685, val_precision: 0.7685 val_recall: 0.7685, val_f1: 0.7685\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 11:27:20\n",
            "loss: 0.0701, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-08-17 11:27:36\n",
            "loss: 0.1058, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-08-17 11:27:52\n",
            "loss: 0.1268, acc: 0.9507\n",
            "E2E-ABSA >>> 2022-08-17 11:28:08\n",
            "loss: 0.1202, acc: 0.9540\n",
            "E2E-ABSA >>> 2022-08-17 11:28:24\n",
            "loss: 0.1078, acc: 0.9572\n",
            "E2E-ABSA >>> 2022-08-17 11:28:41\n",
            ">>> val_acc: 0.7804, val_precision: 0.7804 val_recall: 0.7804, val_f1: 0.7804\n",
            "E2E-ABSA >>> 2022-08-17 11:28:41\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7852, val_precision: 0.7852 val_recall: 0.7852, val_f1: 0.7852\n",
            "you can download the best model from state_dict/lcf_bert_twitter_know_val_f1_0.7852\n",
            ">>> test_acc: 0.7852, test_precision: 0.7852, test_recall: 0.7852, test_f1: 0.7852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **SemEval2014** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "gHPN2AMp6Yzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset SemEval2014 --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D86m8Eod6Y5i",
        "outputId": "40b6255d-4e50-416f-9d5e-1ff3e80cfde7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 246kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 535kB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 71.6MB/s]\n",
            "Bert加载完毕.\n",
            "解析样本出现错误, 已忽略: ['The pizza is the best if you like thin crusted pizza.', 'pizza   1\\n']\n",
            "解析样本出现错误, 已忽略: ['All the money went into the interior decoration, none of it went to the chefs.', 'interior decoration 1\\n']\n",
            "> training dataset count: 3091.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: SemEval2014\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f8fd5ff9b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/train.tsv', 'test': './datasets/laprest14/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 12:10:50\n",
            "loss: 1.0823, acc: 0.4562\n",
            "E2E-ABSA >>> 2022-08-17 12:11:04\n",
            "loss: 1.0404, acc: 0.5234\n",
            "E2E-ABSA >>> 2022-08-17 12:11:18\n",
            "loss: 1.0300, acc: 0.5333\n",
            "E2E-ABSA >>> 2022-08-17 12:11:33\n",
            "loss: 1.0075, acc: 0.5461\n",
            "E2E-ABSA >>> 2022-08-17 12:11:49\n",
            "loss: 0.9749, acc: 0.5613\n",
            "E2E-ABSA >>> 2022-08-17 12:12:05\n",
            "loss: 0.9408, acc: 0.5750\n",
            "E2E-ABSA >>> 2022-08-17 12:12:20\n",
            "loss: 0.9026, acc: 0.5960\n",
            "E2E-ABSA >>> 2022-08-17 12:12:35\n",
            "loss: 0.8585, acc: 0.6180\n",
            "E2E-ABSA >>> 2022-08-17 12:12:50\n",
            "loss: 0.8288, acc: 0.6344\n",
            "E2E-ABSA >>> 2022-08-17 12:13:05\n",
            ">>> val_acc: 0.7812, val_precision: 0.7812 val_recall: 0.7812, val_f1: 0.7812\n",
            ">> saved: state_dict/lcf_bert_SemEval2014_val_f1_0.7812\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 12:13:11\n",
            "loss: 0.6500, acc: 0.7083\n",
            "E2E-ABSA >>> 2022-08-17 12:13:26\n",
            "loss: 0.5152, acc: 0.7909\n",
            "E2E-ABSA >>> 2022-08-17 12:13:41\n",
            "loss: 0.4745, acc: 0.8207\n",
            "E2E-ABSA >>> 2022-08-17 12:13:57\n",
            "loss: 0.5130, acc: 0.8078\n",
            "E2E-ABSA >>> 2022-08-17 12:14:12\n",
            "loss: 0.5043, acc: 0.8074\n",
            "E2E-ABSA >>> 2022-08-17 12:14:27\n",
            "loss: 0.5123, acc: 0.8013\n",
            "E2E-ABSA >>> 2022-08-17 12:14:42\n",
            "loss: 0.5152, acc: 0.7976\n",
            "E2E-ABSA >>> 2022-08-17 12:14:57\n",
            "loss: 0.5107, acc: 0.7988\n",
            "E2E-ABSA >>> 2022-08-17 12:15:12\n",
            "loss: 0.5065, acc: 0.7993\n",
            "E2E-ABSA >>> 2022-08-17 12:15:27\n",
            "loss: 0.4989, acc: 0.8024\n",
            "E2E-ABSA >>> 2022-08-17 12:15:38\n",
            ">>> val_acc: 0.7781, val_precision: 0.7781 val_recall: 0.7781, val_f1: 0.7781\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 12:15:47\n",
            "loss: 0.3621, acc: 0.8490\n",
            "E2E-ABSA >>> 2022-08-17 12:16:03\n",
            "loss: 0.3808, acc: 0.8496\n",
            "E2E-ABSA >>> 2022-08-17 12:16:18\n",
            "loss: 0.3667, acc: 0.8570\n",
            "E2E-ABSA >>> 2022-08-17 12:16:33\n",
            "loss: 0.3672, acc: 0.8498\n",
            "E2E-ABSA >>> 2022-08-17 12:16:48\n",
            "loss: 0.3563, acc: 0.8601\n",
            "E2E-ABSA >>> 2022-08-17 12:17:03\n",
            "loss: 0.3670, acc: 0.8555\n",
            "E2E-ABSA >>> 2022-08-17 12:17:18\n",
            "loss: 0.3675, acc: 0.8546\n",
            "E2E-ABSA >>> 2022-08-17 12:17:34\n",
            "loss: 0.3686, acc: 0.8528\n",
            "E2E-ABSA >>> 2022-08-17 12:17:49\n",
            "loss: 0.3665, acc: 0.8539\n",
            "E2E-ABSA >>> 2022-08-17 12:18:04\n",
            "loss: 0.3680, acc: 0.8519\n",
            "E2E-ABSA >>> 2022-08-17 12:18:10\n",
            ">>> val_acc: 0.7416, val_precision: 0.7416 val_recall: 0.7416, val_f1: 0.7416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 12:18:24\n",
            "loss: 0.3104, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 12:18:39\n",
            "loss: 0.2766, acc: 0.8931\n",
            "E2E-ABSA >>> 2022-08-17 12:18:54\n",
            "loss: 0.2727, acc: 0.8944\n",
            "E2E-ABSA >>> 2022-08-17 12:19:09\n",
            "loss: 0.2549, acc: 0.9030\n",
            "E2E-ABSA >>> 2022-08-17 12:19:24\n",
            "loss: 0.2599, acc: 0.9011\n",
            "E2E-ABSA >>> 2022-08-17 12:19:39\n",
            "loss: 0.2535, acc: 0.9010\n",
            "E2E-ABSA >>> 2022-08-17 12:19:55\n",
            "loss: 0.2502, acc: 0.9040\n",
            "E2E-ABSA >>> 2022-08-17 12:20:10\n",
            "loss: 0.2652, acc: 0.8975\n",
            "E2E-ABSA >>> 2022-08-17 12:20:25\n",
            "loss: 0.2678, acc: 0.8978\n",
            "E2E-ABSA >>> 2022-08-17 12:20:42\n",
            ">>> val_acc: 0.8024, val_precision: 0.8024 val_recall: 0.8024, val_f1: 0.8024\n",
            ">> saved: state_dict/lcf_bert_SemEval2014_val_f1_0.8024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 12:20:46\n",
            "loss: 0.3406, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 12:21:01\n",
            "loss: 0.1874, acc: 0.9193\n",
            "E2E-ABSA >>> 2022-08-17 12:21:17\n",
            "loss: 0.1797, acc: 0.9247\n",
            "E2E-ABSA >>> 2022-08-17 12:21:32\n",
            "loss: 0.1869, acc: 0.9238\n",
            "E2E-ABSA >>> 2022-08-17 12:21:47\n",
            "loss: 0.1898, acc: 0.9263\n",
            "E2E-ABSA >>> 2022-08-17 12:22:02\n",
            "loss: 0.1848, acc: 0.9279\n",
            "E2E-ABSA >>> 2022-08-17 12:22:17\n",
            "loss: 0.1850, acc: 0.9289\n",
            "E2E-ABSA >>> 2022-08-17 12:22:32\n",
            "loss: 0.1833, acc: 0.9280\n",
            "E2E-ABSA >>> 2022-08-17 12:22:48\n",
            "loss: 0.1948, acc: 0.9264\n",
            "E2E-ABSA >>> 2022-08-17 12:23:03\n",
            "loss: 0.1958, acc: 0.9270\n",
            "E2E-ABSA >>> 2022-08-17 12:23:15\n",
            ">>> val_acc: 0.7720, val_precision: 0.7720 val_recall: 0.7720, val_f1: 0.7720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 12:23:23\n",
            "loss: 0.1507, acc: 0.9500\n",
            "E2E-ABSA >>> 2022-08-17 12:23:38\n",
            "loss: 0.1540, acc: 0.9458\n",
            "E2E-ABSA >>> 2022-08-17 12:23:53\n",
            "loss: 0.1724, acc: 0.9337\n",
            "E2E-ABSA >>> 2022-08-17 12:24:08\n",
            "loss: 0.1885, acc: 0.9286\n",
            "E2E-ABSA >>> 2022-08-17 12:24:23\n",
            "loss: 0.1735, acc: 0.9347\n",
            "E2E-ABSA >>> 2022-08-17 12:24:38\n",
            "loss: 0.1682, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 12:24:53\n",
            "loss: 0.1594, acc: 0.9404\n",
            "E2E-ABSA >>> 2022-08-17 12:25:08\n",
            "loss: 0.1580, acc: 0.9425\n",
            "E2E-ABSA >>> 2022-08-17 12:25:24\n",
            "loss: 0.1686, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 12:25:39\n",
            "loss: 0.1725, acc: 0.9349\n",
            "E2E-ABSA >>> 2022-08-17 12:25:47\n",
            ">>> val_acc: 0.8085, val_precision: 0.8085 val_recall: 0.8085, val_f1: 0.8085\n",
            ">> saved: state_dict/lcf_bert_SemEval2014_val_f1_0.8085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 12:26:00\n",
            "loss: 0.0899, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 12:26:15\n",
            "loss: 0.0771, acc: 0.9757\n",
            "E2E-ABSA >>> 2022-08-17 12:26:30\n",
            "loss: 0.0778, acc: 0.9754\n",
            "E2E-ABSA >>> 2022-08-17 12:26:45\n",
            "loss: 0.1096, acc: 0.9638\n",
            "E2E-ABSA >>> 2022-08-17 12:27:00\n",
            "loss: 0.1122, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-08-17 12:27:16\n",
            "loss: 0.1200, acc: 0.9547\n",
            "E2E-ABSA >>> 2022-08-17 12:27:31\n",
            "loss: 0.1250, acc: 0.9536\n",
            "E2E-ABSA >>> 2022-08-17 12:27:46\n",
            "loss: 0.1355, acc: 0.9483\n",
            "E2E-ABSA >>> 2022-08-17 12:28:01\n",
            "loss: 0.1382, acc: 0.9482\n",
            "E2E-ABSA >>> 2022-08-17 12:28:19\n",
            ">>> val_acc: 0.7690, val_precision: 0.7690 val_recall: 0.7690, val_f1: 0.7690\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 12:28:21\n",
            "loss: 0.0251, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-08-17 12:28:36\n",
            "loss: 0.0473, acc: 0.9886\n",
            "E2E-ABSA >>> 2022-08-17 12:28:51\n",
            "loss: 0.0572, acc: 0.9807\n",
            "E2E-ABSA >>> 2022-08-17 12:29:06\n",
            "loss: 0.0576, acc: 0.9808\n",
            "E2E-ABSA >>> 2022-08-17 12:29:21\n",
            "loss: 0.0720, acc: 0.9718\n",
            "E2E-ABSA >>> 2022-08-17 12:29:36\n",
            "loss: 0.0657, acc: 0.9761\n",
            "E2E-ABSA >>> 2022-08-17 12:29:52\n",
            "loss: 0.0643, acc: 0.9769\n",
            "E2E-ABSA >>> 2022-08-17 12:30:07\n",
            "loss: 0.0680, acc: 0.9762\n",
            "E2E-ABSA >>> 2022-08-17 12:30:22\n",
            "loss: 0.0715, acc: 0.9761\n",
            "E2E-ABSA >>> 2022-08-17 12:30:37\n",
            "loss: 0.0785, acc: 0.9739\n",
            "E2E-ABSA >>> 2022-08-17 12:30:51\n",
            ">>> val_acc: 0.7812, val_precision: 0.7812 val_recall: 0.7812, val_f1: 0.7812\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 12:30:57\n",
            "loss: 0.5202, acc: 0.8672\n",
            "E2E-ABSA >>> 2022-08-17 12:31:12\n",
            "loss: 0.3428, acc: 0.8795\n",
            "E2E-ABSA >>> 2022-08-17 12:31:27\n",
            "loss: 0.2628, acc: 0.9102\n",
            "E2E-ABSA >>> 2022-08-17 12:31:42\n",
            "loss: 0.2228, acc: 0.9246\n",
            "E2E-ABSA >>> 2022-08-17 12:31:57\n",
            "loss: 0.2111, acc: 0.9254\n",
            "E2E-ABSA >>> 2022-08-17 12:32:12\n",
            "loss: 0.2020, acc: 0.9288\n",
            "E2E-ABSA >>> 2022-08-17 12:32:28\n",
            "loss: 0.1881, acc: 0.9321\n",
            "E2E-ABSA >>> 2022-08-17 12:32:43\n",
            "loss: 0.1767, acc: 0.9371\n",
            "E2E-ABSA >>> 2022-08-17 12:32:58\n",
            "loss: 0.1711, acc: 0.9390\n",
            "E2E-ABSA >>> 2022-08-17 12:33:13\n",
            "loss: 0.1707, acc: 0.9385\n",
            "E2E-ABSA >>> 2022-08-17 12:33:22\n",
            ">>> val_acc: 0.7842, val_precision: 0.7842 val_recall: 0.7842, val_f1: 0.7842\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 12:33:33\n",
            "loss: 0.0408, acc: 0.9821\n",
            "E2E-ABSA >>> 2022-08-17 12:33:48\n",
            "loss: 0.0487, acc: 0.9816\n",
            "E2E-ABSA >>> 2022-08-17 12:34:03\n",
            "loss: 0.0441, acc: 0.9838\n",
            "E2E-ABSA >>> 2022-08-17 12:34:18\n",
            "loss: 0.0435, acc: 0.9856\n",
            "E2E-ABSA >>> 2022-08-17 12:34:33\n",
            "loss: 0.0441, acc: 0.9867\n",
            "E2E-ABSA >>> 2022-08-17 12:34:48\n",
            "loss: 0.0463, acc: 0.9852\n",
            "E2E-ABSA >>> 2022-08-17 12:35:04\n",
            "loss: 0.0533, acc: 0.9827\n",
            "E2E-ABSA >>> 2022-08-17 12:35:19\n",
            "loss: 0.0597, acc: 0.9805\n",
            "E2E-ABSA >>> 2022-08-17 12:35:34\n",
            "loss: 0.0644, acc: 0.9792\n",
            "E2E-ABSA >>> 2022-08-17 12:35:48\n",
            "loss: 0.0790, acc: 0.9735\n",
            "E2E-ABSA >>> 2022-08-17 12:35:54\n",
            ">>> val_acc: 0.7751, val_precision: 0.7751 val_recall: 0.7751, val_f1: 0.7751\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 12:36:09\n",
            "loss: 0.1008, acc: 0.9656\n",
            "E2E-ABSA >>> 2022-08-17 12:36:24\n",
            "loss: 0.0751, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-08-17 12:36:39\n",
            "loss: 0.0731, acc: 0.9729\n",
            "E2E-ABSA >>> 2022-08-17 12:36:54\n",
            "loss: 0.0879, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 12:37:09\n",
            "loss: 0.1058, acc: 0.9644\n",
            "E2E-ABSA >>> 2022-08-17 12:37:24\n",
            "loss: 0.1231, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-08-17 12:37:39\n",
            "loss: 0.1332, acc: 0.9509\n",
            "E2E-ABSA >>> 2022-08-17 12:37:55\n",
            "loss: 0.1360, acc: 0.9504\n",
            "E2E-ABSA >>> 2022-08-17 12:38:10\n",
            "loss: 0.1361, acc: 0.9497\n",
            "E2E-ABSA >>> 2022-08-17 12:38:25\n",
            ">>> val_acc: 0.8024, val_precision: 0.8024 val_recall: 0.8024, val_f1: 0.8024\n",
            "E2E-ABSA >>> 2022-08-17 12:38:25\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8085, val_precision: 0.8085 val_recall: 0.8085, val_f1: 0.8085\n",
            "you can download the best model from state_dict/lcf_bert_SemEval2014_val_f1_0.8085\n",
            ">>> test_acc: 0.8085, test_precision: 0.8085, test_recall: 0.8085, test_f1: 0.8085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后： Training **SemEval2014** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "8rUNRbKt6Y_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset SemEval2014_know --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "id": "9ETOKs7X6ZEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca5f26c-2a4a-4671-d1d2-a90b27b78d69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 3093.\n",
            "> testing dataset count: 329.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: SemEval2014_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fa59416fb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/laprest14/output_know/train.tsv', 'test': './datasets/laprest14/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 12:39:27\n",
            "loss: 1.0233, acc: 0.5844\n",
            "E2E-ABSA >>> 2022-08-17 12:39:42\n",
            "loss: 0.9811, acc: 0.5844\n",
            "E2E-ABSA >>> 2022-08-17 12:39:58\n",
            "loss: 0.9533, acc: 0.5844\n",
            "E2E-ABSA >>> 2022-08-17 12:40:13\n",
            "loss: 0.9333, acc: 0.5898\n",
            "E2E-ABSA >>> 2022-08-17 12:40:28\n",
            "loss: 0.8821, acc: 0.6138\n",
            "E2E-ABSA >>> 2022-08-17 12:40:43\n",
            "loss: 0.8397, acc: 0.6359\n",
            "E2E-ABSA >>> 2022-08-17 12:40:58\n",
            "loss: 0.8049, acc: 0.6536\n",
            "E2E-ABSA >>> 2022-08-17 12:41:13\n",
            "loss: 0.7784, acc: 0.6645\n",
            "E2E-ABSA >>> 2022-08-17 12:41:29\n",
            "loss: 0.7596, acc: 0.6736\n",
            "E2E-ABSA >>> 2022-08-17 12:41:44\n",
            ">>> val_acc: 0.7903, val_precision: 0.7903 val_recall: 0.7903, val_f1: 0.7903\n",
            ">> saved: state_dict/lcf_bert_SemEval2014_know_val_f1_0.7903\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 12:41:50\n",
            "loss: 0.4964, acc: 0.8542\n",
            "E2E-ABSA >>> 2022-08-17 12:42:05\n",
            "loss: 0.4572, acc: 0.8221\n",
            "E2E-ABSA >>> 2022-08-17 12:42:20\n",
            "loss: 0.4984, acc: 0.7989\n",
            "E2E-ABSA >>> 2022-08-17 12:42:35\n",
            "loss: 0.4880, acc: 0.7917\n",
            "E2E-ABSA >>> 2022-08-17 12:42:51\n",
            "loss: 0.4894, acc: 0.7987\n",
            "E2E-ABSA >>> 2022-08-17 12:43:06\n",
            "loss: 0.4818, acc: 0.8060\n",
            "E2E-ABSA >>> 2022-08-17 12:43:21\n",
            "loss: 0.4824, acc: 0.8041\n",
            "E2E-ABSA >>> 2022-08-17 12:43:36\n",
            "loss: 0.4740, acc: 0.8057\n",
            "E2E-ABSA >>> 2022-08-17 12:43:51\n",
            "loss: 0.4613, acc: 0.8114\n",
            "E2E-ABSA >>> 2022-08-17 12:44:06\n",
            "loss: 0.4598, acc: 0.8112\n",
            "E2E-ABSA >>> 2022-08-17 12:44:17\n",
            ">>> val_acc: 0.8116, val_precision: 0.8116 val_recall: 0.8116, val_f1: 0.8116\n",
            ">> saved: state_dict/lcf_bert_SemEval2014_know_val_f1_0.8116\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 12:44:27\n",
            "loss: 0.2740, acc: 0.8958\n",
            "E2E-ABSA >>> 2022-08-17 12:44:43\n",
            "loss: 0.3128, acc: 0.8789\n",
            "E2E-ABSA >>> 2022-08-17 12:44:58\n",
            "loss: 0.3087, acc: 0.8786\n",
            "E2E-ABSA >>> 2022-08-17 12:45:13\n",
            "loss: 0.3183, acc: 0.8776\n",
            "E2E-ABSA >>> 2022-08-17 12:45:28\n",
            "loss: 0.3133, acc: 0.8777\n",
            "E2E-ABSA >>> 2022-08-17 12:45:43\n",
            "loss: 0.3091, acc: 0.8795\n",
            "E2E-ABSA >>> 2022-08-17 12:45:58\n",
            "loss: 0.3132, acc: 0.8788\n",
            "E2E-ABSA >>> 2022-08-17 12:46:14\n",
            "loss: 0.3134, acc: 0.8775\n",
            "E2E-ABSA >>> 2022-08-17 12:46:29\n",
            "loss: 0.3136, acc: 0.8772\n",
            "E2E-ABSA >>> 2022-08-17 12:46:44\n",
            "loss: 0.3196, acc: 0.8740\n",
            "E2E-ABSA >>> 2022-08-17 12:46:50\n",
            ">>> val_acc: 0.8207, val_precision: 0.8207 val_recall: 0.8207, val_f1: 0.8207\n",
            ">> saved: state_dict/lcf_bert_SemEval2014_know_val_f1_0.8207\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 12:47:05\n",
            "loss: 0.2303, acc: 0.9306\n",
            "E2E-ABSA >>> 2022-08-17 12:47:20\n",
            "loss: 0.2164, acc: 0.9276\n",
            "E2E-ABSA >>> 2022-08-17 12:47:36\n",
            "loss: 0.2280, acc: 0.9192\n",
            "E2E-ABSA >>> 2022-08-17 12:47:51\n",
            "loss: 0.2375, acc: 0.9151\n",
            "E2E-ABSA >>> 2022-08-17 12:48:06\n",
            "loss: 0.2468, acc: 0.9094\n",
            "E2E-ABSA >>> 2022-08-17 12:48:21\n",
            "loss: 0.2407, acc: 0.9100\n",
            "E2E-ABSA >>> 2022-08-17 12:48:36\n",
            "loss: 0.2383, acc: 0.9094\n",
            "E2E-ABSA >>> 2022-08-17 12:48:51\n",
            "loss: 0.2360, acc: 0.9098\n",
            "E2E-ABSA >>> 2022-08-17 12:49:07\n",
            "loss: 0.2325, acc: 0.9112\n",
            "E2E-ABSA >>> 2022-08-17 12:49:24\n",
            ">>> val_acc: 0.8085, val_precision: 0.8085 val_recall: 0.8085, val_f1: 0.8085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 12:49:27\n",
            "loss: 0.0921, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 12:49:42\n",
            "loss: 0.1185, acc: 0.9505\n",
            "E2E-ABSA >>> 2022-08-17 12:49:57\n",
            "loss: 0.1281, acc: 0.9517\n",
            "E2E-ABSA >>> 2022-08-17 12:50:12\n",
            "loss: 0.1481, acc: 0.9521\n",
            "E2E-ABSA >>> 2022-08-17 12:50:27\n",
            "loss: 0.1434, acc: 0.9539\n",
            "E2E-ABSA >>> 2022-08-17 12:50:42\n",
            "loss: 0.1447, acc: 0.9495\n",
            "E2E-ABSA >>> 2022-08-17 12:50:57\n",
            "loss: 0.1606, acc: 0.9435\n",
            "E2E-ABSA >>> 2022-08-17 12:51:13\n",
            "loss: 0.1630, acc: 0.9410\n",
            "E2E-ABSA >>> 2022-08-17 12:51:28\n",
            "loss: 0.1698, acc: 0.9390\n",
            "E2E-ABSA >>> 2022-08-17 12:51:43\n",
            "loss: 0.1776, acc: 0.9361\n",
            "E2E-ABSA >>> 2022-08-17 12:51:55\n",
            ">>> val_acc: 0.8176, val_precision: 0.8176 val_recall: 0.8176, val_f1: 0.8176\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 12:52:03\n",
            "loss: 0.0902, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-08-17 12:52:18\n",
            "loss: 0.0986, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-08-17 12:52:33\n",
            "loss: 0.1124, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 12:52:49\n",
            "loss: 0.1099, acc: 0.9679\n",
            "E2E-ABSA >>> 2022-08-17 12:53:04\n",
            "loss: 0.1197, acc: 0.9632\n",
            "E2E-ABSA >>> 2022-08-17 12:53:19\n",
            "loss: 0.1320, acc: 0.9580\n",
            "E2E-ABSA >>> 2022-08-17 12:53:34\n",
            "loss: 0.1327, acc: 0.9558\n",
            "E2E-ABSA >>> 2022-08-17 12:53:49\n",
            "loss: 0.1446, acc: 0.9500\n",
            "E2E-ABSA >>> 2022-08-17 12:54:04\n",
            "loss: 0.1436, acc: 0.9489\n",
            "E2E-ABSA >>> 2022-08-17 12:54:20\n",
            "loss: 0.1426, acc: 0.9503\n",
            "E2E-ABSA >>> 2022-08-17 12:54:28\n",
            ">>> val_acc: 0.8237, val_precision: 0.8237 val_recall: 0.8237, val_f1: 0.8237\n",
            ">> saved: state_dict/lcf_bert_SemEval2014_know_val_f1_0.8237\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 12:54:41\n",
            "loss: 0.0910, acc: 0.9727\n",
            "E2E-ABSA >>> 2022-08-17 12:54:56\n",
            "loss: 0.0683, acc: 0.9809\n",
            "E2E-ABSA >>> 2022-08-17 12:55:11\n",
            "loss: 0.0902, acc: 0.9699\n",
            "E2E-ABSA >>> 2022-08-17 12:55:26\n",
            "loss: 0.0905, acc: 0.9696\n",
            "E2E-ABSA >>> 2022-08-17 12:55:42\n",
            "loss: 0.0900, acc: 0.9668\n",
            "E2E-ABSA >>> 2022-08-17 12:55:57\n",
            "loss: 0.0985, acc: 0.9607\n",
            "E2E-ABSA >>> 2022-08-17 12:56:12\n",
            "loss: 0.1031, acc: 0.9600\n",
            "E2E-ABSA >>> 2022-08-17 12:56:27\n",
            "loss: 0.1169, acc: 0.9583\n",
            "E2E-ABSA >>> 2022-08-17 12:56:42\n",
            "loss: 0.1212, acc: 0.9553\n",
            "E2E-ABSA >>> 2022-08-17 12:57:01\n",
            ">>> val_acc: 0.8085, val_precision: 0.8085 val_recall: 0.8085, val_f1: 0.8085\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 12:57:02\n",
            "loss: 0.1050, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 12:57:18\n",
            "loss: 0.0582, acc: 0.9886\n",
            "E2E-ABSA >>> 2022-08-17 12:57:33\n",
            "loss: 0.0842, acc: 0.9747\n",
            "E2E-ABSA >>> 2022-08-17 12:57:48\n",
            "loss: 0.0769, acc: 0.9788\n",
            "E2E-ABSA >>> 2022-08-17 12:58:03\n",
            "loss: 0.0722, acc: 0.9809\n",
            "E2E-ABSA >>> 2022-08-17 12:58:18\n",
            "loss: 0.0733, acc: 0.9779\n",
            "E2E-ABSA >>> 2022-08-17 12:58:33\n",
            "loss: 0.0833, acc: 0.9718\n",
            "E2E-ABSA >>> 2022-08-17 12:58:49\n",
            "loss: 0.1064, acc: 0.9595\n",
            "E2E-ABSA >>> 2022-08-17 12:59:04\n",
            "loss: 0.1187, acc: 0.9552\n",
            "E2E-ABSA >>> 2022-08-17 12:59:19\n",
            "loss: 0.1211, acc: 0.9550\n",
            "E2E-ABSA >>> 2022-08-17 12:59:33\n",
            ">>> val_acc: 0.7720, val_precision: 0.7720 val_recall: 0.7720, val_f1: 0.7720\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 12:59:39\n",
            "loss: 0.0640, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-08-17 12:59:54\n",
            "loss: 0.1143, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 13:00:09\n",
            "loss: 0.1202, acc: 0.9622\n",
            "E2E-ABSA >>> 2022-08-17 13:00:24\n",
            "loss: 0.1117, acc: 0.9660\n",
            "E2E-ABSA >>> 2022-08-17 13:00:40\n",
            "loss: 0.0991, acc: 0.9702\n",
            "E2E-ABSA >>> 2022-08-17 13:00:55\n",
            "loss: 0.0934, acc: 0.9728\n",
            "E2E-ABSA >>> 2022-08-17 13:01:10\n",
            "loss: 0.0895, acc: 0.9727\n",
            "E2E-ABSA >>> 2022-08-17 13:01:25\n",
            "loss: 0.0898, acc: 0.9717\n",
            "E2E-ABSA >>> 2022-08-17 13:01:40\n",
            "loss: 0.0972, acc: 0.9695\n",
            "E2E-ABSA >>> 2022-08-17 13:01:55\n",
            "loss: 0.1002, acc: 0.9671\n",
            "E2E-ABSA >>> 2022-08-17 13:02:05\n",
            ">>> val_acc: 0.7872, val_precision: 0.7872 val_recall: 0.7872, val_f1: 0.7872\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 13:02:15\n",
            "loss: 0.1222, acc: 0.9509\n",
            "E2E-ABSA >>> 2022-08-17 13:02:30\n",
            "loss: 0.1270, acc: 0.9522\n",
            "E2E-ABSA >>> 2022-08-17 13:02:46\n",
            "loss: 0.1025, acc: 0.9630\n",
            "E2E-ABSA >>> 2022-08-17 13:03:01\n",
            "loss: 0.0832, acc: 0.9721\n",
            "E2E-ABSA >>> 2022-08-17 13:03:16\n",
            "loss: 0.0777, acc: 0.9747\n",
            "E2E-ABSA >>> 2022-08-17 13:03:31\n",
            "loss: 0.0785, acc: 0.9726\n",
            "E2E-ABSA >>> 2022-08-17 13:03:46\n",
            "loss: 0.0839, acc: 0.9697\n",
            "E2E-ABSA >>> 2022-08-17 13:04:01\n",
            "loss: 0.0866, acc: 0.9692\n",
            "E2E-ABSA >>> 2022-08-17 13:04:16\n",
            "loss: 0.0908, acc: 0.9673\n",
            "E2E-ABSA >>> 2022-08-17 13:04:31\n",
            "loss: 0.0912, acc: 0.9667\n",
            "E2E-ABSA >>> 2022-08-17 13:04:36\n",
            ">>> val_acc: 0.8176, val_precision: 0.8176 val_recall: 0.8176, val_f1: 0.8176\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 13:04:52\n",
            "loss: 0.1927, acc: 0.9156\n",
            "E2E-ABSA >>> 2022-08-17 13:05:07\n",
            "loss: 0.1187, acc: 0.9547\n",
            "E2E-ABSA >>> 2022-08-17 13:05:22\n",
            "loss: 0.0972, acc: 0.9615\n",
            "E2E-ABSA >>> 2022-08-17 13:05:37\n",
            "loss: 0.0953, acc: 0.9609\n",
            "E2E-ABSA >>> 2022-08-17 13:05:52\n",
            "loss: 0.0948, acc: 0.9619\n",
            "E2E-ABSA >>> 2022-08-17 13:06:07\n",
            "loss: 0.0901, acc: 0.9656\n",
            "E2E-ABSA >>> 2022-08-17 13:06:22\n",
            "loss: 0.0914, acc: 0.9652\n",
            "E2E-ABSA >>> 2022-08-17 13:06:38\n",
            "loss: 0.1002, acc: 0.9625\n",
            "E2E-ABSA >>> 2022-08-17 13:06:53\n",
            "loss: 0.0983, acc: 0.9632\n",
            "E2E-ABSA >>> 2022-08-17 13:07:08\n",
            ">>> val_acc: 0.7842, val_precision: 0.7842 val_recall: 0.7842, val_f1: 0.7842\n",
            "E2E-ABSA >>> 2022-08-17 13:07:08\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.8237, val_precision: 0.8237 val_recall: 0.8237, val_f1: 0.8237\n",
            "you can download the best model from state_dict/lcf_bert_SemEval2014_know_val_f1_0.8237\n",
            ">>> test_acc: 0.8237, test_precision: 0.8237, test_recall: 0.8237, test_f1: 0.8237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **SemEval2015** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "zEbjxA6r6ZJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset SemEval2015 --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "id": "CXzf32hO6ZQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005c9c1d-d9fa-4969-b2d0-c8be92e34a7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: SemEval2015\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f2a06054b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/train.tsv', 'test': './datasets/rest15/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 13:07:44\n",
            "loss: 0.7225, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 13:08:00\n",
            "loss: 0.6866, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 13:08:07\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/lcf_bert_SemEval2015_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 13:08:18\n",
            "loss: 0.6427, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 13:08:33\n",
            "loss: 0.6202, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 13:08:45\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">> saved: state_dict/lcf_bert_SemEval2015_val_f1_0.878\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 13:08:50\n",
            "loss: 0.4428, acc: 0.8542\n",
            "E2E-ABSA >>> 2022-08-17 13:09:05\n",
            "loss: 0.3621, acc: 0.8798\n",
            "E2E-ABSA >>> 2022-08-17 13:09:21\n",
            "loss: 0.3048, acc: 0.9008\n",
            "E2E-ABSA >>> 2022-08-17 13:09:23\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 13:09:37\n",
            "loss: 0.1787, acc: 0.9507\n",
            "E2E-ABSA >>> 2022-08-17 13:09:52\n",
            "loss: 0.1673, acc: 0.9535\n",
            "E2E-ABSA >>> 2022-08-17 13:10:00\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">> saved: state_dict/lcf_bert_SemEval2015_val_f1_0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 13:10:10\n",
            "loss: 0.1286, acc: 0.9740\n",
            "E2E-ABSA >>> 2022-08-17 13:10:25\n",
            "loss: 0.1162, acc: 0.9727\n",
            "E2E-ABSA >>> 2022-08-17 13:10:38\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 13:10:41\n",
            "loss: 0.0528, acc: 0.9625\n",
            "E2E-ABSA >>> 2022-08-17 13:10:57\n",
            "loss: 0.0437, acc: 0.9850\n",
            "E2E-ABSA >>> 2022-08-17 13:11:12\n",
            "loss: 0.0571, acc: 0.9806\n",
            "E2E-ABSA >>> 2022-08-17 13:11:15\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 13:11:28\n",
            "loss: 0.0498, acc: 0.9861\n",
            "E2E-ABSA >>> 2022-08-17 13:11:43\n",
            "loss: 0.0413, acc: 0.9901\n",
            "E2E-ABSA >>> 2022-08-17 13:11:51\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 13:12:00\n",
            "loss: 0.0369, acc: 0.9886\n",
            "E2E-ABSA >>> 2022-08-17 13:12:15\n",
            "loss: 0.0233, acc: 0.9919\n",
            "E2E-ABSA >>> 2022-08-17 13:12:28\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 13:12:31\n",
            "loss: 0.0130, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-08-17 13:12:46\n",
            "loss: 0.0182, acc: 0.9974\n",
            "E2E-ABSA >>> 2022-08-17 13:13:02\n",
            "loss: 0.0147, acc: 0.9986\n",
            "E2E-ABSA >>> 2022-08-17 13:13:05\n",
            ">>> val_acc: 0.8780, val_precision: 0.8780 val_recall: 0.8780, val_f1: 0.8780\n",
            "E2E-ABSA >>> 2022-08-17 13:13:05\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            "you can download the best model from state_dict/lcf_bert_SemEval2015_val_f1_0.9024\n",
            ">>> test_acc: 0.9024, test_precision: 0.9024, test_recall: 0.9024, test_f1: 0.9024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后： Training **SemEval2015** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "ycqD9lFT6ZWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset SemEval2015_know --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "id": "3jjH1hud6ZgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78871e4a-dc39-4ad7-cc5f-f3f4bfb0b052"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 750.\n",
            "> testing dataset count: 82.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: SemEval2015_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f82eb115b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/rest15/output_know/train.tsv', 'test': './datasets/rest15/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 13:13:39\n",
            "loss: 0.6923, acc: 0.7688\n",
            "E2E-ABSA >>> 2022-08-17 13:13:55\n",
            "loss: 0.6763, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 13:14:02\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">> saved: state_dict/lcf_bert_SemEval2015_know_val_f1_0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 13:14:13\n",
            "loss: 0.6154, acc: 0.7740\n",
            "E2E-ABSA >>> 2022-08-17 13:14:28\n",
            "loss: 0.6042, acc: 0.7576\n",
            "E2E-ABSA >>> 2022-08-17 13:14:40\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">> saved: state_dict/lcf_bert_SemEval2015_know_val_f1_0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 13:14:46\n",
            "loss: 0.5149, acc: 0.7812\n",
            "E2E-ABSA >>> 2022-08-17 13:15:01\n",
            "loss: 0.4058, acc: 0.8654\n",
            "E2E-ABSA >>> 2022-08-17 13:15:16\n",
            "loss: 0.3390, acc: 0.8845\n",
            "E2E-ABSA >>> 2022-08-17 13:15:18\n",
            ">>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            ">> saved: state_dict/lcf_bert_SemEval2015_know_val_f1_0.9024\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 13:15:34\n",
            "loss: 0.1954, acc: 0.9474\n",
            "E2E-ABSA >>> 2022-08-17 13:15:49\n",
            "loss: 0.1991, acc: 0.9455\n",
            "E2E-ABSA >>> 2022-08-17 13:15:56\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 13:16:05\n",
            "loss: 0.1480, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 13:16:20\n",
            "loss: 0.1327, acc: 0.9492\n",
            "E2E-ABSA >>> 2022-08-17 13:16:33\n",
            ">>> val_acc: 0.8415, val_precision: 0.8415 val_recall: 0.8415, val_f1: 0.8415\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 13:16:37\n",
            "loss: 0.1086, acc: 0.9625\n",
            "E2E-ABSA >>> 2022-08-17 13:16:52\n",
            "loss: 0.0811, acc: 0.9775\n",
            "E2E-ABSA >>> 2022-08-17 13:17:07\n",
            "loss: 0.0829, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-08-17 13:17:10\n",
            ">>> val_acc: 0.8293, val_precision: 0.8293 val_recall: 0.8293, val_f1: 0.8293\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 13:17:24\n",
            "loss: 0.0901, acc: 0.9757\n",
            "E2E-ABSA >>> 2022-08-17 13:17:39\n",
            "loss: 0.0801, acc: 0.9753\n",
            "E2E-ABSA >>> 2022-08-17 13:17:47\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 13:17:55\n",
            "loss: 0.0450, acc: 0.9830\n",
            "E2E-ABSA >>> 2022-08-17 13:18:10\n",
            "loss: 0.0314, acc: 0.9899\n",
            "E2E-ABSA >>> 2022-08-17 13:18:24\n",
            ">>> val_acc: 0.8659, val_precision: 0.8659 val_recall: 0.8659, val_f1: 0.8659\n",
            "E2E-ABSA >>> 2022-08-17 13:18:24\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.9024, val_precision: 0.9024 val_recall: 0.9024, val_f1: 0.9024\n",
            "you can download the best model from state_dict/lcf_bert_SemEval2015_know_val_f1_0.9024\n",
            ">>> test_acc: 0.9024, test_precision: 0.9024, test_recall: 0.9024, test_f1: 0.9024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **SemEval2016** dataset on model**(lfc_bert)**\n"
      ],
      "metadata": {
        "id": "ySnUJH546SpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset SemEval2016 --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSkT07x16WQ5",
        "outputId": "5f35a42f-7135-49b3-d741-f718186f48a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: SemEval2016\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7f756aa22b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/train.tsv', 'test': './datasets/rest16/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 10:43:47\n",
            "loss: 0.7129, acc: 0.6844\n",
            "E2E-ABSA >>> 2022-08-17 10:44:01\n",
            "loss: 0.7391, acc: 0.6969\n",
            "E2E-ABSA >>> 2022-08-17 10:44:16\n",
            "loss: 0.7331, acc: 0.7042\n",
            "E2E-ABSA >>> 2022-08-17 10:44:25\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 10:44:34\n",
            "loss: 0.6990, acc: 0.7125\n",
            "E2E-ABSA >>> 2022-08-17 10:44:49\n",
            "loss: 0.6752, acc: 0.7208\n",
            "E2E-ABSA >>> 2022-08-17 10:45:04\n",
            "loss: 0.6406, acc: 0.7412\n",
            "E2E-ABSA >>> 2022-08-17 10:45:20\n",
            "loss: 0.5908, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 10:45:22\n",
            ">>> val_acc: 0.8220, val_precision: 0.8220 val_recall: 0.8220, val_f1: 0.8220\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_val_f1_0.822\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 10:45:40\n",
            "loss: 0.3493, acc: 0.8906\n",
            "E2E-ABSA >>> 2022-08-17 10:45:55\n",
            "loss: 0.3578, acc: 0.8859\n",
            "E2E-ABSA >>> 2022-08-17 10:46:11\n",
            "loss: 0.3396, acc: 0.8896\n",
            "E2E-ABSA >>> 2022-08-17 10:46:21\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_val_f1_0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 10:46:30\n",
            "loss: 0.1489, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-08-17 10:46:46\n",
            "loss: 0.1890, acc: 0.9437\n",
            "E2E-ABSA >>> 2022-08-17 10:47:02\n",
            "loss: 0.2053, acc: 0.9337\n",
            "E2E-ABSA >>> 2022-08-17 10:47:17\n",
            "loss: 0.2009, acc: 0.9326\n",
            "E2E-ABSA >>> 2022-08-17 10:47:19\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_val_f1_0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 10:47:37\n",
            "loss: 0.1617, acc: 0.9500\n",
            "E2E-ABSA >>> 2022-08-17 10:47:53\n",
            "loss: 0.1434, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 10:48:09\n",
            "loss: 0.1332, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-08-17 10:48:18\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 10:48:26\n",
            "loss: 0.1076, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-08-17 10:48:42\n",
            "loss: 0.0832, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 10:48:58\n",
            "loss: 0.0787, acc: 0.9738\n",
            "E2E-ABSA >>> 2022-08-17 10:49:14\n",
            "loss: 0.0818, acc: 0.9748\n",
            "E2E-ABSA >>> 2022-08-17 10:49:16\n",
            ">>> val_acc: 0.8983, val_precision: 0.8983 val_recall: 0.8983, val_f1: 0.8983\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 10:49:31\n",
            "loss: 0.0427, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-08-17 10:49:47\n",
            "loss: 0.0630, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-08-17 10:50:03\n",
            "loss: 0.0663, acc: 0.9760\n",
            "E2E-ABSA >>> 2022-08-17 10:50:13\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 10:50:21\n",
            "loss: 0.0626, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 10:50:37\n",
            "loss: 0.0760, acc: 0.9771\n",
            "E2E-ABSA >>> 2022-08-17 10:50:53\n",
            "loss: 0.0825, acc: 0.9762\n",
            "E2E-ABSA >>> 2022-08-17 10:51:09\n",
            "loss: 0.0682, acc: 0.9802\n",
            "E2E-ABSA >>> 2022-08-17 10:51:11\n",
            ">>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_val_f1_0.9153\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 10:51:28\n",
            "loss: 0.0347, acc: 0.9906\n",
            "E2E-ABSA >>> 2022-08-17 10:51:44\n",
            "loss: 0.0477, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-08-17 10:52:00\n",
            "loss: 0.0524, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-08-17 10:52:09\n",
            ">>> val_acc: 0.8644, val_precision: 0.8644 val_recall: 0.8644, val_f1: 0.8644\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 10:52:17\n",
            "loss: 0.0307, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-08-17 10:52:33\n",
            "loss: 0.0439, acc: 0.9854\n",
            "E2E-ABSA >>> 2022-08-17 10:52:49\n",
            "loss: 0.0423, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-08-17 10:53:05\n",
            "loss: 0.0458, acc: 0.9883\n",
            "E2E-ABSA >>> 2022-08-17 10:53:07\n",
            ">>> val_acc: 0.8559, val_precision: 0.8559 val_recall: 0.8559, val_f1: 0.8559\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 10:53:23\n",
            "loss: 0.0327, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-08-17 10:53:39\n",
            "loss: 0.0240, acc: 0.9922\n",
            "E2E-ABSA >>> 2022-08-17 10:53:55\n",
            "loss: 0.0208, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-08-17 10:54:04\n",
            ">>> val_acc: 0.8729, val_precision: 0.8729 val_recall: 0.8729, val_f1: 0.8729\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 10:54:12\n",
            "loss: 0.0226, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-08-17 10:54:28\n",
            "loss: 0.0123, acc: 0.9958\n",
            "E2E-ABSA >>> 2022-08-17 10:54:44\n",
            "loss: 0.0211, acc: 0.9938\n",
            "E2E-ABSA >>> 2022-08-17 10:55:00\n",
            "loss: 0.0201, acc: 0.9946\n",
            "E2E-ABSA >>> 2022-08-17 10:55:02\n",
            ">>> val_acc: 0.8983, val_precision: 0.8983 val_recall: 0.8983, val_f1: 0.8983\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 10:55:18\n",
            "loss: 0.0040, acc: 1.0000\n",
            "E2E-ABSA >>> 2022-08-17 10:55:34\n",
            "loss: 0.0093, acc: 0.9984\n",
            "E2E-ABSA >>> 2022-08-17 10:55:50\n",
            "loss: 0.0286, acc: 0.9917\n",
            "E2E-ABSA >>> 2022-08-17 10:55:59\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            "E2E-ABSA >>> 2022-08-17 10:55:59\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            "you can download the best model from state_dict/lcf_bert_SemEval2016_val_f1_0.9153\n",
            ">>> test_acc: 0.9153, test_precision: 0.9153, test_recall: 0.9153, test_f1: 0.9153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后： Training **SemEval2016** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "F14mQtiE6WmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict\n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset SemEval2016_know --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNoDIVFCAbOW",
        "outputId": "173367fc-326d-482d-d995-b69764751ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 1112.\n",
            "> testing dataset count: 118.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: SemEval2016_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fe59396fb00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/rest16/output_know/train.tsv', 'test': './datasets/rest16/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 10:56:38\n",
            "loss: 0.7250, acc: 0.7000\n",
            "E2E-ABSA >>> 2022-08-17 10:56:54\n",
            "loss: 0.7466, acc: 0.7016\n",
            "E2E-ABSA >>> 2022-08-17 10:57:11\n",
            "loss: 0.7366, acc: 0.7073\n",
            "E2E-ABSA >>> 2022-08-17 10:57:20\n",
            ">>> val_acc: 0.7288, val_precision: 0.7288 val_recall: 0.7288, val_f1: 0.7288\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_know_val_f1_0.7288\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 10:57:29\n",
            "loss: 0.6337, acc: 0.7188\n",
            "E2E-ABSA >>> 2022-08-17 10:57:45\n",
            "loss: 0.5907, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 10:58:01\n",
            "loss: 0.5462, acc: 0.7850\n",
            "E2E-ABSA >>> 2022-08-17 10:58:17\n",
            "loss: 0.5292, acc: 0.8031\n",
            "E2E-ABSA >>> 2022-08-17 10:58:19\n",
            ">>> val_acc: 0.7542, val_precision: 0.7542 val_recall: 0.7542, val_f1: 0.7542\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_know_val_f1_0.7542\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 10:58:36\n",
            "loss: 0.3466, acc: 0.8656\n",
            "E2E-ABSA >>> 2022-08-17 10:58:52\n",
            "loss: 0.3254, acc: 0.8828\n",
            "E2E-ABSA >>> 2022-08-17 10:59:08\n",
            "loss: 0.3140, acc: 0.8865\n",
            "E2E-ABSA >>> 2022-08-17 10:59:18\n",
            ">>> val_acc: 0.8814, val_precision: 0.8814 val_recall: 0.8814, val_f1: 0.8814\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_know_val_f1_0.8814\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 10:59:27\n",
            "loss: 0.1507, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-08-17 10:59:43\n",
            "loss: 0.1843, acc: 0.9479\n",
            "E2E-ABSA >>> 2022-08-17 10:59:59\n",
            "loss: 0.1679, acc: 0.9513\n",
            "E2E-ABSA >>> 2022-08-17 11:00:14\n",
            "loss: 0.1695, acc: 0.9469\n",
            "E2E-ABSA >>> 2022-08-17 11:00:16\n",
            ">>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            ">> saved: state_dict/lcf_bert_SemEval2016_know_val_f1_0.9153\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 11:00:34\n",
            "loss: 0.0975, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 11:00:50\n",
            "loss: 0.1011, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 11:01:06\n",
            "loss: 0.1000, acc: 0.9677\n",
            "E2E-ABSA >>> 2022-08-17 11:01:15\n",
            ">>> val_acc: 0.8898, val_precision: 0.8898 val_recall: 0.8898, val_f1: 0.8898\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 11:01:23\n",
            "loss: 0.0445, acc: 0.9875\n",
            "E2E-ABSA >>> 2022-08-17 11:01:39\n",
            "loss: 0.0607, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-08-17 11:01:55\n",
            "loss: 0.0608, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-08-17 11:02:11\n",
            "loss: 0.0710, acc: 0.9784\n",
            "E2E-ABSA >>> 2022-08-17 11:02:13\n",
            ">>> val_acc: 0.9068, val_precision: 0.9068 val_recall: 0.9068, val_f1: 0.9068\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 11:02:29\n",
            "loss: 0.0700, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-08-17 11:02:45\n",
            "loss: 0.0650, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-08-17 11:03:01\n",
            "loss: 0.0639, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-08-17 11:03:11\n",
            ">>> val_acc: 0.8983, val_precision: 0.8983 val_recall: 0.8983, val_f1: 0.8983\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 11:03:19\n",
            "loss: 0.0656, acc: 0.9750\n",
            "E2E-ABSA >>> 2022-08-17 11:03:35\n",
            "loss: 0.0752, acc: 0.9729\n",
            "E2E-ABSA >>> 2022-08-17 11:03:51\n",
            "loss: 0.0785, acc: 0.9712\n",
            "E2E-ABSA >>> 2022-08-17 11:04:06\n",
            "loss: 0.0635, acc: 0.9766\n",
            "E2E-ABSA >>> 2022-08-17 11:04:08\n",
            ">>> val_acc: 0.8814, val_precision: 0.8814 val_recall: 0.8814, val_f1: 0.8814\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 11:04:24\n",
            "loss: 0.0381, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-08-17 11:04:40\n",
            "loss: 0.0446, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-08-17 11:04:56\n",
            "loss: 0.0514, acc: 0.9812\n",
            "E2E-ABSA >>> 2022-08-17 11:05:06\n",
            ">>> val_acc: 0.8983, val_precision: 0.8983 val_recall: 0.8983, val_f1: 0.8983\n",
            "E2E-ABSA >>> 2022-08-17 11:05:06\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.9153, val_precision: 0.9153 val_recall: 0.9153, val_f1: 0.9153\n",
            "you can download the best model from state_dict/lcf_bert_SemEval2016_know_val_f1_0.9153\n",
            ">>> test_acc: 0.9153, test_precision: 0.9153, test_recall: 0.9153, test_f1: 0.9153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training **acl14shortdata** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "ZBzSI8iy6MVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset acl14shortdata --log_step 20  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "id": "waRnFIiAGkuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7abda8b-bdc5-4c9d-816c-c5d509b2bbe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 676kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 490kB/s]\n",
            "Downloading: 100% 440M/440M [00:06<00:00, 69.7MB/s]\n",
            "Bert加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: acl14shortdata\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fdb35192b00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 85\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 20\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/train.tsv', 'test': './datasets/acl14shortdata/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 08:35:01\n",
            "loss: 1.1082, acc: 0.5062\n",
            "E2E-ABSA >>> 2022-08-17 08:35:12\n",
            "loss: 1.0712, acc: 0.4953\n",
            "E2E-ABSA >>> 2022-08-17 08:35:22\n",
            "loss: 1.0655, acc: 0.4750\n",
            "E2E-ABSA >>> 2022-08-17 08:35:32\n",
            "loss: 1.0481, acc: 0.4852\n",
            "E2E-ABSA >>> 2022-08-17 08:35:43\n",
            "loss: 1.0240, acc: 0.4975\n",
            "E2E-ABSA >>> 2022-08-17 08:35:53\n",
            "loss: 1.0030, acc: 0.5094\n",
            "E2E-ABSA >>> 2022-08-17 08:36:04\n",
            "loss: 0.9702, acc: 0.5353\n",
            "E2E-ABSA >>> 2022-08-17 08:36:15\n",
            "loss: 0.9517, acc: 0.5469\n",
            "E2E-ABSA >>> 2022-08-17 08:36:26\n",
            "loss: 0.9371, acc: 0.5559\n",
            "E2E-ABSA >>> 2022-08-17 08:36:38\n",
            "loss: 0.9178, acc: 0.5675\n",
            "E2E-ABSA >>> 2022-08-17 08:36:49\n",
            "loss: 0.9017, acc: 0.5781\n",
            "E2E-ABSA >>> 2022-08-17 08:37:00\n",
            "loss: 0.8878, acc: 0.5867\n",
            "E2E-ABSA >>> 2022-08-17 08:37:11\n",
            "loss: 0.8752, acc: 0.5950\n",
            "E2E-ABSA >>> 2022-08-17 08:37:22\n",
            "loss: 0.8598, acc: 0.6020\n",
            "E2E-ABSA >>> 2022-08-17 08:37:33\n",
            "loss: 0.8488, acc: 0.6079\n",
            "E2E-ABSA >>> 2022-08-17 08:37:44\n",
            "loss: 0.8423, acc: 0.6119\n",
            "E2E-ABSA >>> 2022-08-17 08:37:55\n",
            "loss: 0.8348, acc: 0.6164\n",
            "E2E-ABSA >>> 2022-08-17 08:38:09\n",
            ">>> val_acc: 0.7008, val_precision: 0.7008 val_recall: 0.7008, val_f1: 0.7008\n",
            ">> saved: state_dict/lcf_bert_acl14shortdata_val_f1_0.7008\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 08:38:15\n",
            "loss: 0.6031, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 08:38:27\n",
            "loss: 0.5833, acc: 0.7723\n",
            "E2E-ABSA >>> 2022-08-17 08:38:38\n",
            "loss: 0.5835, acc: 0.7669\n",
            "E2E-ABSA >>> 2022-08-17 08:38:50\n",
            "loss: 0.6034, acc: 0.7509\n",
            "E2E-ABSA >>> 2022-08-17 08:39:01\n",
            "loss: 0.6034, acc: 0.7521\n",
            "E2E-ABSA >>> 2022-08-17 08:39:12\n",
            "loss: 0.6129, acc: 0.7419\n",
            "E2E-ABSA >>> 2022-08-17 08:39:23\n",
            "loss: 0.6210, acc: 0.7363\n",
            "E2E-ABSA >>> 2022-08-17 08:39:34\n",
            "loss: 0.6136, acc: 0.7407\n",
            "E2E-ABSA >>> 2022-08-17 08:39:46\n",
            "loss: 0.6148, acc: 0.7403\n",
            "E2E-ABSA >>> 2022-08-17 08:39:57\n",
            "loss: 0.6054, acc: 0.7447\n",
            "E2E-ABSA >>> 2022-08-17 08:40:08\n",
            "loss: 0.6072, acc: 0.7452\n",
            "E2E-ABSA >>> 2022-08-17 08:40:19\n",
            "loss: 0.6103, acc: 0.7448\n",
            "E2E-ABSA >>> 2022-08-17 08:40:30\n",
            "loss: 0.6143, acc: 0.7432\n",
            "E2E-ABSA >>> 2022-08-17 08:40:41\n",
            "loss: 0.6164, acc: 0.7437\n",
            "E2E-ABSA >>> 2022-08-17 08:40:52\n",
            "loss: 0.6176, acc: 0.7422\n",
            "E2E-ABSA >>> 2022-08-17 08:41:04\n",
            "loss: 0.6169, acc: 0.7425\n",
            "E2E-ABSA >>> 2022-08-17 08:41:15\n",
            "loss: 0.6148, acc: 0.7410\n",
            "E2E-ABSA >>> 2022-08-17 08:41:26\n",
            "loss: 0.6163, acc: 0.7414\n",
            "E2E-ABSA >>> 2022-08-17 08:41:36\n",
            ">>> val_acc: 0.7216, val_precision: 0.7216 val_recall: 0.7216, val_f1: 0.7216\n",
            ">> saved: state_dict/lcf_bert_acl14shortdata_val_f1_0.7216\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 08:41:46\n",
            "loss: 0.4460, acc: 0.8359\n",
            "E2E-ABSA >>> 2022-08-17 08:41:57\n",
            "loss: 0.4450, acc: 0.8333\n",
            "E2E-ABSA >>> 2022-08-17 08:42:08\n",
            "loss: 0.4954, acc: 0.8025\n",
            "E2E-ABSA >>> 2022-08-17 08:42:20\n",
            "loss: 0.5137, acc: 0.7928\n",
            "E2E-ABSA >>> 2022-08-17 08:42:31\n",
            "loss: 0.5111, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 08:42:42\n",
            "loss: 0.5093, acc: 0.7856\n",
            "E2E-ABSA >>> 2022-08-17 08:42:53\n",
            "loss: 0.5010, acc: 0.7886\n",
            "E2E-ABSA >>> 2022-08-17 08:43:04\n",
            "loss: 0.5039, acc: 0.7893\n",
            "E2E-ABSA >>> 2022-08-17 08:43:15\n",
            "loss: 0.5022, acc: 0.7915\n",
            "E2E-ABSA >>> 2022-08-17 08:43:26\n",
            "loss: 0.5067, acc: 0.7895\n",
            "E2E-ABSA >>> 2022-08-17 08:43:37\n",
            "loss: 0.5072, acc: 0.7882\n",
            "E2E-ABSA >>> 2022-08-17 08:43:48\n",
            "loss: 0.5112, acc: 0.7855\n",
            "E2E-ABSA >>> 2022-08-17 08:44:00\n",
            "loss: 0.5171, acc: 0.7825\n",
            "E2E-ABSA >>> 2022-08-17 08:44:11\n",
            "loss: 0.5204, acc: 0.7815\n",
            "E2E-ABSA >>> 2022-08-17 08:44:22\n",
            "loss: 0.5206, acc: 0.7825\n",
            "E2E-ABSA >>> 2022-08-17 08:44:33\n",
            "loss: 0.5202, acc: 0.7824\n",
            "E2E-ABSA >>> 2022-08-17 08:44:44\n",
            "loss: 0.5189, acc: 0.7844\n",
            "E2E-ABSA >>> 2022-08-17 08:45:00\n",
            ">>> val_acc: 0.6800, val_precision: 0.6800 val_recall: 0.6800, val_f1: 0.6800\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 08:45:03\n",
            "loss: 0.3873, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 08:45:14\n",
            "loss: 0.4580, acc: 0.8073\n",
            "E2E-ABSA >>> 2022-08-17 08:45:25\n",
            "loss: 0.4558, acc: 0.8196\n",
            "E2E-ABSA >>> 2022-08-17 08:45:36\n",
            "loss: 0.4296, acc: 0.8340\n",
            "E2E-ABSA >>> 2022-08-17 08:45:47\n",
            "loss: 0.4166, acc: 0.8415\n",
            "E2E-ABSA >>> 2022-08-17 08:45:58\n",
            "loss: 0.4156, acc: 0.8407\n",
            "E2E-ABSA >>> 2022-08-17 08:46:09\n",
            "loss: 0.4241, acc: 0.8382\n",
            "E2E-ABSA >>> 2022-08-17 08:46:21\n",
            "loss: 0.4250, acc: 0.8351\n",
            "E2E-ABSA >>> 2022-08-17 08:46:32\n",
            "loss: 0.4361, acc: 0.8281\n",
            "E2E-ABSA >>> 2022-08-17 08:46:43\n",
            "loss: 0.4381, acc: 0.8278\n",
            "E2E-ABSA >>> 2022-08-17 08:46:54\n",
            "loss: 0.4357, acc: 0.8284\n",
            "E2E-ABSA >>> 2022-08-17 08:47:06\n",
            "loss: 0.4330, acc: 0.8290\n",
            "E2E-ABSA >>> 2022-08-17 08:47:17\n",
            "loss: 0.4366, acc: 0.8263\n",
            "E2E-ABSA >>> 2022-08-17 08:47:28\n",
            "loss: 0.4394, acc: 0.8234\n",
            "E2E-ABSA >>> 2022-08-17 08:47:39\n",
            "loss: 0.4371, acc: 0.8259\n",
            "E2E-ABSA >>> 2022-08-17 08:47:50\n",
            "loss: 0.4388, acc: 0.8250\n",
            "E2E-ABSA >>> 2022-08-17 08:48:01\n",
            "loss: 0.4411, acc: 0.8245\n",
            "E2E-ABSA >>> 2022-08-17 08:48:12\n",
            "loss: 0.4448, acc: 0.8223\n",
            "E2E-ABSA >>> 2022-08-17 08:48:24\n",
            ">>> val_acc: 0.6960, val_precision: 0.6960 val_recall: 0.6960, val_f1: 0.6960\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 08:48:31\n",
            "loss: 0.4002, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 08:48:42\n",
            "loss: 0.3590, acc: 0.8691\n",
            "E2E-ABSA >>> 2022-08-17 08:48:53\n",
            "loss: 0.3447, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 08:49:04\n",
            "loss: 0.3537, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 08:49:15\n",
            "loss: 0.3563, acc: 0.8743\n",
            "E2E-ABSA >>> 2022-08-17 08:49:27\n",
            "loss: 0.3587, acc: 0.8683\n",
            "E2E-ABSA >>> 2022-08-17 08:49:38\n",
            "loss: 0.3420, acc: 0.8755\n",
            "E2E-ABSA >>> 2022-08-17 08:49:49\n",
            "loss: 0.3366, acc: 0.8754\n",
            "E2E-ABSA >>> 2022-08-17 08:50:00\n",
            "loss: 0.3493, acc: 0.8699\n",
            "E2E-ABSA >>> 2022-08-17 08:50:11\n",
            "loss: 0.3588, acc: 0.8646\n",
            "E2E-ABSA >>> 2022-08-17 08:50:23\n",
            "loss: 0.3578, acc: 0.8641\n",
            "E2E-ABSA >>> 2022-08-17 08:50:34\n",
            "loss: 0.3589, acc: 0.8634\n",
            "E2E-ABSA >>> 2022-08-17 08:50:45\n",
            "loss: 0.3653, acc: 0.8599\n",
            "E2E-ABSA >>> 2022-08-17 08:50:56\n",
            "loss: 0.3677, acc: 0.8596\n",
            "E2E-ABSA >>> 2022-08-17 08:51:07\n",
            "loss: 0.3743, acc: 0.8568\n",
            "E2E-ABSA >>> 2022-08-17 08:51:18\n",
            "loss: 0.3809, acc: 0.8536\n",
            "E2E-ABSA >>> 2022-08-17 08:51:29\n",
            "loss: 0.3851, acc: 0.8520\n",
            "E2E-ABSA >>> 2022-08-17 08:51:40\n",
            "loss: 0.3868, acc: 0.8504\n",
            "E2E-ABSA >>> 2022-08-17 08:51:48\n",
            ">>> val_acc: 0.6928, val_precision: 0.6928 val_recall: 0.6928, val_f1: 0.6928\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 08:51:59\n",
            "loss: 0.2365, acc: 0.9187\n",
            "E2E-ABSA >>> 2022-08-17 08:52:10\n",
            "loss: 0.2644, acc: 0.9094\n",
            "E2E-ABSA >>> 2022-08-17 08:52:21\n",
            "loss: 0.2716, acc: 0.9042\n",
            "E2E-ABSA >>> 2022-08-17 08:52:32\n",
            "loss: 0.2807, acc: 0.8984\n",
            "E2E-ABSA >>> 2022-08-17 08:52:43\n",
            "loss: 0.2820, acc: 0.8975\n",
            "E2E-ABSA >>> 2022-08-17 08:52:54\n",
            "loss: 0.3000, acc: 0.8891\n",
            "E2E-ABSA >>> 2022-08-17 08:53:06\n",
            "loss: 0.3119, acc: 0.8839\n",
            "E2E-ABSA >>> 2022-08-17 08:53:17\n",
            "loss: 0.3133, acc: 0.8836\n",
            "E2E-ABSA >>> 2022-08-17 08:53:28\n",
            "loss: 0.3261, acc: 0.8757\n",
            "E2E-ABSA >>> 2022-08-17 08:53:39\n",
            "loss: 0.3339, acc: 0.8744\n",
            "E2E-ABSA >>> 2022-08-17 08:53:50\n",
            "loss: 0.3433, acc: 0.8693\n",
            "E2E-ABSA >>> 2022-08-17 08:54:01\n",
            "loss: 0.3425, acc: 0.8674\n",
            "E2E-ABSA >>> 2022-08-17 08:54:12\n",
            "loss: 0.3447, acc: 0.8651\n",
            "E2E-ABSA >>> 2022-08-17 08:54:23\n",
            "loss: 0.3483, acc: 0.8645\n",
            "E2E-ABSA >>> 2022-08-17 08:54:34\n",
            "loss: 0.3497, acc: 0.8631\n",
            "E2E-ABSA >>> 2022-08-17 08:54:46\n",
            "loss: 0.3534, acc: 0.8621\n",
            "E2E-ABSA >>> 2022-08-17 08:54:57\n",
            "loss: 0.3555, acc: 0.8614\n",
            "E2E-ABSA >>> 2022-08-17 08:55:11\n",
            ">>> val_acc: 0.6784, val_precision: 0.6784 val_recall: 0.6784, val_f1: 0.6784\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 08:55:15\n",
            "loss: 0.4423, acc: 0.8125\n",
            "E2E-ABSA >>> 2022-08-17 08:55:26\n",
            "loss: 0.3093, acc: 0.8795\n",
            "E2E-ABSA >>> 2022-08-17 08:55:37\n",
            "loss: 0.2963, acc: 0.8802\n",
            "E2E-ABSA >>> 2022-08-17 08:55:49\n",
            "loss: 0.2858, acc: 0.8833\n",
            "E2E-ABSA >>> 2022-08-17 08:56:00\n",
            "loss: 0.2959, acc: 0.8828\n",
            "E2E-ABSA >>> 2022-08-17 08:56:11\n",
            "loss: 0.2865, acc: 0.8872\n",
            "E2E-ABSA >>> 2022-08-17 08:56:22\n",
            "loss: 0.2919, acc: 0.8867\n",
            "E2E-ABSA >>> 2022-08-17 08:56:33\n",
            "loss: 0.2891, acc: 0.8881\n",
            "E2E-ABSA >>> 2022-08-17 08:56:44\n",
            "loss: 0.2987, acc: 0.8806\n",
            "E2E-ABSA >>> 2022-08-17 08:56:55\n",
            "loss: 0.3038, acc: 0.8770\n",
            "E2E-ABSA >>> 2022-08-17 08:57:06\n",
            "loss: 0.3061, acc: 0.8777\n",
            "E2E-ABSA >>> 2022-08-17 08:57:18\n",
            "loss: 0.3071, acc: 0.8753\n",
            "E2E-ABSA >>> 2022-08-17 08:57:29\n",
            "loss: 0.3129, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 08:57:40\n",
            "loss: 0.3164, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 08:57:51\n",
            "loss: 0.3159, acc: 0.8763\n",
            "E2E-ABSA >>> 2022-08-17 08:58:02\n",
            "loss: 0.3177, acc: 0.8758\n",
            "E2E-ABSA >>> 2022-08-17 08:58:13\n",
            "loss: 0.3196, acc: 0.8740\n",
            "E2E-ABSA >>> 2022-08-17 08:58:24\n",
            "loss: 0.3213, acc: 0.8746\n",
            "E2E-ABSA >>> 2022-08-17 08:58:34\n",
            ">>> val_acc: 0.6576, val_precision: 0.6576 val_recall: 0.6576, val_f1: 0.6576\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 7.\n",
            "E2E-ABSA >>> 2022-08-17 08:58:43\n",
            "loss: 0.2273, acc: 0.9180\n",
            "E2E-ABSA >>> 2022-08-17 08:58:54\n",
            "loss: 0.2188, acc: 0.9132\n",
            "E2E-ABSA >>> 2022-08-17 08:59:05\n",
            "loss: 0.2124, acc: 0.9152\n",
            "E2E-ABSA >>> 2022-08-17 08:59:16\n",
            "loss: 0.2243, acc: 0.9112\n",
            "E2E-ABSA >>> 2022-08-17 08:59:27\n",
            "loss: 0.2195, acc: 0.9167\n",
            "E2E-ABSA >>> 2022-08-17 08:59:38\n",
            "loss: 0.2484, acc: 0.9089\n",
            "E2E-ABSA >>> 2022-08-17 08:59:49\n",
            "loss: 0.2596, acc: 0.9030\n",
            "E2E-ABSA >>> 2022-08-17 09:00:00\n",
            "loss: 0.2552, acc: 0.9046\n",
            "E2E-ABSA >>> 2022-08-17 09:00:11\n",
            "loss: 0.2566, acc: 0.9031\n",
            "E2E-ABSA >>> 2022-08-17 09:00:22\n",
            "loss: 0.2684, acc: 0.8957\n",
            "E2E-ABSA >>> 2022-08-17 09:00:34\n",
            "loss: 0.2691, acc: 0.8964\n",
            "E2E-ABSA >>> 2022-08-17 09:00:45\n",
            "loss: 0.2826, acc: 0.8925\n",
            "E2E-ABSA >>> 2022-08-17 09:00:56\n",
            "loss: 0.2885, acc: 0.8892\n",
            "E2E-ABSA >>> 2022-08-17 09:01:07\n",
            "loss: 0.2912, acc: 0.8890\n",
            "E2E-ABSA >>> 2022-08-17 09:01:18\n",
            "loss: 0.2982, acc: 0.8862\n",
            "E2E-ABSA >>> 2022-08-17 09:01:29\n",
            "loss: 0.3001, acc: 0.8855\n",
            "E2E-ABSA >>> 2022-08-17 09:01:40\n",
            "loss: 0.3069, acc: 0.8819\n",
            "E2E-ABSA >>> 2022-08-17 09:01:56\n",
            ">>> val_acc: 0.6480, val_precision: 0.6480 val_recall: 0.6480, val_f1: 0.6480\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 8.\n",
            "E2E-ABSA >>> 2022-08-17 09:01:59\n",
            "loss: 0.2700, acc: 0.8750\n",
            "E2E-ABSA >>> 2022-08-17 09:02:10\n",
            "loss: 0.2174, acc: 0.9167\n",
            "E2E-ABSA >>> 2022-08-17 09:02:21\n",
            "loss: 0.2088, acc: 0.9190\n",
            "E2E-ABSA >>> 2022-08-17 09:02:32\n",
            "loss: 0.2400, acc: 0.9004\n",
            "E2E-ABSA >>> 2022-08-17 09:02:43\n",
            "loss: 0.2373, acc: 0.9040\n",
            "E2E-ABSA >>> 2022-08-17 09:02:54\n",
            "loss: 0.2348, acc: 0.9075\n",
            "E2E-ABSA >>> 2022-08-17 09:03:05\n",
            "loss: 0.2343, acc: 0.9088\n",
            "E2E-ABSA >>> 2022-08-17 09:03:16\n",
            "loss: 0.2284, acc: 0.9119\n",
            "E2E-ABSA >>> 2022-08-17 09:03:27\n",
            "loss: 0.2325, acc: 0.9104\n",
            "E2E-ABSA >>> 2022-08-17 09:03:39\n",
            "loss: 0.2353, acc: 0.9113\n",
            "E2E-ABSA >>> 2022-08-17 09:03:50\n",
            "loss: 0.2414, acc: 0.9090\n",
            "E2E-ABSA >>> 2022-08-17 09:04:01\n",
            "loss: 0.2420, acc: 0.9065\n",
            "E2E-ABSA >>> 2022-08-17 09:04:12\n",
            "loss: 0.2466, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-08-17 09:04:23\n",
            "loss: 0.2575, acc: 0.9013\n",
            "E2E-ABSA >>> 2022-08-17 09:04:34\n",
            "loss: 0.2595, acc: 0.9018\n",
            "E2E-ABSA >>> 2022-08-17 09:04:45\n",
            "loss: 0.2633, acc: 0.9017\n",
            "E2E-ABSA >>> 2022-08-17 09:04:56\n",
            "loss: 0.2707, acc: 0.8989\n",
            "E2E-ABSA >>> 2022-08-17 09:05:07\n",
            "loss: 0.2671, acc: 0.9006\n",
            "E2E-ABSA >>> 2022-08-17 09:05:19\n",
            ">>> val_acc: 0.6656, val_precision: 0.6656 val_recall: 0.6656, val_f1: 0.6656\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 9.\n",
            "E2E-ABSA >>> 2022-08-17 09:05:26\n",
            "loss: 0.1289, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 09:05:37\n",
            "loss: 0.1266, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-08-17 09:05:48\n",
            "loss: 0.1547, acc: 0.9471\n",
            "E2E-ABSA >>> 2022-08-17 09:05:59\n",
            "loss: 0.1682, acc: 0.9384\n",
            "E2E-ABSA >>> 2022-08-17 09:06:10\n",
            "loss: 0.1795, acc: 0.9355\n",
            "E2E-ABSA >>> 2022-08-17 09:06:21\n",
            "loss: 0.1909, acc: 0.9302\n",
            "E2E-ABSA >>> 2022-08-17 09:06:32\n",
            "loss: 0.2102, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-08-17 09:06:44\n",
            "loss: 0.2250, acc: 0.9165\n",
            "E2E-ABSA >>> 2022-08-17 09:06:55\n",
            "loss: 0.2377, acc: 0.9117\n",
            "E2E-ABSA >>> 2022-08-17 09:07:06\n",
            "loss: 0.2404, acc: 0.9105\n",
            "E2E-ABSA >>> 2022-08-17 09:07:17\n",
            "loss: 0.2403, acc: 0.9104\n",
            "E2E-ABSA >>> 2022-08-17 09:07:28\n",
            "loss: 0.2452, acc: 0.9081\n",
            "E2E-ABSA >>> 2022-08-17 09:07:39\n",
            "loss: 0.2443, acc: 0.9087\n",
            "E2E-ABSA >>> 2022-08-17 09:07:50\n",
            "loss: 0.2488, acc: 0.9072\n",
            "E2E-ABSA >>> 2022-08-17 09:08:01\n",
            "loss: 0.2505, acc: 0.9062\n",
            "E2E-ABSA >>> 2022-08-17 09:08:12\n",
            "loss: 0.2558, acc: 0.9036\n",
            "E2E-ABSA >>> 2022-08-17 09:08:24\n",
            "loss: 0.2609, acc: 0.9004\n",
            "E2E-ABSA >>> 2022-08-17 09:08:34\n",
            "loss: 0.2606, acc: 0.8995\n",
            "E2E-ABSA >>> 2022-08-17 09:08:42\n",
            ">>> val_acc: 0.6448, val_precision: 0.6448 val_recall: 0.6448, val_f1: 0.6448\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 10.\n",
            "E2E-ABSA >>> 2022-08-17 09:08:53\n",
            "loss: 0.1320, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 09:09:04\n",
            "loss: 0.1474, acc: 0.9406\n",
            "E2E-ABSA >>> 2022-08-17 09:09:15\n",
            "loss: 0.1685, acc: 0.9333\n",
            "E2E-ABSA >>> 2022-08-17 09:09:27\n",
            "loss: 0.1813, acc: 0.9297\n",
            "E2E-ABSA >>> 2022-08-17 09:09:38\n",
            "loss: 0.1844, acc: 0.9300\n",
            "E2E-ABSA >>> 2022-08-17 09:09:49\n",
            "loss: 0.1991, acc: 0.9276\n",
            "E2E-ABSA >>> 2022-08-17 09:10:00\n",
            "loss: 0.1995, acc: 0.9277\n",
            "E2E-ABSA >>> 2022-08-17 09:10:11\n",
            "loss: 0.2122, acc: 0.9227\n",
            "E2E-ABSA >>> 2022-08-17 09:10:22\n",
            "loss: 0.2152, acc: 0.9205\n",
            "E2E-ABSA >>> 2022-08-17 09:10:33\n",
            "loss: 0.2167, acc: 0.9209\n",
            "E2E-ABSA >>> 2022-08-17 09:10:44\n",
            "loss: 0.2190, acc: 0.9196\n",
            "E2E-ABSA >>> 2022-08-17 09:10:56\n",
            "loss: 0.2198, acc: 0.9182\n",
            "E2E-ABSA >>> 2022-08-17 09:11:07\n",
            "loss: 0.2221, acc: 0.9185\n",
            "E2E-ABSA >>> 2022-08-17 09:11:18\n",
            "loss: 0.2291, acc: 0.9165\n",
            "E2E-ABSA >>> 2022-08-17 09:11:29\n",
            "loss: 0.2331, acc: 0.9146\n",
            "E2E-ABSA >>> 2022-08-17 09:11:40\n",
            "loss: 0.2329, acc: 0.9156\n",
            "E2E-ABSA >>> 2022-08-17 09:11:51\n",
            "loss: 0.2306, acc: 0.9162\n",
            "E2E-ABSA >>> 2022-08-17 09:12:05\n",
            ">>> val_acc: 0.6416, val_precision: 0.6416 val_recall: 0.6416, val_f1: 0.6416\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 11.\n",
            "E2E-ABSA >>> 2022-08-17 09:12:10\n",
            "loss: 0.2185, acc: 0.8984\n",
            "E2E-ABSA >>> 2022-08-17 09:12:21\n",
            "loss: 0.1843, acc: 0.9397\n",
            "E2E-ABSA >>> 2022-08-17 09:12:32\n",
            "loss: 0.1940, acc: 0.9349\n",
            "E2E-ABSA >>> 2022-08-17 09:12:43\n",
            "loss: 0.2217, acc: 0.9237\n",
            "E2E-ABSA >>> 2022-08-17 09:12:54\n",
            "loss: 0.2200, acc: 0.9219\n",
            "E2E-ABSA >>> 2022-08-17 09:13:05\n",
            "loss: 0.2120, acc: 0.9248\n",
            "E2E-ABSA >>> 2022-08-17 09:13:16\n",
            "loss: 0.2064, acc: 0.9263\n",
            "E2E-ABSA >>> 2022-08-17 09:13:27\n",
            "loss: 0.2165, acc: 0.9206\n",
            "E2E-ABSA >>> 2022-08-17 09:13:38\n",
            "loss: 0.2098, acc: 0.9237\n",
            "E2E-ABSA >>> 2022-08-17 09:13:50\n",
            "loss: 0.2113, acc: 0.9242\n",
            "E2E-ABSA >>> 2022-08-17 09:14:01\n",
            "loss: 0.2084, acc: 0.9249\n",
            "E2E-ABSA >>> 2022-08-17 09:14:12\n",
            "loss: 0.2056, acc: 0.9246\n",
            "E2E-ABSA >>> 2022-08-17 09:14:23\n",
            "loss: 0.2172, acc: 0.9201\n",
            "E2E-ABSA >>> 2022-08-17 09:14:34\n",
            "loss: 0.2312, acc: 0.9139\n",
            "E2E-ABSA >>> 2022-08-17 09:14:45\n",
            "loss: 0.2361, acc: 0.9104\n",
            "E2E-ABSA >>> 2022-08-17 09:14:56\n",
            "loss: 0.2438, acc: 0.9069\n",
            "E2E-ABSA >>> 2022-08-17 09:15:07\n",
            "loss: 0.2448, acc: 0.9078\n",
            "E2E-ABSA >>> 2022-08-17 09:15:19\n",
            "loss: 0.2466, acc: 0.9070\n",
            "E2E-ABSA >>> 2022-08-17 09:15:28\n",
            ">>> val_acc: 0.6832, val_precision: 0.6832 val_recall: 0.6832, val_f1: 0.6832\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 12.\n",
            "E2E-ABSA >>> 2022-08-17 09:15:37\n",
            "loss: 0.2030, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 09:15:48\n",
            "loss: 0.1628, acc: 0.9462\n",
            "E2E-ABSA >>> 2022-08-17 09:15:59\n",
            "loss: 0.1559, acc: 0.9475\n",
            "E2E-ABSA >>> 2022-08-17 09:16:10\n",
            "loss: 0.1527, acc: 0.9465\n",
            "E2E-ABSA >>> 2022-08-17 09:16:21\n",
            "loss: 0.1552, acc: 0.9492\n",
            "E2E-ABSA >>> 2022-08-17 09:16:33\n",
            "loss: 0.1590, acc: 0.9467\n",
            "E2E-ABSA >>> 2022-08-17 09:16:44\n",
            "loss: 0.1561, acc: 0.9476\n",
            "E2E-ABSA >>> 2022-08-17 09:16:55\n",
            "loss: 0.1605, acc: 0.9443\n",
            "E2E-ABSA >>> 2022-08-17 09:17:06\n",
            "loss: 0.1617, acc: 0.9432\n",
            "E2E-ABSA >>> 2022-08-17 09:17:17\n",
            "loss: 0.1753, acc: 0.9372\n",
            "E2E-ABSA >>> 2022-08-17 09:17:28\n",
            "loss: 0.1757, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 09:17:39\n",
            "loss: 0.1802, acc: 0.9362\n",
            "E2E-ABSA >>> 2022-08-17 09:17:50\n",
            "loss: 0.1869, acc: 0.9336\n",
            "E2E-ABSA >>> 2022-08-17 09:18:01\n",
            "loss: 0.1963, acc: 0.9303\n",
            "E2E-ABSA >>> 2022-08-17 09:18:12\n",
            "loss: 0.1929, acc: 0.9320\n",
            "E2E-ABSA >>> 2022-08-17 09:18:24\n",
            "loss: 0.1930, acc: 0.9310\n",
            "E2E-ABSA >>> 2022-08-17 09:18:35\n",
            "loss: 0.1936, acc: 0.9302\n",
            "E2E-ABSA >>> 2022-08-17 09:18:51\n",
            ">>> val_acc: 0.6544, val_precision: 0.6544 val_recall: 0.6544, val_f1: 0.6544\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 13.\n",
            "E2E-ABSA >>> 2022-08-17 09:18:53\n",
            "loss: 0.0956, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-08-17 09:19:04\n",
            "loss: 0.1032, acc: 0.9661\n",
            "E2E-ABSA >>> 2022-08-17 09:19:15\n",
            "loss: 0.1900, acc: 0.9290\n",
            "E2E-ABSA >>> 2022-08-17 09:19:27\n",
            "loss: 0.1976, acc: 0.9258\n",
            "E2E-ABSA >>> 2022-08-17 09:19:38\n",
            "loss: 0.1865, acc: 0.9338\n",
            "E2E-ABSA >>> 2022-08-17 09:19:49\n",
            "loss: 0.1831, acc: 0.9351\n",
            "E2E-ABSA >>> 2022-08-17 09:20:00\n",
            "loss: 0.1723, acc: 0.9385\n",
            "E2E-ABSA >>> 2022-08-17 09:20:11\n",
            "loss: 0.1691, acc: 0.9401\n",
            "E2E-ABSA >>> 2022-08-17 09:20:22\n",
            "loss: 0.1704, acc: 0.9386\n",
            "E2E-ABSA >>> 2022-08-17 09:20:33\n",
            "loss: 0.1777, acc: 0.9365\n",
            "E2E-ABSA >>> 2022-08-17 09:20:44\n",
            "loss: 0.1762, acc: 0.9369\n",
            "E2E-ABSA >>> 2022-08-17 09:20:55\n",
            "loss: 0.1788, acc: 0.9358\n",
            "E2E-ABSA >>> 2022-08-17 09:21:07\n",
            "loss: 0.1866, acc: 0.9319\n",
            "E2E-ABSA >>> 2022-08-17 09:21:18\n",
            "loss: 0.1851, acc: 0.9321\n",
            "E2E-ABSA >>> 2022-08-17 09:21:29\n",
            "loss: 0.1817, acc: 0.9335\n",
            "E2E-ABSA >>> 2022-08-17 09:21:40\n",
            "loss: 0.1839, acc: 0.9332\n",
            "E2E-ABSA >>> 2022-08-17 09:21:51\n",
            "loss: 0.1850, acc: 0.9321\n",
            "E2E-ABSA >>> 2022-08-17 09:22:02\n",
            "loss: 0.1890, acc: 0.9301\n",
            "E2E-ABSA >>> 2022-08-17 09:22:14\n",
            ">>> val_acc: 0.6624, val_precision: 0.6624 val_recall: 0.6624, val_f1: 0.6624\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 14.\n",
            "E2E-ABSA >>> 2022-08-17 09:22:21\n",
            "loss: 0.1652, acc: 0.9375\n",
            "E2E-ABSA >>> 2022-08-17 09:22:32\n",
            "loss: 0.1780, acc: 0.9336\n",
            "E2E-ABSA >>> 2022-08-17 09:22:43\n",
            "loss: 0.1721, acc: 0.9411\n",
            "E2E-ABSA >>> 2022-08-17 09:22:54\n",
            "loss: 0.1624, acc: 0.9418\n",
            "E2E-ABSA >>> 2022-08-17 09:23:05\n",
            "loss: 0.1524, acc: 0.9463\n",
            "E2E-ABSA >>> 2022-08-17 09:23:16\n",
            "loss: 0.1563, acc: 0.9431\n",
            "E2E-ABSA >>> 2022-08-17 09:23:27\n",
            "loss: 0.1662, acc: 0.9399\n",
            "E2E-ABSA >>> 2022-08-17 09:23:38\n",
            "loss: 0.1683, acc: 0.9400\n",
            "E2E-ABSA >>> 2022-08-17 09:23:49\n",
            "loss: 0.1705, acc: 0.9400\n",
            "E2E-ABSA >>> 2022-08-17 09:24:00\n",
            "loss: 0.1678, acc: 0.9404\n",
            "E2E-ABSA >>> 2022-08-17 09:24:12\n",
            "loss: 0.1631, acc: 0.9425\n",
            "E2E-ABSA >>> 2022-08-17 09:24:23\n",
            "loss: 0.1633, acc: 0.9426\n",
            "E2E-ABSA >>> 2022-08-17 09:24:34\n",
            "loss: 0.1649, acc: 0.9417\n",
            "E2E-ABSA >>> 2022-08-17 09:24:45\n",
            "loss: 0.1654, acc: 0.9416\n",
            "E2E-ABSA >>> 2022-08-17 09:24:56\n",
            "loss: 0.1688, acc: 0.9403\n",
            "E2E-ABSA >>> 2022-08-17 09:25:07\n",
            "loss: 0.1704, acc: 0.9389\n",
            "E2E-ABSA >>> 2022-08-17 09:25:18\n",
            "loss: 0.1738, acc: 0.9383\n",
            "E2E-ABSA >>> 2022-08-17 09:25:29\n",
            "loss: 0.1744, acc: 0.9381\n",
            "E2E-ABSA >>> 2022-08-17 09:25:37\n",
            ">>> val_acc: 0.6400, val_precision: 0.6400 val_recall: 0.6400, val_f1: 0.6400\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 15.\n",
            "E2E-ABSA >>> 2022-08-17 09:25:48\n",
            "loss: 0.1202, acc: 0.9656\n",
            "E2E-ABSA >>> 2022-08-17 09:25:59\n",
            "loss: 0.1090, acc: 0.9656\n",
            "E2E-ABSA >>> 2022-08-17 09:26:10\n",
            "loss: 0.1165, acc: 0.9604\n",
            "E2E-ABSA >>> 2022-08-17 09:26:21\n",
            "loss: 0.1289, acc: 0.9555\n",
            "E2E-ABSA >>> 2022-08-17 09:26:32\n",
            "loss: 0.1258, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-08-17 09:26:43\n",
            "loss: 0.1289, acc: 0.9547\n",
            "E2E-ABSA >>> 2022-08-17 09:26:55\n",
            "loss: 0.1351, acc: 0.9509\n",
            "E2E-ABSA >>> 2022-08-17 09:27:06\n",
            "loss: 0.1356, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 09:27:17\n",
            "loss: 0.1333, acc: 0.9538\n",
            "E2E-ABSA >>> 2022-08-17 09:27:28\n",
            "loss: 0.1348, acc: 0.9541\n",
            "E2E-ABSA >>> 2022-08-17 09:27:39\n",
            "loss: 0.1349, acc: 0.9545\n",
            "E2E-ABSA >>> 2022-08-17 09:27:50\n",
            "loss: 0.1339, acc: 0.9542\n",
            "E2E-ABSA >>> 2022-08-17 09:28:01\n",
            "loss: 0.1344, acc: 0.9543\n",
            "E2E-ABSA >>> 2022-08-17 09:28:12\n",
            "loss: 0.1356, acc: 0.9540\n",
            "E2E-ABSA >>> 2022-08-17 09:28:23\n",
            "loss: 0.1382, acc: 0.9533\n",
            "E2E-ABSA >>> 2022-08-17 09:28:35\n",
            "loss: 0.1439, acc: 0.9508\n",
            "E2E-ABSA >>> 2022-08-17 09:28:46\n",
            "loss: 0.1455, acc: 0.9500\n",
            "E2E-ABSA >>> 2022-08-17 09:29:00\n",
            ">>> val_acc: 0.6336, val_precision: 0.6336 val_recall: 0.6336, val_f1: 0.6336\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 16.\n",
            "E2E-ABSA >>> 2022-08-17 09:29:04\n",
            "loss: 0.1031, acc: 0.9609\n",
            "E2E-ABSA >>> 2022-08-17 09:29:15\n",
            "loss: 0.1050, acc: 0.9643\n",
            "E2E-ABSA >>> 2022-08-17 09:29:26\n",
            "loss: 0.1167, acc: 0.9596\n",
            "E2E-ABSA >>> 2022-08-17 09:29:38\n",
            "loss: 0.1138, acc: 0.9614\n",
            "E2E-ABSA >>> 2022-08-17 09:29:49\n",
            "loss: 0.1204, acc: 0.9567\n",
            "E2E-ABSA >>> 2022-08-17 09:30:00\n",
            "loss: 0.1279, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 09:30:11\n",
            "loss: 0.1359, acc: 0.9512\n",
            "E2E-ABSA >>> 2022-08-17 09:30:22\n",
            "loss: 0.1456, acc: 0.9489\n",
            "E2E-ABSA >>> 2022-08-17 09:30:33\n",
            "loss: 0.1443, acc: 0.9490\n",
            "E2E-ABSA >>> 2022-08-17 09:30:44\n",
            "loss: 0.1412, acc: 0.9511\n",
            "E2E-ABSA >>> 2022-08-17 09:30:55\n",
            "loss: 0.1363, acc: 0.9528\n",
            "E2E-ABSA >>> 2022-08-17 09:31:06\n",
            "loss: 0.1335, acc: 0.9539\n",
            "E2E-ABSA >>> 2022-08-17 09:31:18\n",
            "loss: 0.1429, acc: 0.9514\n",
            "E2E-ABSA >>> 2022-08-17 09:31:29\n",
            "loss: 0.1479, acc: 0.9499\n",
            "E2E-ABSA >>> 2022-08-17 09:31:40\n",
            "loss: 0.1483, acc: 0.9492\n",
            "E2E-ABSA >>> 2022-08-17 09:31:51\n",
            "loss: 0.1484, acc: 0.9491\n",
            "E2E-ABSA >>> 2022-08-17 09:32:02\n",
            "loss: 0.1473, acc: 0.9489\n",
            "E2E-ABSA >>> 2022-08-17 09:32:13\n",
            "loss: 0.1497, acc: 0.9481\n",
            "E2E-ABSA >>> 2022-08-17 09:32:23\n",
            ">>> val_acc: 0.6576, val_precision: 0.6576 val_recall: 0.6576, val_f1: 0.6576\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 17.\n",
            "E2E-ABSA >>> 2022-08-17 09:32:32\n",
            "loss: 0.0997, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-08-17 09:32:43\n",
            "loss: 0.1011, acc: 0.9618\n",
            "E2E-ABSA >>> 2022-08-17 09:32:54\n",
            "loss: 0.1176, acc: 0.9598\n",
            "E2E-ABSA >>> 2022-08-17 09:33:05\n",
            "loss: 0.1168, acc: 0.9630\n",
            "E2E-ABSA >>> 2022-08-17 09:33:16\n",
            "loss: 0.1089, acc: 0.9661\n",
            "E2E-ABSA >>> 2022-08-17 09:33:27\n",
            "loss: 0.1176, acc: 0.9628\n",
            "E2E-ABSA >>> 2022-08-17 09:33:39\n",
            "loss: 0.1201, acc: 0.9623\n",
            "E2E-ABSA >>> 2022-08-17 09:33:50\n",
            "loss: 0.1174, acc: 0.9627\n",
            "E2E-ABSA >>> 2022-08-17 09:34:01\n",
            "loss: 0.1199, acc: 0.9616\n",
            "E2E-ABSA >>> 2022-08-17 09:34:12\n",
            "loss: 0.1251, acc: 0.9585\n",
            "E2E-ABSA >>> 2022-08-17 09:34:23\n",
            "loss: 0.1298, acc: 0.9563\n",
            "E2E-ABSA >>> 2022-08-17 09:34:34\n",
            "loss: 0.1309, acc: 0.9550\n",
            "E2E-ABSA >>> 2022-08-17 09:34:45\n",
            "loss: 0.1320, acc: 0.9543\n",
            "E2E-ABSA >>> 2022-08-17 09:34:56\n",
            "loss: 0.1347, acc: 0.9529\n",
            "E2E-ABSA >>> 2022-08-17 09:35:08\n",
            "loss: 0.1386, acc: 0.9521\n",
            "E2E-ABSA >>> 2022-08-17 09:35:19\n",
            "loss: 0.1387, acc: 0.9513\n",
            "E2E-ABSA >>> 2022-08-17 09:35:30\n",
            "loss: 0.1394, acc: 0.9513\n",
            "E2E-ABSA >>> 2022-08-17 09:35:46\n",
            ">>> val_acc: 0.6496, val_precision: 0.6496 val_recall: 0.6496, val_f1: 0.6496\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 18.\n",
            "E2E-ABSA >>> 2022-08-17 09:35:48\n",
            "loss: 0.0872, acc: 0.9844\n",
            "E2E-ABSA >>> 2022-08-17 09:35:59\n",
            "loss: 0.1832, acc: 0.9401\n",
            "E2E-ABSA >>> 2022-08-17 09:36:10\n",
            "loss: 0.1569, acc: 0.9517\n",
            "E2E-ABSA >>> 2022-08-17 09:36:22\n",
            "loss: 0.1513, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 09:36:33\n",
            "loss: 0.1479, acc: 0.9524\n",
            "E2E-ABSA >>> 2022-08-17 09:36:44\n",
            "loss: 0.1383, acc: 0.9555\n",
            "E2E-ABSA >>> 2022-08-17 09:36:55\n",
            "loss: 0.1333, acc: 0.9577\n",
            "E2E-ABSA >>> 2022-08-17 09:37:06\n",
            "loss: 0.1283, acc: 0.9588\n",
            "E2E-ABSA >>> 2022-08-17 09:37:17\n",
            "loss: 0.1329, acc: 0.9573\n",
            "E2E-ABSA >>> 2022-08-17 09:37:28\n",
            "loss: 0.1299, acc: 0.9575\n",
            "E2E-ABSA >>> 2022-08-17 09:37:39\n",
            "loss: 0.1295, acc: 0.9580\n",
            "E2E-ABSA >>> 2022-08-17 09:37:51\n",
            "loss: 0.1269, acc: 0.9590\n",
            "E2E-ABSA >>> 2022-08-17 09:38:02\n",
            "loss: 0.1286, acc: 0.9593\n",
            "E2E-ABSA >>> 2022-08-17 09:38:13\n",
            "loss: 0.1300, acc: 0.9588\n",
            "E2E-ABSA >>> 2022-08-17 09:38:24\n",
            "loss: 0.1303, acc: 0.9586\n",
            "E2E-ABSA >>> 2022-08-17 09:38:35\n",
            "loss: 0.1307, acc: 0.9579\n",
            "E2E-ABSA >>> 2022-08-17 09:38:46\n",
            "loss: 0.1315, acc: 0.9570\n",
            "E2E-ABSA >>> 2022-08-17 09:38:57\n",
            "loss: 0.1311, acc: 0.9568\n",
            "E2E-ABSA >>> 2022-08-17 09:39:09\n",
            ">>> val_acc: 0.6736, val_precision: 0.6736 val_recall: 0.6736, val_f1: 0.6736\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 19.\n",
            "E2E-ABSA >>> 2022-08-17 09:39:16\n",
            "loss: 0.1384, acc: 0.9531\n",
            "E2E-ABSA >>> 2022-08-17 09:39:27\n",
            "loss: 0.1043, acc: 0.9629\n",
            "E2E-ABSA >>> 2022-08-17 09:39:38\n",
            "loss: 0.0942, acc: 0.9724\n",
            "E2E-ABSA >>> 2022-08-17 09:39:49\n",
            "loss: 0.0998, acc: 0.9696\n",
            "E2E-ABSA >>> 2022-08-17 09:40:00\n",
            "loss: 0.0991, acc: 0.9701\n",
            "E2E-ABSA >>> 2022-08-17 09:40:11\n",
            "loss: 0.1010, acc: 0.9693\n",
            "E2E-ABSA >>> 2022-08-17 09:40:22\n",
            "loss: 0.1058, acc: 0.9673\n",
            "E2E-ABSA >>> 2022-08-17 09:40:34\n",
            "loss: 0.1032, acc: 0.9688\n",
            "E2E-ABSA >>> 2022-08-17 09:40:45\n",
            "loss: 0.1035, acc: 0.9677\n",
            "E2E-ABSA >>> 2022-08-17 09:40:56\n",
            "loss: 0.1048, acc: 0.9671\n",
            "E2E-ABSA >>> 2022-08-17 09:41:07\n",
            "loss: 0.1073, acc: 0.9661\n",
            "E2E-ABSA >>> 2022-08-17 09:41:18\n",
            "loss: 0.1053, acc: 0.9666\n",
            "E2E-ABSA >>> 2022-08-17 09:41:29\n",
            "loss: 0.1039, acc: 0.9668\n",
            "E2E-ABSA >>> 2022-08-17 09:41:40\n",
            "loss: 0.1094, acc: 0.9639\n",
            "E2E-ABSA >>> 2022-08-17 09:41:51\n",
            "loss: 0.1173, acc: 0.9598\n",
            "E2E-ABSA >>> 2022-08-17 09:42:02\n",
            "loss: 0.1195, acc: 0.9585\n",
            "E2E-ABSA >>> 2022-08-17 09:42:14\n",
            "loss: 0.1239, acc: 0.9565\n",
            "E2E-ABSA >>> 2022-08-17 09:42:24\n",
            "loss: 0.1277, acc: 0.9554\n",
            "E2E-ABSA >>> 2022-08-17 09:42:32\n",
            ">>> val_acc: 0.6304, val_precision: 0.6304 val_recall: 0.6304, val_f1: 0.6304\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 20.\n",
            "E2E-ABSA >>> 2022-08-17 09:42:43\n",
            "loss: 0.1436, acc: 0.9469\n",
            "E2E-ABSA >>> 2022-08-17 09:42:54\n",
            "loss: 0.1191, acc: 0.9594\n",
            "E2E-ABSA >>> 2022-08-17 09:43:05\n",
            "loss: 0.1043, acc: 0.9677\n",
            "E2E-ABSA >>> 2022-08-17 09:43:17\n",
            "loss: 0.1020, acc: 0.9656\n",
            "E2E-ABSA >>> 2022-08-17 09:43:28\n",
            "loss: 0.1070, acc: 0.9619\n",
            "E2E-ABSA >>> 2022-08-17 09:43:39\n",
            "loss: 0.1082, acc: 0.9630\n",
            "E2E-ABSA >>> 2022-08-17 09:43:50\n",
            "loss: 0.1047, acc: 0.9652\n",
            "E2E-ABSA >>> 2022-08-17 09:44:01\n",
            "loss: 0.1076, acc: 0.9645\n",
            "E2E-ABSA >>> 2022-08-17 09:44:12\n",
            "loss: 0.1069, acc: 0.9653\n",
            "E2E-ABSA >>> 2022-08-17 09:44:23\n",
            "loss: 0.1121, acc: 0.9644\n",
            "E2E-ABSA >>> 2022-08-17 09:44:34\n",
            "loss: 0.1098, acc: 0.9645\n",
            "E2E-ABSA >>> 2022-08-17 09:44:46\n",
            "loss: 0.1087, acc: 0.9648\n",
            "E2E-ABSA >>> 2022-08-17 09:44:57\n",
            "loss: 0.1090, acc: 0.9632\n",
            "E2E-ABSA >>> 2022-08-17 09:45:08\n",
            "loss: 0.1103, acc: 0.9627\n",
            "E2E-ABSA >>> 2022-08-17 09:45:19\n",
            "loss: 0.1086, acc: 0.9633\n",
            "E2E-ABSA >>> 2022-08-17 09:45:30\n",
            "loss: 0.1089, acc: 0.9631\n",
            "E2E-ABSA >>> 2022-08-17 09:45:41\n",
            "loss: 0.1078, acc: 0.9636\n",
            "E2E-ABSA >>> 2022-08-17 09:45:55\n",
            ">>> val_acc: 0.6368, val_precision: 0.6368 val_recall: 0.6368, val_f1: 0.6368\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 21.\n",
            "E2E-ABSA >>> 2022-08-17 09:46:00\n",
            "loss: 0.1354, acc: 0.9453\n",
            "E2E-ABSA >>> 2022-08-17 09:46:11\n",
            "loss: 0.1154, acc: 0.9621\n",
            "E2E-ABSA >>> 2022-08-17 09:46:22\n",
            "loss: 0.1030, acc: 0.9635\n",
            "E2E-ABSA >>> 2022-08-17 09:46:33\n",
            "loss: 0.0972, acc: 0.9660\n",
            "E2E-ABSA >>> 2022-08-17 09:46:44\n",
            "loss: 0.0874, acc: 0.9709\n",
            "E2E-ABSA >>> 2022-08-17 09:46:55\n",
            "loss: 0.0917, acc: 0.9682\n",
            "E2E-ABSA >>> 2022-08-17 09:47:06\n",
            "loss: 0.0896, acc: 0.9678\n",
            "E2E-ABSA >>> 2022-08-17 09:47:17\n",
            "loss: 0.0929, acc: 0.9683\n",
            "E2E-ABSA >>> 2022-08-17 09:47:29\n",
            "loss: 0.0967, acc: 0.9665\n",
            "E2E-ABSA >>> 2022-08-17 09:47:40\n",
            "loss: 0.0948, acc: 0.9678\n",
            "E2E-ABSA >>> 2022-08-17 09:47:51\n",
            "loss: 0.0961, acc: 0.9669\n",
            "E2E-ABSA >>> 2022-08-17 09:48:02\n",
            "loss: 0.0973, acc: 0.9663\n",
            "E2E-ABSA >>> 2022-08-17 09:48:13\n",
            "loss: 0.0956, acc: 0.9672\n",
            "E2E-ABSA >>> 2022-08-17 09:48:24\n",
            "loss: 0.0928, acc: 0.9685\n",
            "E2E-ABSA >>> 2022-08-17 09:48:35\n",
            "loss: 0.0921, acc: 0.9683\n",
            "E2E-ABSA >>> 2022-08-17 09:48:46\n",
            "loss: 0.0974, acc: 0.9669\n",
            "E2E-ABSA >>> 2022-08-17 09:48:58\n",
            "loss: 0.0980, acc: 0.9670\n",
            "E2E-ABSA >>> 2022-08-17 09:49:09\n",
            "loss: 0.1005, acc: 0.9657\n",
            "E2E-ABSA >>> 2022-08-17 09:49:18\n",
            ">>> val_acc: 0.6464, val_precision: 0.6464 val_recall: 0.6464, val_f1: 0.6464\n",
            "E2E-ABSA >>> 2022-08-17 09:49:18\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7216, val_precision: 0.7216 val_recall: 0.7216, val_f1: 0.7216\n",
            "you can download the best model from state_dict/lcf_bert_acl14shortdata_val_f1_0.7216\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 339, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 335, in main\n",
            "    ins.run()\n",
            "  File \"train.py\", line 180, in run\n",
            "    self.model.load_state_dict(torch.load(best_model_path))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 581, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'state_dict/lcf_bert_acl14shortdata_val_f1_0.7216'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "增加字典知识后： Training **acl14shortdata** dataset on model**(lfc_bert)**"
      ],
      "metadata": {
        "id": "4UXpYO3X6OEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DictionaryFused-E2E-ABSA/state_dict  \n",
        "!cd /content/DictionaryFused-E2E-ABSA && python3 train.py --model_name lcf_bert --dataset acl14shortdata_know --log_step 20 --patience 5 --max_seq_len 125  # batch_size 是16，所以每300个样本测一下"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD6efeMl6EpB",
        "outputId": "99a5db09-cd6a-4bef-b4c7-b094247aa228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> 使用设备:cuda 训练.\n",
            "加载Bert...\n",
            "Bert加载完毕.\n",
            "> training dataset count: 5623.\n",
            "> testing dataset count: 625.\n",
            "cuda memory allocated: 455608832\n",
            "> n_trainable_params: 113617923, n_nontrainable_params: 0\n",
            "> training arguments:\n",
            ">>> model_name: lcf_bert\n",
            ">>> dataset: acl14shortdata_know\n",
            ">>> optimizer: <class 'torch.optim.adam.Adam'>\n",
            ">>> initializer: <function xavier_uniform_ at 0x7fe4ba79db00>\n",
            ">>> lr: 2e-05\n",
            ">>> dropout: 0.1\n",
            ">>> l2reg: 0.01\n",
            ">>> num_epoch: 100\n",
            ">>> batch_size: 16\n",
            ">>> log_step: 20\n",
            ">>> embed_dim: 300\n",
            ">>> hidden_dim: 300\n",
            ">>> bert_dim: 768\n",
            ">>> pretrained_bert_name: bert-base-uncased\n",
            ">>> max_seq_len: 125\n",
            ">>> polarities_dim: 3\n",
            ">>> hops: 3\n",
            ">>> patience: 5\n",
            ">>> device: cuda\n",
            ">>> seed: 1234\n",
            ">>> valset_ratio: 0\n",
            ">>> local_context_focus: cdm\n",
            ">>> SRD: 3\n",
            ">>> model_class: <class 'models.lcf_bert.LCF_BERT'>\n",
            ">>> dataset_file: {'train': './datasets/acl14shortdata/output_know/train.tsv', 'test': './datasets/acl14shortdata/output_know/dev.tsv'}\n",
            ">>> inputs_cols: ['concat_bert_indices', 'concat_segments_indices', 'text_bert_indices', 'aspect_bert_indices']\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 0.\n",
            "E2E-ABSA >>> 2022-08-17 09:50:40\n",
            "loss: 1.0991, acc: 0.5125\n",
            "E2E-ABSA >>> 2022-08-17 09:50:56\n",
            "loss: 1.0701, acc: 0.4922\n",
            "E2E-ABSA >>> 2022-08-17 09:51:12\n",
            "loss: 1.0683, acc: 0.4708\n",
            "E2E-ABSA >>> 2022-08-17 09:51:28\n",
            "loss: 1.0586, acc: 0.4789\n",
            "E2E-ABSA >>> 2022-08-17 09:51:44\n",
            "loss: 1.0467, acc: 0.4919\n",
            "E2E-ABSA >>> 2022-08-17 09:52:00\n",
            "loss: 1.0330, acc: 0.4974\n",
            "E2E-ABSA >>> 2022-08-17 09:52:16\n",
            "loss: 1.0059, acc: 0.5183\n",
            "E2E-ABSA >>> 2022-08-17 09:52:32\n",
            "loss: 0.9846, acc: 0.5312\n",
            "E2E-ABSA >>> 2022-08-17 09:52:48\n",
            "loss: 0.9668, acc: 0.5424\n",
            "E2E-ABSA >>> 2022-08-17 09:53:04\n",
            "loss: 0.9449, acc: 0.5544\n",
            "E2E-ABSA >>> 2022-08-17 09:53:20\n",
            "loss: 0.9210, acc: 0.5676\n",
            "E2E-ABSA >>> 2022-08-17 09:53:36\n",
            "loss: 0.8988, acc: 0.5776\n",
            "E2E-ABSA >>> 2022-08-17 09:53:52\n",
            "loss: 0.8789, acc: 0.5894\n",
            "E2E-ABSA >>> 2022-08-17 09:54:08\n",
            "loss: 0.8589, acc: 0.6009\n",
            "E2E-ABSA >>> 2022-08-17 09:54:24\n",
            "loss: 0.8460, acc: 0.6102\n",
            "E2E-ABSA >>> 2022-08-17 09:54:40\n",
            "loss: 0.8348, acc: 0.6162\n",
            "E2E-ABSA >>> 2022-08-17 09:54:56\n",
            "loss: 0.8224, acc: 0.6226\n",
            "E2E-ABSA >>> 2022-08-17 09:55:16\n",
            ">>> val_acc: 0.7392, val_precision: 0.7392 val_recall: 0.7392, val_f1: 0.7392\n",
            ">> saved: state_dict/lcf_bert_acl14shortdata_know_val_f1_0.7392\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 1.\n",
            "E2E-ABSA >>> 2022-08-17 09:55:25\n",
            "loss: 0.5080, acc: 0.7891\n",
            "E2E-ABSA >>> 2022-08-17 09:55:41\n",
            "loss: 0.5417, acc: 0.7790\n",
            "E2E-ABSA >>> 2022-08-17 09:55:57\n",
            "loss: 0.5544, acc: 0.7734\n",
            "E2E-ABSA >>> 2022-08-17 09:56:13\n",
            "loss: 0.5683, acc: 0.7702\n",
            "E2E-ABSA >>> 2022-08-17 09:56:29\n",
            "loss: 0.5560, acc: 0.7777\n",
            "E2E-ABSA >>> 2022-08-17 09:56:45\n",
            "loss: 0.5611, acc: 0.7749\n",
            "E2E-ABSA >>> 2022-08-17 09:57:01\n",
            "loss: 0.5696, acc: 0.7676\n",
            "E2E-ABSA >>> 2022-08-17 09:57:17\n",
            "loss: 0.5608, acc: 0.7673\n",
            "E2E-ABSA >>> 2022-08-17 09:57:33\n",
            "loss: 0.5588, acc: 0.7671\n",
            "E2E-ABSA >>> 2022-08-17 09:57:49\n",
            "loss: 0.5504, acc: 0.7736\n",
            "E2E-ABSA >>> 2022-08-17 09:58:05\n",
            "loss: 0.5517, acc: 0.7710\n",
            "E2E-ABSA >>> 2022-08-17 09:58:21\n",
            "loss: 0.5562, acc: 0.7678\n",
            "E2E-ABSA >>> 2022-08-17 09:58:37\n",
            "loss: 0.5585, acc: 0.7669\n",
            "E2E-ABSA >>> 2022-08-17 09:58:53\n",
            "loss: 0.5606, acc: 0.7642\n",
            "E2E-ABSA >>> 2022-08-17 09:59:09\n",
            "loss: 0.5630, acc: 0.7632\n",
            "E2E-ABSA >>> 2022-08-17 09:59:25\n",
            "loss: 0.5592, acc: 0.7644\n",
            "E2E-ABSA >>> 2022-08-17 09:59:41\n",
            "loss: 0.5584, acc: 0.7641\n",
            "E2E-ABSA >>> 2022-08-17 09:59:57\n",
            "loss: 0.5615, acc: 0.7622\n",
            "E2E-ABSA >>> 2022-08-17 10:00:11\n",
            ">>> val_acc: 0.7488, val_precision: 0.7488 val_recall: 0.7488, val_f1: 0.7488\n",
            ">> saved: state_dict/lcf_bert_acl14shortdata_know_val_f1_0.7488\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 2.\n",
            "E2E-ABSA >>> 2022-08-17 10:00:26\n",
            "loss: 0.4334, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 10:00:42\n",
            "loss: 0.4379, acc: 0.8264\n",
            "E2E-ABSA >>> 2022-08-17 10:00:58\n",
            "loss: 0.4626, acc: 0.8158\n",
            "E2E-ABSA >>> 2022-08-17 10:01:14\n",
            "loss: 0.4677, acc: 0.8117\n",
            "E2E-ABSA >>> 2022-08-17 10:01:30\n",
            "loss: 0.4710, acc: 0.8073\n",
            "E2E-ABSA >>> 2022-08-17 10:01:46\n",
            "loss: 0.4695, acc: 0.8087\n",
            "E2E-ABSA >>> 2022-08-17 10:02:02\n",
            "loss: 0.4684, acc: 0.8079\n",
            "E2E-ABSA >>> 2022-08-17 10:02:18\n",
            "loss: 0.4679, acc: 0.8073\n",
            "E2E-ABSA >>> 2022-08-17 10:02:34\n",
            "loss: 0.4676, acc: 0.8093\n",
            "E2E-ABSA >>> 2022-08-17 10:02:50\n",
            "loss: 0.4683, acc: 0.8096\n",
            "E2E-ABSA >>> 2022-08-17 10:03:05\n",
            "loss: 0.4686, acc: 0.8087\n",
            "E2E-ABSA >>> 2022-08-17 10:03:22\n",
            "loss: 0.4729, acc: 0.8064\n",
            "E2E-ABSA >>> 2022-08-17 10:03:37\n",
            "loss: 0.4766, acc: 0.8037\n",
            "E2E-ABSA >>> 2022-08-17 10:03:54\n",
            "loss: 0.4817, acc: 0.8019\n",
            "E2E-ABSA >>> 2022-08-17 10:04:10\n",
            "loss: 0.4792, acc: 0.8032\n",
            "E2E-ABSA >>> 2022-08-17 10:04:26\n",
            "loss: 0.4781, acc: 0.8040\n",
            "E2E-ABSA >>> 2022-08-17 10:04:42\n",
            "loss: 0.4782, acc: 0.8036\n",
            "E2E-ABSA >>> 2022-08-17 10:05:05\n",
            ">>> val_acc: 0.7088, val_precision: 0.7088 val_recall: 0.7088, val_f1: 0.7088\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 3.\n",
            "E2E-ABSA >>> 2022-08-17 10:05:08\n",
            "loss: 0.4683, acc: 0.7500\n",
            "E2E-ABSA >>> 2022-08-17 10:05:24\n",
            "loss: 0.4131, acc: 0.8229\n",
            "E2E-ABSA >>> 2022-08-17 10:05:40\n",
            "loss: 0.3787, acc: 0.8438\n",
            "E2E-ABSA >>> 2022-08-17 10:05:56\n",
            "loss: 0.3715, acc: 0.8496\n",
            "E2E-ABSA >>> 2022-08-17 10:06:12\n",
            "loss: 0.3690, acc: 0.8571\n",
            "E2E-ABSA >>> 2022-08-17 10:06:28\n",
            "loss: 0.3849, acc: 0.8486\n",
            "E2E-ABSA >>> 2022-08-17 10:06:44\n",
            "loss: 0.3960, acc: 0.8412\n",
            "E2E-ABSA >>> 2022-08-17 10:07:00\n",
            "loss: 0.4011, acc: 0.8372\n",
            "E2E-ABSA >>> 2022-08-17 10:07:16\n",
            "loss: 0.4138, acc: 0.8327\n",
            "E2E-ABSA >>> 2022-08-17 10:07:32\n",
            "loss: 0.4109, acc: 0.8356\n",
            "E2E-ABSA >>> 2022-08-17 10:07:48\n",
            "loss: 0.4097, acc: 0.8376\n",
            "E2E-ABSA >>> 2022-08-17 10:08:04\n",
            "loss: 0.4094, acc: 0.8362\n",
            "E2E-ABSA >>> 2022-08-17 10:08:20\n",
            "loss: 0.4077, acc: 0.8373\n",
            "E2E-ABSA >>> 2022-08-17 10:08:36\n",
            "loss: 0.4086, acc: 0.8364\n",
            "E2E-ABSA >>> 2022-08-17 10:08:52\n",
            "loss: 0.4118, acc: 0.8345\n",
            "E2E-ABSA >>> 2022-08-17 10:09:07\n",
            "loss: 0.4142, acc: 0.8345\n",
            "E2E-ABSA >>> 2022-08-17 10:09:23\n",
            "loss: 0.4150, acc: 0.8329\n",
            "E2E-ABSA >>> 2022-08-17 10:09:39\n",
            "loss: 0.4160, acc: 0.8328\n",
            "E2E-ABSA >>> 2022-08-17 10:09:56\n",
            ">>> val_acc: 0.7344, val_precision: 0.7344 val_recall: 0.7344, val_f1: 0.7344\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 4.\n",
            "E2E-ABSA >>> 2022-08-17 10:10:06\n",
            "loss: 0.3796, acc: 0.8594\n",
            "E2E-ABSA >>> 2022-08-17 10:10:22\n",
            "loss: 0.3667, acc: 0.8633\n",
            "E2E-ABSA >>> 2022-08-17 10:10:38\n",
            "loss: 0.3515, acc: 0.8630\n",
            "E2E-ABSA >>> 2022-08-17 10:10:54\n",
            "loss: 0.3448, acc: 0.8637\n",
            "E2E-ABSA >>> 2022-08-17 10:11:10\n",
            "loss: 0.3418, acc: 0.8675\n",
            "E2E-ABSA >>> 2022-08-17 10:11:26\n",
            "loss: 0.3431, acc: 0.8650\n",
            "E2E-ABSA >>> 2022-08-17 10:11:42\n",
            "loss: 0.3346, acc: 0.8703\n",
            "E2E-ABSA >>> 2022-08-17 10:11:58\n",
            "loss: 0.3299, acc: 0.8725\n",
            "E2E-ABSA >>> 2022-08-17 10:12:14\n",
            "loss: 0.3325, acc: 0.8692\n",
            "E2E-ABSA >>> 2022-08-17 10:12:30\n",
            "loss: 0.3396, acc: 0.8656\n",
            "E2E-ABSA >>> 2022-08-17 10:12:46\n",
            "loss: 0.3342, acc: 0.8676\n",
            "E2E-ABSA >>> 2022-08-17 10:13:02\n",
            "loss: 0.3442, acc: 0.8637\n",
            "E2E-ABSA >>> 2022-08-17 10:13:18\n",
            "loss: 0.3466, acc: 0.8619\n",
            "E2E-ABSA >>> 2022-08-17 10:13:34\n",
            "loss: 0.3543, acc: 0.8598\n",
            "E2E-ABSA >>> 2022-08-17 10:13:49\n",
            "loss: 0.3598, acc: 0.8587\n",
            "E2E-ABSA >>> 2022-08-17 10:14:05\n",
            "loss: 0.3662, acc: 0.8552\n",
            "E2E-ABSA >>> 2022-08-17 10:14:21\n",
            "loss: 0.3716, acc: 0.8530\n",
            "E2E-ABSA >>> 2022-08-17 10:14:37\n",
            "loss: 0.3751, acc: 0.8519\n",
            "E2E-ABSA >>> 2022-08-17 10:14:48\n",
            ">>> val_acc: 0.7312, val_precision: 0.7312 val_recall: 0.7312, val_f1: 0.7312\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 5.\n",
            "E2E-ABSA >>> 2022-08-17 10:15:04\n",
            "loss: 0.2336, acc: 0.9187\n",
            "E2E-ABSA >>> 2022-08-17 10:15:20\n",
            "loss: 0.2718, acc: 0.8953\n",
            "E2E-ABSA >>> 2022-08-17 10:15:36\n",
            "loss: 0.2815, acc: 0.8854\n",
            "E2E-ABSA >>> 2022-08-17 10:15:52\n",
            "loss: 0.2768, acc: 0.8914\n",
            "E2E-ABSA >>> 2022-08-17 10:16:08\n",
            "loss: 0.2945, acc: 0.8856\n",
            "E2E-ABSA >>> 2022-08-17 10:16:24\n",
            "loss: 0.3043, acc: 0.8812\n",
            "E2E-ABSA >>> 2022-08-17 10:16:40\n",
            "loss: 0.3083, acc: 0.8812\n",
            "E2E-ABSA >>> 2022-08-17 10:16:55\n",
            "loss: 0.3167, acc: 0.8789\n",
            "E2E-ABSA >>> 2022-08-17 10:17:11\n",
            "loss: 0.3230, acc: 0.8743\n",
            "E2E-ABSA >>> 2022-08-17 10:17:27\n",
            "loss: 0.3194, acc: 0.8753\n",
            "E2E-ABSA >>> 2022-08-17 10:17:43\n",
            "loss: 0.3289, acc: 0.8719\n",
            "E2E-ABSA >>> 2022-08-17 10:17:59\n",
            "loss: 0.3302, acc: 0.8714\n",
            "E2E-ABSA >>> 2022-08-17 10:18:15\n",
            "loss: 0.3290, acc: 0.8724\n",
            "E2E-ABSA >>> 2022-08-17 10:18:31\n",
            "loss: 0.3271, acc: 0.8730\n",
            "E2E-ABSA >>> 2022-08-17 10:18:47\n",
            "loss: 0.3284, acc: 0.8710\n",
            "E2E-ABSA >>> 2022-08-17 10:19:03\n",
            "loss: 0.3305, acc: 0.8705\n",
            "E2E-ABSA >>> 2022-08-17 10:19:19\n",
            "loss: 0.3313, acc: 0.8711\n",
            "E2E-ABSA >>> 2022-08-17 10:19:39\n",
            ">>> val_acc: 0.7168, val_precision: 0.7168 val_recall: 0.7168, val_f1: 0.7168\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            ">>> epoch: 6.\n",
            "E2E-ABSA >>> 2022-08-17 10:19:46\n",
            "loss: 0.3585, acc: 0.8516\n",
            "E2E-ABSA >>> 2022-08-17 10:20:02\n",
            "loss: 0.2958, acc: 0.8996\n",
            "E2E-ABSA >>> 2022-08-17 10:20:18\n",
            "loss: 0.2795, acc: 0.9049\n",
            "E2E-ABSA >>> 2022-08-17 10:20:34\n",
            "loss: 0.2641, acc: 0.9053\n",
            "E2E-ABSA >>> 2022-08-17 10:20:50\n",
            "loss: 0.2740, acc: 0.9034\n",
            "E2E-ABSA >>> 2022-08-17 10:21:06\n",
            "loss: 0.2650, acc: 0.9051\n",
            "E2E-ABSA >>> 2022-08-17 10:21:22\n",
            "loss: 0.2757, acc: 0.8994\n",
            "E2E-ABSA >>> 2022-08-17 10:21:37\n",
            "loss: 0.2682, acc: 0.9020\n",
            "E2E-ABSA >>> 2022-08-17 10:21:53\n",
            "loss: 0.2772, acc: 0.8992\n",
            "E2E-ABSA >>> 2022-08-17 10:22:09\n",
            "loss: 0.2793, acc: 0.8966\n",
            "E2E-ABSA >>> 2022-08-17 10:22:25\n",
            "loss: 0.2854, acc: 0.8948\n",
            "E2E-ABSA >>> 2022-08-17 10:22:41\n",
            "loss: 0.2916, acc: 0.8920\n",
            "E2E-ABSA >>> 2022-08-17 10:22:57\n",
            "loss: 0.2933, acc: 0.8909\n",
            "E2E-ABSA >>> 2022-08-17 10:23:13\n",
            "loss: 0.3050, acc: 0.8864\n",
            "E2E-ABSA >>> 2022-08-17 10:23:29\n",
            "loss: 0.3110, acc: 0.8826\n",
            "E2E-ABSA >>> 2022-08-17 10:23:45\n",
            "loss: 0.3168, acc: 0.8793\n",
            "E2E-ABSA >>> 2022-08-17 10:24:01\n",
            "loss: 0.3150, acc: 0.8805\n",
            "E2E-ABSA >>> 2022-08-17 10:24:17\n",
            "loss: 0.3136, acc: 0.8818\n",
            "E2E-ABSA >>> 2022-08-17 10:24:31\n",
            ">>> val_acc: 0.7392, val_precision: 0.7392 val_recall: 0.7392, val_f1: 0.7392\n",
            "E2E-ABSA >>> 2022-08-17 10:24:31\n",
            ">>> early stop.\n",
            "BEST PERFORMANCE(模型最佳表现): >>> val_acc: 0.7488, val_precision: 0.7488 val_recall: 0.7488, val_f1: 0.7488\n",
            "you can download the best model from state_dict/lcf_bert_acl14shortdata_know_val_f1_0.7488\n",
            ">>> test_acc: 0.7488, test_precision: 0.7488, test_recall: 0.7488, test_f1: 0.7488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5V6sT7FXJMca"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}